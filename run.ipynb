{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29178,"status":"ok","timestamp":1650242923773,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"K0FWZuLPwwJL","outputId":"ebcf3a18-4904-4d8c-b56c-31374f5358d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1361,"status":"ok","timestamp":1650242925116,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"XiHSG1Okw2Zp","outputId":"652a9d86-7775-4718-9aea-bedf2e6deb49"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Spoiler Detection/LM-BFF\n"," \u001b[0m\u001b[01;34mauto_label_mapping\u001b[0m/               log.lock\n"," \u001b[01;34mauto_template\u001b[0m/                    README.md\n"," \u001b[01;34mdata\u001b[0m/                             requirements.txt\n"," discord_log                       \u001b[01;34mresult\u001b[0m/\n"," eda.ipynb                         run_experiment.sh\n"," \u001b[01;34mensemble_predict_results\u001b[0m/         run.ipynb\n","\u001b[01;34m'ensemble_predict_results'$'\\r'\u001b[0m/   run_multiple.sh\n"," ensemble.sh                       run.py\n"," \u001b[01;34menv\u001b[0m/                              \u001b[01;34mspoilers_auto_label_mapping\u001b[0m/\n"," evaluate.ipynb                    \u001b[01;34mspoilers_auto_template\u001b[0m/\n"," \u001b[01;34mfigs\u001b[0m/                             \u001b[01;34msrc\u001b[0m/\n"," label_search.sh                   template_search.sh\n"," LICENSE                           \u001b[01;34mtools\u001b[0m/\n"," log\n"]}],"source":["%cd \"/content/drive/MyDrive/Spoiler Detection/LM-BFF\"\n","%ls"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_WPqI4VDkBHl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650243219664,"user_tz":420,"elapsed":294554,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"}},"outputId":"b0026011-5ba9-4749-a48f-7a55b8d2898d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in ./env/lib/python3.7/site-packages (22.0.4)\n"]}],"source":["!source env/bin/activate; python -m pip install --upgrade pip"]},{"cell_type":"code","source":["!apt-get install vim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3xaqMrNXZNt","executionInfo":{"status":"ok","timestamp":1647230107802,"user_tz":420,"elapsed":11632,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"}},"outputId":"3632cda3-59cf-468d-94e7-d214b7fdd0b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  libgpm2 vim-common vim-runtime xxd\n","Suggested packages:\n","  gpm ctags vim-doc vim-scripts\n","The following NEW packages will be installed:\n","  libgpm2 vim vim-common vim-runtime xxd\n","0 upgraded, 5 newly installed, 0 to remove and 39 not upgraded.\n","Need to get 6,725 kB of archives.\n","After this operation, 32.6 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xxd amd64 2:8.0.1453-1ubuntu1.8 [49.9 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 vim-common all 2:8.0.1453-1ubuntu1.8 [71.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgpm2 amd64 1.20.7-5 [15.1 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 vim-runtime all 2:8.0.1453-1ubuntu1.8 [5,435 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 vim amd64 2:8.0.1453-1ubuntu1.8 [1,154 kB]\n","Fetched 6,725 kB in 2s (3,504 kB/s)\n","Selecting previously unselected package xxd.\n","(Reading database ... 155335 files and directories currently installed.)\n","Preparing to unpack .../xxd_2%3a8.0.1453-1ubuntu1.8_amd64.deb ...\n","Unpacking xxd (2:8.0.1453-1ubuntu1.8) ...\n","Selecting previously unselected package vim-common.\n","Preparing to unpack .../vim-common_2%3a8.0.1453-1ubuntu1.8_all.deb ...\n","Unpacking vim-common (2:8.0.1453-1ubuntu1.8) ...\n","Selecting previously unselected package libgpm2:amd64.\n","Preparing to unpack .../libgpm2_1.20.7-5_amd64.deb ...\n","Unpacking libgpm2:amd64 (1.20.7-5) ...\n","Selecting previously unselected package vim-runtime.\n","Preparing to unpack .../vim-runtime_2%3a8.0.1453-1ubuntu1.8_all.deb ...\n","Adding 'diversion of /usr/share/vim/vim80/doc/help.txt to /usr/share/vim/vim80/doc/help.txt.vim-tiny by vim-runtime'\n","Adding 'diversion of /usr/share/vim/vim80/doc/tags to /usr/share/vim/vim80/doc/tags.vim-tiny by vim-runtime'\n","Unpacking vim-runtime (2:8.0.1453-1ubuntu1.8) ...\n","Selecting previously unselected package vim.\n","Preparing to unpack .../vim_2%3a8.0.1453-1ubuntu1.8_amd64.deb ...\n","Unpacking vim (2:8.0.1453-1ubuntu1.8) ...\n","Setting up xxd (2:8.0.1453-1ubuntu1.8) ...\n","Setting up libgpm2:amd64 (1.20.7-5) ...\n","Setting up vim-common (2:8.0.1453-1ubuntu1.8) ...\n","Setting up vim-runtime (2:8.0.1453-1ubuntu1.8) ...\n","Setting up vim (2:8.0.1453-1ubuntu1.8) ...\n","update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vim (vim) in auto mode\n","update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vimdiff (vimdiff) in auto mode\n","update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rvim (rvim) in auto mode\n","update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rview (rview) in auto mode\n","update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vi (vi) in auto mode\n","update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/view (view) in auto mode\n","update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/ex (ex) in auto mode\n","update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/editor (editor) in auto mode\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":199,"status":"ok","timestamp":1650174495516,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"RtJDwadFYfn4","outputId":"82517f35-c7a1-40c1-f6a4-3480a1213b69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Apr 17 05:48:15 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249260,"status":"ok","timestamp":1650243608099,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"XjGtdx8nPIcd","outputId":"62b88db1-519e-4a96-daff-3d0ca1b0f721"},"outputs":[{"output_type":"stream","name":"stdout","text":["K = 250\n","Seed = 100\n","| Task = spoilers\n","Seed = 13\n","| Task = spoilers\n","Seed = 21\n","| Task = spoilers\n","Seed = 42\n","| Task = spoilers\n","Seed = 87\n","| Task = spoilers\n"]}],"source":["!source env/bin/activate; python tools/generate_k_shot_data.py --task spoilers --k 250"]},{"cell_type":"code","source":["!source env/bin/activate; TAG=no_demo TYPE=prompt TASK=spoilers BS=4 LR=2e-5 SEED=21 MODEL=roberta-large bash run_experiment.sh \\\n"," \"--template *cls**sent_0*._Relevant?*mask*.*sep+* --mapping {0:'Yes',1:'No'} --first_sent_limit 502 --other_sent_limit 502\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2A2QUl0Joq-","executionInfo":{"status":"ok","timestamp":1650275515019,"user_tz":420,"elapsed":1254374,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"}},"outputId":"fd47ab0b-0dac-423e-e9f0-ab53eff365b0"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","04/18/2022 09:31:05 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","04/18/2022 09:31:05 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/no_demo-spoilers-prompt-250-21-roberta-large', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Apr18_09-31-05_d993501e1e2a', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/no_demo-spoilers-prompt-250-21-roberta-large', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=True, save_logit_dir='result/no_demo-spoilers-prompt-250-21-roberta-large', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","04/18/2022 09:31:05 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","04/18/2022 09:31:08 - INFO - src.dataset -   Label 0 to word ĠYes (3216)\n","04/18/2022 09:31:08 - INFO - src.dataset -   Label 1 to word ĠNo (440)\n","04/18/2022 09:31:08 - INFO - src.dataset -   Total num_sample for mode train: 1\n","04/18/2022 09:31:08 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/250-21\n","04/18/2022 09:31:08 - INFO - filelock -   Lock 140141155816080 acquired on data/k-shot/spoilers/250-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","04/18/2022 09:31:08 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/250-21/cached_train_RobertaTokenizer_512_spoilers [took 0.006 s]\n","04/18/2022 09:31:08 - INFO - filelock -   Lock 140141155816080 released on data/k-shot/spoilers/250-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","04/18/2022 09:31:08 - INFO - src.dataset -   Label 0 to word ĠYes (3216)\n","04/18/2022 09:31:08 - INFO - src.dataset -   Label 1 to word ĠNo (440)\n","04/18/2022 09:31:08 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","04/18/2022 09:31:08 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/250-21\n","04/18/2022 09:31:08 - INFO - filelock -   Lock 140141155800656 acquired on data/k-shot/spoilers/250-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","04/18/2022 09:31:08 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/250-21/cached_dev_RobertaTokenizer_512_spoilers [took 0.007 s]\n","04/18/2022 09:31:08 - INFO - filelock -   Lock 140141155800656 released on data/k-shot/spoilers/250-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","04/18/2022 09:31:08 - INFO - src.dataset -   *** Example ***\n","04/18/2022 09:31:08 - INFO - src.dataset -   guid: dev-0\n","04/18/2022 09:31:08 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 10127, 38, 5, 129, 65, 54, 16, 5074, 13, 4150, 4306, 116, 10127, 38, 5, 129, 65, 54, 16, 888, 7758, 23, 5, 22, 13584, 8013, 113, 3881, 7, 912, 381, 2558, 116, 50118, 50118, 2264, 16, 49, 380, 563, 116, 50118, 50118, 16197, 4306, 16, 235, 6, 190, 114, 51, 912, 381, 2558, 6, 70, 5, 18864, 2071, 40, 122, 28, 19975, 6000, 8, 40, 28, 1207, 49, 2373, 12808, 6, 14, 18, 101, 5, 2373, 678, 4258, 4, 50118, 50118, 7939, 381, 2558, 18821, 41050, 8, 989, 129, 5, 18864, 2071, 4299, 6, 24, 18, 5, 129, 169, 7, 10, 611, 2088, 1528, 2125, 8, 7, 253, 5, 5964, 9, 13453, 4, 4, 1223, 43986, 116, 50264, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[119], label_word_list=None)\n","04/18/2022 09:31:08 - INFO - src.dataset -   text: <s>Am I the only one who is sad for Floch?Am I the only one who is actually mad at the \"Heroes\" attempts to stop Eren?\n","\n","What is their big plan?\n","\n","Floch is right, even if they stop Eren, all the Eldians will now be hated forever and will be living their worst nightmare, that's like the worst possible outcome.\n","\n","Let Eren reset humankind and leave only the Eldians alive, it's the only way to achive true piece and to end the wheel of hatred.. Relevant?<mask>.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n","04/18/2022 09:31:09 - INFO - src.dataset -   Label 0 to word ĠYes (3216)\n","04/18/2022 09:31:09 - INFO - src.dataset -   Label 1 to word ĠNo (440)\n","04/18/2022 09:31:09 - INFO - src.dataset -   Total num_sample for mode test: 1\n","04/18/2022 09:31:09 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/250-21\n","04/18/2022 09:31:09 - INFO - filelock -   Lock 140141155801808 acquired on data/k-shot/spoilers/250-21/cached_test_RobertaTokenizer_512_spoilers.lock\n","04/18/2022 09:31:09 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/250-21/cached_test_RobertaTokenizer_512_spoilers [took 0.018 s]\n","04/18/2022 09:31:09 - INFO - filelock -   Lock 140141155801808 released on data/k-shot/spoilers/250-21/cached_test_RobertaTokenizer_512_spoilers.lock\n","04/18/2022 09:31:09 - INFO - src.dataset -   *** Example ***\n","04/18/2022 09:31:09 - INFO - src.dataset -   guid: test-0\n","04/18/2022 09:31:09 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 118, 437, 6661, 53, 667, 7, 912, 364, 2558, 16, 1153, 5, 2373, 517, 51, 115, 2999, 24, 2653, 98, 16881, 4, 1223, 43986, 116, 50264, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[26], label_word_list=None)\n","04/18/2022 09:31:09 - INFO - src.dataset -   text: <s>i'm sorry but trying to stop eren is probably the worst move they could pull it feels so dumb. Relevant?<mask>.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","04/18/2022 09:31:28 - INFO - src.trainer -   ***** Running training *****\n","04/18/2022 09:31:28 - INFO - src.trainer -     Num examples = 500\n","04/18/2022 09:31:28 - INFO - src.trainer -     Num Epochs = 8\n","04/18/2022 09:31:28 - INFO - src.trainer -     Instantaneous batch size per device = 4\n","04/18/2022 09:31:28 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 4\n","04/18/2022 09:31:28 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","04/18/2022 09:31:28 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:   0% 0/8 [00:00<?, ?it/s]\n","  0% 0/32 [00:00<?, ?it/s]\u001b[A\n","  6% 2/32 [00:00<00:13,  2.21it/s]\u001b[A\n","  9% 3/32 [00:01<00:17,  1.70it/s]\u001b[A\n"," 12% 4/32 [00:02<00:19,  1.46it/s]\u001b[A\n"," 16% 5/32 [00:03<00:20,  1.33it/s]\u001b[A\n"," 19% 6/32 [00:04<00:20,  1.26it/s]\u001b[A\n"," 22% 7/32 [00:05<00:20,  1.21it/s]\u001b[A\n"," 25% 8/32 [00:06<00:20,  1.17it/s]\u001b[A\n"," 28% 9/32 [00:07<00:19,  1.15it/s]\u001b[A\n"," 31% 10/32 [00:08<00:19,  1.14it/s]\u001b[A\n"," 34% 11/32 [00:09<00:18,  1.13it/s]\u001b[A\n"," 38% 12/32 [00:09<00:17,  1.12it/s]\u001b[A\n"," 41% 13/32 [00:10<00:17,  1.12it/s]\u001b[A\n"," 44% 14/32 [00:11<00:16,  1.11it/s]\u001b[A\n"," 47% 15/32 [00:12<00:15,  1.11it/s]\u001b[A\n"," 50% 16/32 [00:13<00:14,  1.11it/s]\u001b[A\n"," 53% 17/32 [00:14<00:13,  1.11it/s]\u001b[A\n"," 56% 18/32 [00:15<00:12,  1.11it/s]\u001b[A\n"," 59% 19/32 [00:16<00:11,  1.11it/s]\u001b[A\n"," 62% 20/32 [00:17<00:10,  1.10it/s]\u001b[A\n"," 66% 21/32 [00:18<00:09,  1.10it/s]\u001b[A\n"," 69% 22/32 [00:19<00:09,  1.10it/s]\u001b[A\n"," 72% 23/32 [00:19<00:08,  1.10it/s]\u001b[A\n"," 75% 24/32 [00:20<00:07,  1.10it/s]\u001b[A\n"," 78% 25/32 [00:21<00:06,  1.10it/s]\u001b[A\n"," 81% 26/32 [00:22<00:05,  1.10it/s]\u001b[A\n"," 84% 27/32 [00:23<00:04,  1.10it/s]\u001b[A\n"," 88% 28/32 [00:24<00:03,  1.10it/s]\u001b[A\n"," 91% 29/32 [00:25<00:02,  1.10it/s]\u001b[A\n"," 94% 30/32 [00:26<00:01,  1.10it/s]\u001b[A\n"," 97% 31/32 [00:27<00:00,  1.10it/s]\u001b[A\n","100% 32/32 [00:27<00:00,  1.40it/s]\u001b[A04/18/2022 09:33:22 - INFO - src.trainer -   Best dev result: 0.8477439880371094\n","Epoch:  12% 1/8 [02:22<16:35, 142.18s/it]\n","33it [02:00, 28.55s/it]            \u001b[A\n","34it [02:01, 20.26s/it]\u001b[A\n","35it [02:02, 14.45s/it]\u001b[A\n","36it [02:03, 10.39s/it]\u001b[A\n","37it [02:04,  7.54s/it]\u001b[A\n","38it [02:05,  5.55s/it]\u001b[A\n","39it [02:06,  4.16s/it]\u001b[A\n","40it [02:07,  3.18s/it]\u001b[A\n","41it [02:08,  2.50s/it]\u001b[A\n","42it [02:09,  2.02s/it]\u001b[A\n","43it [02:09,  1.69s/it]\u001b[A\n","44it [02:10,  1.45s/it]\u001b[A\n","45it [02:11,  1.29s/it]\u001b[A\n","46it [02:12,  1.17s/it]\u001b[A\n","47it [02:13,  1.09s/it]\u001b[A\n","48it [02:14,  1.04s/it]\u001b[A\n","49it [02:15,  1.00it/s]\u001b[A\n","50it [02:16,  1.03it/s]\u001b[A\n","51it [02:17,  1.05it/s]\u001b[A\n","52it [02:18,  1.07it/s]\u001b[A\n","53it [02:19,  1.08it/s]\u001b[A\n","54it [02:19,  1.09it/s]\u001b[A\n","55it [02:20,  1.09it/s]\u001b[A\n","56it [02:21,  1.09it/s]\u001b[A\n","57it [02:22,  1.10it/s]\u001b[A\n","58it [02:23,  1.10it/s]\u001b[A\n","59it [02:24,  1.10it/s]\u001b[A\n","60it [02:25,  1.10it/s]\u001b[A\n","61it [02:26,  1.10it/s]\u001b[A\n","62it [02:27,  1.10it/s]\u001b[A\n","63it [02:28,  1.10it/s]\u001b[A\n","64it [02:28,  1.40it/s]\u001b[A04/18/2022 09:35:23 - INFO - src.trainer -   Best dev result: 0.8701279759407043\n","Epoch:  25% 2/8 [04:43<14:11, 141.96s/it]\n","65it [04:01, 28.29s/it]\u001b[A\n","66it [04:01, 20.08s/it]\u001b[A\n","67it [04:02, 14.32s/it]\u001b[A\n","68it [04:03, 10.30s/it]\u001b[A\n","69it [04:04,  7.48s/it]\u001b[A\n","70it [04:05,  5.51s/it]\u001b[A\n","71it [04:06,  4.13s/it]\u001b[A\n","72it [04:07,  3.16s/it]\u001b[A\n","73it [04:08,  2.48s/it]\u001b[A\n","74it [04:09,  2.01s/it]\u001b[A\n","75it [04:10,  1.68s/it]\u001b[A\n","76it [04:10,  1.45s/it]\u001b[A\n","77it [04:11,  1.28s/it]\u001b[A\n","78it [04:12,  1.17s/it]\u001b[A\n","79it [04:13,  1.09s/it]\u001b[A\n","80it [04:14,  1.04s/it]\u001b[A\n","81it [04:15,  1.00it/s]\u001b[A\n","82it [04:16,  1.03it/s]\u001b[A\n","83it [04:17,  1.05it/s]\u001b[A\n","84it [04:18,  1.07it/s]\u001b[A\n","85it [04:19,  1.08it/s]\u001b[A\n","86it [04:20,  1.09it/s]\u001b[A\n","87it [04:20,  1.09it/s]\u001b[A\n","88it [04:21,  1.09it/s]\u001b[A\n","89it [04:22,  1.10it/s]\u001b[A\n","90it [04:23,  1.10it/s]\u001b[A\n","91it [04:24,  1.10it/s]\u001b[A\n","92it [04:25,  1.10it/s]\u001b[A\n","93it [04:26,  1.10it/s]\u001b[A\n","94it [04:27,  1.10it/s]\u001b[A\n","95it [04:28,  1.10it/s]\u001b[A\n","Epoch:  38% 3/8 [06:57<11:37, 139.45s/it]\n","97it [05:53, 25.98s/it]\u001b[A\n","98it [05:54, 18.46s/it]\u001b[A\n","99it [05:55, 13.19s/it]\u001b[A\n","100it [05:56,  9.51s/it]\u001b[A\n","101it [05:57,  6.93s/it]\u001b[A\n","102it [05:57,  5.12s/it]\u001b[A\n","103it [05:58,  3.86s/it]\u001b[A\n","104it [05:59,  2.97s/it]\u001b[A\n","105it [06:00,  2.35s/it]\u001b[A\n","106it [06:01,  1.92s/it]\u001b[A\n","107it [06:02,  1.61s/it]\u001b[A\n","108it [06:03,  1.40s/it]\u001b[A\n","109it [06:04,  1.25s/it]\u001b[A\n","110it [06:05,  1.15s/it]\u001b[A\n","111it [06:06,  1.08s/it]\u001b[A\n","112it [06:06,  1.02s/it]\u001b[A\n","113it [06:07,  1.01it/s]\u001b[A\n","114it [06:08,  1.04it/s]\u001b[A\n","115it [06:09,  1.06it/s]\u001b[A\n","116it [06:10,  1.07it/s]\u001b[A\n","117it [06:11,  1.08it/s]\u001b[A\n","118it [06:12,  1.09it/s]\u001b[A\n","119it [06:13,  1.09it/s]\u001b[A\n","120it [06:14,  1.10it/s]\u001b[A\n","121it [06:15,  1.10it/s]\u001b[A\n","122it [06:16,  1.10it/s]\u001b[A\n","123it [06:16,  1.10it/s]\u001b[A\n","124it [06:17,  1.10it/s]\u001b[A\n","125it [06:18,  1.10it/s]\u001b[A\n","126it [06:19,  1.10it/s]\u001b[A\n","127it [06:20,  1.10it/s]\u001b[A\n","128it [06:20,  1.40it/s]\u001b[A\n","129it [07:45, 25.94s/it]\u001b[A\n","130it [07:46, 18.43s/it]\u001b[A\n","131it [07:47, 13.17s/it]\u001b[A\n","132it [07:48,  9.49s/it]\u001b[A\n","133it [07:49,  6.92s/it]\u001b[A\n","134it [07:50,  5.11s/it]\u001b[A\n","135it [07:51,  3.85s/it]\u001b[A\n","136it [07:51,  2.97s/it]\u001b[A\n","137it [07:52,  2.35s/it]\u001b[A\n","138it [07:53,  1.92s/it]\u001b[A\n","139it [07:54,  1.61s/it]\u001b[A\n","140it [07:55,  1.40s/it]\u001b[A\n","141it [07:56,  1.25s/it]\u001b[A\n","142it [07:57,  1.15s/it]\u001b[A\n","143it [07:58,  1.08s/it]\u001b[A\n","144it [07:59,  1.02s/it]\u001b[A\n","145it [08:00,  1.01it/s]\u001b[A\n","146it [08:01,  1.04it/s]\u001b[A\n","147it [08:01,  1.06it/s]\u001b[A\n","148it [08:02,  1.07it/s]\u001b[A\n","149it [08:03,  1.08it/s]\u001b[A\n","150it [08:04,  1.09it/s]\u001b[A\n","151it [08:05,  1.09it/s]\u001b[A\n","152it [08:06,  1.10it/s]\u001b[A\n","153it [08:07,  1.10it/s]\u001b[A\n","154it [08:08,  1.10it/s]\u001b[A\n","155it [08:09,  1.10it/s]\u001b[A\n","156it [08:10,  1.10it/s]\u001b[A\n","157it [08:10,  1.10it/s]\u001b[A\n","158it [08:11,  1.10it/s]\u001b[A\n","159it [08:12,  1.10it/s]\u001b[A\n","Epoch:  50% 4/8 [09:38<09:44, 146.07s/it]\n","161it [09:37, 25.89s/it]\u001b[A\n","162it [09:38, 18.40s/it]\u001b[A\n","163it [09:39, 13.15s/it]\u001b[A\n","164it [09:40,  9.48s/it]\u001b[A\n","165it [09:41,  6.91s/it]\u001b[A\n","166it [09:42,  5.11s/it]\u001b[A\n","167it [09:43,  3.85s/it]\u001b[A\n","168it [09:44,  2.96s/it]\u001b[A\n","169it [09:44,  2.35s/it]\u001b[A\n","170it [09:45,  1.91s/it]\u001b[A\n","171it [09:46,  1.61s/it]\u001b[A\n","172it [09:47,  1.40s/it]\u001b[A\n","173it [09:48,  1.25s/it]\u001b[A\n","174it [09:49,  1.15s/it]\u001b[A\n","175it [09:50,  1.07s/it]\u001b[A\n","176it [09:51,  1.02s/it]\u001b[A\n","177it [09:52,  1.01it/s]\u001b[A\n","178it [09:53,  1.04it/s]\u001b[A\n","179it [09:53,  1.06it/s]\u001b[A\n","180it [09:54,  1.07it/s]\u001b[A\n","181it [09:55,  1.08it/s]\u001b[A\n","182it [09:56,  1.09it/s]\u001b[A\n","183it [09:57,  1.09it/s]\u001b[A\n","184it [09:58,  1.10it/s]\u001b[A\n","185it [09:59,  1.10it/s]\u001b[A\n","186it [10:00,  1.10it/s]\u001b[A\n","187it [10:01,  1.10it/s]\u001b[A\n","188it [10:02,  1.10it/s]\u001b[A\n","189it [10:03,  1.10it/s]\u001b[A\n","190it [10:03,  1.10it/s]\u001b[A\n","191it [10:04,  1.10it/s]\u001b[A\n","Epoch:  62% 5/8 [11:51<07:06, 142.16s/it]\n","193it [11:29, 25.92s/it]\u001b[A\n","194it [11:30, 18.42s/it]\u001b[A\n","195it [11:31, 13.16s/it]\u001b[A\n","196it [11:32,  9.49s/it]\u001b[A\n","197it [11:33,  6.91s/it]\u001b[A\n","198it [11:34,  5.11s/it]\u001b[A\n","199it [11:35,  3.85s/it]\u001b[A\n","200it [11:36,  2.97s/it]\u001b[A\n","201it [11:37,  2.35s/it]\u001b[A\n","202it [11:38,  1.92s/it]\u001b[A\n","203it [11:38,  1.61s/it]\u001b[A\n","204it [11:39,  1.40s/it]\u001b[A\n","205it [11:40,  1.25s/it]\u001b[A\n","206it [11:41,  1.15s/it]\u001b[A\n","207it [11:42,  1.07s/it]\u001b[A\n","208it [11:43,  1.02s/it]\u001b[A\n","209it [11:44,  1.01it/s]\u001b[A\n","210it [11:45,  1.04it/s]\u001b[A\n","211it [11:46,  1.06it/s]\u001b[A\n","212it [11:47,  1.07it/s]\u001b[A\n","213it [11:47,  1.08it/s]\u001b[A\n","214it [11:48,  1.09it/s]\u001b[A\n","215it [11:49,  1.09it/s]\u001b[A\n","216it [11:50,  1.10it/s]\u001b[A\n","217it [11:51,  1.10it/s]\u001b[A\n","218it [11:52,  1.10it/s]\u001b[A\n","219it [11:53,  1.10it/s]\u001b[A\n","220it [11:54,  1.10it/s]\u001b[A\n","221it [11:55,  1.10it/s]\u001b[A\n","222it [11:56,  1.10it/s]\u001b[A\n","223it [11:57,  1.10it/s]\u001b[A\n","Epoch:  75% 6/8 [14:04<04:38, 139.42s/it]\n","225it [13:21, 25.86s/it]\u001b[A\n","226it [13:22, 18.37s/it]\u001b[A\n","227it [13:23, 13.13s/it]\u001b[A\n","228it [13:24,  9.46s/it]\u001b[A\n","229it [13:25,  6.90s/it]\u001b[A\n","230it [13:26,  5.10s/it]\u001b[A\n","231it [13:27,  3.84s/it]\u001b[A\n","232it [13:28,  2.96s/it]\u001b[A\n","233it [13:29,  2.34s/it]\u001b[A\n","234it [13:29,  1.91s/it]\u001b[A\n","235it [13:30,  1.61s/it]\u001b[A\n","236it [13:31,  1.40s/it]\u001b[A\n","237it [13:32,  1.25s/it]\u001b[A\n","238it [13:33,  1.15s/it]\u001b[A\n","239it [13:34,  1.07s/it]\u001b[A\n","240it [13:35,  1.02s/it]\u001b[A\n","241it [13:36,  1.01it/s]\u001b[A\n","242it [13:37,  1.04it/s]\u001b[A\n","243it [13:38,  1.06it/s]\u001b[A\n","244it [13:39,  1.07it/s]\u001b[A\n","245it [13:39,  1.08it/s]\u001b[A\n","246it [13:40,  1.09it/s]\u001b[A\n","247it [13:41,  1.09it/s]\u001b[A\n","248it [13:42,  1.10it/s]\u001b[A\n","249it [13:43,  1.10it/s]\u001b[A\n","250it [13:44,  1.10it/s]\u001b[A\n","251it [13:45,  1.10it/s]\u001b[A\n","252it [13:46,  1.10it/s]\u001b[A\n","253it [13:47,  1.10it/s]\u001b[A\n","254it [13:48,  1.10it/s]\u001b[A\n","255it [13:49,  1.10it/s]\u001b[A\n","Epoch:  88% 7/8 [16:17<02:17, 137.45s/it]\n","257it [15:13, 25.86s/it]\u001b[A\n","258it [15:14, 18.37s/it]\u001b[A\n","259it [15:15, 13.13s/it]\u001b[A\n","260it [15:16,  9.46s/it]\u001b[A\n","261it [15:17,  6.90s/it]\u001b[A\n","262it [15:18,  5.10s/it]\u001b[A\n","263it [15:19,  3.84s/it]\u001b[A\n","264it [15:20,  2.96s/it]\u001b[A\n","265it [15:21,  2.34s/it]\u001b[A\n","266it [15:21,  1.91s/it]\u001b[A\n","267it [15:22,  1.61s/it]\u001b[A\n","268it [15:23,  1.40s/it]\u001b[A\n","269it [15:24,  1.25s/it]\u001b[A\n","270it [15:25,  1.15s/it]\u001b[A\n","271it [15:26,  1.07s/it]\u001b[A\n","272it [15:27,  1.02s/it]\u001b[A\n","273it [15:28,  1.01it/s]\u001b[A\n","274it [15:29,  1.04it/s]\u001b[A\n","275it [15:30,  1.06it/s]\u001b[A\n","276it [15:30,  1.07it/s]\u001b[A\n","277it [15:31,  1.08it/s]\u001b[A\n","278it [15:32,  1.09it/s]\u001b[A\n","279it [15:33,  1.09it/s]\u001b[A\n","280it [15:34,  1.10it/s]\u001b[A\n","281it [15:35,  1.10it/s]\u001b[A\n","282it [15:36,  1.10it/s]\u001b[A\n","283it [15:37,  1.10it/s]\u001b[A\n","284it [15:38,  1.10it/s]\u001b[A\n","285it [15:39,  1.10it/s]\u001b[A\n","286it [15:40,  1.10it/s]\u001b[A\n","287it [15:40,  1.10it/s]\u001b[A\n","288it [15:41,  1.40it/s]\u001b[A\n","289it [17:05, 25.85s/it]\u001b[A\n","290it [17:06, 18.37s/it]\u001b[A\n","291it [17:07, 13.13s/it]\u001b[A\n","292it [17:08,  9.46s/it]\u001b[A\n","293it [17:09,  6.89s/it]\u001b[A\n","294it [17:10,  5.10s/it]\u001b[A\n","295it [17:11,  3.84s/it]\u001b[A\n","296it [17:12,  2.96s/it]\u001b[A\n","297it [17:12,  2.34s/it]\u001b[A\n","298it [17:13,  1.91s/it]\u001b[A\n","299it [17:14,  1.61s/it]\u001b[A\n","300it [17:15,  1.40s/it]\u001b[A\n","301it [17:16,  1.25s/it]\u001b[A\n","302it [17:17,  1.15s/it]\u001b[A\n","303it [17:18,  1.07s/it]\u001b[A\n","304it [17:19,  1.02s/it]\u001b[A\n","305it [17:20,  1.01it/s]\u001b[A\n","306it [17:21,  1.04it/s]\u001b[A\n","307it [17:22,  1.06it/s]\u001b[A\n","308it [17:22,  1.07it/s]\u001b[A\n","309it [17:23,  1.08it/s]\u001b[A\n","310it [17:24,  1.09it/s]\u001b[A\n","311it [17:25,  1.09it/s]\u001b[A\n","312it [17:26,  1.10it/s]\u001b[A\n","313it [17:27,  1.10it/s]\u001b[A\n","314it [17:28,  1.10it/s]\u001b[A\n","315it [17:29,  1.10it/s]\u001b[A\n","316it [17:30,  1.10it/s]\u001b[A\n","317it [17:31,  1.10it/s]\u001b[A\n","318it [17:31,  1.10it/s]\u001b[A\n","319it [17:32,  1.10it/s]\u001b[A\n","Epoch: 100% 8/8 [18:58<00:00, 142.35s/it]\n","04/18/2022 09:50:27 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","04/18/2022 09:50:42 - INFO - __main__ -   *** Validate ***\n","\n","321it [17:48,  5.18s/it]\u001b[A\n","322it [17:49,  3.90s/it]\u001b[A\n","323it [17:50,  3.00s/it]\u001b[A\n","324it [17:51,  2.37s/it]\u001b[A\n","325it [17:52,  1.93s/it]\u001b[A\n","326it [17:53,  1.62s/it]\u001b[A\n","327it [17:54,  1.41s/it]\u001b[A\n","328it [17:55,  1.26s/it]\u001b[A\n","329it [17:55,  1.15s/it]\u001b[A\n","330it [17:56,  1.08s/it]\u001b[A\n","331it [17:57,  1.03s/it]\u001b[A\n","332it [17:58,  1.01it/s]\u001b[A\n","333it [17:59,  1.04it/s]\u001b[A\n","334it [18:00,  1.06it/s]\u001b[A\n","335it [18:01,  1.07it/s]\u001b[A\n","336it [18:02,  1.08it/s]\u001b[A\n","337it [18:03,  1.09it/s]\u001b[A\n","338it [18:04,  1.09it/s]\u001b[A\n","339it [18:05,  1.10it/s]\u001b[A\n","340it [18:05,  1.10it/s]\u001b[A\n","341it [18:06,  1.10it/s]\u001b[A\n","342it [18:07,  1.10it/s]\u001b[A\n","343it [18:08,  1.10it/s]\u001b[A\n","344it [18:09,  1.10it/s]\u001b[A\n","345it [18:10,  1.10it/s]\u001b[A\n","346it [18:11,  1.10it/s]\u001b[A\n","347it [18:12,  1.10it/s]\u001b[A\n","348it [18:13,  1.10it/s]\u001b[A\n","349it [18:14,  1.10it/s]\u001b[A\n","350it [18:15,  1.10it/s]\u001b[A\n","351it [18:15,  1.10it/s]\u001b[A\n","352it [18:16,  1.40it/s]\u001b[A04/18/2022 09:51:10 - INFO - __main__ -   ***** Eval results spoilers *****\n","04/18/2022 09:51:10 - INFO - __main__ -     eval_loss = 0.7026726603507996\n","04/18/2022 09:51:10 - INFO - __main__ -     eval_auroc = 0.8701279759407043\n","04/18/2022 09:51:10 - INFO - __main__ -     eval_recall = 0.7200000286102295\n","04/18/2022 09:51:10 - INFO - __main__ -     eval_f1 = 0.7659575343132019\n","04/18/2022 09:51:10 - INFO - root -   *** Test ***\n","\n","353it [18:17,  1.28it/s]\u001b[A\n","354it [18:18,  1.22it/s]\u001b[A\n","355it [18:18,  1.18it/s]\u001b[A\n","356it [18:19,  1.16it/s]\u001b[A\n","357it [18:20,  1.14it/s]\u001b[A\n","358it [18:21,  1.13it/s]\u001b[A\n","359it [18:22,  1.12it/s]\u001b[A\n","360it [18:23,  1.12it/s]\u001b[A\n","361it [18:24,  1.11it/s]\u001b[A\n","362it [18:25,  1.11it/s]\u001b[A\n","363it [18:26,  1.11it/s]\u001b[A\n","364it [18:27,  1.11it/s]\u001b[A\n","365it [18:27,  1.11it/s]\u001b[A\n","366it [18:28,  1.11it/s]\u001b[A\n","367it [18:29,  1.11it/s]\u001b[A\n","368it [18:30,  1.11it/s]\u001b[A\n","369it [18:31,  1.10it/s]\u001b[A\n","370it [18:32,  1.10it/s]\u001b[A\n","371it [18:33,  1.10it/s]\u001b[A\n","372it [18:34,  1.10it/s]\u001b[A\n","373it [18:35,  1.10it/s]\u001b[A\n","374it [18:36,  1.10it/s]\u001b[A\n","375it [18:37,  1.10it/s]\u001b[A\n","376it [18:37,  1.10it/s]\u001b[A\n","377it [18:38,  1.10it/s]\u001b[A\n","378it [18:39,  1.10it/s]\u001b[A\n","379it [18:40,  1.10it/s]\u001b[A\n","380it [18:41,  1.10it/s]\u001b[A\n","381it [18:42,  1.10it/s]\u001b[A\n","382it [18:43,  1.10it/s]\u001b[A\n","383it [18:44,  1.10it/s]\u001b[A\n","384it [18:45,  1.11it/s]\u001b[A\n","385it [18:46,  1.10it/s]\u001b[A\n","386it [18:46,  1.10it/s]\u001b[A\n","387it [18:47,  1.10it/s]\u001b[A\n","388it [18:48,  1.10it/s]\u001b[A\n","389it [18:49,  1.10it/s]\u001b[A\n","390it [18:50,  1.10it/s]\u001b[A\n","391it [18:51,  1.10it/s]\u001b[A\n","392it [18:52,  1.10it/s]\u001b[A\n","393it [18:53,  1.10it/s]\u001b[A\n","394it [18:54,  1.10it/s]\u001b[A\n","395it [18:55,  1.10it/s]\u001b[A\n","396it [18:56,  1.10it/s]\u001b[A\n","397it [18:56,  1.10it/s]\u001b[A\n","398it [18:57,  1.45it/s]\u001b[A04/18/2022 09:51:51 - INFO - __main__ -   ***** Test results spoilers *****\n","04/18/2022 09:51:51 - INFO - __main__ -     eval_loss = 0.8482718467712402\n","04/18/2022 09:51:51 - INFO - __main__ -     eval_auroc = 0.872575581073761\n","04/18/2022 09:51:51 - INFO - __main__ -     eval_recall = 0.7781456708908081\n","04/18/2022 09:51:51 - INFO - __main__ -     eval_f1 = 0.7580644488334656\n","04/18/2022 09:51:51 - INFO - filelock -   Lock 140141127546832 acquired on log.lock\n","04/18/2022 09:51:51 - INFO - filelock -   Lock 140141127546832 released on log.lock\n","398it [18:57,  2.86s/it]\n"]}]},{"cell_type":"code","source":["!source env/bin/activate; TAG=irrelevant TYPE=prompt-demo TASK=spoilers BS=4 LR=2e-5 SEED=21 MODEL=roberta-large bash run_experiment.sh \\\n"," \"--template *cls**sent_0*._Irrelevant?*mask*.*sep+* --mapping {0:'No',1:'Yes'} --first_sent_limit 163 --other_sent_limit 163\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98KRM_2K09zv","executionInfo":{"status":"ok","timestamp":1650262203180,"user_tz":420,"elapsed":498387,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"}},"outputId":"81a27b64-b60b-469a-bf59-31f4e67f7f16"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","04/18/2022 06:01:48 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","04/18/2022 06:01:48 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/irrelevant-spoilers-prompt-demo-250-21-roberta-large', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Apr18_06-01-48_d993501e1e2a', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/irrelevant-spoilers-prompt-demo-250-21-roberta-large', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=True, save_logit_dir='result/irrelevant-spoilers-prompt-demo-250-21-roberta-large', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","04/18/2022 06:01:48 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","04/18/2022 06:01:48 - INFO - __main__ -   Automatically convert the template to using demonstrations.\n","04/18/2022 06:01:48 - INFO - __main__ -   | *cls**sent_0*._Irrelevant?*mask*.*sep+* => *cls**sent_0*._Irrelevant?*mask*.*sep+**sent_1*._Irrelevant?*label_0*.*sep+**sent_2*._Irrelevant?*label_1*.*sep+*\n","04/18/2022 06:01:51 - INFO - src.dataset -   Use demonstrations\n","04/18/2022 06:01:51 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","04/18/2022 06:01:51 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","04/18/2022 06:01:51 - INFO - src.dataset -   Total num_sample for mode train: 1\n","04/18/2022 06:01:51 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/250-21\n","04/18/2022 06:01:51 - INFO - filelock -   Lock 140152324677584 acquired on data/k-shot/spoilers/250-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","04/18/2022 06:01:51 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/250-21/cached_train_RobertaTokenizer_512_spoilers [took 0.005 s]\n","04/18/2022 06:01:51 - INFO - filelock -   Lock 140152324677584 released on data/k-shot/spoilers/250-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","04/18/2022 06:01:51 - INFO - src.dataset -   Use demonstrations\n","04/18/2022 06:01:51 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","04/18/2022 06:01:51 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","04/18/2022 06:01:51 - INFO - src.dataset -   Total num_sample for mode dev: 16\n","04/18/2022 06:01:51 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/250-21\n","04/18/2022 06:01:51 - INFO - filelock -   Lock 140152028097552 acquired on data/k-shot/spoilers/250-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","04/18/2022 06:01:51 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/250-21/cached_dev_RobertaTokenizer_512_spoilers [took 0.007 s]\n","04/18/2022 06:01:51 - INFO - filelock -   Lock 140152028097552 released on data/k-shot/spoilers/250-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","04/18/2022 06:01:51 - INFO - src.dataset -   *** Example ***\n","04/18/2022 06:01:51 - INFO - src.dataset -   guid: dev-0\n","04/18/2022 06:01:51 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 10127, 38, 5, 129, 65, 54, 16, 5074, 13, 4150, 4306, 116, 10127, 38, 5, 129, 65, 54, 16, 888, 7758, 23, 5, 22, 13584, 8013, 113, 3881, 7, 912, 381, 2558, 116, 50118, 50118, 2264, 16, 49, 380, 563, 116, 50118, 50118, 16197, 4306, 16, 235, 6, 190, 114, 51, 912, 381, 2558, 6, 70, 5, 18864, 2071, 40, 122, 28, 19975, 6000, 8, 40, 28, 1207, 49, 2373, 12808, 6, 14, 18, 101, 5, 2373, 678, 4258, 4, 50118, 50118, 7939, 381, 2558, 18821, 41050, 8, 989, 129, 5, 18864, 2071, 4299, 6, 24, 18, 5, 129, 169, 7, 10, 611, 2088, 1528, 2125, 8, 7, 253, 5, 5964, 9, 13453, 4, 4, 9139, 41017, 116, 50264, 4, 2, 16197, 4306, 197, 33, 684, 4, 9341, 118, 393, 16857, 4, 4, 9139, 41017, 116, 440, 4, 2, 627, 1280, 9, 82, 259, 14, 224, 22, 100, 437, 15, 381, 2558, 18, 526, 727, 47460, 8, 2045, 7, 14528, 45, 1346, 141, 84, 1049, 5188, 36, 8138, 4691, 6, 12355, 8810, 6, 289, 10987, 4753, 1592, 236, 7, 912, 123, 95, 32327, 162, 4, 6319, 52, 70, 57, 2494, 430, 924, 70, 42, 86, 116, 50118, 50118, 31157, 2028, 29, 9, 141, 683, 77, 38, 21, 2600, 289, 10987, 18, 485, 1901, 7, 5363, 8, 12355, 8810, 36, 100, 341, 7, 1166, 5, 38996, 124, 172, 6, 8, 38, 218, 75, 122, 111, 98, 38, 218, 75, 216, 99, 2594, 220, 95, 11, 403, 238, 70, 38, 115, 206, 21, 32463, 20381, 32191, 41812, 16646, 3703, 29391, 1862, 39973, 31534, 5433, 14662, 226, 5061, 495, 6, 8, 172, 38, 439, 7, 5, 1129, 2810, 129, 7, 192, 14, 1812, 207, 9, 5, 82, 58, 1765, 289, 10987, 12103, 734, 4, 9139, 41017, 116, 3216, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[119], label_word_list=None)\n","04/18/2022 06:01:51 - INFO - src.dataset -   text: <s>Am I the only one who is sad for Floch?Am I the only one who is actually mad at the \"Heroes\" attempts to stop Eren?\n","\n","What is their big plan?\n","\n","Floch is right, even if they stop Eren, all the Eldians will now be hated forever and will be living their worst nightmare, that's like the worst possible outcome.\n","\n","Let Eren reset humankind and leave only the Eldians alive, it's the only way to achive true piece and to end the wheel of hatred.. Irrelevant?<mask>.</s>Floch should have known. Gabi never misses.. Irrelevant? No.</s>the amount of people here that say \"I'm on Eren's side 100%\" and seem to genuinely not understand how our main gang (Armin, Mikasa, Hange etc.) want to stop him just scares me. Have we all been watching different shows all this time?\n","\n","Reminds of how once when I was reading Hange's recent speech to Jean and Mikasa (I used to read the manga back then, and I don't now - so I don't know what happens next just in case), all I could think was YES FINALLY SOMEONE IS SAYING THESE WORDS OUT LOUD, and then I went to the comment section only to see that 80% of the people were calling Hange stupid.... Irrelevant? Yes.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n","04/18/2022 06:02:28 - INFO - __main__ -   *** Validate ***\n","100% 500/500 [07:30<00:00,  1.11it/s]/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","04/18/2022 06:10:00 - INFO - __main__ -   ***** Eval results spoilers *****\n","04/18/2022 06:10:00 - INFO - __main__ -     eval_loss = 0.5342997312545776\n","04/18/2022 06:10:00 - INFO - __main__ -     eval_auroc = 0.8655363321304321\n","04/18/2022 06:10:00 - INFO - __main__ -     eval_recall = 0.7519999742507935\n","04/18/2022 06:10:00 - INFO - __main__ -     eval_f1 = 0.7752577066421509\n","04/18/2022 06:10:00 - INFO - filelock -   Lock 140152022201424 acquired on log.lock\n","04/18/2022 06:10:00 - INFO - filelock -   Lock 140152022201424 released on log.lock\n","100% 500/500 [07:30<00:00,  1.11it/s]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","test = np.load('result/relevant-spoilers-prompt-demo-250-21-roberta-large/spoilers--1--1.npy')"],"metadata":{"id":"yF2iQDC6oSjP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghT8qutEoko5","executionInfo":{"status":"ok","timestamp":1650184243906,"user_tz":420,"elapsed":195,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"}},"outputId":"c19bc8d0-5659-4b0e-c20c-ed18e35c512e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[48.384468, 45.80882 ],\n","       [44.264988, 46.22074 ],\n","       [50.94467 , 45.77304 ],\n","       ...,\n","       [50.57451 , 46.24546 ],\n","       [47.357555, 45.11906 ],\n","       [46.39147 , 46.88502 ]], dtype=float32)"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AVCVR68dVjC","outputId":"13d198ac-e83c-45c0-b8e9-08cca8727f6b","executionInfo":{"status":"ok","timestamp":1646285008785,"user_tz":480,"elapsed":3496689,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","2220it [15:34,  4.33it/s]\u001b[A\n","2221it [15:34,  4.33it/s]\u001b[A\n","2222it [15:34,  4.33it/s]\u001b[A\n","2223it [15:35,  4.33it/s]\u001b[A\n","2224it [15:35,  4.34it/s]\u001b[A\n","2225it [15:35,  4.34it/s]\u001b[A\n","2226it [15:35,  4.33it/s]\u001b[A\n","2227it [15:35,  4.33it/s]\u001b[A\n","2228it [15:36,  4.34it/s]\u001b[A\n","2229it [15:36,  4.33it/s]\u001b[A\n","2230it [15:36,  4.33it/s]\u001b[A\n","2231it [15:36,  4.33it/s]\u001b[A\n","2232it [15:37,  4.33it/s]\u001b[A\n","2233it [15:37,  4.33it/s]\u001b[A\n","2234it [15:37,  4.33it/s]\u001b[A\n","2235it [15:37,  4.34it/s]\u001b[A\n","2236it [15:38,  4.33it/s]\u001b[A\n","2237it [15:38,  4.33it/s]\u001b[A\n","2238it [15:38,  4.33it/s]\u001b[A\n","2239it [15:38,  4.33it/s]\u001b[A\n","2240it [15:38,  4.33it/s]\u001b[A\n","2241it [15:39,  4.33it/s]\u001b[A\n","2242it [15:39,  4.33it/s]\u001b[A\n","2243it [15:39,  4.33it/s]\u001b[A\n","2244it [15:39,  4.33it/s]\u001b[A\n","2245it [15:40,  4.34it/s]\u001b[A\n","2246it [15:40,  4.34it/s]\u001b[A\n","2247it [15:40,  4.34it/s]\u001b[A\n","2248it [15:40,  4.34it/s]\u001b[A\n","2249it [15:41,  4.34it/s]\u001b[A\n","2250it [15:41,  4.34it/s]\u001b[A\n","2251it [15:41,  4.34it/s]\u001b[A\n","2252it [15:41,  4.33it/s]\u001b[A\n","2253it [15:41,  4.33it/s]\u001b[A\n","2254it [15:42,  4.33it/s]\u001b[A\n","2255it [15:42,  4.33it/s]\u001b[A\n","2256it [15:42,  4.33it/s]\u001b[A\n","2257it [15:42,  4.33it/s]\u001b[A\n","2258it [15:43,  4.34it/s]\u001b[A\n","2259it [15:43,  4.34it/s]\u001b[A\n","2260it [15:43,  4.33it/s]\u001b[A\n","2261it [15:43,  4.34it/s]\u001b[A\n","2262it [15:44,  4.33it/s]\u001b[A\n","2263it [15:44,  4.33it/s]\u001b[A\n","2264it [15:44,  4.33it/s]\u001b[A\n","2265it [15:44,  4.33it/s]\u001b[A\n","2266it [15:44,  4.34it/s]\u001b[A\n","2267it [15:45,  4.34it/s]\u001b[A\n","2268it [15:45,  4.33it/s]\u001b[A\n","2269it [15:45,  4.33it/s]\u001b[A\n","2270it [15:45,  4.33it/s]\u001b[A\n","2271it [15:46,  4.33it/s]\u001b[A\n","2272it [15:46,  4.33it/s]\u001b[A\n","2273it [15:46,  4.33it/s]\u001b[A\n","2274it [15:46,  4.33it/s]\u001b[A\n","2275it [15:47,  4.33it/s]\u001b[A\n","2276it [15:47,  4.34it/s]\u001b[A\n","2277it [15:47,  4.34it/s]\u001b[A\n","2278it [15:47,  4.33it/s]\u001b[A\n","2279it [15:47,  4.33it/s]\u001b[A\n","2280it [15:48,  4.33it/s]\u001b[A\n","2281it [15:48,  4.33it/s]\u001b[A\n","2282it [15:48,  4.33it/s]\u001b[A\n","2283it [15:48,  4.33it/s]\u001b[A\n","2284it [15:49,  4.33it/s]\u001b[A\n","2285it [15:49,  4.34it/s]\u001b[A\n","2286it [15:49,  4.34it/s]\u001b[A\n","2287it [15:49,  4.34it/s]\u001b[A\n","2288it [15:50,  4.34it/s]\u001b[A\n","2289it [15:50,  4.34it/s]\u001b[A\n","2290it [15:50,  4.34it/s]\u001b[A\n","2291it [15:50,  4.34it/s]\u001b[A\n","2292it [15:50,  4.33it/s]\u001b[A\n","2293it [15:51,  4.33it/s]\u001b[A\n","2294it [15:51,  4.33it/s]\u001b[A\n","2295it [15:51,  4.33it/s]\u001b[A\n","2296it [15:51,  4.33it/s]\u001b[A\n","2297it [15:52,  4.34it/s]\u001b[A\n","2298it [15:52,  4.34it/s]\u001b[A\n","2299it [15:52,  4.33it/s]\u001b[A\n","2300it [15:52,  4.33it/s]\u001b[A\n","2301it [15:53,  4.33it/s]\u001b[A\n","2302it [15:53,  4.33it/s]\u001b[A\n","2303it [15:53,  4.33it/s]\u001b[A\n","2304it [15:53,  4.33it/s]\u001b[A\n","2305it [15:53,  4.33it/s]\u001b[A\n","2306it [15:54,  4.33it/s]\u001b[A\n","2307it [15:54,  4.34it/s]\u001b[A\n","2308it [15:54,  4.34it/s]\u001b[A\n","2309it [15:54,  4.33it/s]\u001b[A\n","2310it [15:55,  4.33it/s]\u001b[A\n","2311it [15:55,  4.33it/s]\u001b[A\n","2312it [15:55,  4.33it/s]\u001b[A\n","2313it [15:55,  4.33it/s]\u001b[A\n","2314it [15:56,  4.33it/s]\u001b[A\n","2315it [15:56,  4.33it/s]\u001b[A\n","2316it [15:56,  4.34it/s]\u001b[A\n","2317it [15:56,  4.34it/s]\u001b[A\n","2318it [15:56,  4.34it/s]\u001b[A\n","2319it [15:57,  4.34it/s]\u001b[A\n","2320it [15:57,  4.33it/s]\u001b[A\n","2321it [15:57,  4.34it/s]\u001b[A\n","2322it [15:57,  4.34it/s]\u001b[A\n","2323it [15:58,  4.33it/s]\u001b[A\n","2324it [15:58,  4.34it/s]\u001b[A\n","2325it [15:58,  4.34it/s]\u001b[A\n","2326it [15:58,  4.34it/s]\u001b[A\n","2327it [15:59,  4.34it/s]\u001b[A\n","2328it [15:59,  4.34it/s]\u001b[A\n","2329it [15:59,  4.34it/s]\u001b[A\n","2330it [15:59,  4.34it/s]\u001b[A\n","2331it [15:59,  4.34it/s]\u001b[A\n","2332it [16:00,  4.33it/s]\u001b[A\n","2333it [16:00,  4.33it/s]\u001b[A\n","2334it [16:00,  4.33it/s]\u001b[A\n","2335it [16:00,  4.34it/s]\u001b[A\n","2336it [16:01,  4.34it/s]\u001b[A\n","2337it [16:01,  4.34it/s]\u001b[A\n","2338it [16:01,  4.34it/s]\u001b[A\n","2339it [16:01,  4.34it/s]\u001b[A\n","2340it [16:02,  4.33it/s]\u001b[A\n","2341it [16:02,  4.33it/s]\u001b[A\n","2342it [16:02,  4.33it/s]\u001b[A\n","2343it [16:02,  4.33it/s]\u001b[A\n","2344it [16:02,  4.33it/s]\u001b[A\n","2345it [16:03,  4.33it/s]\u001b[A\n","2346it [16:03,  4.33it/s]\u001b[A\n","2347it [16:03,  4.33it/s]\u001b[A\n","2348it [16:03,  4.33it/s]\u001b[A\n","2349it [16:04,  4.33it/s]\u001b[A\n","2350it [16:04,  4.33it/s]\u001b[A\n","2351it [16:04,  4.33it/s]\u001b[A\n","2352it [16:04,  4.33it/s]\u001b[A\n","2353it [16:05,  4.33it/s]\u001b[A\n","2354it [16:05,  4.33it/s]\u001b[A03/03/2022 04:49:18 - INFO - __main__ -   ***** Test results spoilers *****\n","03/03/2022 04:49:18 - INFO - __main__ -     eval_loss = 3.9962873458862305\n","03/03/2022 04:49:18 - INFO - __main__ -     eval_auroc = 0.8906779885292053\n","03/03/2022 04:49:18 - INFO - __main__ -     eval_recall = 0.9583333134651184\n","03/03/2022 04:49:18 - INFO - __main__ -     eval_f1 = 0.29677414894104004\n","03/03/2022 04:49:18 - INFO - filelock -   Lock 140023586046736 acquired on log.lock\n","03/03/2022 04:49:18 - INFO - filelock -   Lock 140023586046736 released on log.lock\n","2354it [16:05,  2.44it/s]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/03/2022 04:49:25 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/03/2022 04:49:25 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-demo-16-100-roberta-large-4613', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar03_04-49-25_8087200c1e26', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=100, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-demo-16-100-roberta-large-4613', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","03/03/2022 04:49:25 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/03/2022 04:49:25 - INFO - __main__ -   Automatically convert the template to using demonstrations.\n","03/03/2022 04:49:25 - INFO - __main__ -   | *cls**sent_0*._Spoiler?*mask*.*sep+* => *cls**sent_0*._Spoiler?*mask*.*sep+**sent_1*._Spoiler?*label_0*.*sep+**sent_2*._Spoiler?*label_1*.*sep+*\n","03/03/2022 04:49:27 - INFO - src.dataset -   Use demonstrations\n","03/03/2022 04:49:27 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/03/2022 04:49:27 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/03/2022 04:49:27 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/03/2022 04:49:27 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-100\n","03/03/2022 04:49:27 - INFO - filelock -   Lock 139831435038032 acquired on data/k-shot-10x/spoilers/16-100/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 04:49:27 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-100/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/03/2022 04:49:27 - INFO - filelock -   Lock 139831435038032 released on data/k-shot-10x/spoilers/16-100/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 04:49:27 - INFO - src.dataset -   Use demonstrations\n","03/03/2022 04:49:27 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/03/2022 04:49:27 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/03/2022 04:49:27 - INFO - src.dataset -   Total num_sample for mode dev: 16\n","03/03/2022 04:49:27 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-100\n","03/03/2022 04:49:27 - INFO - filelock -   Lock 139831435015440 acquired on data/k-shot-10x/spoilers/16-100/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 04:49:27 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-100/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/03/2022 04:49:27 - INFO - filelock -   Lock 139831435015440 released on data/k-shot-10x/spoilers/16-100/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 04:49:27 - INFO - src.dataset -   *** Example ***\n","03/03/2022 04:49:27 - INFO - src.dataset -   guid: dev-0\n","03/03/2022 04:49:27 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 1594, 129, 5, 22067, 39996, 19269, 21, 23, 10, 723, 5120, 731, 4, 43005, 116, 50264, 4, 2, 36446, 4, 43005, 116, 440, 4, 2, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 4, 43005, 116, 3216, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[16], label_word_list=None)\n","03/03/2022 04:49:27 - INFO - src.dataset -   text: <s>if only the beast titan animation was at a higher frame rate. Spoiler?<mask>.</s>Wait. Spoiler? No.</s>Guys, why goth mikasa was shown?. Spoiler? Yes.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","03/03/2022 04:49:30 - INFO - src.dataset -   Use demonstrations\n","03/03/2022 04:49:30 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/03/2022 04:49:30 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/03/2022 04:49:30 - INFO - src.dataset -   Total num_sample for mode test: 16\n","03/03/2022 04:49:30 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-100\n","03/03/2022 04:49:30 - INFO - filelock -   Lock 139831422267152 acquired on data/k-shot-10x/spoilers/16-100/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 04:49:30 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-100/cached_test_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/03/2022 04:49:30 - INFO - filelock -   Lock 139831422267152 released on data/k-shot-10x/spoilers/16-100/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 04:49:30 - INFO - src.dataset -   *** Example ***\n","03/03/2022 04:49:30 - INFO - src.dataset -   guid: test-0\n","03/03/2022 04:49:30 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 8663, 18, 269, 15867, 154, 123, 4, 43005, 116, 50264, 4, 2, 41010, 8108, 4, 43005, 116, 440, 4, 2, 119, 967, 8810, 14964, 4709, 4691, 18, 385, 12, 4, 43005, 116, 3216, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[10], label_word_list=None)\n","03/03/2022 04:49:30 - INFO - src.dataset -   text: <s>eren's really battering him. Spoiler?<mask>.</s>YESSS. Spoiler? No.</s>mikasa ate armin's d-. Spoiler? Yes.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/03/2022 04:49:51 - INFO - src.trainer -   ***** Running training *****\n","03/03/2022 04:49:51 - INFO - src.trainer -     Num examples = 32\n","03/03/2022 04:49:51 - INFO - src.trainer -     Num Epochs = 250\n","03/03/2022 04:49:51 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/03/2022 04:49:51 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/03/2022 04:49:51 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/03/2022 04:49:51 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:17,  1.67s/it]\n","  0% 0/185 [00:00<?, ?it/s]\u001b[A\n","  1% 2/185 [00:00<00:21,  8.68it/s]\u001b[A\n","  2% 3/185 [00:00<00:27,  6.68it/s]\u001b[A\n","  2% 4/185 [00:00<00:31,  5.75it/s]\u001b[A\n","  3% 5/185 [00:00<00:34,  5.24it/s]\u001b[A\n","  3% 6/185 [00:01<00:36,  4.93it/s]\u001b[A\n","  4% 7/185 [00:01<00:37,  4.73it/s]\u001b[A\n","  4% 8/185 [00:01<00:38,  4.61it/s]\u001b[A\n","  5% 9/185 [00:01<00:38,  4.52it/s]\u001b[A\n","  5% 10/185 [00:02<00:39,  4.46it/s]\u001b[A\n","  6% 11/185 [00:02<00:39,  4.42it/s]\u001b[A\n","  6% 12/185 [00:02<00:39,  4.40it/s]\u001b[A\n","  7% 13/185 [00:02<00:39,  4.38it/s]\u001b[A\n","  8% 14/185 [00:02<00:39,  4.36it/s]\u001b[A\n","  8% 15/185 [00:03<00:39,  4.35it/s]\u001b[A\n","  9% 16/185 [00:03<00:38,  4.35it/s]\u001b[A\n","  9% 17/185 [00:03<00:38,  4.34it/s]\u001b[A\n"," 10% 18/185 [00:03<00:38,  4.34it/s]\u001b[A\n"," 10% 19/185 [00:04<00:38,  4.34it/s]\u001b[A\n"," 11% 20/185 [00:04<00:38,  4.34it/s]\u001b[A\n"," 11% 21/185 [00:04<00:37,  4.34it/s]\u001b[A\n"," 12% 22/185 [00:04<00:37,  4.34it/s]\u001b[A\n"," 12% 23/185 [00:05<00:37,  4.34it/s]\u001b[A\n"," 13% 24/185 [00:05<00:37,  4.34it/s]\u001b[A\n"," 14% 25/185 [00:05<00:36,  4.33it/s]\u001b[A\n"," 14% 26/185 [00:05<00:36,  4.33it/s]\u001b[A\n"," 15% 27/185 [00:05<00:36,  4.33it/s]\u001b[A\n"," 15% 28/185 [00:06<00:36,  4.33it/s]\u001b[A\n"," 16% 29/185 [00:06<00:36,  4.33it/s]\u001b[A\n"," 16% 30/185 [00:06<00:35,  4.33it/s]\u001b[A\n"," 17% 31/185 [00:06<00:35,  4.33it/s]\u001b[A\n"," 17% 32/185 [00:07<00:35,  4.34it/s]\u001b[A\n"," 18% 33/185 [00:07<00:35,  4.34it/s]\u001b[A\n"," 18% 34/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 35/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 36/185 [00:08<00:34,  4.33it/s]\u001b[A\n"," 20% 37/185 [00:08<00:34,  4.33it/s]\u001b[A\n"," 21% 38/185 [00:08<00:33,  4.33it/s]\u001b[A\n"," 21% 39/185 [00:08<00:33,  4.33it/s]\u001b[A\n"," 22% 40/185 [00:08<00:33,  4.34it/s]\u001b[A\n"," 22% 41/185 [00:09<00:33,  4.34it/s]\u001b[A\n"," 23% 42/185 [00:09<00:32,  4.34it/s]\u001b[A\n"," 23% 43/185 [00:09<00:32,  4.33it/s]\u001b[A\n"," 24% 44/185 [00:09<00:32,  4.33it/s]\u001b[A\n"," 24% 45/185 [00:10<00:32,  4.34it/s]\u001b[A\n"," 25% 46/185 [00:10<00:32,  4.33it/s]\u001b[A\n"," 25% 47/185 [00:10<00:31,  4.33it/s]\u001b[A\n"," 26% 48/185 [00:10<00:31,  4.33it/s]\u001b[A\n"," 26% 49/185 [00:11<00:31,  4.34it/s]\u001b[A\n"," 27% 50/185 [00:11<00:31,  4.34it/s]\u001b[A\n"," 28% 51/185 [00:11<00:30,  4.34it/s]\u001b[A\n"," 28% 52/185 [00:11<00:30,  4.34it/s]\u001b[A\n"," 29% 53/185 [00:11<00:30,  4.34it/s]\u001b[A\n"," 29% 54/185 [00:12<00:30,  4.34it/s]\u001b[A\n"," 30% 55/185 [00:12<00:29,  4.34it/s]\u001b[A\n"," 30% 56/185 [00:12<00:29,  4.33it/s]\u001b[A\n"," 31% 57/185 [00:12<00:29,  4.33it/s]\u001b[A\n"," 31% 58/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 59/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 60/185 [00:13<00:28,  4.34it/s]\u001b[A\n"," 33% 61/185 [00:13<00:28,  4.34it/s]\u001b[A\n"," 34% 62/185 [00:14<00:28,  4.34it/s]\u001b[A\n"," 34% 63/185 [00:14<00:28,  4.34it/s]\u001b[A\n"," 35% 64/185 [00:14<00:27,  4.34it/s]\u001b[A\n"," 35% 65/185 [00:14<00:27,  4.34it/s]\u001b[A\n"," 36% 66/185 [00:14<00:27,  4.33it/s]\u001b[A\n"," 36% 67/185 [00:15<00:27,  4.33it/s]\u001b[A\n"," 37% 68/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 37% 69/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 38% 70/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 38% 71/185 [00:16<00:26,  4.33it/s]\u001b[A\n"," 39% 72/185 [00:16<00:26,  4.34it/s]\u001b[A\n"," 39% 73/185 [00:16<00:25,  4.33it/s]\u001b[A\n"," 40% 74/185 [00:16<00:25,  4.33it/s]\u001b[A\n"," 41% 75/185 [00:17<00:25,  4.34it/s]\u001b[A\n"," 41% 76/185 [00:17<00:25,  4.33it/s]\u001b[A\n"," 42% 77/185 [00:17<00:24,  4.33it/s]\u001b[A\n"," 42% 78/185 [00:17<00:24,  4.33it/s]\u001b[A\n"," 43% 79/185 [00:17<00:24,  4.34it/s]\u001b[A\n"," 43% 80/185 [00:18<00:24,  4.34it/s]\u001b[A\n"," 44% 81/185 [00:18<00:23,  4.34it/s]\u001b[A\n"," 44% 82/185 [00:18<00:23,  4.34it/s]\u001b[A\n"," 45% 83/185 [00:18<00:23,  4.34it/s]\u001b[A\n"," 45% 84/185 [00:19<00:23,  4.33it/s]\u001b[A\n"," 46% 85/185 [00:19<00:23,  4.33it/s]\u001b[A\n"," 46% 86/185 [00:19<00:22,  4.33it/s]\u001b[A\n"," 47% 87/185 [00:19<00:22,  4.33it/s]\u001b[A\n"," 48% 88/185 [00:20<00:22,  4.33it/s]\u001b[A\n"," 48% 89/185 [00:20<00:22,  4.33it/s]\u001b[A\n"," 49% 90/185 [00:20<00:21,  4.34it/s]\u001b[A\n"," 49% 91/185 [00:20<00:21,  4.34it/s]\u001b[A\n"," 50% 92/185 [00:20<00:21,  4.34it/s]\u001b[A\n"," 50% 93/185 [00:21<00:21,  4.34it/s]\u001b[A\n"," 51% 94/185 [00:21<00:20,  4.33it/s]\u001b[A\n"," 51% 95/185 [00:21<00:20,  4.33it/s]\u001b[A\n"," 52% 96/185 [00:21<00:20,  4.33it/s]\u001b[A\n"," 52% 97/185 [00:22<00:20,  4.33it/s]\u001b[A\n"," 53% 98/185 [00:22<00:20,  4.33it/s]\u001b[A\n"," 54% 99/185 [00:22<00:19,  4.33it/s]\u001b[A\n"," 54% 100/185 [00:22<00:19,  4.33it/s]\u001b[A\n"," 55% 101/185 [00:23<00:19,  4.33it/s]\u001b[A\n"," 55% 102/185 [00:23<00:19,  4.33it/s]\u001b[A\n"," 56% 103/185 [00:23<00:18,  4.33it/s]\u001b[A\n"," 56% 104/185 [00:23<00:18,  4.33it/s]\u001b[A\n"," 57% 105/185 [00:23<00:18,  4.33it/s]\u001b[A\n"," 57% 106/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 58% 107/185 [00:24<00:17,  4.33it/s]\u001b[A\n"," 58% 108/185 [00:24<00:17,  4.34it/s]\u001b[A\n"," 59% 109/185 [00:24<00:17,  4.34it/s]\u001b[A\n"," 59% 110/185 [00:25<00:17,  4.34it/s]\u001b[A\n"," 60% 111/185 [00:25<00:17,  4.34it/s]\u001b[A\n"," 61% 112/185 [00:25<00:16,  4.34it/s]\u001b[A\n"," 61% 113/185 [00:25<00:16,  4.34it/s]\u001b[A\n"," 62% 114/185 [00:26<00:16,  4.33it/s]\u001b[A\n"," 62% 115/185 [00:26<00:16,  4.34it/s]\u001b[A\n"," 63% 116/185 [00:26<00:15,  4.34it/s]\u001b[A\n"," 63% 117/185 [00:26<00:15,  4.33it/s]\u001b[A\n"," 64% 118/185 [00:26<00:15,  4.34it/s]\u001b[A\n"," 64% 119/185 [00:27<00:15,  4.34it/s]\u001b[A\n"," 65% 120/185 [00:27<00:14,  4.34it/s]\u001b[A\n"," 65% 121/185 [00:27<00:14,  4.34it/s]\u001b[A\n"," 66% 122/185 [00:27<00:14,  4.34it/s]\u001b[A\n"," 66% 123/185 [00:28<00:14,  4.34it/s]\u001b[A\n"," 67% 124/185 [00:28<00:14,  4.33it/s]\u001b[A\n"," 68% 125/185 [00:28<00:13,  4.34it/s]\u001b[A\n"," 68% 126/185 [00:28<00:13,  4.33it/s]\u001b[A\n"," 69% 127/185 [00:29<00:13,  4.33it/s]\u001b[A\n"," 69% 128/185 [00:29<00:13,  4.33it/s]\u001b[A\n"," 70% 129/185 [00:29<00:12,  4.33it/s]\u001b[A\n"," 70% 130/185 [00:29<00:12,  4.33it/s]\u001b[A\n"," 71% 131/185 [00:29<00:12,  4.33it/s]\u001b[A\n"," 71% 132/185 [00:30<00:12,  4.33it/s]\u001b[A\n"," 72% 133/185 [00:30<00:12,  4.33it/s]\u001b[A\n"," 72% 134/185 [00:30<00:11,  4.33it/s]\u001b[A\n"," 73% 135/185 [00:30<00:11,  4.33it/s]\u001b[A\n"," 74% 136/185 [00:31<00:11,  4.33it/s]\u001b[A\n"," 74% 137/185 [00:31<00:11,  4.33it/s]\u001b[A\n"," 75% 138/185 [00:31<00:10,  4.33it/s]\u001b[A\n"," 75% 139/185 [00:31<00:10,  4.33it/s]\u001b[A\n"," 76% 140/185 [00:32<00:10,  4.34it/s]\u001b[A\n"," 76% 141/185 [00:32<00:10,  4.34it/s]\u001b[A\n"," 77% 142/185 [00:32<00:09,  4.34it/s]\u001b[A\n"," 77% 143/185 [00:32<00:09,  4.34it/s]\u001b[A\n"," 78% 144/185 [00:32<00:09,  4.34it/s]\u001b[A\n"," 78% 145/185 [00:33<00:09,  4.34it/s]\u001b[A\n"," 79% 146/185 [00:33<00:08,  4.34it/s]\u001b[A\n"," 79% 147/185 [00:33<00:08,  4.34it/s]\u001b[A\n"," 80% 148/185 [00:33<00:08,  4.34it/s]\u001b[A\n"," 81% 149/185 [00:34<00:08,  4.34it/s]\u001b[A\n"," 81% 150/185 [00:34<00:08,  4.34it/s]\u001b[A\n"," 82% 151/185 [00:34<00:07,  4.34it/s]\u001b[A\n"," 82% 152/185 [00:34<00:07,  4.34it/s]\u001b[A\n"," 83% 153/185 [00:35<00:07,  4.33it/s]\u001b[A\n"," 83% 154/185 [00:35<00:07,  4.33it/s]\u001b[A\n"," 84% 155/185 [00:35<00:06,  4.34it/s]\u001b[A\n"," 84% 156/185 [00:35<00:06,  4.33it/s]\u001b[A\n"," 85% 157/185 [00:35<00:06,  4.33it/s]\u001b[A\n"," 85% 158/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 86% 159/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 86% 160/185 [00:36<00:05,  4.33it/s]\u001b[A\n"," 87% 161/185 [00:36<00:05,  4.33it/s]\u001b[A\n"," 88% 162/185 [00:37<00:05,  4.33it/s]\u001b[A\n"," 88% 163/185 [00:37<00:05,  4.33it/s]\u001b[A\n"," 89% 164/185 [00:37<00:04,  4.33it/s]\u001b[A\n"," 89% 165/185 [00:37<00:04,  4.33it/s]\u001b[A\n"," 90% 166/185 [00:38<00:04,  4.33it/s]\u001b[A\n"," 90% 167/185 [00:38<00:04,  4.33it/s]\u001b[A\n"," 91% 168/185 [00:38<00:03,  4.33it/s]\u001b[A\n"," 91% 169/185 [00:38<00:03,  4.33it/s]\u001b[A\n"," 92% 170/185 [00:38<00:03,  4.33it/s]\u001b[A\n"," 92% 171/185 [00:39<00:03,  4.33it/s]\u001b[A\n"," 93% 172/185 [00:39<00:02,  4.33it/s]\u001b[A\n"," 94% 173/185 [00:39<00:02,  4.33it/s]\u001b[A\n"," 94% 174/185 [00:39<00:02,  4.33it/s]\u001b[A\n"," 95% 175/185 [00:40<00:02,  4.34it/s]\u001b[A\n"," 95% 176/185 [00:40<00:02,  4.33it/s]\u001b[A\n"," 96% 177/185 [00:40<00:01,  4.33it/s]\u001b[A\n"," 96% 178/185 [00:40<00:01,  4.34it/s]\u001b[A\n"," 97% 179/185 [00:41<00:01,  4.34it/s]\u001b[A\n"," 97% 180/185 [00:41<00:01,  4.34it/s]\u001b[A\n"," 98% 181/185 [00:41<00:00,  4.34it/s]\u001b[A\n"," 98% 182/185 [00:41<00:00,  4.34it/s]\u001b[A\n"," 99% 183/185 [00:41<00:00,  4.34it/s]\u001b[A\n"," 99% 184/185 [00:42<00:00,  4.33it/s]\u001b[A\n","100% 185/185 [00:42<00:00,  4.33it/s]\u001b[A03/03/2022 04:51:16 - INFO - src.trainer -   Best dev result: 0.875\n","Epoch:  20% 49/250 [02:12<05:38,  1.69s/it]\n","186it [01:32, 15.16s/it]             \u001b[A\n","187it [01:32, 10.68s/it]\u001b[A\n","188it [01:32,  7.55s/it]\u001b[A\n","189it [01:33,  5.35s/it]\u001b[A\n","190it [01:33,  3.82s/it]\u001b[A\n","191it [01:33,  2.74s/it]\u001b[A\n","192it [01:33,  1.99s/it]\u001b[A\n","193it [01:34,  1.46s/it]\u001b[A\n","194it [01:34,  1.09s/it]\u001b[A\n","195it [01:34,  1.20it/s]\u001b[A\n","196it [01:34,  1.53it/s]\u001b[A\n","197it [01:34,  1.90it/s]\u001b[A\n","198it [01:35,  2.29it/s]\u001b[A\n","199it [01:35,  2.66it/s]\u001b[A\n","200it [01:35,  3.01it/s]\u001b[A\n","201it [01:35,  3.32it/s]\u001b[A\n","202it [01:36,  3.57it/s]\u001b[A\n","203it [01:36,  3.77it/s]\u001b[A\n","204it [01:36,  3.92it/s]\u001b[A\n","205it [01:36,  4.04it/s]\u001b[A\n","206it [01:37,  4.12it/s]\u001b[A\n","207it [01:37,  4.18it/s]\u001b[A\n","208it [01:37,  4.23it/s]\u001b[A\n","209it [01:37,  4.26it/s]\u001b[A\n","210it [01:37,  4.28it/s]\u001b[A\n","211it [01:38,  4.30it/s]\u001b[A\n","212it [01:38,  4.31it/s]\u001b[A\n","213it [01:38,  4.32it/s]\u001b[A\n","214it [01:38,  4.32it/s]\u001b[A\n","215it [01:39,  4.33it/s]\u001b[A\n","216it [01:39,  4.33it/s]\u001b[A\n","217it [01:39,  4.33it/s]\u001b[A\n","218it [01:39,  4.33it/s]\u001b[A\n","219it [01:40,  4.33it/s]\u001b[A\n","220it [01:40,  4.33it/s]\u001b[A\n","221it [01:40,  4.33it/s]\u001b[A\n","222it [01:40,  4.33it/s]\u001b[A\n","223it [01:40,  4.33it/s]\u001b[A\n","224it [01:41,  4.33it/s]\u001b[A\n","225it [01:41,  4.33it/s]\u001b[A\n","226it [01:41,  4.33it/s]\u001b[A\n","227it [01:41,  4.33it/s]\u001b[A\n","228it [01:42,  4.33it/s]\u001b[A\n","229it [01:42,  4.33it/s]\u001b[A\n","230it [01:42,  4.33it/s]\u001b[A\n","231it [01:42,  4.33it/s]\u001b[A\n","232it [01:43,  4.33it/s]\u001b[A\n","233it [01:43,  4.33it/s]\u001b[A\n","234it [01:43,  4.33it/s]\u001b[A\n","235it [01:43,  4.33it/s]\u001b[A\n","236it [01:43,  4.33it/s]\u001b[A\n","237it [01:44,  4.34it/s]\u001b[A\n","238it [01:44,  4.34it/s]\u001b[A\n","239it [01:44,  4.33it/s]\u001b[A\n","240it [01:44,  4.33it/s]\u001b[A\n","241it [01:45,  4.33it/s]\u001b[A\n","242it [01:45,  4.33it/s]\u001b[A\n","243it [01:45,  4.34it/s]\u001b[A\n","244it [01:45,  4.34it/s]\u001b[A\n","245it [01:46,  4.34it/s]\u001b[A\n","246it [01:46,  4.33it/s]\u001b[A\n","247it [01:46,  4.33it/s]\u001b[A\n","248it [01:46,  4.33it/s]\u001b[A\n","249it [01:46,  4.33it/s]\u001b[A\n","250it [01:47,  4.33it/s]\u001b[A\n","251it [01:47,  4.33it/s]\u001b[A\n","252it [01:47,  4.34it/s]\u001b[A\n","253it [01:47,  4.34it/s]\u001b[A\n","254it [01:48,  4.34it/s]\u001b[A\n","255it [01:48,  4.34it/s]\u001b[A\n","256it [01:48,  4.34it/s]\u001b[A\n","257it [01:48,  4.34it/s]\u001b[A\n","258it [01:49,  4.33it/s]\u001b[A\n","259it [01:49,  4.33it/s]\u001b[A\n","260it [01:49,  4.34it/s]\u001b[A\n","261it [01:49,  4.34it/s]\u001b[A\n","262it [01:49,  4.34it/s]\u001b[A\n","263it [01:50,  4.34it/s]\u001b[A\n","264it [01:50,  4.33it/s]\u001b[A\n","265it [01:50,  4.33it/s]\u001b[A\n","266it [01:50,  4.33it/s]\u001b[A\n","267it [01:51,  4.33it/s]\u001b[A\n","268it [01:51,  4.33it/s]\u001b[A\n","269it [01:51,  4.33it/s]\u001b[A\n","270it [01:51,  4.33it/s]\u001b[A\n","271it [01:52,  4.33it/s]\u001b[A\n","272it [01:52,  4.33it/s]\u001b[A\n","273it [01:52,  4.33it/s]\u001b[A\n","274it [01:52,  4.33it/s]\u001b[A\n","275it [01:52,  4.33it/s]\u001b[A\n","276it [01:53,  4.33it/s]\u001b[A\n","277it [01:53,  4.33it/s]\u001b[A\n","278it [01:53,  4.33it/s]\u001b[A\n","279it [01:53,  4.33it/s]\u001b[A\n","280it [01:54,  4.33it/s]\u001b[A\n","281it [01:54,  4.34it/s]\u001b[A\n","282it [01:54,  4.34it/s]\u001b[A\n","283it [01:54,  4.34it/s]\u001b[A\n","284it [01:55,  4.34it/s]\u001b[A\n","285it [01:55,  4.34it/s]\u001b[A\n","286it [01:55,  4.33it/s]\u001b[A\n","287it [01:55,  4.33it/s]\u001b[A\n","288it [01:55,  4.33it/s]\u001b[A\n","289it [01:56,  4.33it/s]\u001b[A\n","290it [01:56,  4.33it/s]\u001b[A\n","291it [01:56,  4.33it/s]\u001b[A\n","292it [01:56,  4.33it/s]\u001b[A\n","293it [01:57,  4.33it/s]\u001b[A\n","294it [01:57,  4.33it/s]\u001b[A\n","295it [01:57,  4.33it/s]\u001b[A\n","296it [01:57,  4.34it/s]\u001b[A\n","297it [01:58,  4.34it/s]\u001b[A\n","298it [01:58,  4.33it/s]\u001b[A\n","299it [01:58,  4.33it/s]\u001b[A\n","300it [01:58,  4.34it/s]\u001b[A\n","301it [01:58,  4.34it/s]\u001b[A\n","302it [01:59,  4.34it/s]\u001b[A\n","303it [01:59,  4.34it/s]\u001b[A\n","304it [01:59,  4.34it/s]\u001b[A\n","305it [01:59,  4.34it/s]\u001b[A\n","306it [02:00,  4.34it/s]\u001b[A\n","307it [02:00,  4.34it/s]\u001b[A\n","308it [02:00,  4.33it/s]\u001b[A\n","309it [02:00,  4.33it/s]\u001b[A\n","310it [02:01,  4.33it/s]\u001b[A\n","311it [02:01,  4.33it/s]\u001b[A\n","312it [02:01,  4.33it/s]\u001b[A\n","313it [02:01,  4.33it/s]\u001b[A\n","314it [02:01,  4.34it/s]\u001b[A\n","315it [02:02,  4.33it/s]\u001b[A\n","316it [02:02,  4.33it/s]\u001b[A\n","317it [02:02,  4.33it/s]\u001b[A\n","318it [02:02,  4.33it/s]\u001b[A\n","319it [02:03,  4.33it/s]\u001b[A\n","320it [02:03,  4.33it/s]\u001b[A\n","321it [02:03,  4.33it/s]\u001b[A\n","322it [02:03,  4.34it/s]\u001b[A\n","323it [02:04,  4.34it/s]\u001b[A\n","324it [02:04,  4.34it/s]\u001b[A\n","325it [02:04,  4.34it/s]\u001b[A\n","326it [02:04,  4.34it/s]\u001b[A\n","327it [02:04,  4.34it/s]\u001b[A\n","328it [02:05,  4.34it/s]\u001b[A\n","329it [02:05,  4.33it/s]\u001b[A\n","330it [02:05,  4.34it/s]\u001b[A\n","331it [02:05,  4.34it/s]\u001b[A\n","332it [02:06,  4.33it/s]\u001b[A\n","333it [02:06,  4.33it/s]\u001b[A\n","334it [02:06,  4.33it/s]\u001b[A\n","335it [02:06,  4.33it/s]\u001b[A\n","336it [02:07,  4.33it/s]\u001b[A\n","337it [02:07,  4.33it/s]\u001b[A\n","338it [02:07,  4.33it/s]\u001b[A\n","339it [02:07,  4.33it/s]\u001b[A\n","340it [02:07,  4.33it/s]\u001b[A\n","341it [02:08,  4.33it/s]\u001b[A\n","342it [02:08,  4.33it/s]\u001b[A\n","343it [02:08,  4.33it/s]\u001b[A\n","344it [02:08,  4.33it/s]\u001b[A\n","345it [02:09,  4.33it/s]\u001b[A\n","346it [02:09,  4.33it/s]\u001b[A\n","347it [02:09,  4.33it/s]\u001b[A\n","348it [02:09,  4.33it/s]\u001b[A\n","349it [02:10,  4.33it/s]\u001b[A\n","350it [02:10,  4.33it/s]\u001b[A\n","351it [02:10,  4.33it/s]\u001b[A\n","352it [02:10,  4.34it/s]\u001b[A\n","353it [02:10,  4.34it/s]\u001b[A\n","354it [02:11,  4.34it/s]\u001b[A\n","355it [02:11,  4.33it/s]\u001b[A\n","356it [02:11,  4.33it/s]\u001b[A\n","357it [02:11,  4.33it/s]\u001b[A\n","358it [02:12,  4.33it/s]\u001b[A\n","359it [02:12,  4.33it/s]\u001b[A\n","360it [02:12,  4.33it/s]\u001b[A\n","361it [02:12,  4.34it/s]\u001b[A\n","362it [02:13,  4.34it/s]\u001b[A\n","363it [02:13,  4.34it/s]\u001b[A\n","364it [02:13,  4.34it/s]\u001b[A\n","365it [02:13,  4.33it/s]\u001b[A\n","366it [02:13,  4.33it/s]\u001b[A\n","367it [02:14,  4.34it/s]\u001b[A\n","368it [02:14,  4.34it/s]\u001b[A\n","369it [02:14,  4.33it/s]\u001b[A\n","Epoch:  30% 74/250 [03:37<04:54,  1.67s/it]\n","371it [02:56, 12.77s/it]\u001b[A\n","372it [02:57,  9.01s/it]\u001b[A\n","373it [02:57,  6.37s/it]\u001b[A\n","374it [02:57,  4.53s/it]\u001b[A\n","375it [02:57,  3.24s/it]\u001b[A\n","376it [02:58,  2.34s/it]\u001b[A\n","377it [02:58,  1.71s/it]\u001b[A\n","378it [02:58,  1.26s/it]\u001b[A\n","379it [02:58,  1.05it/s]\u001b[A\n","380it [02:59,  1.36it/s]\u001b[A\n","381it [02:59,  1.71it/s]\u001b[A\n","382it [02:59,  2.09it/s]\u001b[A\n","383it [02:59,  2.47it/s]\u001b[A\n","384it [02:59,  2.84it/s]\u001b[A\n","385it [03:00,  3.17it/s]\u001b[A\n","386it [03:00,  3.45it/s]\u001b[A\n","387it [03:00,  3.67it/s]\u001b[A\n","388it [03:00,  3.85it/s]\u001b[A\n","389it [03:01,  3.98it/s]\u001b[A\n","390it [03:01,  4.08it/s]\u001b[A\n","391it [03:01,  4.15it/s]\u001b[A\n","392it [03:01,  4.21it/s]\u001b[A\n","393it [03:02,  4.25it/s]\u001b[A\n","394it [03:02,  4.27it/s]\u001b[A\n","395it [03:02,  4.29it/s]\u001b[A\n","396it [03:02,  4.30it/s]\u001b[A\n","397it [03:02,  4.31it/s]\u001b[A\n","398it [03:03,  4.32it/s]\u001b[A\n","399it [03:03,  4.32it/s]\u001b[A\n","400it [03:03,  4.32it/s]\u001b[A\n","401it [03:03,  4.33it/s]\u001b[A\n","402it [03:04,  4.33it/s]\u001b[A\n","403it [03:04,  4.33it/s]\u001b[A\n","404it [03:04,  4.33it/s]\u001b[A\n","405it [03:04,  4.34it/s]\u001b[A\n","406it [03:05,  4.34it/s]\u001b[A\n","407it [03:05,  4.33it/s]\u001b[A\n","408it [03:05,  4.34it/s]\u001b[A\n","409it [03:05,  4.33it/s]\u001b[A\n","410it [03:05,  4.33it/s]\u001b[A\n","411it [03:06,  4.33it/s]\u001b[A\n","412it [03:06,  4.33it/s]\u001b[A\n","413it [03:06,  4.33it/s]\u001b[A\n","414it [03:06,  4.34it/s]\u001b[A\n","415it [03:07,  4.33it/s]\u001b[A\n","416it [03:07,  4.33it/s]\u001b[A\n","417it [03:07,  4.33it/s]\u001b[A\n","418it [03:07,  4.33it/s]\u001b[A\n","419it [03:08,  4.33it/s]\u001b[A\n","420it [03:08,  4.33it/s]\u001b[A\n","421it [03:08,  4.33it/s]\u001b[A\n","422it [03:08,  4.33it/s]\u001b[A\n","423it [03:08,  4.34it/s]\u001b[A\n","424it [03:09,  4.34it/s]\u001b[A\n","425it [03:09,  4.34it/s]\u001b[A\n","426it [03:09,  4.34it/s]\u001b[A\n","427it [03:09,  4.33it/s]\u001b[A\n","428it [03:10,  4.33it/s]\u001b[A\n","429it [03:10,  4.33it/s]\u001b[A\n","430it [03:10,  4.33it/s]\u001b[A\n","431it [03:10,  4.34it/s]\u001b[A\n","432it [03:11,  4.34it/s]\u001b[A\n","433it [03:11,  4.34it/s]\u001b[A\n","434it [03:11,  4.33it/s]\u001b[A\n","435it [03:11,  4.33it/s]\u001b[A\n","436it [03:11,  4.33it/s]\u001b[A\n","437it [03:12,  4.33it/s]\u001b[A\n","438it [03:12,  4.33it/s]\u001b[A\n","439it [03:12,  4.33it/s]\u001b[A\n","440it [03:12,  4.33it/s]\u001b[A\n","441it [03:13,  4.33it/s]\u001b[A\n","442it [03:13,  4.33it/s]\u001b[A\n","443it [03:13,  4.33it/s]\u001b[A\n","444it [03:13,  4.33it/s]\u001b[A\n","445it [03:14,  4.33it/s]\u001b[A\n","446it [03:14,  4.33it/s]\u001b[A\n","447it [03:14,  4.33it/s]\u001b[A\n","448it [03:14,  4.33it/s]\u001b[A\n","449it [03:14,  4.33it/s]\u001b[A\n","450it [03:15,  4.33it/s]\u001b[A\n","451it [03:15,  4.33it/s]\u001b[A\n","452it [03:15,  4.33it/s]\u001b[A\n","453it [03:15,  4.33it/s]\u001b[A\n","454it [03:16,  4.33it/s]\u001b[A\n","455it [03:16,  4.34it/s]\u001b[A\n","456it [03:16,  4.34it/s]\u001b[A\n","457it [03:16,  4.33it/s]\u001b[A\n","458it [03:17,  4.33it/s]\u001b[A\n","459it [03:17,  4.33it/s]\u001b[A\n","460it [03:17,  4.33it/s]\u001b[A\n","461it [03:17,  4.33it/s]\u001b[A\n","462it [03:17,  4.33it/s]\u001b[A\n","463it [03:18,  4.33it/s]\u001b[A\n","464it [03:18,  4.34it/s]\u001b[A\n","465it [03:18,  4.34it/s]\u001b[A\n","466it [03:18,  4.33it/s]\u001b[A\n","467it [03:19,  4.33it/s]\u001b[A\n","468it [03:19,  4.34it/s]\u001b[A\n","469it [03:19,  4.34it/s]\u001b[A\n","470it [03:19,  4.34it/s]\u001b[A\n","471it [03:20,  4.34it/s]\u001b[A\n","472it [03:20,  4.34it/s]\u001b[A\n","473it [03:20,  4.33it/s]\u001b[A\n","474it [03:20,  4.33it/s]\u001b[A\n","475it [03:20,  4.33it/s]\u001b[A\n","476it [03:21,  4.33it/s]\u001b[A\n","477it [03:21,  4.33it/s]\u001b[A\n","478it [03:21,  4.33it/s]\u001b[A\n","479it [03:21,  4.33it/s]\u001b[A\n","480it [03:22,  4.33it/s]\u001b[A\n","481it [03:22,  4.33it/s]\u001b[A\n","482it [03:22,  4.33it/s]\u001b[A\n","483it [03:22,  4.33it/s]\u001b[A\n","484it [03:23,  4.33it/s]\u001b[A\n","485it [03:23,  4.33it/s]\u001b[A\n","486it [03:23,  4.34it/s]\u001b[A\n","487it [03:23,  4.33it/s]\u001b[A\n","488it [03:23,  4.33it/s]\u001b[A\n","489it [03:24,  4.33it/s]\u001b[A\n","490it [03:24,  4.33it/s]\u001b[A\n","491it [03:24,  4.34it/s]\u001b[A\n","492it [03:24,  4.33it/s]\u001b[A\n","493it [03:25,  4.33it/s]\u001b[A\n","494it [03:25,  4.33it/s]\u001b[A\n","495it [03:25,  4.33it/s]\u001b[A\n","496it [03:25,  4.33it/s]\u001b[A\n","497it [03:26,  4.34it/s]\u001b[A\n","498it [03:26,  4.34it/s]\u001b[A\n","499it [03:26,  4.34it/s]\u001b[A\n","500it [03:26,  4.34it/s]\u001b[A\n","501it [03:26,  4.34it/s]\u001b[A\n","502it [03:27,  4.34it/s]\u001b[A\n","503it [03:27,  4.33it/s]\u001b[A\n","504it [03:27,  4.33it/s]\u001b[A\n","505it [03:27,  4.34it/s]\u001b[A\n","506it [03:28,  4.34it/s]\u001b[A\n","507it [03:28,  4.34it/s]\u001b[A\n","508it [03:28,  4.34it/s]\u001b[A\n","509it [03:28,  4.34it/s]\u001b[A\n","510it [03:28,  4.33it/s]\u001b[A\n","511it [03:29,  4.33it/s]\u001b[A\n","512it [03:29,  4.33it/s]\u001b[A\n","513it [03:29,  4.33it/s]\u001b[A\n","514it [03:29,  4.33it/s]\u001b[A\n","515it [03:30,  4.34it/s]\u001b[A\n","516it [03:30,  4.34it/s]\u001b[A\n","517it [03:30,  4.34it/s]\u001b[A\n","518it [03:30,  4.34it/s]\u001b[A\n","519it [03:31,  4.34it/s]\u001b[A\n","520it [03:31,  4.33it/s]\u001b[A\n","521it [03:31,  4.34it/s]\u001b[A\n","522it [03:31,  4.34it/s]\u001b[A\n","523it [03:31,  4.33it/s]\u001b[A\n","524it [03:32,  4.33it/s]\u001b[A\n","525it [03:32,  4.34it/s]\u001b[A\n","526it [03:32,  4.33it/s]\u001b[A\n","527it [03:32,  4.33it/s]\u001b[A\n","528it [03:33,  4.33it/s]\u001b[A\n","529it [03:33,  4.33it/s]\u001b[A\n","530it [03:33,  4.33it/s]\u001b[A\n","531it [03:33,  4.33it/s]\u001b[A\n","532it [03:34,  4.33it/s]\u001b[A\n","533it [03:34,  4.33it/s]\u001b[A\n","534it [03:34,  4.33it/s]\u001b[A\n","535it [03:34,  4.33it/s]\u001b[A\n","536it [03:34,  4.33it/s]\u001b[A\n","537it [03:35,  4.34it/s]\u001b[A\n","538it [03:35,  4.34it/s]\u001b[A\n","539it [03:35,  4.34it/s]\u001b[A\n","540it [03:35,  4.33it/s]\u001b[A\n","541it [03:36,  4.33it/s]\u001b[A\n","542it [03:36,  4.33it/s]\u001b[A\n","543it [03:36,  4.33it/s]\u001b[A\n","544it [03:36,  4.33it/s]\u001b[A\n","545it [03:37,  4.34it/s]\u001b[A\n","546it [03:37,  4.34it/s]\u001b[A\n","547it [03:37,  4.34it/s]\u001b[A\n","548it [03:37,  4.34it/s]\u001b[A\n","549it [03:37,  4.34it/s]\u001b[A\n","550it [03:38,  4.33it/s]\u001b[A\n","551it [03:38,  4.34it/s]\u001b[A\n","552it [03:38,  4.33it/s]\u001b[A\n","553it [03:38,  4.33it/s]\u001b[A\n","554it [03:39,  4.34it/s]\u001b[A\n","Epoch:  40% 99/250 [05:01<04:12,  1.67s/it]\n","556it [04:21, 12.77s/it]\u001b[A\n","557it [04:21,  9.01s/it]\u001b[A\n","558it [04:21,  6.38s/it]\u001b[A\n","559it [04:22,  4.53s/it]\u001b[A\n","560it [04:22,  3.24s/it]\u001b[A\n","561it [04:22,  2.34s/it]\u001b[A\n","562it [04:22,  1.71s/it]\u001b[A\n","563it [04:23,  1.26s/it]\u001b[A\n","564it [04:23,  1.05it/s]\u001b[A\n","565it [04:23,  1.36it/s]\u001b[A\n","566it [04:23,  1.71it/s]\u001b[A\n","567it [04:23,  2.09it/s]\u001b[A\n","568it [04:24,  2.47it/s]\u001b[A\n","569it [04:24,  2.84it/s]\u001b[A\n","570it [04:24,  3.17it/s]\u001b[A\n","571it [04:24,  3.45it/s]\u001b[A\n","572it [04:25,  3.67it/s]\u001b[A\n","573it [04:25,  3.85it/s]\u001b[A\n","574it [04:25,  3.98it/s]\u001b[A\n","575it [04:25,  4.08it/s]\u001b[A\n","576it [04:26,  4.16it/s]\u001b[A\n","577it [04:26,  4.21it/s]\u001b[A\n","578it [04:26,  4.25it/s]\u001b[A\n","579it [04:26,  4.27it/s]\u001b[A\n","580it [04:26,  4.28it/s]\u001b[A\n","581it [04:27,  4.30it/s]\u001b[A\n","582it [04:27,  4.31it/s]\u001b[A\n","583it [04:27,  4.32it/s]\u001b[A\n","584it [04:27,  4.33it/s]\u001b[A\n","585it [04:28,  4.33it/s]\u001b[A\n","586it [04:28,  4.33it/s]\u001b[A\n","587it [04:28,  4.33it/s]\u001b[A\n","588it [04:28,  4.33it/s]\u001b[A\n","589it [04:29,  4.33it/s]\u001b[A\n","590it [04:29,  4.33it/s]\u001b[A\n","591it [04:29,  4.33it/s]\u001b[A\n","592it [04:29,  4.33it/s]\u001b[A\n","593it [04:29,  4.33it/s]\u001b[A\n","594it [04:30,  4.33it/s]\u001b[A\n","595it [04:30,  4.34it/s]\u001b[A\n","596it [04:30,  4.34it/s]\u001b[A\n","597it [04:30,  4.33it/s]\u001b[A\n","598it [04:31,  4.33it/s]\u001b[A\n","599it [04:31,  4.34it/s]\u001b[A\n","600it [04:31,  4.34it/s]\u001b[A\n","601it [04:31,  4.34it/s]\u001b[A\n","602it [04:32,  4.34it/s]\u001b[A\n","603it [04:32,  4.34it/s]\u001b[A\n","604it [04:32,  4.34it/s]\u001b[A\n","605it [04:32,  4.34it/s]\u001b[A\n","606it [04:32,  4.34it/s]\u001b[A\n","607it [04:33,  4.34it/s]\u001b[A\n","608it [04:33,  4.34it/s]\u001b[A\n","609it [04:33,  4.33it/s]\u001b[A\n","610it [04:33,  4.33it/s]\u001b[A\n","611it [04:34,  4.34it/s]\u001b[A\n","612it [04:34,  4.34it/s]\u001b[A\n","613it [04:34,  4.34it/s]\u001b[A\n","614it [04:34,  4.34it/s]\u001b[A\n","615it [04:35,  4.34it/s]\u001b[A\n","616it [04:35,  4.34it/s]\u001b[A\n","617it [04:35,  4.34it/s]\u001b[A\n","618it [04:35,  4.34it/s]\u001b[A\n","619it [04:35,  4.34it/s]\u001b[A\n","620it [04:36,  4.33it/s]\u001b[A\n","621it [04:36,  4.33it/s]\u001b[A\n","622it [04:36,  4.33it/s]\u001b[A\n","623it [04:36,  4.34it/s]\u001b[A\n","624it [04:37,  4.34it/s]\u001b[A\n","625it [04:37,  4.34it/s]\u001b[A\n","626it [04:37,  4.34it/s]\u001b[A\n","627it [04:37,  4.34it/s]\u001b[A\n","628it [04:38,  4.34it/s]\u001b[A\n","629it [04:38,  4.34it/s]\u001b[A\n","630it [04:38,  4.33it/s]\u001b[A\n","631it [04:38,  4.33it/s]\u001b[A\n","632it [04:38,  4.33it/s]\u001b[A\n","633it [04:39,  4.34it/s]\u001b[A\n","634it [04:39,  4.34it/s]\u001b[A\n","635it [04:39,  4.34it/s]\u001b[A\n","636it [04:39,  4.34it/s]\u001b[A\n","637it [04:40,  4.34it/s]\u001b[A\n","638it [04:40,  4.34it/s]\u001b[A\n","639it [04:40,  4.33it/s]\u001b[A\n","640it [04:40,  4.33it/s]\u001b[A\n","641it [04:41,  4.33it/s]\u001b[A\n","642it [04:41,  4.34it/s]\u001b[A\n","643it [04:41,  4.34it/s]\u001b[A\n","644it [04:41,  4.34it/s]\u001b[A\n","645it [04:41,  4.34it/s]\u001b[A\n","646it [04:42,  4.34it/s]\u001b[A\n","647it [04:42,  4.33it/s]\u001b[A\n","648it [04:42,  4.34it/s]\u001b[A\n","649it [04:42,  4.34it/s]\u001b[A\n","650it [04:43,  4.33it/s]\u001b[A\n","651it [04:43,  4.33it/s]\u001b[A\n","652it [04:43,  4.33it/s]\u001b[A\n","653it [04:43,  4.33it/s]\u001b[A\n","654it [04:44,  4.33it/s]\u001b[A\n","655it [04:44,  4.34it/s]\u001b[A\n","656it [04:44,  4.34it/s]\u001b[A\n","657it [04:44,  4.34it/s]\u001b[A\n","658it [04:44,  4.34it/s]\u001b[A\n","659it [04:45,  4.34it/s]\u001b[A\n","660it [04:45,  4.33it/s]\u001b[A\n","661it [04:45,  4.33it/s]\u001b[A\n","662it [04:45,  4.33it/s]\u001b[A\n","663it [04:46,  4.33it/s]\u001b[A\n","664it [04:46,  4.33it/s]\u001b[A\n","665it [04:46,  4.33it/s]\u001b[A\n","666it [04:46,  4.33it/s]\u001b[A\n","667it [04:47,  4.33it/s]\u001b[A\n","668it [04:47,  4.33it/s]\u001b[A\n","669it [04:47,  4.33it/s]\u001b[A\n","670it [04:47,  4.33it/s]\u001b[A\n","671it [04:47,  4.33it/s]\u001b[A\n","672it [04:48,  4.34it/s]\u001b[A\n","673it [04:48,  4.34it/s]\u001b[A\n","674it [04:48,  4.34it/s]\u001b[A\n","675it [04:48,  4.34it/s]\u001b[A\n","676it [04:49,  4.33it/s]\u001b[A\n","677it [04:49,  4.33it/s]\u001b[A\n","678it [04:49,  4.33it/s]\u001b[A\n","679it [04:49,  4.33it/s]\u001b[A\n","680it [04:50,  4.33it/s]\u001b[A\n","681it [04:50,  4.34it/s]\u001b[A\n","682it [04:50,  4.34it/s]\u001b[A\n","683it [04:50,  4.33it/s]\u001b[A\n","684it [04:50,  4.34it/s]\u001b[A\n","685it [04:51,  4.34it/s]\u001b[A\n","686it [04:51,  4.34it/s]\u001b[A\n","687it [04:51,  4.33it/s]\u001b[A\n","688it [04:51,  4.33it/s]\u001b[A\n","689it [04:52,  4.33it/s]\u001b[A\n","690it [04:52,  4.33it/s]\u001b[A\n","691it [04:52,  4.33it/s]\u001b[A\n","692it [04:52,  4.33it/s]\u001b[A\n","693it [04:53,  4.34it/s]\u001b[A\n","694it [04:53,  4.34it/s]\u001b[A\n","695it [04:53,  4.33it/s]\u001b[A\n","696it [04:53,  4.33it/s]\u001b[A\n","697it [04:53,  4.33it/s]\u001b[A\n","698it [04:54,  4.33it/s]\u001b[A\n","699it [04:54,  4.33it/s]\u001b[A\n","700it [04:54,  4.33it/s]\u001b[A\n","701it [04:54,  4.33it/s]\u001b[A\n","702it [04:55,  4.34it/s]\u001b[A\n","703it [04:55,  4.34it/s]\u001b[A\n","704it [04:55,  4.34it/s]\u001b[A\n","705it [04:55,  4.34it/s]\u001b[A\n","706it [04:56,  4.34it/s]\u001b[A\n","707it [04:56,  4.33it/s]\u001b[A\n","708it [04:56,  4.34it/s]\u001b[A\n","709it [04:56,  4.34it/s]\u001b[A\n","710it [04:56,  4.34it/s]\u001b[A\n","711it [04:57,  4.34it/s]\u001b[A\n","712it [04:57,  4.34it/s]\u001b[A\n","713it [04:57,  4.34it/s]\u001b[A\n","714it [04:57,  4.34it/s]\u001b[A\n","715it [04:58,  4.34it/s]\u001b[A\n","716it [04:58,  4.34it/s]\u001b[A\n","717it [04:58,  4.33it/s]\u001b[A\n","718it [04:58,  4.34it/s]\u001b[A\n","719it [04:59,  4.34it/s]\u001b[A\n","720it [04:59,  4.34it/s]\u001b[A\n","721it [04:59,  4.34it/s]\u001b[A\n","722it [04:59,  4.33it/s]\u001b[A\n","723it [04:59,  4.33it/s]\u001b[A\n","724it [05:00,  4.33it/s]\u001b[A\n","725it [05:00,  4.33it/s]\u001b[A\n","726it [05:00,  4.33it/s]\u001b[A\n","727it [05:00,  4.33it/s]\u001b[A\n","728it [05:01,  4.34it/s]\u001b[A\n","729it [05:01,  4.33it/s]\u001b[A\n","730it [05:01,  4.33it/s]\u001b[A\n","731it [05:01,  4.33it/s]\u001b[A\n","732it [05:02,  4.34it/s]\u001b[A\n","733it [05:02,  4.33it/s]\u001b[A\n","734it [05:02,  4.34it/s]\u001b[A\n","735it [05:02,  4.33it/s]\u001b[A\n","736it [05:02,  4.33it/s]\u001b[A\n","737it [05:03,  4.33it/s]\u001b[A\n","738it [05:03,  4.33it/s]\u001b[A\n","739it [05:03,  4.33it/s]\u001b[A\n","Epoch:  50% 124/250 [06:26<03:31,  1.67s/it]\n","741it [05:45, 12.78s/it]\u001b[A\n","742it [05:46,  9.01s/it]\u001b[A\n","743it [05:46,  6.38s/it]\u001b[A\n","744it [05:46,  4.53s/it]\u001b[A\n","745it [05:46,  3.24s/it]\u001b[A\n","746it [05:47,  2.34s/it]\u001b[A\n","747it [05:47,  1.71s/it]\u001b[A\n","748it [05:47,  1.26s/it]\u001b[A\n","749it [05:47,  1.05it/s]\u001b[A\n","750it [05:47,  1.36it/s]\u001b[A\n","751it [05:48,  1.71it/s]\u001b[A\n","752it [05:48,  2.09it/s]\u001b[A\n","753it [05:48,  2.47it/s]\u001b[A\n","754it [05:48,  2.84it/s]\u001b[A\n","755it [05:49,  3.17it/s]\u001b[A\n","756it [05:49,  3.45it/s]\u001b[A\n","757it [05:49,  3.67it/s]\u001b[A\n","758it [05:49,  3.85it/s]\u001b[A\n","759it [05:50,  3.98it/s]\u001b[A\n","760it [05:50,  4.08it/s]\u001b[A\n","761it [05:50,  4.15it/s]\u001b[A\n","762it [05:50,  4.21it/s]\u001b[A\n","763it [05:50,  4.25it/s]\u001b[A\n","764it [05:51,  4.27it/s]\u001b[A\n","765it [05:51,  4.29it/s]\u001b[A\n","766it [05:51,  4.31it/s]\u001b[A\n","767it [05:51,  4.31it/s]\u001b[A\n","768it [05:52,  4.32it/s]\u001b[A\n","769it [05:52,  4.32it/s]\u001b[A\n","770it [05:52,  4.32it/s]\u001b[A\n","771it [05:52,  4.33it/s]\u001b[A\n","772it [05:53,  4.33it/s]\u001b[A\n","773it [05:53,  4.33it/s]\u001b[A\n","774it [05:53,  4.33it/s]\u001b[A\n","775it [05:53,  4.33it/s]\u001b[A\n","776it [05:53,  4.33it/s]\u001b[A\n","777it [05:54,  4.33it/s]\u001b[A\n","778it [05:54,  4.33it/s]\u001b[A\n","779it [05:54,  4.33it/s]\u001b[A\n","780it [05:54,  4.33it/s]\u001b[A\n","781it [05:55,  4.33it/s]\u001b[A\n","782it [05:55,  4.34it/s]\u001b[A\n","783it [05:55,  4.33it/s]\u001b[A\n","784it [05:55,  4.33it/s]\u001b[A\n","785it [05:56,  4.34it/s]\u001b[A\n","786it [05:56,  4.34it/s]\u001b[A\n","787it [05:56,  4.34it/s]\u001b[A\n","788it [05:56,  4.34it/s]\u001b[A\n","789it [05:56,  4.34it/s]\u001b[A\n","790it [05:57,  4.34it/s]\u001b[A\n","791it [05:57,  4.34it/s]\u001b[A\n","792it [05:57,  4.33it/s]\u001b[A\n","793it [05:57,  4.33it/s]\u001b[A\n","794it [05:58,  4.33it/s]\u001b[A\n","795it [05:58,  4.33it/s]\u001b[A\n","796it [05:58,  4.33it/s]\u001b[A\n","797it [05:58,  4.33it/s]\u001b[A\n","798it [05:59,  4.34it/s]\u001b[A\n","799it [05:59,  4.33it/s]\u001b[A\n","800it [05:59,  4.33it/s]\u001b[A\n","801it [05:59,  4.33it/s]\u001b[A\n","802it [05:59,  4.33it/s]\u001b[A\n","803it [06:00,  4.33it/s]\u001b[A\n","804it [06:00,  4.34it/s]\u001b[A\n","805it [06:00,  4.34it/s]\u001b[A\n","806it [06:00,  4.34it/s]\u001b[A\n","807it [06:01,  4.33it/s]\u001b[A\n","808it [06:01,  4.33it/s]\u001b[A\n","809it [06:01,  4.33it/s]\u001b[A\n","810it [06:01,  4.33it/s]\u001b[A\n","811it [06:02,  4.33it/s]\u001b[A\n","812it [06:02,  4.33it/s]\u001b[A\n","813it [06:02,  4.33it/s]\u001b[A\n","814it [06:02,  4.33it/s]\u001b[A\n","815it [06:02,  4.33it/s]\u001b[A\n","816it [06:03,  4.33it/s]\u001b[A\n","817it [06:03,  4.33it/s]\u001b[A\n","818it [06:03,  4.33it/s]\u001b[A\n","819it [06:03,  4.33it/s]\u001b[A\n","820it [06:04,  4.33it/s]\u001b[A\n","821it [06:04,  4.34it/s]\u001b[A\n","822it [06:04,  4.34it/s]\u001b[A\n","823it [06:04,  4.34it/s]\u001b[A\n","824it [06:05,  4.34it/s]\u001b[A\n","825it [06:05,  4.34it/s]\u001b[A\n","826it [06:05,  4.34it/s]\u001b[A\n","827it [06:05,  4.34it/s]\u001b[A\n","828it [06:05,  4.34it/s]\u001b[A\n","829it [06:06,  4.33it/s]\u001b[A\n","830it [06:06,  4.33it/s]\u001b[A\n","831it [06:06,  4.33it/s]\u001b[A\n","832it [06:06,  4.33it/s]\u001b[A\n","833it [06:07,  4.33it/s]\u001b[A\n","834it [06:07,  4.33it/s]\u001b[A\n","835it [06:07,  4.34it/s]\u001b[A\n","836it [06:07,  4.34it/s]\u001b[A\n","837it [06:08,  4.34it/s]\u001b[A\n","838it [06:08,  4.34it/s]\u001b[A\n","839it [06:08,  4.34it/s]\u001b[A\n","840it [06:08,  4.33it/s]\u001b[A\n","841it [06:08,  4.33it/s]\u001b[A\n","842it [06:09,  4.33it/s]\u001b[A\n","843it [06:09,  4.33it/s]\u001b[A\n","844it [06:09,  4.33it/s]\u001b[A\n","845it [06:09,  4.33it/s]\u001b[A\n","846it [06:10,  4.33it/s]\u001b[A\n","847it [06:10,  4.33it/s]\u001b[A\n","848it [06:10,  4.34it/s]\u001b[A\n","849it [06:10,  4.33it/s]\u001b[A\n","850it [06:11,  4.33it/s]\u001b[A\n","851it [06:11,  4.33it/s]\u001b[A\n","852it [06:11,  4.33it/s]\u001b[A\n","853it [06:11,  4.33it/s]\u001b[A\n","854it [06:11,  4.33it/s]\u001b[A\n","855it [06:12,  4.33it/s]\u001b[A\n","856it [06:12,  4.33it/s]\u001b[A\n","857it [06:12,  4.34it/s]\u001b[A\n","858it [06:12,  4.34it/s]\u001b[A\n","859it [06:13,  4.33it/s]\u001b[A\n","860it [06:13,  4.33it/s]\u001b[A\n","861it [06:13,  4.33it/s]\u001b[A\n","862it [06:13,  4.33it/s]\u001b[A\n","863it [06:14,  4.33it/s]\u001b[A\n","864it [06:14,  4.33it/s]\u001b[A\n","865it [06:14,  4.33it/s]\u001b[A\n","866it [06:14,  4.34it/s]\u001b[A\n","867it [06:14,  4.34it/s]\u001b[A\n","868it [06:15,  4.34it/s]\u001b[A\n","869it [06:15,  4.33it/s]\u001b[A\n","870it [06:15,  4.33it/s]\u001b[A\n","871it [06:15,  4.33it/s]\u001b[A\n","872it [06:16,  4.33it/s]\u001b[A\n","873it [06:16,  4.33it/s]\u001b[A\n","874it [06:16,  4.33it/s]\u001b[A\n","875it [06:16,  4.33it/s]\u001b[A\n","876it [06:17,  4.34it/s]\u001b[A\n","877it [06:17,  4.34it/s]\u001b[A\n","878it [06:17,  4.34it/s]\u001b[A\n","879it [06:17,  4.33it/s]\u001b[A\n","880it [06:17,  4.34it/s]\u001b[A\n","881it [06:18,  4.34it/s]\u001b[A\n","882it [06:18,  4.34it/s]\u001b[A\n","883it [06:18,  4.33it/s]\u001b[A\n","884it [06:18,  4.34it/s]\u001b[A\n","885it [06:19,  4.34it/s]\u001b[A\n","886it [06:19,  4.34it/s]\u001b[A\n","887it [06:19,  4.33it/s]\u001b[A\n","888it [06:19,  4.33it/s]\u001b[A\n","889it [06:20,  4.33it/s]\u001b[A\n","890it [06:20,  4.33it/s]\u001b[A\n","891it [06:20,  4.34it/s]\u001b[A\n","892it [06:20,  4.34it/s]\u001b[A\n","893it [06:20,  4.33it/s]\u001b[A\n","894it [06:21,  4.33it/s]\u001b[A\n","895it [06:21,  4.33it/s]\u001b[A\n","896it [06:21,  4.34it/s]\u001b[A\n","897it [06:21,  4.34it/s]\u001b[A\n","898it [06:22,  4.34it/s]\u001b[A\n","899it [06:22,  4.33it/s]\u001b[A\n","900it [06:22,  4.33it/s]\u001b[A\n","901it [06:22,  4.33it/s]\u001b[A\n","902it [06:23,  4.33it/s]\u001b[A\n","903it [06:23,  4.33it/s]\u001b[A\n","904it [06:23,  4.33it/s]\u001b[A\n","905it [06:23,  4.33it/s]\u001b[A\n","906it [06:23,  4.34it/s]\u001b[A\n","907it [06:24,  4.34it/s]\u001b[A\n","908it [06:24,  4.34it/s]\u001b[A\n","909it [06:24,  4.33it/s]\u001b[A\n","910it [06:24,  4.33it/s]\u001b[A\n","911it [06:25,  4.33it/s]\u001b[A\n","912it [06:25,  4.33it/s]\u001b[A\n","913it [06:25,  4.33it/s]\u001b[A\n","914it [06:25,  4.33it/s]\u001b[A\n","915it [06:26,  4.33it/s]\u001b[A\n","916it [06:26,  4.34it/s]\u001b[A\n","917it [06:26,  4.34it/s]\u001b[A\n","918it [06:26,  4.34it/s]\u001b[A\n","919it [06:26,  4.34it/s]\u001b[A\n","920it [06:27,  4.34it/s]\u001b[A\n","921it [06:27,  4.34it/s]\u001b[A\n","922it [06:27,  4.34it/s]\u001b[A\n","923it [06:27,  4.33it/s]\u001b[A\n","924it [06:28,  4.33it/s]\u001b[A\n","Epoch:  60% 149/250 [07:50<02:48,  1.67s/it]\n","926it [07:10, 12.76s/it]\u001b[A\n","927it [07:10,  9.00s/it]\u001b[A\n","928it [07:10,  6.37s/it]\u001b[A\n","929it [07:11,  4.53s/it]\u001b[A\n","930it [07:11,  3.24s/it]\u001b[A\n","931it [07:11,  2.34s/it]\u001b[A\n","932it [07:11,  1.70s/it]\u001b[A\n","933it [07:11,  1.26s/it]\u001b[A\n","934it [07:12,  1.05it/s]\u001b[A\n","935it [07:12,  1.36it/s]\u001b[A\n","936it [07:12,  1.71it/s]\u001b[A\n","937it [07:12,  2.09it/s]\u001b[A\n","938it [07:13,  2.47it/s]\u001b[A\n","939it [07:13,  2.84it/s]\u001b[A\n","940it [07:13,  3.17it/s]\u001b[A\n","941it [07:13,  3.45it/s]\u001b[A\n","942it [07:14,  3.67it/s]\u001b[A\n","943it [07:14,  3.85it/s]\u001b[A\n","944it [07:14,  3.98it/s]\u001b[A\n","945it [07:14,  4.08it/s]\u001b[A\n","946it [07:14,  4.15it/s]\u001b[A\n","947it [07:15,  4.21it/s]\u001b[A\n","948it [07:15,  4.24it/s]\u001b[A\n","949it [07:15,  4.27it/s]\u001b[A\n","950it [07:15,  4.29it/s]\u001b[A\n","951it [07:16,  4.30it/s]\u001b[A\n","952it [07:16,  4.31it/s]\u001b[A\n","953it [07:16,  4.32it/s]\u001b[A\n","954it [07:16,  4.32it/s]\u001b[A\n","955it [07:17,  4.32it/s]\u001b[A\n","956it [07:17,  4.33it/s]\u001b[A\n","957it [07:17,  4.33it/s]\u001b[A\n","958it [07:17,  4.33it/s]\u001b[A\n","959it [07:17,  4.33it/s]\u001b[A\n","960it [07:18,  4.34it/s]\u001b[A\n","961it [07:18,  4.34it/s]\u001b[A\n","962it [07:18,  4.33it/s]\u001b[A\n","963it [07:18,  4.33it/s]\u001b[A\n","964it [07:19,  4.33it/s]\u001b[A\n","965it [07:19,  4.33it/s]\u001b[A\n","966it [07:19,  4.33it/s]\u001b[A\n","967it [07:19,  4.33it/s]\u001b[A\n","968it [07:20,  4.33it/s]\u001b[A\n","969it [07:20,  4.34it/s]\u001b[A\n","970it [07:20,  4.34it/s]\u001b[A\n","971it [07:20,  4.33it/s]\u001b[A\n","972it [07:20,  4.33it/s]\u001b[A\n","973it [07:21,  4.33it/s]\u001b[A\n","974it [07:21,  4.33it/s]\u001b[A\n","975it [07:21,  4.33it/s]\u001b[A\n","976it [07:21,  4.33it/s]\u001b[A\n","977it [07:22,  4.33it/s]\u001b[A\n","978it [07:22,  4.33it/s]\u001b[A\n","979it [07:22,  4.34it/s]\u001b[A\n","980it [07:22,  4.34it/s]\u001b[A\n","981it [07:23,  4.33it/s]\u001b[A\n","982it [07:23,  4.33it/s]\u001b[A\n","983it [07:23,  4.34it/s]\u001b[A\n","984it [07:23,  4.33it/s]\u001b[A\n","985it [07:23,  4.33it/s]\u001b[A\n","986it [07:24,  4.33it/s]\u001b[A\n","987it [07:24,  4.33it/s]\u001b[A\n","988it [07:24,  4.34it/s]\u001b[A\n","989it [07:24,  4.34it/s]\u001b[A\n","990it [07:25,  4.34it/s]\u001b[A\n","991it [07:25,  4.34it/s]\u001b[A\n","992it [07:25,  4.33it/s]\u001b[A\n","993it [07:25,  4.33it/s]\u001b[A\n","994it [07:26,  4.33it/s]\u001b[A\n","995it [07:26,  4.33it/s]\u001b[A\n","996it [07:26,  4.33it/s]\u001b[A\n","997it [07:26,  4.33it/s]\u001b[A\n","998it [07:26,  4.33it/s]\u001b[A\n","999it [07:27,  4.34it/s]\u001b[A\n","1000it [07:27,  4.34it/s]\u001b[A\n","1001it [07:27,  4.34it/s]\u001b[A\n","1002it [07:27,  4.33it/s]\u001b[A\n","1003it [07:28,  4.33it/s]\u001b[A\n","1004it [07:28,  4.33it/s]\u001b[A\n","1005it [07:28,  4.33it/s]\u001b[A\n","1006it [07:28,  4.34it/s]\u001b[A\n","1007it [07:29,  4.34it/s]\u001b[A\n","1008it [07:29,  4.34it/s]\u001b[A\n","1009it [07:29,  4.34it/s]\u001b[A\n","1010it [07:29,  4.34it/s]\u001b[A\n","1011it [07:29,  4.33it/s]\u001b[A\n","1012it [07:30,  4.33it/s]\u001b[A\n","1013it [07:30,  4.34it/s]\u001b[A\n","1014it [07:30,  4.33it/s]\u001b[A\n","1015it [07:30,  4.33it/s]\u001b[A\n","1016it [07:31,  4.33it/s]\u001b[A\n","1017it [07:31,  4.33it/s]\u001b[A\n","1018it [07:31,  4.33it/s]\u001b[A\n","1019it [07:31,  4.33it/s]\u001b[A\n","1020it [07:32,  4.34it/s]\u001b[A\n","1021it [07:32,  4.33it/s]\u001b[A\n","1022it [07:32,  4.33it/s]\u001b[A\n","1023it [07:32,  4.33it/s]\u001b[A\n","1024it [07:32,  4.33it/s]\u001b[A\n","1025it [07:33,  4.33it/s]\u001b[A\n","1026it [07:33,  4.33it/s]\u001b[A\n","1027it [07:33,  4.33it/s]\u001b[A\n","1028it [07:33,  4.33it/s]\u001b[A\n","1029it [07:34,  4.34it/s]\u001b[A\n","1030it [07:34,  4.34it/s]\u001b[A\n","1031it [07:34,  4.34it/s]\u001b[A\n","1032it [07:34,  4.34it/s]\u001b[A\n","1033it [07:35,  4.34it/s]\u001b[A\n","1034it [07:35,  4.33it/s]\u001b[A\n","1035it [07:35,  4.33it/s]\u001b[A\n","1036it [07:35,  4.33it/s]\u001b[A\n","1037it [07:35,  4.34it/s]\u001b[A\n","1038it [07:36,  4.34it/s]\u001b[A\n","1039it [07:36,  4.34it/s]\u001b[A\n","1040it [07:36,  4.34it/s]\u001b[A\n","1041it [07:36,  4.34it/s]\u001b[A\n","1042it [07:37,  4.33it/s]\u001b[A\n","1043it [07:37,  4.33it/s]\u001b[A\n","1044it [07:37,  4.33it/s]\u001b[A\n","1045it [07:37,  4.33it/s]\u001b[A\n","1046it [07:38,  4.34it/s]\u001b[A\n","1047it [07:38,  4.34it/s]\u001b[A\n","1048it [07:38,  4.34it/s]\u001b[A\n","1049it [07:38,  4.34it/s]\u001b[A\n","1050it [07:38,  4.33it/s]\u001b[A\n","1051it [07:39,  4.33it/s]\u001b[A\n","1052it [07:39,  4.33it/s]\u001b[A\n","1053it [07:39,  4.33it/s]\u001b[A\n","1054it [07:39,  4.33it/s]\u001b[A\n","1055it [07:40,  4.33it/s]\u001b[A\n","1056it [07:40,  4.33it/s]\u001b[A\n","1057it [07:40,  4.33it/s]\u001b[A\n","1058it [07:40,  4.34it/s]\u001b[A\n","1059it [07:41,  4.34it/s]\u001b[A\n","1060it [07:41,  4.34it/s]\u001b[A\n","1061it [07:41,  4.33it/s]\u001b[A\n","1062it [07:41,  4.33it/s]\u001b[A\n","1063it [07:41,  4.34it/s]\u001b[A\n","1064it [07:42,  4.34it/s]\u001b[A\n","1065it [07:42,  4.33it/s]\u001b[A\n","1066it [07:42,  4.33it/s]\u001b[A\n","1067it [07:42,  4.34it/s]\u001b[A\n","1068it [07:43,  4.34it/s]\u001b[A\n","1069it [07:43,  4.34it/s]\u001b[A\n","1070it [07:43,  4.34it/s]\u001b[A\n","1071it [07:43,  4.34it/s]\u001b[A\n","1072it [07:44,  4.34it/s]\u001b[A\n","1073it [07:44,  4.34it/s]\u001b[A\n","1074it [07:44,  4.33it/s]\u001b[A\n","1075it [07:44,  4.33it/s]\u001b[A\n","1076it [07:44,  4.33it/s]\u001b[A\n","1077it [07:45,  4.33it/s]\u001b[A\n","1078it [07:45,  4.33it/s]\u001b[A\n","1079it [07:45,  4.33it/s]\u001b[A\n","1080it [07:45,  4.33it/s]\u001b[A\n","1081it [07:46,  4.33it/s]\u001b[A\n","1082it [07:46,  4.33it/s]\u001b[A\n","1083it [07:46,  4.33it/s]\u001b[A\n","1084it [07:46,  4.33it/s]\u001b[A\n","1085it [07:47,  4.33it/s]\u001b[A\n","1086it [07:47,  4.33it/s]\u001b[A\n","1087it [07:47,  4.33it/s]\u001b[A\n","1088it [07:47,  4.33it/s]\u001b[A\n","1089it [07:47,  4.33it/s]\u001b[A\n","1090it [07:48,  4.33it/s]\u001b[A\n","1091it [07:48,  4.33it/s]\u001b[A\n","1092it [07:48,  4.33it/s]\u001b[A\n","1093it [07:48,  4.33it/s]\u001b[A\n","1094it [07:49,  4.33it/s]\u001b[A\n","1095it [07:49,  4.33it/s]\u001b[A\n","1096it [07:49,  4.33it/s]\u001b[A\n","1097it [07:49,  4.33it/s]\u001b[A\n","1098it [07:50,  4.34it/s]\u001b[A\n","1099it [07:50,  4.34it/s]\u001b[A\n","1100it [07:50,  4.34it/s]\u001b[A\n","1101it [07:50,  4.34it/s]\u001b[A\n","1102it [07:50,  4.34it/s]\u001b[A\n","1103it [07:51,  4.34it/s]\u001b[A\n","1104it [07:51,  4.34it/s]\u001b[A\n","1105it [07:51,  4.33it/s]\u001b[A\n","1106it [07:51,  4.33it/s]\u001b[A\n","1107it [07:52,  4.33it/s]\u001b[A\n","1108it [07:52,  4.33it/s]\u001b[A\n","1109it [07:52,  4.34it/s]\u001b[A\n","Epoch:  70% 174/250 [09:15<02:07,  1.67s/it]\n","1111it [08:34, 12.76s/it]\u001b[A\n","1112it [08:35,  9.00s/it]\u001b[A\n","1113it [08:35,  6.37s/it]\u001b[A\n","1114it [08:35,  4.53s/it]\u001b[A\n","1115it [08:35,  3.24s/it]\u001b[A\n","1116it [08:35,  2.34s/it]\u001b[A\n","1117it [08:36,  1.71s/it]\u001b[A\n","1118it [08:36,  1.26s/it]\u001b[A\n","1119it [08:36,  1.05it/s]\u001b[A\n","1120it [08:36,  1.36it/s]\u001b[A\n","1121it [08:37,  1.71it/s]\u001b[A\n","1122it [08:37,  2.09it/s]\u001b[A\n","1123it [08:37,  2.47it/s]\u001b[A\n","1124it [08:37,  2.84it/s]\u001b[A\n","1125it [08:38,  3.17it/s]\u001b[A\n","1126it [08:38,  3.44it/s]\u001b[A\n","1127it [08:38,  3.67it/s]\u001b[A\n","1128it [08:38,  3.85it/s]\u001b[A\n","1129it [08:38,  3.98it/s]\u001b[A\n","1130it [08:39,  4.09it/s]\u001b[A\n","1131it [08:39,  4.16it/s]\u001b[A\n","1132it [08:39,  4.21it/s]\u001b[A\n","1133it [08:39,  4.25it/s]\u001b[A\n","1134it [08:40,  4.27it/s]\u001b[A\n","1135it [08:40,  4.29it/s]\u001b[A\n","1136it [08:40,  4.30it/s]\u001b[A\n","1137it [08:40,  4.31it/s]\u001b[A\n","1138it [08:41,  4.32it/s]\u001b[A\n","1139it [08:41,  4.32it/s]\u001b[A\n","1140it [08:41,  4.33it/s]\u001b[A\n","1141it [08:41,  4.33it/s]\u001b[A\n","1142it [08:41,  4.33it/s]\u001b[A\n","1143it [08:42,  4.33it/s]\u001b[A\n","1144it [08:42,  4.33it/s]\u001b[A\n","1145it [08:42,  4.33it/s]\u001b[A\n","1146it [08:42,  4.33it/s]\u001b[A\n","1147it [08:43,  4.33it/s]\u001b[A\n","1148it [08:43,  4.33it/s]\u001b[A\n","1149it [08:43,  4.33it/s]\u001b[A\n","1150it [08:43,  4.33it/s]\u001b[A\n","1151it [08:44,  4.34it/s]\u001b[A\n","1152it [08:44,  4.34it/s]\u001b[A\n","1153it [08:44,  4.34it/s]\u001b[A\n","1154it [08:44,  4.34it/s]\u001b[A\n","1155it [08:44,  4.33it/s]\u001b[A\n","1156it [08:45,  4.33it/s]\u001b[A\n","1157it [08:45,  4.33it/s]\u001b[A\n","1158it [08:45,  4.33it/s]\u001b[A\n","1159it [08:45,  4.33it/s]\u001b[A\n","1160it [08:46,  4.33it/s]\u001b[A\n","1161it [08:46,  4.33it/s]\u001b[A\n","1162it [08:46,  4.33it/s]\u001b[A\n","1163it [08:46,  4.33it/s]\u001b[A\n","1164it [08:47,  4.33it/s]\u001b[A\n","1165it [08:47,  4.33it/s]\u001b[A\n","1166it [08:47,  4.33it/s]\u001b[A\n","1167it [08:47,  4.34it/s]\u001b[A\n","1168it [08:47,  4.34it/s]\u001b[A\n","1169it [08:48,  4.34it/s]\u001b[A\n","1170it [08:48,  4.34it/s]\u001b[A\n","1171it [08:48,  4.34it/s]\u001b[A\n","1172it [08:48,  4.34it/s]\u001b[A\n","1173it [08:49,  4.33it/s]\u001b[A\n","1174it [08:49,  4.34it/s]\u001b[A\n","1175it [08:49,  4.33it/s]\u001b[A\n","1176it [08:49,  4.33it/s]\u001b[A\n","1177it [08:50,  4.34it/s]\u001b[A\n","1178it [08:50,  4.34it/s]\u001b[A\n","1179it [08:50,  4.34it/s]\u001b[A\n","1180it [08:50,  4.34it/s]\u001b[A\n","1181it [08:50,  4.34it/s]\u001b[A\n","1182it [08:51,  4.33it/s]\u001b[A\n","1183it [08:51,  4.33it/s]\u001b[A\n","1184it [08:51,  4.33it/s]\u001b[A\n","1185it [08:51,  4.33it/s]\u001b[A\n","1186it [08:52,  4.33it/s]\u001b[A\n","1187it [08:52,  4.33it/s]\u001b[A\n","1188it [08:52,  4.33it/s]\u001b[A\n","1189it [08:52,  4.34it/s]\u001b[A\n","1190it [08:53,  4.34it/s]\u001b[A\n","1191it [08:53,  4.34it/s]\u001b[A\n","1192it [08:53,  4.34it/s]\u001b[A\n","1193it [08:53,  4.33it/s]\u001b[A\n","1194it [08:53,  4.34it/s]\u001b[A\n","1195it [08:54,  4.33it/s]\u001b[A\n","1196it [08:54,  4.33it/s]\u001b[A\n","1197it [08:54,  4.33it/s]\u001b[A\n","1198it [08:54,  4.33it/s]\u001b[A\n","1199it [08:55,  4.34it/s]\u001b[A\n","1200it [08:55,  4.34it/s]\u001b[A\n","1201it [08:55,  4.34it/s]\u001b[A\n","1202it [08:55,  4.34it/s]\u001b[A\n","1203it [08:56,  4.33it/s]\u001b[A\n","1204it [08:56,  4.34it/s]\u001b[A\n","1205it [08:56,  4.33it/s]\u001b[A\n","1206it [08:56,  4.33it/s]\u001b[A\n","1207it [08:56,  4.33it/s]\u001b[A\n","1208it [08:57,  4.34it/s]\u001b[A\n","1209it [08:57,  4.34it/s]\u001b[A\n","1210it [08:57,  4.34it/s]\u001b[A\n","1211it [08:57,  4.34it/s]\u001b[A\n","1212it [08:58,  4.34it/s]\u001b[A\n","1213it [08:58,  4.33it/s]\u001b[A\n","1214it [08:58,  4.34it/s]\u001b[A\n","1215it [08:58,  4.33it/s]\u001b[A\n","1216it [08:59,  4.33it/s]\u001b[A\n","1217it [08:59,  4.34it/s]\u001b[A\n","1218it [08:59,  4.34it/s]\u001b[A\n","1219it [08:59,  4.34it/s]\u001b[A\n","1220it [08:59,  4.34it/s]\u001b[A\n","1221it [09:00,  4.34it/s]\u001b[A\n","1222it [09:00,  4.34it/s]\u001b[A\n","1223it [09:00,  4.34it/s]\u001b[A\n","1224it [09:00,  4.34it/s]\u001b[A\n","1225it [09:01,  4.34it/s]\u001b[A\n","1226it [09:01,  4.33it/s]\u001b[A\n","1227it [09:01,  4.34it/s]\u001b[A\n","1228it [09:01,  4.33it/s]\u001b[A\n","1229it [09:02,  4.34it/s]\u001b[A\n","1230it [09:02,  4.34it/s]\u001b[A\n","1231it [09:02,  4.34it/s]\u001b[A\n","1232it [09:02,  4.34it/s]\u001b[A\n","1233it [09:02,  4.34it/s]\u001b[A\n","1234it [09:03,  4.34it/s]\u001b[A\n","1235it [09:03,  4.33it/s]\u001b[A\n","1236it [09:03,  4.33it/s]\u001b[A\n","1237it [09:03,  4.32it/s]\u001b[A\n","1238it [09:04,  4.33it/s]\u001b[A\n","1239it [09:04,  4.33it/s]\u001b[A\n","1240it [09:04,  4.33it/s]\u001b[A\n","1241it [09:04,  4.33it/s]\u001b[A\n","1242it [09:05,  4.33it/s]\u001b[A\n","1243it [09:05,  4.33it/s]\u001b[A\n","1244it [09:05,  4.33it/s]\u001b[A\n","1245it [09:05,  4.33it/s]\u001b[A\n","1246it [09:05,  4.33it/s]\u001b[A\n","1247it [09:06,  4.34it/s]\u001b[A\n","1248it [09:06,  4.34it/s]\u001b[A\n","1249it [09:06,  4.34it/s]\u001b[A\n","1250it [09:06,  4.34it/s]\u001b[A\n","1251it [09:07,  4.34it/s]\u001b[A\n","1252it [09:07,  4.34it/s]\u001b[A\n","1253it [09:07,  4.34it/s]\u001b[A\n","1254it [09:07,  4.34it/s]\u001b[A\n","1255it [09:08,  4.34it/s]\u001b[A\n","1256it [09:08,  4.34it/s]\u001b[A\n","1257it [09:08,  4.34it/s]\u001b[A\n","1258it [09:08,  4.34it/s]\u001b[A\n","1259it [09:08,  4.34it/s]\u001b[A\n","1260it [09:09,  4.34it/s]\u001b[A\n","1261it [09:09,  4.34it/s]\u001b[A\n","1262it [09:09,  4.34it/s]\u001b[A\n","1263it [09:09,  4.33it/s]\u001b[A\n","1264it [09:10,  4.33it/s]\u001b[A\n","1265it [09:10,  4.33it/s]\u001b[A\n","1266it [09:10,  4.33it/s]\u001b[A\n","1267it [09:10,  4.33it/s]\u001b[A\n","1268it [09:11,  4.33it/s]\u001b[A\n","1269it [09:11,  4.33it/s]\u001b[A\n","1270it [09:11,  4.33it/s]\u001b[A\n","1271it [09:11,  4.34it/s]\u001b[A\n","1272it [09:11,  4.33it/s]\u001b[A\n","1273it [09:12,  4.33it/s]\u001b[A\n","1274it [09:12,  4.33it/s]\u001b[A\n","1275it [09:12,  4.33it/s]\u001b[A\n","1276it [09:12,  4.33it/s]\u001b[A\n","1277it [09:13,  4.33it/s]\u001b[A\n","1278it [09:13,  4.33it/s]\u001b[A\n","1279it [09:13,  4.33it/s]\u001b[A\n","1280it [09:13,  4.33it/s]\u001b[A\n","1281it [09:14,  4.33it/s]\u001b[A\n","1282it [09:14,  4.33it/s]\u001b[A\n","1283it [09:14,  4.33it/s]\u001b[A\n","1284it [09:14,  4.33it/s]\u001b[A\n","1285it [09:14,  4.33it/s]\u001b[A\n","1286it [09:15,  4.33it/s]\u001b[A\n","1287it [09:15,  4.34it/s]\u001b[A\n","1288it [09:15,  4.34it/s]\u001b[A\n","1289it [09:15,  4.34it/s]\u001b[A\n","1290it [09:16,  4.34it/s]\u001b[A\n","1291it [09:16,  4.34it/s]\u001b[A\n","1292it [09:16,  4.34it/s]\u001b[A\n","1293it [09:16,  4.34it/s]\u001b[A\n","1294it [09:17,  4.34it/s]\u001b[A\n","Epoch:  80% 199/250 [10:39<01:25,  1.67s/it]\n","1296it [09:59, 12.77s/it]\u001b[A\n","1297it [09:59,  9.01s/it]\u001b[A\n","1298it [09:59,  6.37s/it]\u001b[A\n","1299it [09:59,  4.53s/it]\u001b[A\n","1300it [10:00,  3.24s/it]\u001b[A\n","1301it [10:00,  2.34s/it]\u001b[A\n","1302it [10:00,  1.71s/it]\u001b[A\n","1303it [10:00,  1.26s/it]\u001b[A\n","1304it [10:01,  1.05it/s]\u001b[A\n","1305it [10:01,  1.36it/s]\u001b[A\n","1306it [10:01,  1.71it/s]\u001b[A\n","1307it [10:01,  2.09it/s]\u001b[A\n","1308it [10:02,  2.47it/s]\u001b[A\n","1309it [10:02,  2.84it/s]\u001b[A\n","1310it [10:02,  3.17it/s]\u001b[A\n","1311it [10:02,  3.45it/s]\u001b[A\n","1312it [10:02,  3.67it/s]\u001b[A\n","1313it [10:03,  3.85it/s]\u001b[A\n","1314it [10:03,  3.98it/s]\u001b[A\n","1315it [10:03,  4.08it/s]\u001b[A\n","1316it [10:03,  4.16it/s]\u001b[A\n","1317it [10:04,  4.21it/s]\u001b[A\n","1318it [10:04,  4.25it/s]\u001b[A\n","1319it [10:04,  4.27it/s]\u001b[A\n","1320it [10:04,  4.29it/s]\u001b[A\n","1321it [10:05,  4.31it/s]\u001b[A\n","1322it [10:05,  4.32it/s]\u001b[A\n","1323it [10:05,  4.32it/s]\u001b[A\n","1324it [10:05,  4.33it/s]\u001b[A\n","1325it [10:05,  4.33it/s]\u001b[A\n","1326it [10:06,  4.33it/s]\u001b[A\n","1327it [10:06,  4.33it/s]\u001b[A\n","1328it [10:06,  4.33it/s]\u001b[A\n","1329it [10:06,  4.33it/s]\u001b[A\n","1330it [10:07,  4.33it/s]\u001b[A\n","1331it [10:07,  4.34it/s]\u001b[A\n","1332it [10:07,  4.34it/s]\u001b[A\n","1333it [10:07,  4.34it/s]\u001b[A\n","1334it [10:08,  4.33it/s]\u001b[A\n","1335it [10:08,  4.34it/s]\u001b[A\n","1336it [10:08,  4.34it/s]\u001b[A\n","1337it [10:08,  4.34it/s]\u001b[A\n","1338it [10:08,  4.34it/s]\u001b[A\n","1339it [10:09,  4.34it/s]\u001b[A\n","1340it [10:09,  4.34it/s]\u001b[A\n","1341it [10:09,  4.34it/s]\u001b[A\n","1342it [10:09,  4.34it/s]\u001b[A\n","1343it [10:10,  4.34it/s]\u001b[A\n","1344it [10:10,  4.34it/s]\u001b[A\n","1345it [10:10,  4.33it/s]\u001b[A\n","1346it [10:10,  4.33it/s]\u001b[A\n","1347it [10:11,  4.33it/s]\u001b[A\n","1348it [10:11,  4.33it/s]\u001b[A\n","1349it [10:11,  4.33it/s]\u001b[A\n","1350it [10:11,  4.33it/s]\u001b[A\n","1351it [10:11,  4.33it/s]\u001b[A\n","1352it [10:12,  4.33it/s]\u001b[A\n","1353it [10:12,  4.33it/s]\u001b[A\n","1354it [10:12,  4.34it/s]\u001b[A\n","1355it [10:12,  4.34it/s]\u001b[A\n","1356it [10:13,  4.33it/s]\u001b[A\n","1357it [10:13,  4.33it/s]\u001b[A\n","1358it [10:13,  4.33it/s]\u001b[A\n","1359it [10:13,  4.34it/s]\u001b[A\n","1360it [10:14,  4.34it/s]\u001b[A\n","1361it [10:14,  4.34it/s]\u001b[A\n","1362it [10:14,  4.34it/s]\u001b[A\n","1363it [10:14,  4.34it/s]\u001b[A\n","1364it [10:14,  4.34it/s]\u001b[A\n","1365it [10:15,  4.33it/s]\u001b[A\n","1366it [10:15,  4.33it/s]\u001b[A\n","1367it [10:15,  4.33it/s]\u001b[A\n","1368it [10:15,  4.33it/s]\u001b[A\n","1369it [10:16,  4.33it/s]\u001b[A\n","1370it [10:16,  4.33it/s]\u001b[A\n","1371it [10:16,  4.33it/s]\u001b[A\n","1372it [10:16,  4.33it/s]\u001b[A\n","1373it [10:17,  4.33it/s]\u001b[A\n","1374it [10:17,  4.33it/s]\u001b[A\n","1375it [10:17,  4.33it/s]\u001b[A\n","1376it [10:17,  4.33it/s]\u001b[A\n","1377it [10:17,  4.33it/s]\u001b[A\n","1378it [10:18,  4.33it/s]\u001b[A\n","1379it [10:18,  4.34it/s]\u001b[A\n","1380it [10:18,  4.34it/s]\u001b[A\n","1381it [10:18,  4.34it/s]\u001b[A\n","1382it [10:19,  4.33it/s]\u001b[A\n","1383it [10:19,  4.33it/s]\u001b[A\n","1384it [10:19,  4.33it/s]\u001b[A\n","1385it [10:19,  4.33it/s]\u001b[A\n","1386it [10:20,  4.33it/s]\u001b[A\n","1387it [10:20,  4.33it/s]\u001b[A\n","1388it [10:20,  4.34it/s]\u001b[A\n","1389it [10:20,  4.34it/s]\u001b[A\n","1390it [10:20,  4.34it/s]\u001b[A\n","1391it [10:21,  4.34it/s]\u001b[A\n","1392it [10:21,  4.34it/s]\u001b[A\n","1393it [10:21,  4.33it/s]\u001b[A\n","1394it [10:21,  4.33it/s]\u001b[A\n","1395it [10:22,  4.33it/s]\u001b[A\n","1396it [10:22,  4.33it/s]\u001b[A\n","1397it [10:22,  4.33it/s]\u001b[A\n","1398it [10:22,  4.33it/s]\u001b[A\n","1399it [10:23,  4.34it/s]\u001b[A\n","1400it [10:23,  4.33it/s]\u001b[A\n","1401it [10:23,  4.33it/s]\u001b[A\n","1402it [10:23,  4.33it/s]\u001b[A\n","1403it [10:23,  4.33it/s]\u001b[A\n","1404it [10:24,  4.33it/s]\u001b[A\n","1405it [10:24,  4.33it/s]\u001b[A\n","1406it [10:24,  4.33it/s]\u001b[A\n","1407it [10:24,  4.33it/s]\u001b[A\n","1408it [10:25,  4.33it/s]\u001b[A\n","1409it [10:25,  4.34it/s]\u001b[A\n","1410it [10:25,  4.33it/s]\u001b[A\n","1411it [10:25,  4.34it/s]\u001b[A\n","1412it [10:26,  4.34it/s]\u001b[A\n","1413it [10:26,  4.33it/s]\u001b[A\n","1414it [10:26,  4.33it/s]\u001b[A\n","1415it [10:26,  4.34it/s]\u001b[A\n","1416it [10:26,  4.34it/s]\u001b[A\n","1417it [10:27,  4.34it/s]\u001b[A\n","1418it [10:27,  4.34it/s]\u001b[A\n","1419it [10:27,  4.34it/s]\u001b[A\n","1420it [10:27,  4.34it/s]\u001b[A\n","1421it [10:28,  4.34it/s]\u001b[A\n","1422it [10:28,  4.34it/s]\u001b[A\n","1423it [10:28,  4.34it/s]\u001b[A\n","1424it [10:28,  4.34it/s]\u001b[A\n","1425it [10:29,  4.34it/s]\u001b[A\n","1426it [10:29,  4.34it/s]\u001b[A\n","1427it [10:29,  4.34it/s]\u001b[A\n","1428it [10:29,  4.33it/s]\u001b[A\n","1429it [10:29,  4.33it/s]\u001b[A\n","1430it [10:30,  4.33it/s]\u001b[A\n","1431it [10:30,  4.33it/s]\u001b[A\n","1432it [10:30,  4.33it/s]\u001b[A\n","1433it [10:30,  4.33it/s]\u001b[A\n","1434it [10:31,  4.33it/s]\u001b[A\n","1435it [10:31,  4.33it/s]\u001b[A\n","1436it [10:31,  4.33it/s]\u001b[A\n","1437it [10:31,  4.33it/s]\u001b[A\n","1438it [10:32,  4.33it/s]\u001b[A\n","1439it [10:32,  4.34it/s]\u001b[A\n","1440it [10:32,  4.34it/s]\u001b[A\n","1441it [10:32,  4.34it/s]\u001b[A\n","1442it [10:32,  4.34it/s]\u001b[A\n","1443it [10:33,  4.33it/s]\u001b[A\n","1444it [10:33,  4.33it/s]\u001b[A\n","1445it [10:33,  4.33it/s]\u001b[A\n","1446it [10:33,  4.33it/s]\u001b[A\n","1447it [10:34,  4.33it/s]\u001b[A\n","1448it [10:34,  4.34it/s]\u001b[A\n","1449it [10:34,  4.34it/s]\u001b[A\n","1450it [10:34,  4.34it/s]\u001b[A\n","1451it [10:35,  4.34it/s]\u001b[A\n","1452it [10:35,  4.34it/s]\u001b[A\n","1453it [10:35,  4.34it/s]\u001b[A\n","1454it [10:35,  4.34it/s]\u001b[A\n","1455it [10:35,  4.34it/s]\u001b[A\n","1456it [10:36,  4.34it/s]\u001b[A\n","1457it [10:36,  4.33it/s]\u001b[A\n","1458it [10:36,  4.34it/s]\u001b[A\n","1459it [10:36,  4.34it/s]\u001b[A\n","1460it [10:37,  4.34it/s]\u001b[A\n","1461it [10:37,  4.34it/s]\u001b[A\n","1462it [10:37,  4.34it/s]\u001b[A\n","1463it [10:37,  4.33it/s]\u001b[A\n","1464it [10:38,  4.33it/s]\u001b[A\n","1465it [10:38,  4.34it/s]\u001b[A\n","1466it [10:38,  4.34it/s]\u001b[A\n","1467it [10:38,  4.34it/s]\u001b[A\n","1468it [10:38,  4.34it/s]\u001b[A\n","1469it [10:39,  4.33it/s]\u001b[A\n","1470it [10:39,  4.34it/s]\u001b[A\n","1471it [10:39,  4.34it/s]\u001b[A\n","1472it [10:39,  4.34it/s]\u001b[A\n","1473it [10:40,  4.34it/s]\u001b[A\n","1474it [10:40,  4.34it/s]\u001b[A\n","1475it [10:40,  4.34it/s]\u001b[A\n","1476it [10:40,  4.33it/s]\u001b[A\n","1477it [10:41,  4.33it/s]\u001b[A\n","1478it [10:41,  4.33it/s]\u001b[A\n","1479it [10:41,  4.33it/s]\u001b[A\n","Epoch:  90% 224/250 [12:04<00:43,  1.68s/it]\n","1481it [11:23, 12.77s/it]\u001b[A\n","1482it [11:23,  9.01s/it]\u001b[A\n","1483it [11:24,  6.37s/it]\u001b[A\n","1484it [11:24,  4.53s/it]\u001b[A\n","1485it [11:24,  3.24s/it]\u001b[A\n","1486it [11:24,  2.34s/it]\u001b[A\n","1487it [11:25,  1.71s/it]\u001b[A\n","1488it [11:25,  1.26s/it]\u001b[A\n","1489it [11:25,  1.05it/s]\u001b[A\n","1490it [11:25,  1.36it/s]\u001b[A\n","1491it [11:26,  1.71it/s]\u001b[A\n","1492it [11:26,  2.09it/s]\u001b[A\n","1493it [11:26,  2.47it/s]\u001b[A\n","1494it [11:26,  2.84it/s]\u001b[A\n","1495it [11:26,  3.17it/s]\u001b[A\n","1496it [11:27,  3.45it/s]\u001b[A\n","1497it [11:27,  3.67it/s]\u001b[A\n","1498it [11:27,  3.85it/s]\u001b[A\n","1499it [11:27,  3.98it/s]\u001b[A\n","1500it [11:28,  4.08it/s]\u001b[A\n","1501it [11:28,  4.15it/s]\u001b[A\n","1502it [11:28,  4.21it/s]\u001b[A\n","1503it [11:28,  4.25it/s]\u001b[A\n","1504it [11:29,  4.27it/s]\u001b[A\n","1505it [11:29,  4.29it/s]\u001b[A\n","1506it [11:29,  4.31it/s]\u001b[A\n","1507it [11:29,  4.32it/s]\u001b[A\n","1508it [11:29,  4.32it/s]\u001b[A\n","1509it [11:30,  4.32it/s]\u001b[A\n","1510it [11:30,  4.32it/s]\u001b[A\n","1511it [11:30,  4.33it/s]\u001b[A\n","1512it [11:30,  4.33it/s]\u001b[A\n","1513it [11:31,  4.33it/s]\u001b[A\n","1514it [11:31,  4.33it/s]\u001b[A\n","1515it [11:31,  4.33it/s]\u001b[A\n","1516it [11:31,  4.33it/s]\u001b[A\n","1517it [11:32,  4.34it/s]\u001b[A\n","1518it [11:32,  4.33it/s]\u001b[A\n","1519it [11:32,  4.34it/s]\u001b[A\n","1520it [11:32,  4.33it/s]\u001b[A\n","1521it [11:32,  4.32it/s]\u001b[A\n","1522it [11:33,  4.32it/s]\u001b[A\n","1523it [11:33,  4.33it/s]\u001b[A\n","1524it [11:33,  4.33it/s]\u001b[A\n","1525it [11:33,  4.33it/s]\u001b[A\n","1526it [11:34,  4.34it/s]\u001b[A\n","1527it [11:34,  4.34it/s]\u001b[A\n","1528it [11:34,  4.34it/s]\u001b[A\n","1529it [11:34,  4.34it/s]\u001b[A\n","1530it [11:35,  4.33it/s]\u001b[A\n","1531it [11:35,  4.33it/s]\u001b[A\n","1532it [11:35,  4.33it/s]\u001b[A\n","1533it [11:35,  4.33it/s]\u001b[A\n","1534it [11:35,  4.34it/s]\u001b[A\n","1535it [11:36,  4.34it/s]\u001b[A\n","1536it [11:36,  4.34it/s]\u001b[A\n","1537it [11:36,  4.34it/s]\u001b[A\n","1538it [11:36,  4.33it/s]\u001b[A\n","1539it [11:37,  4.34it/s]\u001b[A\n","1540it [11:37,  4.33it/s]\u001b[A\n","1541it [11:37,  4.33it/s]\u001b[A\n","1542it [11:37,  4.33it/s]\u001b[A\n","1543it [11:38,  4.33it/s]\u001b[A\n","1544it [11:38,  4.33it/s]\u001b[A\n","1545it [11:38,  4.33it/s]\u001b[A\n","1546it [11:38,  4.33it/s]\u001b[A\n","1547it [11:38,  4.33it/s]\u001b[A\n","1548it [11:39,  4.33it/s]\u001b[A\n","1549it [11:39,  4.33it/s]\u001b[A\n","1550it [11:39,  4.33it/s]\u001b[A\n","1551it [11:39,  4.33it/s]\u001b[A\n","1552it [11:40,  4.33it/s]\u001b[A\n","1553it [11:40,  4.33it/s]\u001b[A\n","1554it [11:40,  4.34it/s]\u001b[A\n","1555it [11:40,  4.34it/s]\u001b[A\n","1556it [11:41,  4.34it/s]\u001b[A\n","1557it [11:41,  4.34it/s]\u001b[A\n","1558it [11:41,  4.34it/s]\u001b[A\n","1559it [11:41,  4.34it/s]\u001b[A\n","1560it [11:41,  4.33it/s]\u001b[A\n","1561it [11:42,  4.33it/s]\u001b[A\n","1562it [11:42,  4.34it/s]\u001b[A\n","1563it [11:42,  4.34it/s]\u001b[A\n","1564it [11:42,  4.34it/s]\u001b[A\n","1565it [11:43,  4.34it/s]\u001b[A\n","1566it [11:43,  4.34it/s]\u001b[A\n","1567it [11:43,  4.34it/s]\u001b[A\n","1568it [11:43,  4.34it/s]\u001b[A\n","1569it [11:44,  4.34it/s]\u001b[A\n","1570it [11:44,  4.33it/s]\u001b[A\n","1571it [11:44,  4.33it/s]\u001b[A\n","1572it [11:44,  4.34it/s]\u001b[A\n","1573it [11:44,  4.34it/s]\u001b[A\n","1574it [11:45,  4.34it/s]\u001b[A\n","1575it [11:45,  4.34it/s]\u001b[A\n","1576it [11:45,  4.34it/s]\u001b[A\n","1577it [11:45,  4.34it/s]\u001b[A\n","1578it [11:46,  4.34it/s]\u001b[A\n","1579it [11:46,  4.34it/s]\u001b[A\n","1580it [11:46,  4.33it/s]\u001b[A\n","1581it [11:46,  4.33it/s]\u001b[A\n","1582it [11:47,  4.33it/s]\u001b[A\n","1583it [11:47,  4.33it/s]\u001b[A\n","1584it [11:47,  4.34it/s]\u001b[A\n","1585it [11:47,  4.34it/s]\u001b[A\n","1586it [11:47,  4.34it/s]\u001b[A\n","1587it [11:48,  4.33it/s]\u001b[A\n","1588it [11:48,  4.33it/s]\u001b[A\n","1589it [11:48,  4.34it/s]\u001b[A\n","1590it [11:48,  4.33it/s]\u001b[A\n","1591it [11:49,  4.33it/s]\u001b[A\n","1592it [11:49,  4.34it/s]\u001b[A\n","1593it [11:49,  4.33it/s]\u001b[A\n","1594it [11:49,  4.34it/s]\u001b[A\n","1595it [11:50,  4.34it/s]\u001b[A\n","1596it [11:50,  4.34it/s]\u001b[A\n","1597it [11:50,  4.34it/s]\u001b[A\n","1598it [11:50,  4.34it/s]\u001b[A\n","1599it [11:50,  4.34it/s]\u001b[A\n","1600it [11:51,  4.33it/s]\u001b[A\n","1601it [11:51,  4.33it/s]\u001b[A\n","1602it [11:51,  4.33it/s]\u001b[A\n","1603it [11:51,  4.34it/s]\u001b[A\n","1604it [11:52,  4.34it/s]\u001b[A\n","1605it [11:52,  4.34it/s]\u001b[A\n","1606it [11:52,  4.34it/s]\u001b[A\n","1607it [11:52,  4.34it/s]\u001b[A\n","1608it [11:53,  4.33it/s]\u001b[A\n","1609it [11:53,  4.34it/s]\u001b[A\n","1610it [11:53,  4.34it/s]\u001b[A\n","1611it [11:53,  4.33it/s]\u001b[A\n","1612it [11:53,  4.34it/s]\u001b[A\n","1613it [11:54,  4.34it/s]\u001b[A\n","1614it [11:54,  4.34it/s]\u001b[A\n","1615it [11:54,  4.34it/s]\u001b[A\n","1616it [11:54,  4.33it/s]\u001b[A\n","1617it [11:55,  4.33it/s]\u001b[A\n","1618it [11:55,  4.33it/s]\u001b[A\n","1619it [11:55,  4.33it/s]\u001b[A\n","1620it [11:55,  4.33it/s]\u001b[A\n","1621it [11:56,  4.33it/s]\u001b[A\n","1622it [11:56,  4.33it/s]\u001b[A\n","1623it [11:56,  4.33it/s]\u001b[A\n","1624it [11:56,  4.33it/s]\u001b[A\n","1625it [11:56,  4.33it/s]\u001b[A\n","1626it [11:57,  4.34it/s]\u001b[A\n","1627it [11:57,  4.34it/s]\u001b[A\n","1628it [11:57,  4.34it/s]\u001b[A\n","1629it [11:57,  4.34it/s]\u001b[A\n","1630it [11:58,  4.33it/s]\u001b[A\n","1631it [11:58,  4.33it/s]\u001b[A\n","1632it [11:58,  4.33it/s]\u001b[A\n","1633it [11:58,  4.34it/s]\u001b[A\n","1634it [11:59,  4.34it/s]\u001b[A\n","1635it [11:59,  4.34it/s]\u001b[A\n","1636it [11:59,  4.34it/s]\u001b[A\n","1637it [11:59,  4.34it/s]\u001b[A\n","1638it [11:59,  4.34it/s]\u001b[A\n","1639it [12:00,  4.34it/s]\u001b[A\n","1640it [12:00,  4.34it/s]\u001b[A\n","1641it [12:00,  4.33it/s]\u001b[A\n","1642it [12:00,  4.33it/s]\u001b[A\n","1643it [12:01,  4.34it/s]\u001b[A\n","1644it [12:01,  4.34it/s]\u001b[A\n","1645it [12:01,  4.34it/s]\u001b[A\n","1646it [12:01,  4.34it/s]\u001b[A\n","1647it [12:02,  4.34it/s]\u001b[A\n","1648it [12:02,  4.33it/s]\u001b[A\n","1649it [12:02,  4.33it/s]\u001b[A\n","1650it [12:02,  4.33it/s]\u001b[A\n","1651it [12:02,  4.33it/s]\u001b[A\n","1652it [12:03,  4.33it/s]\u001b[A\n","1653it [12:03,  4.33it/s]\u001b[A\n","1654it [12:03,  4.33it/s]\u001b[A\n","1655it [12:03,  4.33it/s]\u001b[A\n","1656it [12:04,  4.34it/s]\u001b[A\n","1657it [12:04,  4.34it/s]\u001b[A\n","1658it [12:04,  4.34it/s]\u001b[A\n","1659it [12:04,  4.34it/s]\u001b[A\n","1660it [12:05,  4.34it/s]\u001b[A\n","1661it [12:05,  4.33it/s]\u001b[A\n","1662it [12:05,  4.34it/s]\u001b[A\n","1663it [12:05,  4.34it/s]\u001b[A\n","1664it [12:05,  4.34it/s]\u001b[A\n","Epoch: 100% 249/250 [13:28<00:01,  1.67s/it]\n","1666it [12:48, 12.76s/it]\u001b[A\n","1667it [12:48,  9.00s/it]\u001b[A\n","1668it [12:48,  6.37s/it]\u001b[A\n","1669it [12:48,  4.53s/it]\u001b[A\n","1670it [12:49,  3.24s/it]\u001b[A\n","1671it [12:49,  2.34s/it]\u001b[A\n","1672it [12:49,  1.71s/it]\u001b[A\n","1673it [12:49,  1.26s/it]\u001b[A\n","1674it [12:50,  1.05it/s]\u001b[A\n","1675it [12:50,  1.36it/s]\u001b[A\n","1676it [12:50,  1.71it/s]\u001b[A\n","1677it [12:50,  2.09it/s]\u001b[A\n","1678it [12:50,  2.47it/s]\u001b[A\n","1679it [12:51,  2.84it/s]\u001b[A\n","1680it [12:51,  3.17it/s]\u001b[A\n","1681it [12:51,  3.45it/s]\u001b[A\n","1682it [12:51,  3.67it/s]\u001b[A\n","1683it [12:52,  3.85it/s]\u001b[A\n","1684it [12:52,  3.98it/s]\u001b[A\n","1685it [12:52,  4.08it/s]\u001b[A\n","1686it [12:52,  4.15it/s]\u001b[A\n","1687it [12:53,  4.21it/s]\u001b[A\n","1688it [12:53,  4.25it/s]\u001b[A\n","1689it [12:53,  4.27it/s]\u001b[A\n","1690it [12:53,  4.29it/s]\u001b[A\n","1691it [12:53,  4.30it/s]\u001b[A\n","1692it [12:54,  4.31it/s]\u001b[A\n","1693it [12:54,  4.32it/s]\u001b[A\n","1694it [12:54,  4.33it/s]\u001b[A\n","1695it [12:54,  4.33it/s]\u001b[A\n","1696it [12:55,  4.33it/s]\u001b[A\n","1697it [12:55,  4.33it/s]\u001b[A\n","1698it [12:55,  4.33it/s]\u001b[A\n","1699it [12:55,  4.33it/s]\u001b[A\n","1700it [12:56,  4.33it/s]\u001b[A\n","1701it [12:56,  4.33it/s]\u001b[A\n","1702it [12:56,  4.34it/s]\u001b[A\n","1703it [12:56,  4.34it/s]\u001b[A\n","1704it [12:56,  4.34it/s]\u001b[A\n","1705it [12:57,  4.34it/s]\u001b[A\n","1706it [12:57,  4.34it/s]\u001b[A\n","1707it [12:57,  4.34it/s]\u001b[A\n","1708it [12:57,  4.33it/s]\u001b[A\n","1709it [12:58,  4.33it/s]\u001b[A\n","1710it [12:58,  4.33it/s]\u001b[A\n","1711it [12:58,  4.33it/s]\u001b[A\n","1712it [12:58,  4.33it/s]\u001b[A\n","1713it [12:59,  4.34it/s]\u001b[A\n","1714it [12:59,  4.34it/s]\u001b[A\n","1715it [12:59,  4.34it/s]\u001b[A\n","1716it [12:59,  4.33it/s]\u001b[A\n","1717it [12:59,  4.33it/s]\u001b[A\n","1718it [13:00,  4.33it/s]\u001b[A\n","1719it [13:00,  4.33it/s]\u001b[A\n","1720it [13:00,  4.33it/s]\u001b[A\n","1721it [13:00,  4.33it/s]\u001b[A\n","1722it [13:01,  4.33it/s]\u001b[A\n","1723it [13:01,  4.33it/s]\u001b[A\n","1724it [13:01,  4.33it/s]\u001b[A\n","1725it [13:01,  4.33it/s]\u001b[A\n","1726it [13:02,  4.33it/s]\u001b[A\n","1727it [13:02,  4.33it/s]\u001b[A\n","1728it [13:02,  4.33it/s]\u001b[A\n","1729it [13:02,  4.33it/s]\u001b[A\n","1730it [13:02,  4.34it/s]\u001b[A\n","1731it [13:03,  4.34it/s]\u001b[A\n","1732it [13:03,  4.33it/s]\u001b[A\n","1733it [13:03,  4.34it/s]\u001b[A\n","1734it [13:03,  4.34it/s]\u001b[A\n","1735it [13:04,  4.33it/s]\u001b[A\n","1736it [13:04,  4.33it/s]\u001b[A\n","1737it [13:04,  4.33it/s]\u001b[A\n","1738it [13:04,  4.33it/s]\u001b[A\n","1739it [13:05,  4.33it/s]\u001b[A\n","1740it [13:05,  4.33it/s]\u001b[A\n","1741it [13:05,  4.33it/s]\u001b[A\n","1742it [13:05,  4.33it/s]\u001b[A\n","1743it [13:05,  4.33it/s]\u001b[A\n","1744it [13:06,  4.34it/s]\u001b[A\n","1745it [13:06,  4.34it/s]\u001b[A\n","1746it [13:06,  4.34it/s]\u001b[A\n","1747it [13:06,  4.34it/s]\u001b[A\n","1748it [13:07,  4.33it/s]\u001b[A\n","1749it [13:07,  4.33it/s]\u001b[A\n","1750it [13:07,  4.33it/s]\u001b[A\n","1751it [13:07,  4.34it/s]\u001b[A\n","1752it [13:08,  4.34it/s]\u001b[A\n","1753it [13:08,  4.34it/s]\u001b[A\n","1754it [13:08,  4.34it/s]\u001b[A\n","1755it [13:08,  4.34it/s]\u001b[A\n","1756it [13:08,  4.33it/s]\u001b[A\n","1757it [13:09,  4.33it/s]\u001b[A\n","1758it [13:09,  4.33it/s]\u001b[A\n","1759it [13:09,  4.33it/s]\u001b[A\n","1760it [13:09,  4.33it/s]\u001b[A\n","1761it [13:10,  4.33it/s]\u001b[A\n","1762it [13:10,  4.33it/s]\u001b[A\n","1763it [13:10,  4.33it/s]\u001b[A\n","1764it [13:10,  4.33it/s]\u001b[A\n","1765it [13:11,  4.33it/s]\u001b[A\n","1766it [13:11,  4.33it/s]\u001b[A\n","1767it [13:11,  4.33it/s]\u001b[A\n","1768it [13:11,  4.33it/s]\u001b[A\n","1769it [13:11,  4.33it/s]\u001b[A\n","1770it [13:12,  4.33it/s]\u001b[A\n","1771it [13:12,  4.33it/s]\u001b[A\n","1772it [13:12,  4.33it/s]\u001b[A\n","1773it [13:12,  4.33it/s]\u001b[A\n","1774it [13:13,  4.33it/s]\u001b[A\n","1775it [13:13,  4.34it/s]\u001b[A\n","1776it [13:13,  4.34it/s]\u001b[A\n","1777it [13:13,  4.34it/s]\u001b[A\n","1778it [13:14,  4.33it/s]\u001b[A\n","1779it [13:14,  4.33it/s]\u001b[A\n","1780it [13:14,  4.33it/s]\u001b[A\n","1781it [13:14,  4.33it/s]\u001b[A\n","1782it [13:14,  4.33it/s]\u001b[A\n","1783it [13:15,  4.34it/s]\u001b[A\n","1784it [13:15,  4.33it/s]\u001b[A\n","1785it [13:15,  4.33it/s]\u001b[A\n","1786it [13:15,  4.33it/s]\u001b[A\n","1787it [13:16,  4.33it/s]\u001b[A\n","1788it [13:16,  4.33it/s]\u001b[A\n","1789it [13:16,  4.33it/s]\u001b[A\n","1790it [13:16,  4.34it/s]\u001b[A\n","1791it [13:17,  4.33it/s]\u001b[A\n","1792it [13:17,  4.33it/s]\u001b[A\n","1793it [13:17,  4.33it/s]\u001b[A\n","1794it [13:17,  4.33it/s]\u001b[A\n","1795it [13:17,  4.34it/s]\u001b[A\n","1796it [13:18,  4.34it/s]\u001b[A\n","1797it [13:18,  4.34it/s]\u001b[A\n","1798it [13:18,  4.34it/s]\u001b[A\n","1799it [13:18,  4.34it/s]\u001b[A\n","1800it [13:19,  4.34it/s]\u001b[A\n","1801it [13:19,  4.33it/s]\u001b[A\n","1802it [13:19,  4.33it/s]\u001b[A\n","1803it [13:19,  4.33it/s]\u001b[A\n","1804it [13:20,  4.33it/s]\u001b[A\n","1805it [13:20,  4.33it/s]\u001b[A\n","1806it [13:20,  4.33it/s]\u001b[A\n","1807it [13:20,  4.34it/s]\u001b[A\n","1808it [13:20,  4.33it/s]\u001b[A\n","1809it [13:21,  4.33it/s]\u001b[A\n","1810it [13:21,  4.33it/s]\u001b[A\n","1811it [13:21,  4.33it/s]\u001b[A\n","1812it [13:21,  4.33it/s]\u001b[A\n","1813it [13:22,  4.33it/s]\u001b[A\n","1814it [13:22,  4.33it/s]\u001b[A\n","1815it [13:22,  4.34it/s]\u001b[A\n","1816it [13:22,  4.34it/s]\u001b[A\n","1817it [13:23,  4.34it/s]\u001b[A\n","1818it [13:23,  4.33it/s]\u001b[A\n","1819it [13:23,  4.33it/s]\u001b[A\n","1820it [13:23,  4.34it/s]\u001b[A\n","1821it [13:23,  4.34it/s]\u001b[A\n","1822it [13:24,  4.33it/s]\u001b[A\n","1823it [13:24,  4.34it/s]\u001b[A\n","1824it [13:24,  4.34it/s]\u001b[A\n","1825it [13:24,  4.34it/s]\u001b[A\n","1826it [13:25,  4.34it/s]\u001b[A\n","1827it [13:25,  4.34it/s]\u001b[A\n","1828it [13:25,  4.34it/s]\u001b[A\n","1829it [13:25,  4.34it/s]\u001b[A\n","1830it [13:26,  4.34it/s]\u001b[A\n","1831it [13:26,  4.33it/s]\u001b[A\n","1832it [13:26,  4.33it/s]\u001b[A\n","1833it [13:26,  4.33it/s]\u001b[A\n","1834it [13:26,  4.33it/s]\u001b[A\n","1835it [13:27,  4.33it/s]\u001b[A\n","1836it [13:27,  4.33it/s]\u001b[A\n","1837it [13:27,  4.34it/s]\u001b[A\n","1838it [13:27,  4.34it/s]\u001b[A\n","1839it [13:28,  4.34it/s]\u001b[A\n","1840it [13:28,  4.34it/s]\u001b[A\n","1841it [13:28,  4.33it/s]\u001b[A\n","1842it [13:28,  4.33it/s]\u001b[A\n","1843it [13:29,  4.33it/s]\u001b[A\n","1844it [13:29,  4.33it/s]\u001b[A\n","1845it [13:29,  4.33it/s]\u001b[A\n","1846it [13:29,  4.34it/s]\u001b[A\n","1847it [13:29,  4.34it/s]\u001b[A\n","1848it [13:30,  4.34it/s]\u001b[A\n","1849it [13:30,  4.33it/s]\u001b[A\n","1850it [13:30,  4.33it/s]\u001b[A03/03/2022 05:04:05 - INFO - src.trainer -   Best dev result: 0.8828749656677246\n","Epoch: 100% 250/250 [14:19<00:00,  3.44s/it]\n","03/03/2022 05:04:11 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/03/2022 05:04:26 - INFO - __main__ -   *** Validate ***\n","\n","1851it [13:52,  6.66s/it]\u001b[A\n","1852it [13:52,  4.73s/it]\u001b[A\n","1853it [13:52,  3.38s/it]\u001b[A\n","1854it [13:53,  2.44s/it]\u001b[A\n","1855it [13:53,  1.78s/it]\u001b[A\n","1856it [13:53,  1.31s/it]\u001b[A\n","1857it [13:53,  1.01it/s]\u001b[A\n","1858it [13:53,  1.31it/s]\u001b[A\n","1859it [13:54,  1.66it/s]\u001b[A\n","1860it [13:54,  2.04it/s]\u001b[A\n","1861it [13:54,  2.42it/s]\u001b[A\n","1862it [13:54,  2.79it/s]\u001b[A\n","1863it [13:55,  3.12it/s]\u001b[A\n","1864it [13:55,  3.41it/s]\u001b[A\n","1865it [13:55,  3.63it/s]\u001b[A\n","1866it [13:55,  3.81it/s]\u001b[A\n","1867it [13:56,  3.95it/s]\u001b[A\n","1868it [13:56,  4.06it/s]\u001b[A\n","1869it [13:56,  4.13it/s]\u001b[A\n","1870it [13:56,  4.19it/s]\u001b[A\n","1871it [13:56,  4.23it/s]\u001b[A\n","1872it [13:57,  4.25it/s]\u001b[A\n","1873it [13:57,  4.27it/s]\u001b[A\n","1874it [13:57,  4.28it/s]\u001b[A\n","1875it [13:57,  4.29it/s]\u001b[A\n","1876it [13:58,  4.29it/s]\u001b[A\n","1877it [13:58,  4.29it/s]\u001b[A\n","1878it [13:58,  4.30it/s]\u001b[A\n","1879it [13:58,  4.30it/s]\u001b[A\n","1880it [13:59,  4.30it/s]\u001b[A\n","1881it [13:59,  4.29it/s]\u001b[A\n","1882it [13:59,  4.29it/s]\u001b[A\n","1883it [13:59,  4.29it/s]\u001b[A\n","1884it [13:59,  4.30it/s]\u001b[A\n","1885it [14:00,  4.30it/s]\u001b[A\n","1886it [14:00,  4.31it/s]\u001b[A\n","1887it [14:00,  4.29it/s]\u001b[A\n","1888it [14:00,  4.29it/s]\u001b[A\n","1889it [14:01,  4.29it/s]\u001b[A\n","1890it [14:01,  4.30it/s]\u001b[A\n","1891it [14:01,  4.30it/s]\u001b[A\n","1892it [14:01,  4.30it/s]\u001b[A\n","1893it [14:02,  4.31it/s]\u001b[A\n","1894it [14:02,  4.30it/s]\u001b[A\n","1895it [14:02,  4.31it/s]\u001b[A\n","1896it [14:02,  4.31it/s]\u001b[A\n","1897it [14:02,  4.31it/s]\u001b[A\n","1898it [14:03,  4.32it/s]\u001b[A\n","1899it [14:03,  4.32it/s]\u001b[A\n","1900it [14:03,  4.32it/s]\u001b[A\n","1901it [14:03,  4.33it/s]\u001b[A\n","1902it [14:04,  4.33it/s]\u001b[A\n","1903it [14:04,  4.33it/s]\u001b[A\n","1904it [14:04,  4.33it/s]\u001b[A\n","1905it [14:04,  4.33it/s]\u001b[A\n","1906it [14:05,  4.33it/s]\u001b[A\n","1907it [14:05,  4.33it/s]\u001b[A\n","1908it [14:05,  4.33it/s]\u001b[A\n","1909it [14:05,  4.33it/s]\u001b[A\n","1910it [14:06,  4.31it/s]\u001b[A\n","1911it [14:06,  4.28it/s]\u001b[A\n","1912it [14:06,  4.29it/s]\u001b[A\n","1913it [14:06,  4.30it/s]\u001b[A\n","1914it [14:06,  4.30it/s]\u001b[A\n","1915it [14:07,  4.31it/s]\u001b[A\n","1916it [14:07,  4.31it/s]\u001b[A\n","1917it [14:07,  4.32it/s]\u001b[A\n","1918it [14:07,  4.32it/s]\u001b[A\n","1919it [14:08,  4.32it/s]\u001b[A\n","1920it [14:08,  4.32it/s]\u001b[A\n","1921it [14:08,  4.32it/s]\u001b[A\n","1922it [14:08,  4.33it/s]\u001b[A\n","1923it [14:09,  4.32it/s]\u001b[A\n","1924it [14:09,  4.33it/s]\u001b[A\n","1925it [14:09,  4.32it/s]\u001b[A\n","1926it [14:09,  4.32it/s]\u001b[A\n","1927it [14:09,  4.32it/s]\u001b[A\n","1928it [14:10,  4.33it/s]\u001b[A\n","1929it [14:10,  4.32it/s]\u001b[A\n","1930it [14:10,  4.32it/s]\u001b[A\n","1931it [14:10,  4.32it/s]\u001b[A\n","1932it [14:11,  4.33it/s]\u001b[A\n","1933it [14:11,  4.33it/s]\u001b[A\n","1934it [14:11,  4.33it/s]\u001b[A\n","1935it [14:11,  4.33it/s]\u001b[A\n","1936it [14:12,  4.33it/s]\u001b[A\n","1937it [14:12,  4.33it/s]\u001b[A\n","1938it [14:12,  4.33it/s]\u001b[A\n","1939it [14:12,  4.33it/s]\u001b[A\n","1940it [14:12,  4.33it/s]\u001b[A\n","1941it [14:13,  4.33it/s]\u001b[A\n","1942it [14:13,  4.33it/s]\u001b[A\n","1943it [14:13,  4.33it/s]\u001b[A\n","1944it [14:13,  4.34it/s]\u001b[A\n","1945it [14:14,  4.34it/s]\u001b[A\n","1946it [14:14,  4.34it/s]\u001b[A\n","1947it [14:14,  4.33it/s]\u001b[A\n","1948it [14:14,  4.34it/s]\u001b[A\n","1949it [14:15,  4.33it/s]\u001b[A\n","1950it [14:15,  4.33it/s]\u001b[A\n","1951it [14:15,  4.33it/s]\u001b[A\n","1952it [14:15,  4.33it/s]\u001b[A\n","1953it [14:15,  4.34it/s]\u001b[A\n","1954it [14:16,  4.34it/s]\u001b[A\n","1955it [14:16,  4.34it/s]\u001b[A\n","1956it [14:16,  4.34it/s]\u001b[A\n","1957it [14:16,  4.34it/s]\u001b[A\n","1958it [14:17,  4.34it/s]\u001b[A\n","1959it [14:17,  4.33it/s]\u001b[A\n","1960it [14:17,  4.33it/s]\u001b[A\n","1961it [14:17,  4.34it/s]\u001b[A\n","1962it [14:18,  4.34it/s]\u001b[A\n","1963it [14:18,  4.34it/s]\u001b[A\n","1964it [14:18,  4.34it/s]\u001b[A\n","1965it [14:18,  4.34it/s]\u001b[A\n","1966it [14:18,  4.34it/s]\u001b[A\n","1967it [14:19,  4.34it/s]\u001b[A\n","1968it [14:19,  4.34it/s]\u001b[A\n","1969it [14:19,  4.34it/s]\u001b[A\n","1970it [14:19,  4.34it/s]\u001b[A\n","1971it [14:20,  4.34it/s]\u001b[A\n","1972it [14:20,  4.34it/s]\u001b[A\n","1973it [14:20,  4.34it/s]\u001b[A\n","1974it [14:20,  4.34it/s]\u001b[A\n","1975it [14:21,  4.33it/s]\u001b[A\n","1976it [14:21,  4.33it/s]\u001b[A\n","1977it [14:21,  4.33it/s]\u001b[A\n","1978it [14:21,  4.34it/s]\u001b[A\n","1979it [14:21,  4.33it/s]\u001b[A\n","1980it [14:22,  4.33it/s]\u001b[A\n","1981it [14:22,  4.33it/s]\u001b[A\n","1982it [14:22,  4.33it/s]\u001b[A\n","1983it [14:22,  4.33it/s]\u001b[A\n","1984it [14:23,  4.33it/s]\u001b[A\n","1985it [14:23,  4.33it/s]\u001b[A\n","1986it [14:23,  4.33it/s]\u001b[A\n","1987it [14:23,  4.33it/s]\u001b[A\n","1988it [14:24,  4.33it/s]\u001b[A\n","1989it [14:24,  4.33it/s]\u001b[A\n","1990it [14:24,  4.33it/s]\u001b[A\n","1991it [14:24,  4.33it/s]\u001b[A\n","1992it [14:24,  4.34it/s]\u001b[A\n","1993it [14:25,  4.34it/s]\u001b[A\n","1994it [14:25,  4.34it/s]\u001b[A\n","1995it [14:25,  4.34it/s]\u001b[A\n","1996it [14:25,  4.34it/s]\u001b[A\n","1997it [14:26,  4.33it/s]\u001b[A\n","1998it [14:26,  4.33it/s]\u001b[A\n","1999it [14:26,  4.33it/s]\u001b[A\n","2000it [14:26,  4.34it/s]\u001b[A\n","2001it [14:27,  4.34it/s]\u001b[A\n","2002it [14:27,  4.34it/s]\u001b[A\n","2003it [14:27,  4.34it/s]\u001b[A\n","2004it [14:27,  4.33it/s]\u001b[A\n","2005it [14:27,  4.34it/s]\u001b[A\n","2006it [14:28,  4.34it/s]\u001b[A\n","2007it [14:28,  4.33it/s]\u001b[A\n","2008it [14:28,  4.34it/s]\u001b[A\n","2009it [14:28,  4.34it/s]\u001b[A\n","2010it [14:29,  4.34it/s]\u001b[A\n","2011it [14:29,  4.34it/s]\u001b[A\n","2012it [14:29,  4.34it/s]\u001b[A\n","2013it [14:29,  4.33it/s]\u001b[A\n","2014it [14:30,  4.33it/s]\u001b[A\n","2015it [14:30,  4.34it/s]\u001b[A\n","2016it [14:30,  4.33it/s]\u001b[A\n","2017it [14:30,  4.33it/s]\u001b[A\n","2018it [14:30,  4.34it/s]\u001b[A\n","2019it [14:31,  4.34it/s]\u001b[A\n","2020it [14:31,  4.34it/s]\u001b[A\n","2021it [14:31,  4.33it/s]\u001b[A\n","2022it [14:31,  4.33it/s]\u001b[A\n","2023it [14:32,  4.34it/s]\u001b[A\n","2024it [14:32,  4.34it/s]\u001b[A\n","2025it [14:32,  4.34it/s]\u001b[A\n","2026it [14:32,  4.34it/s]\u001b[A\n","2027it [14:33,  4.34it/s]\u001b[A\n","2028it [14:33,  4.34it/s]\u001b[A\n","2029it [14:33,  4.34it/s]\u001b[A\n","2030it [14:33,  4.34it/s]\u001b[A\n","2031it [14:33,  4.34it/s]\u001b[A\n","2032it [14:34,  4.34it/s]\u001b[A\n","2033it [14:34,  4.34it/s]\u001b[A\n","2034it [14:34,  4.34it/s]\u001b[A\n","2035it [14:34,  4.34it/s]\u001b[A03/03/2022 05:05:09 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/03/2022 05:05:09 - INFO - __main__ -     eval_loss = 4.30712890625\n","03/03/2022 05:05:09 - INFO - __main__ -     eval_auroc = 0.8828749656677246\n","03/03/2022 05:05:09 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/03/2022 05:05:09 - INFO - __main__ -     eval_f1 = 0.4848484694957733\n","03/03/2022 05:05:09 - INFO - root -   *** Test ***\n","\n","2036it [14:35,  4.27it/s]\u001b[A\n","2037it [14:35,  4.29it/s]\u001b[A\n","2038it [14:35,  4.30it/s]\u001b[A\n","2039it [14:35,  4.31it/s]\u001b[A\n","2040it [14:36,  4.32it/s]\u001b[A\n","2041it [14:36,  4.33it/s]\u001b[A\n","2042it [14:36,  4.33it/s]\u001b[A\n","2043it [14:36,  4.33it/s]\u001b[A\n","2044it [14:36,  4.33it/s]\u001b[A\n","2045it [14:37,  4.33it/s]\u001b[A\n","2046it [14:37,  4.34it/s]\u001b[A\n","2047it [14:37,  4.33it/s]\u001b[A\n","2048it [14:37,  4.33it/s]\u001b[A\n","2049it [14:38,  4.33it/s]\u001b[A\n","2050it [14:38,  4.33it/s]\u001b[A\n","2051it [14:38,  4.34it/s]\u001b[A\n","2052it [14:38,  4.34it/s]\u001b[A\n","2053it [14:39,  4.34it/s]\u001b[A\n","2054it [14:39,  4.34it/s]\u001b[A\n","2055it [14:39,  4.34it/s]\u001b[A\n","2056it [14:39,  4.34it/s]\u001b[A\n","2057it [14:39,  4.33it/s]\u001b[A\n","2058it [14:40,  4.33it/s]\u001b[A\n","2059it [14:40,  4.33it/s]\u001b[A\n","2060it [14:40,  4.33it/s]\u001b[A\n","2061it [14:40,  4.33it/s]\u001b[A\n","2062it [14:41,  4.33it/s]\u001b[A\n","2063it [14:41,  4.33it/s]\u001b[A\n","2064it [14:41,  4.34it/s]\u001b[A\n","2065it [14:41,  4.34it/s]\u001b[A\n","2066it [14:42,  4.34it/s]\u001b[A\n","2067it [14:42,  4.34it/s]\u001b[A\n","2068it [14:42,  4.34it/s]\u001b[A\n","2069it [14:42,  4.33it/s]\u001b[A\n","2070it [14:42,  4.33it/s]\u001b[A\n","2071it [14:43,  4.33it/s]\u001b[A\n","2072it [14:43,  4.34it/s]\u001b[A\n","2073it [14:43,  4.34it/s]\u001b[A\n","2074it [14:43,  4.34it/s]\u001b[A\n","2075it [14:44,  4.34it/s]\u001b[A\n","2076it [14:44,  4.34it/s]\u001b[A\n","2077it [14:44,  4.34it/s]\u001b[A\n","2078it [14:44,  4.34it/s]\u001b[A\n","2079it [14:45,  4.33it/s]\u001b[A\n","2080it [14:45,  4.33it/s]\u001b[A\n","2081it [14:45,  4.33it/s]\u001b[A\n","2082it [14:45,  4.33it/s]\u001b[A\n","2083it [14:45,  4.33it/s]\u001b[A\n","2084it [14:46,  4.33it/s]\u001b[A\n","2085it [14:46,  4.34it/s]\u001b[A\n","2086it [14:46,  4.34it/s]\u001b[A\n","2087it [14:46,  4.34it/s]\u001b[A\n","2088it [14:47,  4.34it/s]\u001b[A\n","2089it [14:47,  4.33it/s]\u001b[A\n","2090it [14:47,  4.33it/s]\u001b[A\n","2091it [14:47,  4.33it/s]\u001b[A\n","2092it [14:48,  4.33it/s]\u001b[A\n","2093it [14:48,  4.33it/s]\u001b[A\n","2094it [14:48,  4.33it/s]\u001b[A\n","2095it [14:48,  4.33it/s]\u001b[A\n","2096it [14:48,  4.33it/s]\u001b[A\n","2097it [14:49,  4.33it/s]\u001b[A\n","2098it [14:49,  4.33it/s]\u001b[A\n","2099it [14:49,  4.33it/s]\u001b[A\n","2100it [14:49,  4.33it/s]\u001b[A\n","2101it [14:50,  4.33it/s]\u001b[A\n","2102it [14:50,  4.33it/s]\u001b[A\n","2103it [14:50,  4.34it/s]\u001b[A\n","2104it [14:50,  4.34it/s]\u001b[A\n","2105it [14:51,  4.34it/s]\u001b[A\n","2106it [14:51,  4.33it/s]\u001b[A\n","2107it [14:51,  4.33it/s]\u001b[A\n","2108it [14:51,  4.33it/s]\u001b[A\n","2109it [14:51,  4.33it/s]\u001b[A\n","2110it [14:52,  4.33it/s]\u001b[A\n","2111it [14:52,  4.33it/s]\u001b[A\n","2112it [14:52,  4.34it/s]\u001b[A\n","2113it [14:52,  4.34it/s]\u001b[A\n","2114it [14:53,  4.34it/s]\u001b[A\n","2115it [14:53,  4.34it/s]\u001b[A\n","2116it [14:53,  4.34it/s]\u001b[A\n","2117it [14:53,  4.34it/s]\u001b[A\n","2118it [14:54,  4.34it/s]\u001b[A\n","2119it [14:54,  4.34it/s]\u001b[A\n","2120it [14:54,  4.34it/s]\u001b[A\n","2121it [14:54,  4.34it/s]\u001b[A\n","2122it [14:54,  4.34it/s]\u001b[A\n","2123it [14:55,  4.34it/s]\u001b[A\n","2124it [14:55,  4.34it/s]\u001b[A\n","2125it [14:55,  4.34it/s]\u001b[A\n","2126it [14:55,  4.34it/s]\u001b[A\n","2127it [14:56,  4.33it/s]\u001b[A\n","2128it [14:56,  4.33it/s]\u001b[A\n","2129it [14:56,  4.33it/s]\u001b[A\n","2130it [14:56,  4.33it/s]\u001b[A\n","2131it [14:57,  4.33it/s]\u001b[A\n","2132it [14:57,  4.33it/s]\u001b[A\n","2133it [14:57,  4.33it/s]\u001b[A\n","2134it [14:57,  4.33it/s]\u001b[A\n","2135it [14:57,  4.34it/s]\u001b[A\n","2136it [14:58,  4.33it/s]\u001b[A\n","2137it [14:58,  4.33it/s]\u001b[A\n","2138it [14:58,  4.34it/s]\u001b[A\n","2139it [14:58,  4.33it/s]\u001b[A\n","2140it [14:59,  4.33it/s]\u001b[A\n","2141it [14:59,  4.34it/s]\u001b[A\n","2142it [14:59,  4.34it/s]\u001b[A\n","2143it [14:59,  4.34it/s]\u001b[A\n","2144it [15:00,  4.34it/s]\u001b[A\n","2145it [15:00,  4.34it/s]\u001b[A\n","2146it [15:00,  4.34it/s]\u001b[A\n","2147it [15:00,  4.33it/s]\u001b[A\n","2148it [15:00,  4.33it/s]\u001b[A\n","2149it [15:01,  4.33it/s]\u001b[A\n","2150it [15:01,  4.33it/s]\u001b[A\n","2151it [15:01,  4.33it/s]\u001b[A\n","2152it [15:01,  4.34it/s]\u001b[A\n","2153it [15:02,  4.34it/s]\u001b[A\n","2154it [15:02,  4.34it/s]\u001b[A\n","2155it [15:02,  4.34it/s]\u001b[A\n","2156it [15:02,  4.34it/s]\u001b[A\n","2157it [15:03,  4.34it/s]\u001b[A\n","2158it [15:03,  4.34it/s]\u001b[A\n","2159it [15:03,  4.34it/s]\u001b[A\n","2160it [15:03,  4.33it/s]\u001b[A\n","2161it [15:03,  4.33it/s]\u001b[A\n","2162it [15:04,  4.34it/s]\u001b[A\n","2163it [15:04,  4.34it/s]\u001b[A\n","2164it [15:04,  4.34it/s]\u001b[A\n","2165it [15:04,  4.34it/s]\u001b[A\n","2166it [15:05,  4.34it/s]\u001b[A\n","2167it [15:05,  4.34it/s]\u001b[A\n","2168it [15:05,  4.34it/s]\u001b[A\n","2169it [15:05,  4.33it/s]\u001b[A\n","2170it [15:06,  4.33it/s]\u001b[A\n","2171it [15:06,  4.33it/s]\u001b[A\n","2172it [15:06,  4.33it/s]\u001b[A\n","2173it [15:06,  4.34it/s]\u001b[A\n","2174it [15:06,  4.34it/s]\u001b[A\n","2175it [15:07,  4.34it/s]\u001b[A\n","2176it [15:07,  4.34it/s]\u001b[A\n","2177it [15:07,  4.33it/s]\u001b[A\n","2178it [15:07,  4.33it/s]\u001b[A\n","2179it [15:08,  4.33it/s]\u001b[A\n","2180it [15:08,  4.33it/s]\u001b[A\n","2181it [15:08,  4.33it/s]\u001b[A\n","2182it [15:08,  4.34it/s]\u001b[A\n","2183it [15:09,  4.34it/s]\u001b[A\n","2184it [15:09,  4.34it/s]\u001b[A\n","2185it [15:09,  4.34it/s]\u001b[A\n","2186it [15:09,  4.34it/s]\u001b[A\n","2187it [15:09,  4.33it/s]\u001b[A\n","2188it [15:10,  4.34it/s]\u001b[A\n","2189it [15:10,  4.33it/s]\u001b[A\n","2190it [15:10,  4.33it/s]\u001b[A\n","2191it [15:10,  4.34it/s]\u001b[A\n","2192it [15:11,  4.34it/s]\u001b[A\n","2193it [15:11,  4.34it/s]\u001b[A\n","2194it [15:11,  4.34it/s]\u001b[A\n","2195it [15:11,  4.34it/s]\u001b[A\n","2196it [15:12,  4.34it/s]\u001b[A\n","2197it [15:12,  4.34it/s]\u001b[A\n","2198it [15:12,  4.33it/s]\u001b[A\n","2199it [15:12,  4.33it/s]\u001b[A\n","2200it [15:12,  4.33it/s]\u001b[A\n","2201it [15:13,  4.33it/s]\u001b[A\n","2202it [15:13,  4.33it/s]\u001b[A\n","2203it [15:13,  4.33it/s]\u001b[A\n","2204it [15:13,  4.33it/s]\u001b[A\n","2205it [15:14,  4.34it/s]\u001b[A\n","2206it [15:14,  4.34it/s]\u001b[A\n","2207it [15:14,  4.34it/s]\u001b[A\n","2208it [15:14,  4.34it/s]\u001b[A\n","2209it [15:15,  4.33it/s]\u001b[A\n","2210it [15:15,  4.33it/s]\u001b[A\n","2211it [15:15,  4.33it/s]\u001b[A\n","2212it [15:15,  4.34it/s]\u001b[A\n","2213it [15:15,  4.34it/s]\u001b[A\n","2214it [15:16,  4.34it/s]\u001b[A\n","2215it [15:16,  4.34it/s]\u001b[A\n","2216it [15:16,  4.34it/s]\u001b[A\n","2217it [15:16,  4.34it/s]\u001b[A\n","2218it [15:17,  4.34it/s]\u001b[A\n","2219it [15:17,  4.34it/s]\u001b[A\n","2220it [15:17,  4.33it/s]\u001b[A\n","2221it [15:17,  4.34it/s]\u001b[A\n","2222it [15:18,  4.34it/s]\u001b[A\n","2223it [15:18,  4.34it/s]\u001b[A\n","2224it [15:18,  4.34it/s]\u001b[A\n","2225it [15:18,  4.34it/s]\u001b[A\n","2226it [15:18,  4.33it/s]\u001b[A\n","2227it [15:19,  4.33it/s]\u001b[A\n","2228it [15:19,  4.33it/s]\u001b[A\n","2229it [15:19,  4.33it/s]\u001b[A\n","2230it [15:19,  4.33it/s]\u001b[A\n","2231it [15:20,  4.34it/s]\u001b[A\n","2232it [15:20,  4.34it/s]\u001b[A\n","2233it [15:20,  4.34it/s]\u001b[A\n","2234it [15:20,  4.34it/s]\u001b[A\n","2235it [15:21,  4.34it/s]\u001b[A\n","2236it [15:21,  4.34it/s]\u001b[A\n","2237it [15:21,  4.33it/s]\u001b[A\n","2238it [15:21,  4.33it/s]\u001b[A\n","2239it [15:21,  4.33it/s]\u001b[A\n","2240it [15:22,  4.33it/s]\u001b[A\n","2241it [15:22,  4.33it/s]\u001b[A\n","2242it [15:22,  4.33it/s]\u001b[A\n","2243it [15:22,  4.33it/s]\u001b[A\n","2244it [15:23,  4.33it/s]\u001b[A\n","2245it [15:23,  4.33it/s]\u001b[A\n","2246it [15:23,  4.33it/s]\u001b[A\n","2247it [15:23,  4.33it/s]\u001b[A\n","2248it [15:24,  4.34it/s]\u001b[A\n","2249it [15:24,  4.33it/s]\u001b[A\n","2250it [15:24,  4.33it/s]\u001b[A\n","2251it [15:24,  4.33it/s]\u001b[A\n","2252it [15:24,  4.33it/s]\u001b[A\n","2253it [15:25,  4.34it/s]\u001b[A\n","2254it [15:25,  4.34it/s]\u001b[A\n","2255it [15:25,  4.34it/s]\u001b[A\n","2256it [15:25,  4.34it/s]\u001b[A\n","2257it [15:26,  4.33it/s]\u001b[A\n","2258it [15:26,  4.34it/s]\u001b[A\n","2259it [15:26,  4.34it/s]\u001b[A\n","2260it [15:26,  4.33it/s]\u001b[A\n","2261it [15:27,  4.34it/s]\u001b[A\n","2262it [15:27,  4.34it/s]\u001b[A\n","2263it [15:27,  4.34it/s]\u001b[A\n","2264it [15:27,  4.34it/s]\u001b[A\n","2265it [15:27,  4.33it/s]\u001b[A\n","2266it [15:28,  4.33it/s]\u001b[A\n","2267it [15:28,  4.33it/s]\u001b[A\n","2268it [15:28,  4.33it/s]\u001b[A\n","2269it [15:28,  4.33it/s]\u001b[A\n","2270it [15:29,  4.33it/s]\u001b[A\n","2271it [15:29,  4.34it/s]\u001b[A\n","2272it [15:29,  4.34it/s]\u001b[A\n","2273it [15:29,  4.33it/s]\u001b[A\n","2274it [15:30,  4.34it/s]\u001b[A\n","2275it [15:30,  4.34it/s]\u001b[A\n","2276it [15:30,  4.34it/s]\u001b[A\n","2277it [15:30,  4.34it/s]\u001b[A\n","2278it [15:30,  4.34it/s]\u001b[A\n","2279it [15:31,  4.33it/s]\u001b[A\n","2280it [15:31,  4.33it/s]\u001b[A\n","2281it [15:31,  4.33it/s]\u001b[A\n","2282it [15:31,  4.33it/s]\u001b[A\n","2283it [15:32,  4.33it/s]\u001b[A\n","2284it [15:32,  4.34it/s]\u001b[A\n","2285it [15:32,  4.34it/s]\u001b[A\n","2286it [15:32,  4.34it/s]\u001b[A\n","2287it [15:33,  4.34it/s]\u001b[A\n","2288it [15:33,  4.34it/s]\u001b[A\n","2289it [15:33,  4.34it/s]\u001b[A\n","2290it [15:33,  4.33it/s]\u001b[A\n","2291it [15:33,  4.33it/s]\u001b[A\n","2292it [15:34,  4.34it/s]\u001b[A\n","2293it [15:34,  4.34it/s]\u001b[A\n","2294it [15:34,  4.34it/s]\u001b[A\n","2295it [15:34,  4.34it/s]\u001b[A\n","2296it [15:35,  4.34it/s]\u001b[A\n","2297it [15:35,  4.34it/s]\u001b[A\n","2298it [15:35,  4.34it/s]\u001b[A\n","2299it [15:35,  4.34it/s]\u001b[A\n","2300it [15:35,  4.33it/s]\u001b[A\n","2301it [15:36,  4.33it/s]\u001b[A\n","2302it [15:36,  4.33it/s]\u001b[A\n","2303it [15:36,  4.34it/s]\u001b[A\n","2304it [15:36,  4.34it/s]\u001b[A\n","2305it [15:37,  4.34it/s]\u001b[A\n","2306it [15:37,  4.34it/s]\u001b[A\n","2307it [15:37,  4.34it/s]\u001b[A\n","2308it [15:37,  4.34it/s]\u001b[A\n","2309it [15:38,  4.33it/s]\u001b[A\n","2310it [15:38,  4.33it/s]\u001b[A\n","2311it [15:38,  4.34it/s]\u001b[A\n","2312it [15:38,  4.34it/s]\u001b[A\n","2313it [15:38,  4.34it/s]\u001b[A\n","2314it [15:39,  4.34it/s]\u001b[A\n","2315it [15:39,  4.34it/s]\u001b[A\n","2316it [15:39,  4.34it/s]\u001b[A\n","2317it [15:39,  4.33it/s]\u001b[A\n","2318it [15:40,  4.33it/s]\u001b[A\n","2319it [15:40,  4.33it/s]\u001b[A\n","2320it [15:40,  4.33it/s]\u001b[A\n","2321it [15:40,  4.33it/s]\u001b[A\n","2322it [15:41,  4.33it/s]\u001b[A\n","2323it [15:41,  4.33it/s]\u001b[A\n","2324it [15:41,  4.33it/s]\u001b[A\n","2325it [15:41,  4.33it/s]\u001b[A\n","2326it [15:41,  4.33it/s]\u001b[A\n","2327it [15:42,  4.33it/s]\u001b[A\n","2328it [15:42,  4.34it/s]\u001b[A\n","2329it [15:42,  4.34it/s]\u001b[A\n","2330it [15:42,  4.33it/s]\u001b[A\n","2331it [15:43,  4.34it/s]\u001b[A\n","2332it [15:43,  4.34it/s]\u001b[A\n","2333it [15:43,  4.34it/s]\u001b[A\n","2334it [15:43,  4.34it/s]\u001b[A\n","2335it [15:44,  4.34it/s]\u001b[A\n","2336it [15:44,  4.34it/s]\u001b[A\n","2337it [15:44,  4.33it/s]\u001b[A\n","2338it [15:44,  4.33it/s]\u001b[A\n","2339it [15:44,  4.34it/s]\u001b[A\n","2340it [15:45,  4.33it/s]\u001b[A\n","2341it [15:45,  4.33it/s]\u001b[A\n","2342it [15:45,  4.33it/s]\u001b[A\n","2343it [15:45,  4.33it/s]\u001b[A\n","2344it [15:46,  4.33it/s]\u001b[A\n","2345it [15:46,  4.34it/s]\u001b[A\n","2346it [15:46,  4.34it/s]\u001b[A\n","2347it [15:46,  4.33it/s]\u001b[A\n","2348it [15:47,  4.34it/s]\u001b[A\n","2349it [15:47,  4.33it/s]\u001b[A\n","2350it [15:47,  4.33it/s]\u001b[A\n","2351it [15:47,  4.34it/s]\u001b[A\n","2352it [15:47,  4.34it/s]\u001b[A\n","2353it [15:48,  4.34it/s]\u001b[A\n","2354it [15:48,  4.34it/s]\u001b[A03/03/2022 05:06:22 - INFO - __main__ -   ***** Test results spoilers *****\n","03/03/2022 05:06:22 - INFO - __main__ -     eval_loss = 3.971660614013672\n","03/03/2022 05:06:22 - INFO - __main__ -     eval_auroc = 0.8785310983657837\n","03/03/2022 05:06:22 - INFO - __main__ -     eval_recall = 1.0\n","03/03/2022 05:06:22 - INFO - __main__ -     eval_f1 = 0.36090224981307983\n","03/03/2022 05:06:22 - INFO - filelock -   Lock 139831407117840 acquired on log.lock\n","03/03/2022 05:06:22 - INFO - filelock -   Lock 139831407117840 released on log.lock\n","2354it [15:48,  2.48it/s]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/03/2022 05:06:29 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/03/2022 05:06:29 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-demo-16-100-roberta-large-14322', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar03_05-06-29_8087200c1e26', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=100, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-demo-16-100-roberta-large-14322', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","03/03/2022 05:06:29 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/03/2022 05:06:29 - INFO - __main__ -   Automatically convert the template to using demonstrations.\n","03/03/2022 05:06:29 - INFO - __main__ -   | *cls**sent_0*._Spoiler?*mask*.*sep+* => *cls**sent_0*._Spoiler?*mask*.*sep+**sent_1*._Spoiler?*label_0*.*sep+**sent_2*._Spoiler?*label_1*.*sep+*\n","03/03/2022 05:06:31 - INFO - src.dataset -   Use demonstrations\n","03/03/2022 05:06:31 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/03/2022 05:06:31 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/03/2022 05:06:31 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/03/2022 05:06:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-100\n","03/03/2022 05:06:31 - INFO - filelock -   Lock 139631861044176 acquired on data/k-shot-10x/spoilers/16-100/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 05:06:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-100/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/03/2022 05:06:31 - INFO - filelock -   Lock 139631861044176 released on data/k-shot-10x/spoilers/16-100/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 05:06:31 - INFO - src.dataset -   Use demonstrations\n","03/03/2022 05:06:31 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/03/2022 05:06:31 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/03/2022 05:06:31 - INFO - src.dataset -   Total num_sample for mode dev: 16\n","03/03/2022 05:06:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-100\n","03/03/2022 05:06:31 - INFO - filelock -   Lock 139631861044176 acquired on data/k-shot-10x/spoilers/16-100/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 05:06:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-100/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/03/2022 05:06:31 - INFO - filelock -   Lock 139631861044176 released on data/k-shot-10x/spoilers/16-100/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 05:06:31 - INFO - src.dataset -   *** Example ***\n","03/03/2022 05:06:31 - INFO - src.dataset -   guid: dev-0\n","03/03/2022 05:06:31 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 1594, 129, 5, 22067, 39996, 19269, 21, 23, 10, 723, 5120, 731, 4, 43005, 116, 50264, 4, 2, 36446, 4, 43005, 116, 440, 4, 2, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 4, 43005, 116, 3216, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[16], label_word_list=None)\n","03/03/2022 05:06:31 - INFO - src.dataset -   text: <s>if only the beast titan animation was at a higher frame rate. Spoiler?<mask>.</s>Wait. Spoiler? No.</s>Guys, why goth mikasa was shown?. Spoiler? Yes.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","03/03/2022 05:06:34 - INFO - src.dataset -   Use demonstrations\n","03/03/2022 05:06:34 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/03/2022 05:06:34 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/03/2022 05:06:34 - INFO - src.dataset -   Total num_sample for mode test: 16\n","03/03/2022 05:06:34 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-100\n","03/03/2022 05:06:34 - INFO - filelock -   Lock 139632077776400 acquired on data/k-shot-10x/spoilers/16-100/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 05:06:34 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-100/cached_test_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/03/2022 05:06:34 - INFO - filelock -   Lock 139632077776400 released on data/k-shot-10x/spoilers/16-100/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/03/2022 05:06:34 - INFO - src.dataset -   *** Example ***\n","03/03/2022 05:06:34 - INFO - src.dataset -   guid: test-0\n","03/03/2022 05:06:34 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 8663, 18, 269, 15867, 154, 123, 4, 43005, 116, 50264, 4, 2, 41010, 8108, 4, 43005, 116, 440, 4, 2, 119, 967, 8810, 14964, 4709, 4691, 18, 385, 12, 4, 43005, 116, 3216, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[10], label_word_list=None)\n","03/03/2022 05:06:34 - INFO - src.dataset -   text: <s>eren's really battering him. Spoiler?<mask>.</s>YESSS. Spoiler? No.</s>mikasa ate armin's d-. Spoiler? Yes.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/03/2022 05:06:56 - INFO - src.trainer -   ***** Running training *****\n","03/03/2022 05:06:56 - INFO - src.trainer -     Num examples = 32\n","03/03/2022 05:06:56 - INFO - src.trainer -     Num Epochs = 250\n","03/03/2022 05:06:56 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/03/2022 05:06:56 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/03/2022 05:06:56 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/03/2022 05:06:56 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:21,  1.69s/it]\n","  0% 0/185 [00:00<?, ?it/s]\u001b[A\n","  1% 2/185 [00:00<00:21,  8.69it/s]\u001b[A\n","  2% 3/185 [00:00<00:27,  6.69it/s]\u001b[A\n","  2% 4/185 [00:00<00:31,  5.75it/s]\u001b[A\n","  3% 5/185 [00:00<00:34,  5.24it/s]\u001b[A\n","  3% 6/185 [00:01<00:36,  4.93it/s]\u001b[A\n","  4% 7/185 [00:01<00:37,  4.73it/s]\u001b[A\n","  4% 8/185 [00:01<00:38,  4.61it/s]\u001b[A\n","  5% 9/185 [00:01<00:38,  4.52it/s]\u001b[A\n","  5% 10/185 [00:02<00:39,  4.47it/s]\u001b[A\n","  6% 11/185 [00:02<00:39,  4.43it/s]\u001b[A\n","  6% 12/185 [00:02<00:39,  4.40it/s]\u001b[A\n","  7% 13/185 [00:02<00:39,  4.38it/s]\u001b[A\n","  8% 14/185 [00:02<00:39,  4.36it/s]\u001b[A\n","  8% 15/185 [00:03<00:39,  4.35it/s]\u001b[A\n","  9% 16/185 [00:03<00:38,  4.35it/s]\u001b[A\n","  9% 17/185 [00:03<00:38,  4.34it/s]\u001b[A\n"," 10% 18/185 [00:03<00:38,  4.34it/s]\u001b[A\n"," 10% 19/185 [00:04<00:38,  4.34it/s]\u001b[A\n"," 11% 20/185 [00:04<00:38,  4.34it/s]\u001b[A\n"," 11% 21/185 [00:04<00:37,  4.34it/s]\u001b[A\n"," 12% 22/185 [00:04<00:37,  4.34it/s]\u001b[A\n"," 12% 23/185 [00:05<00:37,  4.34it/s]\u001b[A\n"," 13% 24/185 [00:05<00:37,  4.34it/s]\u001b[A\n"," 14% 25/185 [00:05<00:36,  4.34it/s]\u001b[A\n"," 14% 26/185 [00:05<00:36,  4.34it/s]\u001b[A\n"," 15% 27/185 [00:05<00:36,  4.34it/s]\u001b[A\n"," 15% 28/185 [00:06<00:36,  4.34it/s]\u001b[A\n"," 16% 29/185 [00:06<00:35,  4.34it/s]\u001b[A\n"," 16% 30/185 [00:06<00:35,  4.34it/s]\u001b[A\n"," 17% 31/185 [00:06<00:35,  4.34it/s]\u001b[A\n"," 17% 32/185 [00:07<00:35,  4.34it/s]\u001b[A\n"," 18% 33/185 [00:07<00:35,  4.34it/s]\u001b[A\n"," 18% 34/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 35/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 36/185 [00:08<00:34,  4.34it/s]\u001b[A\n"," 20% 37/185 [00:08<00:34,  4.33it/s]\u001b[A\n"," 21% 38/185 [00:08<00:33,  4.34it/s]\u001b[A\n"," 21% 39/185 [00:08<00:33,  4.34it/s]\u001b[A\n"," 22% 40/185 [00:08<00:33,  4.34it/s]\u001b[A\n"," 22% 41/185 [00:09<00:33,  4.34it/s]\u001b[A\n"," 23% 42/185 [00:09<00:32,  4.34it/s]\u001b[A\n"," 23% 43/185 [00:09<00:32,  4.34it/s]\u001b[A\n"," 24% 44/185 [00:09<00:32,  4.34it/s]\u001b[A\n"," 24% 45/185 [00:10<00:32,  4.34it/s]\u001b[A\n"," 25% 46/185 [00:10<00:32,  4.34it/s]\u001b[A\n"," 25% 47/185 [00:10<00:31,  4.34it/s]\u001b[A\n"," 26% 48/185 [00:10<00:31,  4.33it/s]\u001b[A\n"," 26% 49/185 [00:11<00:31,  4.34it/s]\u001b[A\n"," 27% 50/185 [00:11<00:31,  4.34it/s]\u001b[A\n"," 28% 51/185 [00:11<00:30,  4.34it/s]\u001b[A\n"," 28% 52/185 [00:11<00:30,  4.34it/s]\u001b[A\n"," 29% 53/185 [00:11<00:30,  4.34it/s]\u001b[A\n"," 29% 54/185 [00:12<00:30,  4.34it/s]\u001b[A\n"," 30% 55/185 [00:12<00:29,  4.34it/s]\u001b[A\n"," 30% 56/185 [00:12<00:29,  4.34it/s]\u001b[A\n"," 31% 57/185 [00:12<00:29,  4.33it/s]\u001b[A\n"," 31% 58/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 59/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 60/185 [00:13<00:28,  4.34it/s]\u001b[A\n"," 33% 61/185 [00:13<00:28,  4.34it/s]\u001b[A\n"," 34% 62/185 [00:14<00:28,  4.34it/s]\u001b[A\n"," 34% 63/185 [00:14<00:28,  4.34it/s]\u001b[A\n"," 35% 64/185 [00:14<00:27,  4.34it/s]\u001b[A\n"," 35% 65/185 [00:14<00:27,  4.34it/s]\u001b[A\n"," 36% 66/185 [00:14<00:27,  4.34it/s]\u001b[A\n"," 36% 67/185 [00:15<00:27,  4.34it/s]\u001b[A\n"," 37% 68/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 37% 69/185 [00:15<00:26,  4.34it/s]\u001b[A\n"," 38% 70/185 [00:15<00:26,  4.34it/s]\u001b[A\n"," 38% 71/185 [00:16<00:26,  4.34it/s]\u001b[A\n"," 39% 72/185 [00:16<00:26,  4.34it/s]\u001b[A\n"," 39% 73/185 [00:16<00:25,  4.34it/s]\u001b[A\n"," 40% 74/185 [00:16<00:25,  4.34it/s]\u001b[A\n"," 41% 75/185 [00:17<00:25,  4.34it/s]\u001b[A\n"," 41% 76/185 [00:17<00:25,  4.34it/s]\u001b[A\n"," 42% 77/185 [00:17<00:24,  4.34it/s]\u001b[A\n"," 42% 78/185 [00:17<00:24,  4.34it/s]\u001b[A\n"," 43% 79/185 [00:17<00:24,  4.34it/s]\u001b[A\n"," 43% 80/185 [00:18<00:24,  4.34it/s]\u001b[A\n"," 44% 81/185 [00:18<00:23,  4.34it/s]\u001b[A\n"," 44% 82/185 [00:18<00:23,  4.34it/s]\u001b[A\n"," 45% 83/185 [00:18<00:23,  4.34it/s]\u001b[A\n"," 45% 84/185 [00:19<00:23,  4.34it/s]\u001b[A\n"," 46% 85/185 [00:19<00:23,  4.34it/s]\u001b[A\n"," 46% 86/185 [00:19<00:22,  4.34it/s]\u001b[A\n"," 47% 87/185 [00:19<00:22,  4.34it/s]\u001b[A\n"," 48% 88/185 [00:20<00:22,  4.33it/s]\u001b[A\n"," 48% 89/185 [00:20<00:22,  4.33it/s]\u001b[A\n"," 49% 90/185 [00:20<00:21,  4.33it/s]\u001b[A\n"," 49% 91/185 [00:20<00:21,  4.33it/s]\u001b[A\n"," 50% 92/185 [00:20<00:21,  4.33it/s]\u001b[A\n"," 50% 93/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 51% 94/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 51% 95/185 [00:21<00:20,  4.33it/s]\u001b[A\n"," 52% 96/185 [00:21<00:20,  4.34it/s]\u001b[A\n"," 52% 97/185 [00:22<00:20,  4.34it/s]\u001b[A\n"," 53% 98/185 [00:22<00:20,  4.34it/s]\u001b[A\n"," 54% 99/185 [00:22<00:19,  4.34it/s]\u001b[A\n"," 54% 100/185 [00:22<00:19,  4.34it/s]\u001b[A\n"," 55% 101/185 [00:23<00:19,  4.34it/s]\u001b[A\n"," 55% 102/185 [00:23<00:19,  4.34it/s]\u001b[A\n"," 56% 103/185 [00:23<00:18,  4.34it/s]\u001b[A\n"," 56% 104/185 [00:23<00:18,  4.34it/s]\u001b[A\n"," 57% 105/185 [00:23<00:18,  4.34it/s]\u001b[A\n"," 57% 106/185 [00:24<00:18,  4.34it/s]\u001b[A\n"," 58% 107/185 [00:24<00:17,  4.34it/s]\u001b[A\n"," 58% 108/185 [00:24<00:17,  4.34it/s]\u001b[A\n"," 59% 109/185 [00:24<00:17,  4.34it/s]\u001b[A\n"," 59% 110/185 [00:25<00:17,  4.34it/s]\u001b[A\n"," 60% 111/185 [00:25<00:17,  4.34it/s]\u001b[A\n"," 61% 112/185 [00:25<00:16,  4.34it/s]\u001b[A\n"," 61% 113/185 [00:25<00:16,  4.34it/s]\u001b[A\n"," 62% 114/185 [00:26<00:16,  4.34it/s]\u001b[A\n"," 62% 115/185 [00:26<00:16,  4.33it/s]\u001b[A\n"," 63% 116/185 [00:26<00:15,  4.34it/s]\u001b[A\n"," 63% 117/185 [00:26<00:15,  4.34it/s]\u001b[A\n"," 64% 118/185 [00:26<00:15,  4.34it/s]\u001b[A\n"," 64% 119/185 [00:27<00:15,  4.34it/s]\u001b[A\n"," 65% 120/185 [00:27<00:14,  4.34it/s]\u001b[A\n"," 65% 121/185 [00:27<00:14,  4.34it/s]\u001b[A\n"," 66% 122/185 [00:27<00:14,  4.34it/s]\u001b[A\n"," 66% 123/185 [00:28<00:14,  4.34it/s]\u001b[A\n"," 67% 124/185 [00:28<00:14,  4.34it/s]\u001b[A\n"," 68% 125/185 [00:28<00:13,  4.34it/s]\u001b[A\n"," 68% 126/185 [00:28<00:13,  4.34it/s]\u001b[A\n"," 69% 127/185 [00:29<00:13,  4.33it/s]\u001b[A\n"," 69% 128/185 [00:29<00:13,  4.33it/s]\u001b[A\n"," 70% 129/185 [00:29<00:12,  4.33it/s]\u001b[A\n"," 70% 130/185 [00:29<00:12,  4.34it/s]\u001b[A\n"," 71% 131/185 [00:29<00:12,  4.33it/s]\u001b[A\n"," 71% 132/185 [00:30<00:12,  4.34it/s]\u001b[A\n"," 72% 133/185 [00:30<00:11,  4.34it/s]\u001b[A\n"," 72% 134/185 [00:30<00:11,  4.34it/s]\u001b[A\n"," 73% 135/185 [00:30<00:11,  4.33it/s]\u001b[A\n"," 74% 136/185 [00:31<00:11,  4.34it/s]\u001b[A\n"," 74% 137/185 [00:31<00:11,  4.34it/s]\u001b[A\n"," 75% 138/185 [00:31<00:10,  4.34it/s]\u001b[A\n"," 75% 139/185 [00:31<00:10,  4.34it/s]\u001b[A\n"," 76% 140/185 [00:32<00:10,  4.34it/s]\u001b[A\n"," 76% 141/185 [00:32<00:10,  4.34it/s]\u001b[A\n"," 77% 142/185 [00:32<00:09,  4.34it/s]\u001b[A\n"," 77% 143/185 [00:32<00:09,  4.34it/s]\u001b[A\n"," 78% 144/185 [00:32<00:09,  4.34it/s]\u001b[A\n"," 78% 145/185 [00:33<00:09,  4.34it/s]\u001b[A\n"," 79% 146/185 [00:33<00:08,  4.34it/s]\u001b[A\n"," 79% 147/185 [00:33<00:08,  4.34it/s]\u001b[A\n"," 80% 148/185 [00:33<00:08,  4.34it/s]\u001b[A\n"," 81% 149/185 [00:34<00:08,  4.34it/s]\u001b[A\n"," 81% 150/185 [00:34<00:08,  4.34it/s]\u001b[A\n"," 82% 151/185 [00:34<00:07,  4.34it/s]\u001b[A\n"," 82% 152/185 [00:34<00:07,  4.34it/s]\u001b[A\n"," 83% 153/185 [00:35<00:07,  4.34it/s]\u001b[A\n"," 83% 154/185 [00:35<00:07,  4.33it/s]\u001b[A\n"," 84% 155/185 [00:35<00:06,  4.33it/s]\u001b[A\n"," 84% 156/185 [00:35<00:06,  4.33it/s]\u001b[A\n"," 85% 157/185 [00:35<00:06,  4.34it/s]\u001b[A\n"," 85% 158/185 [00:36<00:06,  4.34it/s]\u001b[A\n"," 86% 159/185 [00:36<00:05,  4.34it/s]\u001b[A\n"," 86% 160/185 [00:36<00:05,  4.34it/s]\u001b[A\n"," 87% 161/185 [00:36<00:05,  4.34it/s]\u001b[A\n"," 88% 162/185 [00:37<00:05,  4.34it/s]\u001b[A\n"," 88% 163/185 [00:37<00:05,  4.33it/s]\u001b[A\n"," 89% 164/185 [00:37<00:04,  4.33it/s]\u001b[A\n"," 89% 165/185 [00:37<00:04,  4.33it/s]\u001b[A\n"," 90% 166/185 [00:38<00:04,  4.34it/s]\u001b[A\n"," 90% 167/185 [00:38<00:04,  4.34it/s]\u001b[A\n"," 91% 168/185 [00:38<00:03,  4.34it/s]\u001b[A\n"," 91% 169/185 [00:38<00:03,  4.34it/s]\u001b[A\n"," 92% 170/185 [00:38<00:03,  4.34it/s]\u001b[A\n"," 92% 171/185 [00:39<00:03,  4.34it/s]\u001b[A\n"," 93% 172/185 [00:39<00:02,  4.34it/s]\u001b[A\n"," 94% 173/185 [00:39<00:02,  4.34it/s]\u001b[A\n"," 94% 174/185 [00:39<00:02,  4.34it/s]\u001b[A\n"," 95% 175/185 [00:40<00:02,  4.34it/s]\u001b[A\n"," 95% 176/185 [00:40<00:02,  4.34it/s]\u001b[A\n"," 96% 177/185 [00:40<00:01,  4.34it/s]\u001b[A\n"," 96% 178/185 [00:40<00:01,  4.34it/s]\u001b[A\n"," 97% 179/185 [00:41<00:01,  4.34it/s]\u001b[A\n"," 97% 180/185 [00:41<00:01,  4.34it/s]\u001b[A\n"," 98% 181/185 [00:41<00:00,  4.34it/s]\u001b[A\n"," 98% 182/185 [00:41<00:00,  4.34it/s]\u001b[A\n"," 99% 183/185 [00:41<00:00,  4.34it/s]\u001b[A\n"," 99% 184/185 [00:42<00:00,  4.34it/s]\u001b[A\n","100% 185/185 [00:42<00:00,  4.34it/s]\u001b[A03/03/2022 05:08:21 - INFO - src.trainer -   Best dev result: 0.9151250720024109\n","Epoch:  20% 49/250 [02:12<05:39,  1.69s/it]\n","186it [01:32, 15.08s/it]             \u001b[A\n","187it [01:32, 10.63s/it]\u001b[A\n","188it [01:32,  7.51s/it]\u001b[A\n","189it [01:32,  5.32s/it]\u001b[A\n","190it [01:33,  3.80s/it]\u001b[A\n","191it [01:33,  2.73s/it]\u001b[A\n","192it [01:33,  1.98s/it]\u001b[A\n","193it [01:33,  1.45s/it]\u001b[A\n","194it [01:34,  1.09s/it]\u001b[A\n","195it [01:34,  1.21it/s]\u001b[A\n","196it [01:34,  1.54it/s]\u001b[A\n","197it [01:34,  1.91it/s]\u001b[A\n","198it [01:34,  2.29it/s]\u001b[A\n","199it [01:35,  2.67it/s]\u001b[A\n","200it [01:35,  3.02it/s]\u001b[A\n","201it [01:35,  3.32it/s]\u001b[A\n","202it [01:35,  3.57it/s]\u001b[A\n","203it [01:36,  3.77it/s]\u001b[A\n","204it [01:36,  3.93it/s]\u001b[A\n","205it [01:36,  4.04it/s]\u001b[A\n","206it [01:36,  4.12it/s]\u001b[A\n","207it [01:37,  4.18it/s]\u001b[A\n","208it [01:37,  4.23it/s]\u001b[A\n","209it [01:37,  4.26it/s]\u001b[A\n","210it [01:37,  4.28it/s]\u001b[A\n","211it [01:37,  4.30it/s]\u001b[A\n","212it [01:38,  4.31it/s]\u001b[A\n","213it [01:38,  4.32it/s]\u001b[A\n","214it [01:38,  4.33it/s]\u001b[A\n","215it [01:38,  4.33it/s]\u001b[A\n","216it [01:39,  4.33it/s]\u001b[A\n","217it [01:39,  4.33it/s]\u001b[A\n","218it [01:39,  4.33it/s]\u001b[A\n","219it [01:39,  4.33it/s]\u001b[A\n","220it [01:40,  4.33it/s]\u001b[A\n","221it [01:40,  4.34it/s]\u001b[A\n","222it [01:40,  4.34it/s]\u001b[A\n","223it [01:40,  4.34it/s]\u001b[A\n","224it [01:40,  4.34it/s]\u001b[A\n","225it [01:41,  4.34it/s]\u001b[A\n","226it [01:41,  4.34it/s]\u001b[A\n","227it [01:41,  4.34it/s]\u001b[A\n","228it [01:41,  4.33it/s]\u001b[A\n","229it [01:42,  4.33it/s]\u001b[A\n","230it [01:42,  4.33it/s]\u001b[A\n","231it [01:42,  4.33it/s]\u001b[A\n","232it [01:42,  4.33it/s]\u001b[A\n","233it [01:43,  4.33it/s]\u001b[A\n","234it [01:43,  4.34it/s]\u001b[A\n","235it [01:43,  4.34it/s]\u001b[A\n","236it [01:43,  4.33it/s]\u001b[A\n","237it [01:43,  4.33it/s]\u001b[A\n","238it [01:44,  4.34it/s]\u001b[A\n","239it [01:44,  4.34it/s]\u001b[A\n","240it [01:44,  4.34it/s]\u001b[A\n","241it [01:44,  4.34it/s]\u001b[A\n","242it [01:45,  4.34it/s]\u001b[A\n","243it [01:45,  4.34it/s]\u001b[A\n","244it [01:45,  4.34it/s]\u001b[A\n","245it [01:45,  4.33it/s]\u001b[A\n","246it [01:45,  4.34it/s]\u001b[A\n","247it [01:46,  4.34it/s]\u001b[A\n","248it [01:46,  4.34it/s]\u001b[A\n","249it [01:46,  4.34it/s]\u001b[A\n","250it [01:46,  4.34it/s]\u001b[A\n","251it [01:47,  4.34it/s]\u001b[A\n","252it [01:47,  4.34it/s]\u001b[A\n","253it [01:47,  4.33it/s]\u001b[A\n","254it [01:47,  4.34it/s]\u001b[A\n","255it [01:48,  4.33it/s]\u001b[A\n","256it [01:48,  4.33it/s]\u001b[A\n","257it [01:48,  4.34it/s]\u001b[A\n","258it [01:48,  4.34it/s]\u001b[A\n","259it [01:48,  4.34it/s]\u001b[A\n","260it [01:49,  4.34it/s]\u001b[A\n","261it [01:49,  4.34it/s]\u001b[A\n","262it [01:49,  4.34it/s]\u001b[A\n","263it [01:49,  4.34it/s]\u001b[A\n","264it [01:50,  4.34it/s]\u001b[A\n","265it [01:50,  4.34it/s]\u001b[A\n","266it [01:50,  4.33it/s]\u001b[A\n","267it [01:50,  4.33it/s]\u001b[A\n","268it [01:51,  4.34it/s]\u001b[A\n","269it [01:51,  4.34it/s]\u001b[A\n","270it [01:51,  4.34it/s]\u001b[A\n","271it [01:51,  4.34it/s]\u001b[A\n","272it [01:51,  4.34it/s]\u001b[A\n","273it [01:52,  4.34it/s]\u001b[A\n","274it [01:52,  4.33it/s]\u001b[A\n","275it [01:52,  4.33it/s]\u001b[A\n","276it [01:52,  4.33it/s]\u001b[A\n","277it [01:53,  4.33it/s]\u001b[A\n","278it [01:53,  4.34it/s]\u001b[A\n","279it [01:53,  4.34it/s]\u001b[A\n","280it [01:53,  4.34it/s]\u001b[A\n","281it [01:54,  4.34it/s]\u001b[A\n","282it [01:54,  4.33it/s]\u001b[A\n","283it [01:54,  4.33it/s]\u001b[A\n","284it [01:54,  4.33it/s]\u001b[A\n","285it [01:54,  4.33it/s]\u001b[A\n","286it [01:55,  4.33it/s]\u001b[A\n","287it [01:55,  4.34it/s]\u001b[A\n","288it [01:55,  4.34it/s]\u001b[A\n","289it [01:55,  4.34it/s]\u001b[A\n","290it [01:56,  4.34it/s]\u001b[A\n","291it [01:56,  4.34it/s]\u001b[A\n","292it [01:56,  4.34it/s]\u001b[A\n","293it [01:56,  4.34it/s]\u001b[A\n","294it [01:57,  4.34it/s]\u001b[A\n","295it [01:57,  4.34it/s]\u001b[A\n","296it [01:57,  4.34it/s]\u001b[A\n","297it [01:57,  4.34it/s]\u001b[A\n","298it [01:57,  4.34it/s]\u001b[A\n","299it [01:58,  4.34it/s]\u001b[A\n","300it [01:58,  4.34it/s]\u001b[A\n","301it [01:58,  4.34it/s]\u001b[A\n","302it [01:58,  4.34it/s]\u001b[A\n","303it [01:59,  4.34it/s]\u001b[A\n","304it [01:59,  4.34it/s]\u001b[A\n","305it [01:59,  4.33it/s]\u001b[A\n","306it [01:59,  4.33it/s]\u001b[A\n","307it [02:00,  4.33it/s]\u001b[A\n","308it [02:00,  4.33it/s]\u001b[A\n","309it [02:00,  4.33it/s]\u001b[A\n","310it [02:00,  4.33it/s]\u001b[A\n","311it [02:00,  4.33it/s]\u001b[A\n","312it [02:01,  4.33it/s]\u001b[A\n","313it [02:01,  4.33it/s]\u001b[A\n","314it [02:01,  4.34it/s]\u001b[A\n","315it [02:01,  4.33it/s]\u001b[A\n","316it [02:02,  4.33it/s]\u001b[A\n","317it [02:02,  4.34it/s]\u001b[A\n","318it [02:02,  4.34it/s]\u001b[A\n","319it [02:02,  4.34it/s]\u001b[A\n","320it [02:03,  4.34it/s]\u001b[A\n","321it [02:03,  4.34it/s]\u001b[A\n","322it [02:03,  4.34it/s]\u001b[A\n","323it [02:03,  4.34it/s]\u001b[A\n","324it [02:03,  4.34it/s]\u001b[A\n","325it [02:04,  4.34it/s]\u001b[A\n","326it [02:04,  4.34it/s]\u001b[A\n","327it [02:04,  4.34it/s]\u001b[A\n","328it [02:04,  4.34it/s]\u001b[A\n","329it [02:05,  4.34it/s]\u001b[A\n","330it [02:05,  4.34it/s]\u001b[A\n","331it [02:05,  4.34it/s]\u001b[A\n","332it [02:05,  4.34it/s]\u001b[A\n","333it [02:06,  4.33it/s]\u001b[A\n","334it [02:06,  4.34it/s]\u001b[A\n","335it [02:06,  4.33it/s]\u001b[A\n","336it [02:06,  4.33it/s]\u001b[A\n","337it [02:06,  4.34it/s]\u001b[A\n","338it [02:07,  4.33it/s]\u001b[A\n","339it [02:07,  4.33it/s]\u001b[A\n","340it [02:07,  4.33it/s]\u001b[A\n","341it [02:07,  4.33it/s]\u001b[A\n","342it [02:08,  4.33it/s]\u001b[A\n","343it [02:08,  4.33it/s]\u001b[A\n","344it [02:08,  4.33it/s]\u001b[A\n","345it [02:08,  4.33it/s]\u001b[A\n","346it [02:09,  4.33it/s]\u001b[A\n","347it [02:09,  4.33it/s]\u001b[A\n","348it [02:09,  4.33it/s]\u001b[A\n","349it [02:09,  4.34it/s]\u001b[A\n","350it [02:09,  4.34it/s]\u001b[A\n","351it [02:10,  4.34it/s]\u001b[A\n","352it [02:10,  4.34it/s]\u001b[A\n","353it [02:10,  4.33it/s]\u001b[A\n","354it [02:10,  4.34it/s]\u001b[A\n","355it [02:11,  4.33it/s]\u001b[A\n","356it [02:11,  4.33it/s]\u001b[A\n","357it [02:11,  4.34it/s]\u001b[A\n","358it [02:11,  4.34it/s]\u001b[A\n","359it [02:12,  4.34it/s]\u001b[A\n","360it [02:12,  4.34it/s]\u001b[A\n","361it [02:12,  4.34it/s]\u001b[A\n","362it [02:12,  4.34it/s]\u001b[A\n","363it [02:12,  4.34it/s]\u001b[A\n","364it [02:13,  4.34it/s]\u001b[A\n","365it [02:13,  4.34it/s]\u001b[A\n","366it [02:13,  4.34it/s]\u001b[A\n","367it [02:13,  4.34it/s]\u001b[A\n","368it [02:14,  4.34it/s]\u001b[A\n","369it [02:14,  4.34it/s]\u001b[A\n","370it [02:14,  4.34it/s]\u001b[A03/03/2022 05:09:53 - INFO - src.trainer -   Best dev result: 0.9481249451637268\n","Epoch:  30% 74/250 [03:44<04:58,  1.69s/it]\n","371it [03:04, 15.05s/it]\u001b[A\n","372it [03:04, 10.60s/it]\u001b[A\n","373it [03:04,  7.49s/it]\u001b[A\n","374it [03:04,  5.31s/it]\u001b[A\n","375it [03:05,  3.79s/it]\u001b[A\n","376it [03:05,  2.72s/it]\u001b[A\n","377it [03:05,  1.97s/it]\u001b[A\n","378it [03:05,  1.45s/it]\u001b[A\n","379it [03:06,  1.09s/it]\u001b[A\n","380it [03:06,  1.21it/s]\u001b[A\n","381it [03:06,  1.54it/s]\u001b[A\n","382it [03:06,  1.91it/s]\u001b[A\n","383it [03:07,  2.29it/s]\u001b[A\n","384it [03:07,  2.67it/s]\u001b[A\n","385it [03:07,  3.02it/s]\u001b[A\n","386it [03:07,  3.32it/s]\u001b[A\n","387it [03:07,  3.57it/s]\u001b[A\n","388it [03:08,  3.77it/s]\u001b[A\n","389it [03:08,  3.93it/s]\u001b[A\n","390it [03:08,  4.04it/s]\u001b[A\n","391it [03:08,  4.13it/s]\u001b[A\n","392it [03:09,  4.19it/s]\u001b[A\n","393it [03:09,  4.23it/s]\u001b[A\n","394it [03:09,  4.26it/s]\u001b[A\n","395it [03:09,  4.28it/s]\u001b[A\n","396it [03:09,  4.30it/s]\u001b[A\n","397it [03:10,  4.31it/s]\u001b[A\n","398it [03:10,  4.32it/s]\u001b[A\n","399it [03:10,  4.33it/s]\u001b[A\n","400it [03:10,  4.33it/s]\u001b[A\n","401it [03:11,  4.33it/s]\u001b[A\n","402it [03:11,  4.33it/s]\u001b[A\n","403it [03:11,  4.33it/s]\u001b[A\n","404it [03:11,  4.33it/s]\u001b[A\n","405it [03:12,  4.33it/s]\u001b[A\n","406it [03:12,  4.33it/s]\u001b[A\n","407it [03:12,  4.34it/s]\u001b[A\n","408it [03:12,  4.34it/s]\u001b[A\n","409it [03:12,  4.34it/s]\u001b[A\n","410it [03:13,  4.34it/s]\u001b[A\n","411it [03:13,  4.34it/s]\u001b[A\n","412it [03:13,  4.34it/s]\u001b[A\n","413it [03:13,  4.34it/s]\u001b[A\n","414it [03:14,  4.33it/s]\u001b[A\n","415it [03:14,  4.33it/s]\u001b[A\n","416it [03:14,  4.33it/s]\u001b[A\n","417it [03:14,  4.34it/s]\u001b[A\n","418it [03:15,  4.34it/s]\u001b[A\n","419it [03:15,  4.34it/s]\u001b[A\n","420it [03:15,  4.34it/s]\u001b[A\n","421it [03:15,  4.34it/s]\u001b[A\n","422it [03:15,  4.34it/s]\u001b[A\n","423it [03:16,  4.34it/s]\u001b[A\n","424it [03:16,  4.34it/s]\u001b[A\n","425it [03:16,  4.34it/s]\u001b[A\n","426it [03:16,  4.34it/s]\u001b[A\n","427it [03:17,  4.34it/s]\u001b[A\n","428it [03:17,  4.34it/s]\u001b[A\n","429it [03:17,  4.34it/s]\u001b[A\n","430it [03:17,  4.34it/s]\u001b[A\n","431it [03:18,  4.34it/s]\u001b[A\n","432it [03:18,  4.34it/s]\u001b[A\n","433it [03:18,  4.34it/s]\u001b[A\n","434it [03:18,  4.34it/s]\u001b[A\n","435it [03:18,  4.33it/s]\u001b[A\n","436it [03:19,  4.34it/s]\u001b[A\n","437it [03:19,  4.34it/s]\u001b[A\n","438it [03:19,  4.34it/s]\u001b[A\n","439it [03:19,  4.34it/s]\u001b[A\n","440it [03:20,  4.34it/s]\u001b[A\n","441it [03:20,  4.33it/s]\u001b[A\n","442it [03:20,  4.33it/s]\u001b[A\n","443it [03:20,  4.33it/s]\u001b[A\n","444it [03:21,  4.33it/s]\u001b[A\n","445it [03:21,  4.33it/s]\u001b[A\n","446it [03:21,  4.34it/s]\u001b[A\n","447it [03:21,  4.34it/s]\u001b[A\n","448it [03:21,  4.34it/s]\u001b[A\n","449it [03:22,  4.34it/s]\u001b[A\n","450it [03:22,  4.34it/s]\u001b[A\n","451it [03:22,  4.34it/s]\u001b[A\n","452it [03:22,  4.33it/s]\u001b[A\n","453it [03:23,  4.34it/s]\u001b[A\n","454it [03:23,  4.34it/s]\u001b[A\n","455it [03:23,  4.34it/s]\u001b[A\n","456it [03:23,  4.34it/s]\u001b[A\n","457it [03:24,  4.34it/s]\u001b[A\n","458it [03:24,  4.34it/s]\u001b[A\n","459it [03:24,  4.34it/s]\u001b[A\n","460it [03:24,  4.34it/s]\u001b[A\n","461it [03:24,  4.34it/s]\u001b[A\n","462it [03:25,  4.34it/s]\u001b[A\n","463it [03:25,  4.34it/s]\u001b[A\n","464it [03:25,  4.34it/s]\u001b[A\n","465it [03:25,  4.34it/s]\u001b[A\n","466it [03:26,  4.34it/s]\u001b[A\n","467it [03:26,  4.34it/s]\u001b[A\n","468it [03:26,  4.34it/s]\u001b[A\n","469it [03:26,  4.34it/s]\u001b[A\n","470it [03:27,  4.34it/s]\u001b[A\n","471it [03:27,  4.34it/s]\u001b[A\n","472it [03:27,  4.34it/s]\u001b[A\n","473it [03:27,  4.34it/s]\u001b[A\n","474it [03:27,  4.34it/s]\u001b[A\n","475it [03:28,  4.34it/s]\u001b[A\n","476it [03:28,  4.34it/s]\u001b[A\n","477it [03:28,  4.34it/s]\u001b[A\n","478it [03:28,  4.34it/s]\u001b[A\n","479it [03:29,  4.34it/s]\u001b[A\n","480it [03:29,  4.34it/s]\u001b[A\n","481it [03:29,  4.34it/s]\u001b[A\n","482it [03:29,  4.34it/s]\u001b[A\n","483it [03:30,  4.33it/s]\u001b[A\n","484it [03:30,  4.33it/s]\u001b[A\n","485it [03:30,  4.34it/s]\u001b[A\n","486it [03:30,  4.34it/s]\u001b[A\n","487it [03:30,  4.34it/s]\u001b[A\n","488it [03:31,  4.34it/s]\u001b[A\n","489it [03:31,  4.33it/s]\u001b[A\n","490it [03:31,  4.33it/s]\u001b[A\n","491it [03:31,  4.33it/s]\u001b[A\n","492it [03:32,  4.33it/s]\u001b[A\n","493it [03:32,  4.34it/s]\u001b[A\n","494it [03:32,  4.34it/s]\u001b[A\n","495it [03:32,  4.34it/s]\u001b[A\n","496it [03:33,  4.34it/s]\u001b[A\n","497it [03:33,  4.34it/s]\u001b[A\n","498it [03:33,  4.34it/s]\u001b[A\n","499it [03:33,  4.33it/s]\u001b[A\n","500it [03:33,  4.34it/s]\u001b[A\n","501it [03:34,  4.34it/s]\u001b[A\n","502it [03:34,  4.34it/s]\u001b[A\n","503it [03:34,  4.34it/s]\u001b[A\n","504it [03:34,  4.34it/s]\u001b[A\n","505it [03:35,  4.34it/s]\u001b[A\n","506it [03:35,  4.34it/s]\u001b[A\n","507it [03:35,  4.34it/s]\u001b[A\n","508it [03:35,  4.32it/s]\u001b[A\n","509it [03:36,  4.32it/s]\u001b[A\n","510it [03:36,  4.33it/s]\u001b[A\n","511it [03:36,  4.33it/s]\u001b[A\n","512it [03:36,  4.33it/s]\u001b[A\n","513it [03:36,  4.33it/s]\u001b[A\n","514it [03:37,  4.33it/s]\u001b[A\n","515it [03:37,  4.33it/s]\u001b[A\n","516it [03:37,  4.33it/s]\u001b[A\n","517it [03:37,  4.34it/s]\u001b[A\n","518it [03:38,  4.34it/s]\u001b[A\n","519it [03:38,  4.33it/s]\u001b[A\n","520it [03:38,  4.33it/s]\u001b[A\n","521it [03:38,  4.33it/s]\u001b[A\n","522it [03:39,  4.33it/s]\u001b[A\n","523it [03:39,  4.33it/s]\u001b[A\n","524it [03:39,  4.33it/s]\u001b[A\n","525it [03:39,  4.33it/s]\u001b[A\n","526it [03:39,  4.34it/s]\u001b[A\n","527it [03:40,  4.34it/s]\u001b[A\n","528it [03:40,  4.34it/s]\u001b[A\n","529it [03:40,  4.34it/s]\u001b[A\n","530it [03:40,  4.34it/s]\u001b[A\n","531it [03:41,  4.34it/s]\u001b[A\n","532it [03:41,  4.33it/s]\u001b[A\n","533it [03:41,  4.34it/s]\u001b[A\n","534it [03:41,  4.34it/s]\u001b[A\n","535it [03:42,  4.34it/s]\u001b[A\n","536it [03:42,  4.34it/s]\u001b[A\n","537it [03:42,  4.34it/s]\u001b[A\n","538it [03:42,  4.34it/s]\u001b[A\n","539it [03:42,  4.34it/s]\u001b[A\n","540it [03:43,  4.34it/s]\u001b[A\n","541it [03:43,  4.34it/s]\u001b[A\n","542it [03:43,  4.34it/s]\u001b[A\n","543it [03:43,  4.34it/s]\u001b[A\n","544it [03:44,  4.34it/s]\u001b[A\n","545it [03:44,  4.34it/s]\u001b[A\n","546it [03:44,  4.34it/s]\u001b[A\n","547it [03:44,  4.34it/s]\u001b[A\n","548it [03:45,  4.34it/s]\u001b[A\n","549it [03:45,  4.34it/s]\u001b[A\n","550it [03:45,  4.34it/s]\u001b[A\n","551it [03:45,  4.34it/s]\u001b[A\n","552it [03:45,  4.34it/s]\u001b[A\n","553it [03:46,  4.34it/s]\u001b[A\n","554it [03:46,  4.34it/s]\u001b[A\n","Epoch:  40% 99/250 [05:09<04:12,  1.67s/it]\n","556it [04:28, 12.77s/it]\u001b[A\n","557it [04:28,  9.01s/it]\u001b[A\n","558it [04:29,  6.38s/it]\u001b[A\n","559it [04:29,  4.53s/it]\u001b[A\n","560it [04:29,  3.24s/it]\u001b[A\n","561it [04:29,  2.34s/it]\u001b[A\n","562it [04:30,  1.71s/it]\u001b[A\n","563it [04:30,  1.26s/it]\u001b[A\n","564it [04:30,  1.05it/s]\u001b[A\n","565it [04:30,  1.36it/s]\u001b[A\n","566it [04:31,  1.71it/s]\u001b[A\n","567it [04:31,  2.09it/s]\u001b[A\n","568it [04:31,  2.47it/s]\u001b[A\n","569it [04:31,  2.84it/s]\u001b[A\n","570it [04:31,  3.17it/s]\u001b[A\n","571it [04:32,  3.45it/s]\u001b[A\n","572it [04:32,  3.68it/s]\u001b[A\n","573it [04:32,  3.85it/s]\u001b[A\n","574it [04:32,  3.99it/s]\u001b[A\n","575it [04:33,  4.08it/s]\u001b[A\n","576it [04:33,  4.15it/s]\u001b[A\n","577it [04:33,  4.21it/s]\u001b[A\n","578it [04:33,  4.25it/s]\u001b[A\n","579it [04:34,  4.26it/s]\u001b[A\n","580it [04:34,  4.28it/s]\u001b[A\n","581it [04:34,  4.30it/s]\u001b[A\n","582it [04:34,  4.31it/s]\u001b[A\n","583it [04:34,  4.32it/s]\u001b[A\n","584it [04:35,  4.33it/s]\u001b[A\n","585it [04:35,  4.33it/s]\u001b[A\n","586it [04:35,  4.33it/s]\u001b[A\n","587it [04:35,  4.33it/s]\u001b[A\n","588it [04:36,  4.33it/s]\u001b[A\n","589it [04:36,  4.33it/s]\u001b[A\n","590it [04:36,  4.33it/s]\u001b[A\n","591it [04:36,  4.34it/s]\u001b[A\n","592it [04:37,  4.33it/s]\u001b[A\n","593it [04:37,  4.34it/s]\u001b[A\n","594it [04:37,  4.34it/s]\u001b[A\n","595it [04:37,  4.33it/s]\u001b[A\n","596it [04:37,  4.34it/s]\u001b[A\n","597it [04:38,  4.34it/s]\u001b[A\n","598it [04:38,  4.34it/s]\u001b[A\n","599it [04:38,  4.33it/s]\u001b[A\n","600it [04:38,  4.34it/s]\u001b[A\n","601it [04:39,  4.34it/s]\u001b[A\n","602it [04:39,  4.34it/s]\u001b[A\n","603it [04:39,  4.34it/s]\u001b[A\n","604it [04:39,  4.34it/s]\u001b[A\n","605it [04:40,  4.34it/s]\u001b[A\n","606it [04:40,  4.34it/s]\u001b[A\n","607it [04:40,  4.34it/s]\u001b[A\n","608it [04:40,  4.33it/s]\u001b[A\n","609it [04:40,  4.33it/s]\u001b[A\n","610it [04:41,  4.33it/s]\u001b[A\n","611it [04:41,  4.33it/s]\u001b[A\n","612it [04:41,  4.34it/s]\u001b[A\n","613it [04:41,  4.34it/s]\u001b[A\n","614it [04:42,  4.34it/s]\u001b[A\n","615it [04:42,  4.34it/s]\u001b[A\n","616it [04:42,  4.34it/s]\u001b[A\n","617it [04:42,  4.34it/s]\u001b[A\n","618it [04:43,  4.34it/s]\u001b[A\n","619it [04:43,  4.33it/s]\u001b[A\n","620it [04:43,  4.34it/s]\u001b[A\n","621it [04:43,  4.34it/s]\u001b[A\n","622it [04:43,  4.34it/s]\u001b[A\n","623it [04:44,  4.34it/s]\u001b[A\n","624it [04:44,  4.34it/s]\u001b[A\n","625it [04:44,  4.33it/s]\u001b[A\n","626it [04:44,  4.33it/s]\u001b[A\n","627it [04:45,  4.34it/s]\u001b[A\n","628it [04:45,  4.33it/s]\u001b[A\n","629it [04:45,  4.33it/s]\u001b[A\n","630it [04:45,  4.33it/s]\u001b[A\n","631it [04:46,  4.34it/s]\u001b[A\n","632it [04:46,  4.34it/s]\u001b[A\n","633it [04:46,  4.34it/s]\u001b[A\n","634it [04:46,  4.34it/s]\u001b[A\n","635it [04:46,  4.34it/s]\u001b[A\n","636it [04:47,  4.33it/s]\u001b[A\n","637it [04:47,  4.34it/s]\u001b[A\n","638it [04:47,  4.34it/s]\u001b[A\n","639it [04:47,  4.33it/s]\u001b[A\n","640it [04:48,  4.34it/s]\u001b[A\n","641it [04:48,  4.34it/s]\u001b[A\n","642it [04:48,  4.34it/s]\u001b[A\n","643it [04:48,  4.34it/s]\u001b[A\n","644it [04:49,  4.34it/s]\u001b[A\n","645it [04:49,  4.34it/s]\u001b[A\n","646it [04:49,  4.34it/s]\u001b[A\n","647it [04:49,  4.34it/s]\u001b[A\n","648it [04:49,  4.34it/s]\u001b[A\n","649it [04:50,  4.33it/s]\u001b[A\n","650it [04:50,  4.33it/s]\u001b[A\n","651it [04:50,  4.34it/s]\u001b[A\n","652it [04:50,  4.34it/s]\u001b[A\n","653it [04:51,  4.34it/s]\u001b[A\n","654it [04:51,  4.34it/s]\u001b[A\n","655it [04:51,  4.33it/s]\u001b[A\n","656it [04:51,  4.33it/s]\u001b[A\n","657it [04:52,  4.33it/s]\u001b[A\n","658it [04:52,  4.33it/s]\u001b[A\n","659it [04:52,  4.33it/s]\u001b[A\n","660it [04:52,  4.34it/s]\u001b[A\n","661it [04:52,  4.34it/s]\u001b[A\n","662it [04:53,  4.33it/s]\u001b[A\n","663it [04:53,  4.33it/s]\u001b[A\n","664it [04:53,  4.34it/s]\u001b[A\n","665it [04:53,  4.33it/s]\u001b[A\n","666it [04:54,  4.33it/s]\u001b[A\n","667it [04:54,  4.33it/s]\u001b[A\n","668it [04:54,  4.33it/s]\u001b[A\n","669it [04:54,  4.33it/s]\u001b[A\n","670it [04:55,  4.34it/s]\u001b[A\n","671it [04:55,  4.34it/s]\u001b[A\n","672it [04:55,  4.34it/s]\u001b[A\n","673it [04:55,  4.34it/s]\u001b[A\n","674it [04:55,  4.34it/s]\u001b[A\n","675it [04:56,  4.34it/s]\u001b[A\n","676it [04:56,  4.34it/s]\u001b[A\n","677it [04:56,  4.34it/s]\u001b[A\n","678it [04:56,  4.34it/s]\u001b[A\n","679it [04:57,  4.34it/s]\u001b[A\n","680it [04:57,  4.34it/s]\u001b[A\n","681it [04:57,  4.34it/s]\u001b[A\n","682it [04:57,  4.34it/s]\u001b[A\n","683it [04:57,  4.34it/s]\u001b[A\n","684it [04:58,  4.34it/s]\u001b[A\n","685it [04:58,  4.34it/s]\u001b[A\n","686it [04:58,  4.34it/s]\u001b[A\n","687it [04:58,  4.34it/s]\u001b[A\n","688it [04:59,  4.34it/s]\u001b[A\n","689it [04:59,  4.34it/s]\u001b[A\n","690it [04:59,  4.34it/s]\u001b[A\n","691it [04:59,  4.34it/s]\u001b[A\n","692it [05:00,  4.34it/s]\u001b[A\n","693it [05:00,  4.33it/s]\u001b[A\n","694it [05:00,  4.33it/s]\u001b[A\n","695it [05:00,  4.33it/s]\u001b[A\n","696it [05:00,  4.33it/s]\u001b[A\n","697it [05:01,  4.33it/s]\u001b[A\n","698it [05:01,  4.34it/s]\u001b[A\n","699it [05:01,  4.34it/s]\u001b[A\n","700it [05:01,  4.34it/s]\u001b[A\n","701it [05:02,  4.34it/s]\u001b[A\n","702it [05:02,  4.34it/s]\u001b[A\n","703it [05:02,  4.34it/s]\u001b[A\n","704it [05:02,  4.34it/s]\u001b[A\n","705it [05:03,  4.34it/s]\u001b[A\n","706it [05:03,  4.34it/s]\u001b[A\n","707it [05:03,  4.34it/s]\u001b[A\n","708it [05:03,  4.34it/s]\u001b[A\n","709it [05:03,  4.34it/s]\u001b[A\n","710it [05:04,  4.34it/s]\u001b[A\n","711it [05:04,  4.34it/s]\u001b[A\n","712it [05:04,  4.34it/s]\u001b[A\n","713it [05:04,  4.34it/s]\u001b[A\n","714it [05:05,  4.34it/s]\u001b[A\n","715it [05:05,  4.34it/s]\u001b[A\n","716it [05:05,  4.34it/s]\u001b[A\n","717it [05:05,  4.34it/s]\u001b[A\n","718it [05:06,  4.34it/s]\u001b[A\n","719it [05:06,  4.34it/s]\u001b[A\n","720it [05:06,  4.34it/s]\u001b[A\n","721it [05:06,  4.34it/s]\u001b[A\n","722it [05:06,  4.34it/s]\u001b[A\n","723it [05:07,  4.34it/s]\u001b[A\n","724it [05:07,  4.34it/s]\u001b[A\n","725it [05:07,  4.34it/s]\u001b[A\n","726it [05:07,  4.34it/s]\u001b[A\n","727it [05:08,  4.34it/s]\u001b[A\n","728it [05:08,  4.34it/s]\u001b[A\n","729it [05:08,  4.34it/s]\u001b[A\n","730it [05:08,  4.34it/s]\u001b[A\n","731it [05:09,  4.34it/s]\u001b[A\n","732it [05:09,  4.33it/s]\u001b[A\n","733it [05:09,  4.33it/s]\u001b[A\n","734it [05:09,  4.34it/s]\u001b[A\n","735it [05:09,  4.34it/s]\u001b[A\n","736it [05:10,  4.34it/s]\u001b[A\n","737it [05:10,  4.34it/s]\u001b[A\n","738it [05:10,  4.34it/s]\u001b[A\n","739it [05:10,  4.34it/s]\u001b[A\n","Epoch:  50% 124/250 [06:33<03:31,  1.68s/it]\n","741it [05:53, 12.78s/it]\u001b[A\n","742it [05:53,  9.01s/it]\u001b[A\n","743it [05:53,  6.38s/it]\u001b[A\n","744it [05:53,  4.53s/it]\u001b[A\n","745it [05:54,  3.24s/it]\u001b[A\n","746it [05:54,  2.34s/it]\u001b[A\n","747it [05:54,  1.71s/it]\u001b[A\n","748it [05:54,  1.26s/it]\u001b[A\n","749it [05:55,  1.05it/s]\u001b[A\n","750it [05:55,  1.36it/s]\u001b[A\n","751it [05:55,  1.71it/s]\u001b[A\n","752it [05:55,  2.09it/s]\u001b[A\n","753it [05:55,  2.47it/s]\u001b[A\n","754it [05:56,  2.84it/s]\u001b[A\n","755it [05:56,  3.17it/s]\u001b[A\n","756it [05:56,  3.44it/s]\u001b[A\n","757it [05:56,  3.67it/s]\u001b[A\n","758it [05:57,  3.85it/s]\u001b[A\n","759it [05:57,  3.98it/s]\u001b[A\n","760it [05:57,  4.09it/s]\u001b[A\n","761it [05:57,  4.16it/s]\u001b[A\n","762it [05:58,  4.21it/s]\u001b[A\n","763it [05:58,  4.25it/s]\u001b[A\n","764it [05:58,  4.27it/s]\u001b[A\n","765it [05:58,  4.29it/s]\u001b[A\n","766it [05:58,  4.30it/s]\u001b[A\n","767it [05:59,  4.31it/s]\u001b[A\n","768it [05:59,  4.32it/s]\u001b[A\n","769it [05:59,  4.32it/s]\u001b[A\n","770it [05:59,  4.33it/s]\u001b[A\n","771it [06:00,  4.33it/s]\u001b[A\n","772it [06:00,  4.33it/s]\u001b[A\n","773it [06:00,  4.33it/s]\u001b[A\n","774it [06:00,  4.33it/s]\u001b[A\n","775it [06:01,  4.33it/s]\u001b[A\n","776it [06:01,  4.33it/s]\u001b[A\n","777it [06:01,  4.34it/s]\u001b[A\n","778it [06:01,  4.34it/s]\u001b[A\n","779it [06:01,  4.34it/s]\u001b[A\n","780it [06:02,  4.34it/s]\u001b[A\n","781it [06:02,  4.34it/s]\u001b[A\n","782it [06:02,  4.34it/s]\u001b[A\n","783it [06:02,  4.34it/s]\u001b[A\n","784it [06:03,  4.34it/s]\u001b[A\n","785it [06:03,  4.34it/s]\u001b[A\n","786it [06:03,  4.33it/s]\u001b[A\n","787it [06:03,  4.34it/s]\u001b[A\n","788it [06:04,  4.34it/s]\u001b[A\n","789it [06:04,  4.34it/s]\u001b[A\n","790it [06:04,  4.34it/s]\u001b[A\n","791it [06:04,  4.34it/s]\u001b[A\n","792it [06:04,  4.34it/s]\u001b[A\n","793it [06:05,  4.34it/s]\u001b[A\n","794it [06:05,  4.34it/s]\u001b[A\n","795it [06:05,  4.34it/s]\u001b[A\n","796it [06:05,  4.34it/s]\u001b[A\n","797it [06:06,  4.34it/s]\u001b[A\n","798it [06:06,  4.34it/s]\u001b[A\n","799it [06:06,  4.33it/s]\u001b[A\n","800it [06:06,  4.33it/s]\u001b[A\n","801it [06:07,  4.33it/s]\u001b[A\n","802it [06:07,  4.33it/s]\u001b[A\n","803it [06:07,  4.33it/s]\u001b[A\n","804it [06:07,  4.33it/s]\u001b[A\n","805it [06:07,  4.33it/s]\u001b[A\n","806it [06:08,  4.33it/s]\u001b[A\n","807it [06:08,  4.34it/s]\u001b[A\n","808it [06:08,  4.34it/s]\u001b[A\n","809it [06:08,  4.34it/s]\u001b[A\n","810it [06:09,  4.33it/s]\u001b[A\n","811it [06:09,  4.34it/s]\u001b[A\n","812it [06:09,  4.34it/s]\u001b[A\n","813it [06:09,  4.34it/s]\u001b[A\n","814it [06:10,  4.34it/s]\u001b[A\n","815it [06:10,  4.34it/s]\u001b[A\n","816it [06:10,  4.34it/s]\u001b[A\n","817it [06:10,  4.34it/s]\u001b[A\n","818it [06:10,  4.34it/s]\u001b[A\n","819it [06:11,  4.34it/s]\u001b[A\n","820it [06:11,  4.34it/s]\u001b[A\n","821it [06:11,  4.34it/s]\u001b[A\n","822it [06:11,  4.34it/s]\u001b[A\n","823it [06:12,  4.34it/s]\u001b[A\n","824it [06:12,  4.34it/s]\u001b[A\n","825it [06:12,  4.34it/s]\u001b[A\n","826it [06:12,  4.34it/s]\u001b[A\n","827it [06:13,  4.34it/s]\u001b[A\n","828it [06:13,  4.34it/s]\u001b[A\n","829it [06:13,  4.34it/s]\u001b[A\n","830it [06:13,  4.34it/s]\u001b[A\n","831it [06:13,  4.34it/s]\u001b[A\n","832it [06:14,  4.34it/s]\u001b[A\n","833it [06:14,  4.33it/s]\u001b[A\n","834it [06:14,  4.34it/s]\u001b[A\n","835it [06:14,  4.34it/s]\u001b[A\n","836it [06:15,  4.34it/s]\u001b[A\n","837it [06:15,  4.34it/s]\u001b[A\n","838it [06:15,  4.34it/s]\u001b[A\n","839it [06:15,  4.34it/s]\u001b[A\n","840it [06:16,  4.34it/s]\u001b[A\n","841it [06:16,  4.34it/s]\u001b[A\n","842it [06:16,  4.33it/s]\u001b[A\n","843it [06:16,  4.33it/s]\u001b[A\n","844it [06:16,  4.34it/s]\u001b[A\n","845it [06:17,  4.34it/s]\u001b[A\n","846it [06:17,  4.34it/s]\u001b[A\n","847it [06:17,  4.34it/s]\u001b[A\n","848it [06:17,  4.34it/s]\u001b[A\n","849it [06:18,  4.34it/s]\u001b[A\n","850it [06:18,  4.34it/s]\u001b[A\n","851it [06:18,  4.34it/s]\u001b[A\n","852it [06:18,  4.34it/s]\u001b[A\n","853it [06:19,  4.33it/s]\u001b[A\n","854it [06:19,  4.33it/s]\u001b[A\n","855it [06:19,  4.34it/s]\u001b[A\n","856it [06:19,  4.34it/s]\u001b[A\n","857it [06:19,  4.34it/s]\u001b[A\n","858it [06:20,  4.34it/s]\u001b[A\n","859it [06:20,  4.34it/s]\u001b[A\n","860it [06:20,  4.33it/s]\u001b[A\n","861it [06:20,  4.33it/s]\u001b[A\n","862it [06:21,  4.33it/s]\u001b[A\n","863it [06:21,  4.33it/s]\u001b[A\n","864it [06:21,  4.33it/s]\u001b[A\n","865it [06:21,  4.33it/s]\u001b[A\n","866it [06:22,  4.34it/s]\u001b[A\n","867it [06:22,  4.34it/s]\u001b[A\n","868it [06:22,  4.34it/s]\u001b[A\n","869it [06:22,  4.34it/s]\u001b[A\n","870it [06:22,  4.34it/s]\u001b[A\n","871it [06:23,  4.34it/s]\u001b[A\n","872it [06:23,  4.33it/s]\u001b[A\n","873it [06:23,  4.33it/s]\u001b[A\n","874it [06:23,  4.33it/s]\u001b[A\n","875it [06:24,  4.33it/s]\u001b[A\n","876it [06:24,  4.34it/s]\u001b[A\n","877it [06:24,  4.34it/s]\u001b[A\n","878it [06:24,  4.33it/s]\u001b[A\n","879it [06:25,  4.33it/s]\u001b[A\n","880it [06:25,  4.33it/s]\u001b[A\n","881it [06:25,  4.34it/s]\u001b[A\n","882it [06:25,  4.33it/s]\u001b[A\n","883it [06:25,  4.33it/s]\u001b[A\n","884it [06:26,  4.34it/s]\u001b[A\n","885it [06:26,  4.34it/s]\u001b[A\n","886it [06:26,  4.34it/s]\u001b[A\n","887it [06:26,  4.34it/s]\u001b[A\n","888it [06:27,  4.34it/s]\u001b[A\n","889it [06:27,  4.34it/s]\u001b[A\n","890it [06:27,  4.34it/s]\u001b[A\n","891it [06:27,  4.34it/s]\u001b[A\n","892it [06:28,  4.34it/s]\u001b[A\n","893it [06:28,  4.34it/s]\u001b[A\n","894it [06:28,  4.34it/s]\u001b[A\n","895it [06:28,  4.34it/s]\u001b[A\n","896it [06:28,  4.34it/s]\u001b[A\n","897it [06:29,  4.34it/s]\u001b[A\n","898it [06:29,  4.34it/s]\u001b[A\n","899it [06:29,  4.34it/s]\u001b[A\n","900it [06:29,  4.33it/s]\u001b[A\n","901it [06:30,  4.33it/s]\u001b[A\n","902it [06:30,  4.33it/s]\u001b[A\n","903it [06:30,  4.33it/s]\u001b[A\n","904it [06:30,  4.33it/s]\u001b[A\n","905it [06:31,  4.34it/s]\u001b[A\n","906it [06:31,  4.34it/s]\u001b[A\n","907it [06:31,  4.34it/s]\u001b[A\n","908it [06:31,  4.34it/s]\u001b[A\n","909it [06:31,  4.34it/s]\u001b[A\n","910it [06:32,  4.33it/s]\u001b[A\n","911it [06:32,  4.33it/s]\u001b[A\n","912it [06:32,  4.33it/s]\u001b[A\n","913it [06:32,  4.33it/s]\u001b[A\n","914it [06:33,  4.33it/s]\u001b[A\n","915it [06:33,  4.33it/s]\u001b[A\n","916it [06:33,  4.34it/s]\u001b[A\n","917it [06:33,  4.34it/s]\u001b[A\n","918it [06:34,  4.34it/s]\u001b[A\n","919it [06:34,  4.34it/s]\u001b[A\n","920it [06:34,  4.34it/s]\u001b[A\n","921it [06:34,  4.34it/s]\u001b[A\n","922it [06:34,  4.34it/s]\u001b[A\n","923it [06:35,  4.33it/s]\u001b[A\n","924it [06:35,  4.34it/s]\u001b[A\n","Epoch:  60% 149/250 [07:58<02:49,  1.68s/it]\n","926it [07:17, 12.78s/it]\u001b[A\n","927it [07:17,  9.02s/it]\u001b[A\n","928it [07:18,  6.38s/it]\u001b[A\n","929it [07:18,  4.54s/it]\u001b[A\n","930it [07:18,  3.24s/it]\u001b[A\n","931it [07:18,  2.34s/it]\u001b[A\n","932it [07:19,  1.71s/it]\u001b[A\n","933it [07:19,  1.26s/it]\u001b[A\n","934it [07:19,  1.05it/s]\u001b[A\n","935it [07:19,  1.36it/s]\u001b[A\n","936it [07:19,  1.71it/s]\u001b[A\n","937it [07:20,  2.09it/s]\u001b[A\n","938it [07:20,  2.47it/s]\u001b[A\n","939it [07:20,  2.84it/s]\u001b[A\n","940it [07:20,  3.17it/s]\u001b[A\n","941it [07:21,  3.45it/s]\u001b[A\n","942it [07:21,  3.67it/s]\u001b[A\n","943it [07:21,  3.85it/s]\u001b[A\n","944it [07:21,  3.98it/s]\u001b[A\n","945it [07:22,  4.08it/s]\u001b[A\n","946it [07:22,  4.16it/s]\u001b[A\n","947it [07:22,  4.21it/s]\u001b[A\n","948it [07:22,  4.25it/s]\u001b[A\n","949it [07:22,  4.27it/s]\u001b[A\n","950it [07:23,  4.29it/s]\u001b[A\n","951it [07:23,  4.31it/s]\u001b[A\n","952it [07:23,  4.32it/s]\u001b[A\n","953it [07:23,  4.33it/s]\u001b[A\n","954it [07:24,  4.33it/s]\u001b[A\n","955it [07:24,  4.33it/s]\u001b[A\n","956it [07:24,  4.33it/s]\u001b[A\n","957it [07:24,  4.33it/s]\u001b[A\n","958it [07:25,  4.33it/s]\u001b[A\n","959it [07:25,  4.33it/s]\u001b[A\n","960it [07:25,  4.33it/s]\u001b[A\n","961it [07:25,  4.34it/s]\u001b[A\n","962it [07:25,  4.34it/s]\u001b[A\n","963it [07:26,  4.34it/s]\u001b[A\n","964it [07:26,  4.34it/s]\u001b[A\n","965it [07:26,  4.34it/s]\u001b[A\n","966it [07:26,  4.34it/s]\u001b[A\n","967it [07:27,  4.34it/s]\u001b[A\n","968it [07:27,  4.34it/s]\u001b[A\n","969it [07:27,  4.34it/s]\u001b[A\n","970it [07:27,  4.34it/s]\u001b[A\n","971it [07:28,  4.34it/s]\u001b[A\n","972it [07:28,  4.34it/s]\u001b[A\n","973it [07:28,  4.34it/s]\u001b[A\n","974it [07:28,  4.34it/s]\u001b[A\n","975it [07:28,  4.34it/s]\u001b[A\n","976it [07:29,  4.34it/s]\u001b[A\n","977it [07:29,  4.34it/s]\u001b[A\n","978it [07:29,  4.34it/s]\u001b[A\n","979it [07:29,  4.34it/s]\u001b[A\n","980it [07:30,  4.34it/s]\u001b[A\n","981it [07:30,  4.34it/s]\u001b[A\n","982it [07:30,  4.34it/s]\u001b[A\n","983it [07:30,  4.34it/s]\u001b[A\n","984it [07:31,  4.33it/s]\u001b[A\n","985it [07:31,  4.33it/s]\u001b[A\n","986it [07:31,  4.33it/s]\u001b[A\n","987it [07:31,  4.33it/s]\u001b[A\n","988it [07:31,  4.34it/s]\u001b[A\n","989it [07:32,  4.34it/s]\u001b[A\n","990it [07:32,  4.34it/s]\u001b[A\n","991it [07:32,  4.34it/s]\u001b[A\n","992it [07:32,  4.34it/s]\u001b[A\n","993it [07:33,  4.34it/s]\u001b[A\n","994it [07:33,  4.34it/s]\u001b[A\n","995it [07:33,  4.34it/s]\u001b[A\n","996it [07:33,  4.34it/s]\u001b[A\n","997it [07:34,  4.34it/s]\u001b[A\n","998it [07:34,  4.34it/s]\u001b[A\n","999it [07:34,  4.34it/s]\u001b[A\n","1000it [07:34,  4.34it/s]\u001b[A\n","1001it [07:34,  4.34it/s]\u001b[A\n","1002it [07:35,  4.34it/s]\u001b[A\n","1003it [07:35,  4.33it/s]\u001b[A\n","1004it [07:35,  4.33it/s]\u001b[A\n","1005it [07:35,  4.34it/s]\u001b[A\n","1006it [07:36,  4.34it/s]\u001b[A\n","1007it [07:36,  4.34it/s]\u001b[A\n","1008it [07:36,  4.34it/s]\u001b[A\n","1009it [07:36,  4.34it/s]\u001b[A\n","1010it [07:37,  4.34it/s]\u001b[A\n","1011it [07:37,  4.34it/s]\u001b[A\n","1012it [07:37,  4.34it/s]\u001b[A\n","1013it [07:37,  4.34it/s]\u001b[A\n","1014it [07:37,  4.34it/s]\u001b[A\n","1015it [07:38,  4.34it/s]\u001b[A\n","1016it [07:38,  4.33it/s]\u001b[A\n","1017it [07:38,  4.33it/s]\u001b[A\n","1018it [07:38,  4.33it/s]\u001b[A\n","1019it [07:39,  4.33it/s]\u001b[A\n","1020it [07:39,  4.33it/s]\u001b[A\n","1021it [07:39,  4.34it/s]\u001b[A\n","1022it [07:39,  4.34it/s]\u001b[A\n","1023it [07:40,  4.33it/s]\u001b[A\n","1024it [07:40,  4.34it/s]\u001b[A\n","1025it [07:40,  4.34it/s]\u001b[A\n","1026it [07:40,  4.34it/s]\u001b[A\n","1027it [07:40,  4.34it/s]\u001b[A\n","1028it [07:41,  4.34it/s]\u001b[A\n","1029it [07:41,  4.34it/s]\u001b[A\n","1030it [07:41,  4.34it/s]\u001b[A\n","1031it [07:41,  4.34it/s]\u001b[A\n","1032it [07:42,  4.34it/s]\u001b[A\n","1033it [07:42,  4.34it/s]\u001b[A\n","1034it [07:42,  4.34it/s]\u001b[A\n","1035it [07:42,  4.33it/s]\u001b[A\n","1036it [07:43,  4.33it/s]\u001b[A\n","1037it [07:43,  4.34it/s]\u001b[A\n","1038it [07:43,  4.34it/s]\u001b[A\n","1039it [07:43,  4.34it/s]\u001b[A\n","1040it [07:43,  4.34it/s]\u001b[A\n","1041it [07:44,  4.34it/s]\u001b[A\n","1042it [07:44,  4.34it/s]\u001b[A\n","1043it [07:44,  4.34it/s]\u001b[A\n","1044it [07:44,  4.33it/s]\u001b[A\n","1045it [07:45,  4.33it/s]\u001b[A\n","1046it [07:45,  4.33it/s]\u001b[A\n","1047it [07:45,  4.33it/s]\u001b[A\n","1048it [07:45,  4.34it/s]\u001b[A\n","1049it [07:46,  4.34it/s]\u001b[A\n","1050it [07:46,  4.33it/s]\u001b[A\n","1051it [07:46,  4.34it/s]\u001b[A\n","1052it [07:46,  4.34it/s]\u001b[A\n","1053it [07:46,  4.33it/s]\u001b[A\n","1054it [07:47,  4.33it/s]\u001b[A\n","1055it [07:47,  4.33it/s]\u001b[A\n","1056it [07:47,  4.33it/s]\u001b[A\n","1057it [07:47,  4.33it/s]\u001b[A\n","1058it [07:48,  4.34it/s]\u001b[A\n","1059it [07:48,  4.34it/s]\u001b[A\n","1060it [07:48,  4.34it/s]\u001b[A\n","1061it [07:48,  4.34it/s]\u001b[A\n","1062it [07:49,  4.34it/s]\u001b[A\n","1063it [07:49,  4.34it/s]\u001b[A\n","1064it [07:49,  4.34it/s]\u001b[A\n","1065it [07:49,  4.34it/s]\u001b[A\n","1066it [07:49,  4.34it/s]\u001b[A\n","1067it [07:50,  4.34it/s]\u001b[A\n","1068it [07:50,  4.34it/s]\u001b[A\n","1069it [07:50,  4.34it/s]\u001b[A\n","1070it [07:50,  4.34it/s]\u001b[A\n","1071it [07:51,  4.34it/s]\u001b[A\n","1072it [07:51,  4.34it/s]\u001b[A\n","1073it [07:51,  4.33it/s]\u001b[A\n","1074it [07:51,  4.34it/s]\u001b[A\n","1075it [07:52,  4.34it/s]\u001b[A\n","1076it [07:52,  4.33it/s]\u001b[A\n","1077it [07:52,  4.33it/s]\u001b[A\n","1078it [07:52,  4.33it/s]\u001b[A\n","1079it [07:52,  4.34it/s]\u001b[A\n","1080it [07:53,  4.34it/s]\u001b[A\n","1081it [07:53,  4.34it/s]\u001b[A\n","1082it [07:53,  4.34it/s]\u001b[A\n","1083it [07:53,  4.34it/s]\u001b[A\n","1084it [07:54,  4.34it/s]\u001b[A\n","1085it [07:54,  4.34it/s]\u001b[A\n","1086it [07:54,  4.34it/s]\u001b[A\n","1087it [07:54,  4.34it/s]\u001b[A\n","1088it [07:55,  4.34it/s]\u001b[A\n","1089it [07:55,  4.34it/s]\u001b[A\n","1090it [07:55,  4.34it/s]\u001b[A\n","1091it [07:55,  4.34it/s]\u001b[A\n","1092it [07:55,  4.34it/s]\u001b[A\n","1093it [07:56,  4.34it/s]\u001b[A\n","1094it [07:56,  4.34it/s]\u001b[A\n","1095it [07:56,  4.34it/s]\u001b[A\n","1096it [07:56,  4.34it/s]\u001b[A\n","1097it [07:57,  4.34it/s]\u001b[A\n","1098it [07:57,  4.34it/s]\u001b[A\n","1099it [07:57,  4.34it/s]\u001b[A\n","1100it [07:57,  4.34it/s]\u001b[A\n","1101it [07:58,  4.34it/s]\u001b[A\n","1102it [07:58,  4.34it/s]\u001b[A\n","1103it [07:58,  4.33it/s]\u001b[A\n","1104it [07:58,  4.34it/s]\u001b[A\n","1105it [07:58,  4.34it/s]\u001b[A\n","1106it [07:59,  4.34it/s]\u001b[A\n","1107it [07:59,  4.34it/s]\u001b[A\n","1108it [07:59,  4.34it/s]\u001b[A\n","1109it [07:59,  4.33it/s]\u001b[A\n","Epoch:  70% 174/250 [09:22<02:07,  1.68s/it]\n","1111it [08:42, 12.78s/it]\u001b[A\n","1112it [08:42,  9.02s/it]\u001b[A\n","1113it [08:42,  6.38s/it]\u001b[A\n","1114it [08:42,  4.54s/it]\u001b[A\n","1115it [08:43,  3.24s/it]\u001b[A\n","1116it [08:43,  2.34s/it]\u001b[A\n","1117it [08:43,  1.71s/it]\u001b[A\n","1118it [08:43,  1.26s/it]\u001b[A\n","1119it [08:44,  1.05it/s]\u001b[A\n","1120it [08:44,  1.36it/s]\u001b[A\n","1121it [08:44,  1.71it/s]\u001b[A\n","1122it [08:44,  2.09it/s]\u001b[A\n","1123it [08:44,  2.47it/s]\u001b[A\n","1124it [08:45,  2.84it/s]\u001b[A\n","1125it [08:45,  3.17it/s]\u001b[A\n","1126it [08:45,  3.44it/s]\u001b[A\n","1127it [08:45,  3.67it/s]\u001b[A\n","1128it [08:46,  3.85it/s]\u001b[A\n","1129it [08:46,  3.98it/s]\u001b[A\n","1130it [08:46,  4.08it/s]\u001b[A\n","1131it [08:46,  4.16it/s]\u001b[A\n","1132it [08:47,  4.21it/s]\u001b[A\n","1133it [08:47,  4.25it/s]\u001b[A\n","1134it [08:47,  4.27it/s]\u001b[A\n","1135it [08:47,  4.29it/s]\u001b[A\n","1136it [08:47,  4.30it/s]\u001b[A\n","1137it [08:48,  4.31it/s]\u001b[A\n","1138it [08:48,  4.32it/s]\u001b[A\n","1139it [08:48,  4.32it/s]\u001b[A\n","1140it [08:48,  4.33it/s]\u001b[A\n","1141it [08:49,  4.33it/s]\u001b[A\n","1142it [08:49,  4.33it/s]\u001b[A\n","1143it [08:49,  4.34it/s]\u001b[A\n","1144it [08:49,  4.34it/s]\u001b[A\n","1145it [08:50,  4.34it/s]\u001b[A\n","1146it [08:50,  4.34it/s]\u001b[A\n","1147it [08:50,  4.34it/s]\u001b[A\n","1148it [08:50,  4.34it/s]\u001b[A\n","1149it [08:50,  4.33it/s]\u001b[A\n","1150it [08:51,  4.33it/s]\u001b[A\n","1151it [08:51,  4.33it/s]\u001b[A\n","1152it [08:51,  4.34it/s]\u001b[A\n","1153it [08:51,  4.34it/s]\u001b[A\n","1154it [08:52,  4.34it/s]\u001b[A\n","1155it [08:52,  4.34it/s]\u001b[A\n","1156it [08:52,  4.33it/s]\u001b[A\n","1157it [08:52,  4.33it/s]\u001b[A\n","1158it [08:53,  4.33it/s]\u001b[A\n","1159it [08:53,  4.33it/s]\u001b[A\n","1160it [08:53,  4.34it/s]\u001b[A\n","1161it [08:53,  4.34it/s]\u001b[A\n","1162it [08:53,  4.34it/s]\u001b[A\n","1163it [08:54,  4.34it/s]\u001b[A\n","1164it [08:54,  4.34it/s]\u001b[A\n","1165it [08:54,  4.34it/s]\u001b[A\n","1166it [08:54,  4.34it/s]\u001b[A\n","1167it [08:55,  4.34it/s]\u001b[A\n","1168it [08:55,  4.33it/s]\u001b[A\n","1169it [08:55,  4.34it/s]\u001b[A\n","1170it [08:55,  4.33it/s]\u001b[A\n","1171it [08:56,  4.33it/s]\u001b[A\n","1172it [08:56,  4.33it/s]\u001b[A\n","1173it [08:56,  4.33it/s]\u001b[A\n","1174it [08:56,  4.34it/s]\u001b[A\n","1175it [08:56,  4.34it/s]\u001b[A\n","1176it [08:57,  4.33it/s]\u001b[A\n","1177it [08:57,  4.34it/s]\u001b[A\n","1178it [08:57,  4.34it/s]\u001b[A\n","1179it [08:57,  4.33it/s]\u001b[A\n","1180it [08:58,  4.33it/s]\u001b[A\n","1181it [08:58,  4.34it/s]\u001b[A\n","1182it [08:58,  4.34it/s]\u001b[A\n","1183it [08:58,  4.34it/s]\u001b[A\n","1184it [08:59,  4.34it/s]\u001b[A\n","1185it [08:59,  4.34it/s]\u001b[A\n","1186it [08:59,  4.34it/s]\u001b[A\n","1187it [08:59,  4.34it/s]\u001b[A\n","1188it [08:59,  4.33it/s]\u001b[A\n","1189it [09:00,  4.33it/s]\u001b[A\n","1190it [09:00,  4.33it/s]\u001b[A\n","1191it [09:00,  4.34it/s]\u001b[A\n","1192it [09:00,  4.34it/s]\u001b[A\n","1193it [09:01,  4.34it/s]\u001b[A\n","1194it [09:01,  4.34it/s]\u001b[A\n","1195it [09:01,  4.34it/s]\u001b[A\n","1196it [09:01,  4.34it/s]\u001b[A\n","1197it [09:02,  4.34it/s]\u001b[A\n","1198it [09:02,  4.33it/s]\u001b[A\n","1199it [09:02,  4.33it/s]\u001b[A\n","1200it [09:02,  4.34it/s]\u001b[A\n","1201it [09:02,  4.34it/s]\u001b[A\n","1202it [09:03,  4.33it/s]\u001b[A\n","1203it [09:03,  4.34it/s]\u001b[A\n","1204it [09:03,  4.34it/s]\u001b[A\n","1205it [09:03,  4.34it/s]\u001b[A\n","1206it [09:04,  4.33it/s]\u001b[A\n","1207it [09:04,  4.33it/s]\u001b[A\n","1208it [09:04,  4.33it/s]\u001b[A\n","1209it [09:04,  4.33it/s]\u001b[A\n","1210it [09:05,  4.33it/s]\u001b[A\n","1211it [09:05,  4.33it/s]\u001b[A\n","1212it [09:05,  4.33it/s]\u001b[A\n","1213it [09:05,  4.34it/s]\u001b[A\n","1214it [09:05,  4.34it/s]\u001b[A\n","1215it [09:06,  4.34it/s]\u001b[A\n","1216it [09:06,  4.33it/s]\u001b[A\n","1217it [09:06,  4.34it/s]\u001b[A\n","1218it [09:06,  4.34it/s]\u001b[A\n","1219it [09:07,  4.33it/s]\u001b[A\n","1220it [09:07,  4.33it/s]\u001b[A\n","1221it [09:07,  4.33it/s]\u001b[A\n","1222it [09:07,  4.33it/s]\u001b[A\n","1223it [09:08,  4.33it/s]\u001b[A\n","1224it [09:08,  4.33it/s]\u001b[A\n","1225it [09:08,  4.33it/s]\u001b[A\n","1226it [09:08,  4.33it/s]\u001b[A\n","1227it [09:08,  4.33it/s]\u001b[A\n","1228it [09:09,  4.33it/s]\u001b[A\n","1229it [09:09,  4.33it/s]\u001b[A\n","1230it [09:09,  4.33it/s]\u001b[A\n","1231it [09:09,  4.34it/s]\u001b[A\n","1232it [09:10,  4.34it/s]\u001b[A\n","1233it [09:10,  4.34it/s]\u001b[A\n","1234it [09:10,  4.34it/s]\u001b[A\n","1235it [09:10,  4.34it/s]\u001b[A\n","1236it [09:11,  4.34it/s]\u001b[A\n","1237it [09:11,  4.34it/s]\u001b[A\n","1238it [09:11,  4.33it/s]\u001b[A\n","1239it [09:11,  4.34it/s]\u001b[A\n","1240it [09:11,  4.34it/s]\u001b[A\n","1241it [09:12,  4.34it/s]\u001b[A\n","1242it [09:12,  4.33it/s]\u001b[A\n","1243it [09:12,  4.34it/s]\u001b[A\n","1244it [09:12,  4.34it/s]\u001b[A\n","1245it [09:13,  4.34it/s]\u001b[A\n","1246it [09:13,  4.34it/s]\u001b[A\n","1247it [09:13,  4.34it/s]\u001b[A\n","1248it [09:13,  4.34it/s]\u001b[A\n","1249it [09:14,  4.33it/s]\u001b[A\n","1250it [09:14,  4.34it/s]\u001b[A\n","1251it [09:14,  4.33it/s]\u001b[A\n","1252it [09:14,  4.34it/s]\u001b[A\n","1253it [09:14,  4.34it/s]\u001b[A\n","1254it [09:15,  4.34it/s]\u001b[A\n","1255it [09:15,  4.33it/s]\u001b[A\n","1256it [09:15,  4.33it/s]\u001b[A\n","1257it [09:15,  4.34it/s]\u001b[A\n","1258it [09:16,  4.33it/s]\u001b[A\n","1259it [09:16,  4.33it/s]\u001b[A\n","1260it [09:16,  4.33it/s]\u001b[A\n","1261it [09:16,  4.34it/s]\u001b[A\n","1262it [09:17,  4.34it/s]\u001b[A\n","1263it [09:17,  4.34it/s]\u001b[A\n","1264it [09:17,  4.34it/s]\u001b[A\n","1265it [09:17,  4.33it/s]\u001b[A\n","1266it [09:17,  4.33it/s]\u001b[A\n","1267it [09:18,  4.33it/s]\u001b[A\n","1268it [09:18,  4.33it/s]\u001b[A\n","1269it [09:18,  4.33it/s]\u001b[A\n","1270it [09:18,  4.33it/s]\u001b[A\n","1271it [09:19,  4.33it/s]\u001b[A\n","1272it [09:19,  4.33it/s]\u001b[A\n","1273it [09:19,  4.33it/s]\u001b[A\n","1274it [09:19,  4.33it/s]\u001b[A\n","1275it [09:20,  4.34it/s]\u001b[A\n","1276it [09:20,  4.34it/s]\u001b[A\n","1277it [09:20,  4.34it/s]\u001b[A\n","1278it [09:20,  4.33it/s]\u001b[A\n","1279it [09:20,  4.33it/s]\u001b[A\n","1280it [09:21,  4.33it/s]\u001b[A\n","1281it [09:21,  4.33it/s]\u001b[A\n","1282it [09:21,  4.34it/s]\u001b[A\n","1283it [09:21,  4.34it/s]\u001b[A\n","1284it [09:22,  4.34it/s]\u001b[A\n","1285it [09:22,  4.34it/s]\u001b[A\n","1286it [09:22,  4.34it/s]\u001b[A\n","1287it [09:22,  4.34it/s]\u001b[A\n","1288it [09:23,  4.34it/s]\u001b[A\n","1289it [09:23,  4.34it/s]\u001b[A\n","1290it [09:23,  4.34it/s]\u001b[A\n","1291it [09:23,  4.34it/s]\u001b[A\n","1292it [09:23,  4.34it/s]\u001b[A\n","1293it [09:24,  4.34it/s]\u001b[A\n","1294it [09:24,  4.34it/s]\u001b[A\n","Epoch:  80% 199/250 [10:47<01:25,  1.68s/it]\n","1296it [10:06, 12.79s/it]\u001b[A\n","1297it [10:06,  9.02s/it]\u001b[A\n","1298it [10:07,  6.38s/it]\u001b[A\n","1299it [10:07,  4.54s/it]\u001b[A\n","1300it [10:07,  3.25s/it]\u001b[A\n","1301it [10:07,  2.34s/it]\u001b[A\n","1302it [10:08,  1.71s/it]\u001b[A\n","1303it [10:08,  1.26s/it]\u001b[A\n","1304it [10:08,  1.05it/s]\u001b[A\n","1305it [10:08,  1.36it/s]\u001b[A\n","1306it [10:09,  1.71it/s]\u001b[A\n","1307it [10:09,  2.09it/s]\u001b[A\n","1308it [10:09,  2.47it/s]\u001b[A\n","1309it [10:09,  2.84it/s]\u001b[A\n","1310it [10:09,  3.17it/s]\u001b[A\n","1311it [10:10,  3.44it/s]\u001b[A\n","1312it [10:10,  3.67it/s]\u001b[A\n","1313it [10:10,  3.85it/s]\u001b[A\n","1314it [10:10,  3.98it/s]\u001b[A\n","1315it [10:11,  4.08it/s]\u001b[A\n","1316it [10:11,  4.16it/s]\u001b[A\n","1317it [10:11,  4.21it/s]\u001b[A\n","1318it [10:11,  4.25it/s]\u001b[A\n","1319it [10:12,  4.27it/s]\u001b[A\n","1320it [10:12,  4.29it/s]\u001b[A\n","1321it [10:12,  4.30it/s]\u001b[A\n","1322it [10:12,  4.31it/s]\u001b[A\n","1323it [10:12,  4.32it/s]\u001b[A\n","1324it [10:13,  4.33it/s]\u001b[A\n","1325it [10:13,  4.33it/s]\u001b[A\n","1326it [10:13,  4.33it/s]\u001b[A\n","1327it [10:13,  4.33it/s]\u001b[A\n","1328it [10:14,  4.33it/s]\u001b[A\n","1329it [10:14,  4.33it/s]\u001b[A\n","1330it [10:14,  4.34it/s]\u001b[A\n","1331it [10:14,  4.34it/s]\u001b[A\n","1332it [10:15,  4.34it/s]\u001b[A\n","1333it [10:15,  4.34it/s]\u001b[A\n","1334it [10:15,  4.34it/s]\u001b[A\n","1335it [10:15,  4.34it/s]\u001b[A\n","1336it [10:15,  4.34it/s]\u001b[A\n","1337it [10:16,  4.34it/s]\u001b[A\n","1338it [10:16,  4.34it/s]\u001b[A\n","1339it [10:16,  4.34it/s]\u001b[A\n","1340it [10:16,  4.34it/s]\u001b[A\n","1341it [10:17,  4.33it/s]\u001b[A\n","1342it [10:17,  4.34it/s]\u001b[A\n","1343it [10:17,  4.34it/s]\u001b[A\n","1344it [10:17,  4.34it/s]\u001b[A\n","1345it [10:18,  4.34it/s]\u001b[A\n","1346it [10:18,  4.34it/s]\u001b[A\n","1347it [10:18,  4.34it/s]\u001b[A\n","1348it [10:18,  4.34it/s]\u001b[A\n","1349it [10:18,  4.34it/s]\u001b[A\n","1350it [10:19,  4.34it/s]\u001b[A\n","1351it [10:19,  4.33it/s]\u001b[A\n","1352it [10:19,  4.34it/s]\u001b[A\n","1353it [10:19,  4.34it/s]\u001b[A\n","1354it [10:20,  4.34it/s]\u001b[A\n","1355it [10:20,  4.34it/s]\u001b[A\n","1356it [10:20,  4.34it/s]\u001b[A\n","1357it [10:20,  4.34it/s]\u001b[A\n","1358it [10:21,  4.33it/s]\u001b[A\n","1359it [10:21,  4.33it/s]\u001b[A\n","1360it [10:21,  4.33it/s]\u001b[A\n","1361it [10:21,  4.33it/s]\u001b[A\n","1362it [10:21,  4.33it/s]\u001b[A\n","1363it [10:22,  4.34it/s]\u001b[A\n","1364it [10:22,  4.34it/s]\u001b[A\n","1365it [10:22,  4.34it/s]\u001b[A\n","1366it [10:22,  4.34it/s]\u001b[A\n","1367it [10:23,  4.34it/s]\u001b[A\n","1368it [10:23,  4.34it/s]\u001b[A\n","1369it [10:23,  4.34it/s]\u001b[A\n","1370it [10:23,  4.33it/s]\u001b[A\n","1371it [10:24,  4.33it/s]\u001b[A\n","1372it [10:24,  4.33it/s]\u001b[A\n","1373it [10:24,  4.34it/s]\u001b[A\n","1374it [10:24,  4.34it/s]\u001b[A\n","1375it [10:24,  4.34it/s]\u001b[A\n","1376it [10:25,  4.34it/s]\u001b[A\n","1377it [10:25,  4.34it/s]\u001b[A\n","1378it [10:25,  4.33it/s]\u001b[A\n","1379it [10:25,  4.33it/s]\u001b[A\n","1380it [10:26,  4.33it/s]\u001b[A\n","1381it [10:26,  4.33it/s]\u001b[A\n","1382it [10:26,  4.33it/s]\u001b[A\n","1383it [10:26,  4.33it/s]\u001b[A\n","1384it [10:27,  4.33it/s]\u001b[A\n","1385it [10:27,  4.34it/s]\u001b[A\n","1386it [10:27,  4.34it/s]\u001b[A\n","1387it [10:27,  4.34it/s]\u001b[A\n","1388it [10:27,  4.34it/s]\u001b[A\n","1389it [10:28,  4.34it/s]\u001b[A\n","1390it [10:28,  4.34it/s]\u001b[A\n","1391it [10:28,  4.33it/s]\u001b[A\n","1392it [10:28,  4.34it/s]\u001b[A\n","1393it [10:29,  4.34it/s]\u001b[A\n","1394it [10:29,  4.34it/s]\u001b[A\n","1395it [10:29,  4.34it/s]\u001b[A\n","1396it [10:29,  4.34it/s]\u001b[A\n","1397it [10:30,  4.34it/s]\u001b[A\n","1398it [10:30,  4.34it/s]\u001b[A\n","1399it [10:30,  4.34it/s]\u001b[A\n","1400it [10:30,  4.34it/s]\u001b[A\n","1401it [10:30,  4.33it/s]\u001b[A\n","1402it [10:31,  4.33it/s]\u001b[A\n","1403it [10:31,  4.33it/s]\u001b[A\n","1404it [10:31,  4.33it/s]\u001b[A\n","1405it [10:31,  4.33it/s]\u001b[A\n","1406it [10:32,  4.33it/s]\u001b[A\n","1407it [10:32,  4.33it/s]\u001b[A\n","1408it [10:32,  4.33it/s]\u001b[A\n","1409it [10:32,  4.33it/s]\u001b[A\n","1410it [10:33,  4.34it/s]\u001b[A\n","1411it [10:33,  4.33it/s]\u001b[A\n","1412it [10:33,  4.34it/s]\u001b[A\n","1413it [10:33,  4.34it/s]\u001b[A\n","1414it [10:33,  4.34it/s]\u001b[A\n","1415it [10:34,  4.33it/s]\u001b[A\n","1416it [10:34,  4.34it/s]\u001b[A\n","1417it [10:34,  4.34it/s]\u001b[A\n","1418it [10:34,  4.34it/s]\u001b[A\n","1419it [10:35,  4.34it/s]\u001b[A\n","1420it [10:35,  4.34it/s]\u001b[A\n","1421it [10:35,  4.34it/s]\u001b[A\n","1422it [10:35,  4.34it/s]\u001b[A\n","1423it [10:36,  4.34it/s]\u001b[A\n","1424it [10:36,  4.34it/s]\u001b[A\n","1425it [10:36,  4.33it/s]\u001b[A\n","1426it [10:36,  4.34it/s]\u001b[A\n","1427it [10:36,  4.34it/s]\u001b[A\n","1428it [10:37,  4.33it/s]\u001b[A\n","1429it [10:37,  4.33it/s]\u001b[A\n","1430it [10:37,  4.33it/s]\u001b[A\n","1431it [10:37,  4.33it/s]\u001b[A\n","1432it [10:38,  4.34it/s]\u001b[A\n","1433it [10:38,  4.34it/s]\u001b[A\n","1434it [10:38,  4.34it/s]\u001b[A\n","1435it [10:38,  4.34it/s]\u001b[A\n","1436it [10:39,  4.34it/s]\u001b[A\n","1437it [10:39,  4.33it/s]\u001b[A\n","1438it [10:39,  4.33it/s]\u001b[A\n","1439it [10:39,  4.34it/s]\u001b[A\n","1440it [10:39,  4.34it/s]\u001b[A\n","1441it [10:40,  4.34it/s]\u001b[A\n","1442it [10:40,  4.34it/s]\u001b[A\n","1443it [10:40,  4.34it/s]\u001b[A\n","1444it [10:40,  4.34it/s]\u001b[A\n","1445it [10:41,  4.34it/s]\u001b[A\n","1446it [10:41,  4.34it/s]\u001b[A\n","1447it [10:41,  4.34it/s]\u001b[A\n","1448it [10:41,  4.33it/s]\u001b[A\n","1449it [10:42,  4.33it/s]\u001b[A\n","1450it [10:42,  4.33it/s]\u001b[A\n","1451it [10:42,  4.33it/s]\u001b[A\n","1452it [10:42,  4.33it/s]\u001b[A\n","1453it [10:42,  4.34it/s]\u001b[A\n","1454it [10:43,  4.33it/s]\u001b[A\n","1455it [10:43,  4.33it/s]\u001b[A\n","1456it [10:43,  4.34it/s]\u001b[A\n","1457it [10:43,  4.34it/s]\u001b[A\n","1458it [10:44,  4.33it/s]\u001b[A\n","1459it [10:44,  4.33it/s]\u001b[A\n","1460it [10:44,  4.33it/s]\u001b[A\n","1461it [10:44,  4.33it/s]\u001b[A\n","1462it [10:45,  4.33it/s]\u001b[A\n","1463it [10:45,  4.34it/s]\u001b[A\n","1464it [10:45,  4.34it/s]\u001b[A\n","1465it [10:45,  4.34it/s]\u001b[A\n","1466it [10:45,  4.34it/s]\u001b[A\n","1467it [10:46,  4.34it/s]\u001b[A\n","1468it [10:46,  4.34it/s]\u001b[A\n","1469it [10:46,  4.34it/s]\u001b[A\n","1470it [10:46,  4.34it/s]\u001b[A\n","1471it [10:47,  4.34it/s]\u001b[A\n","1472it [10:47,  4.34it/s]\u001b[A\n","1473it [10:47,  4.34it/s]\u001b[A\n","1474it [10:47,  4.34it/s]\u001b[A\n","1475it [10:48,  4.34it/s]\u001b[A\n","1476it [10:48,  4.34it/s]\u001b[A\n","1477it [10:48,  4.34it/s]\u001b[A\n","1478it [10:48,  4.33it/s]\u001b[A\n","1479it [10:48,  4.34it/s]\u001b[A\n","Epoch:  90% 224/250 [12:11<00:43,  1.68s/it]\n","1481it [11:31, 12.79s/it]\u001b[A\n","1482it [11:31,  9.02s/it]\u001b[A\n","1483it [11:31,  6.38s/it]\u001b[A\n","1484it [11:31,  4.54s/it]\u001b[A\n","1485it [11:32,  3.25s/it]\u001b[A\n","1486it [11:32,  2.34s/it]\u001b[A\n","1487it [11:32,  1.71s/it]\u001b[A\n","1488it [11:32,  1.27s/it]\u001b[A\n","1489it [11:33,  1.05it/s]\u001b[A\n","1490it [11:33,  1.36it/s]\u001b[A\n","1491it [11:33,  1.71it/s]\u001b[A\n","1492it [11:33,  2.09it/s]\u001b[A\n","1493it [11:34,  2.47it/s]\u001b[A\n","1494it [11:34,  2.84it/s]\u001b[A\n","1495it [11:34,  3.17it/s]\u001b[A\n","1496it [11:34,  3.45it/s]\u001b[A\n","1497it [11:34,  3.67it/s]\u001b[A\n","1498it [11:35,  3.85it/s]\u001b[A\n","1499it [11:35,  3.98it/s]\u001b[A\n","1500it [11:35,  4.08it/s]\u001b[A\n","1501it [11:35,  4.16it/s]\u001b[A\n","1502it [11:36,  4.21it/s]\u001b[A\n","1503it [11:36,  4.24it/s]\u001b[A\n","1504it [11:36,  4.27it/s]\u001b[A\n","1505it [11:36,  4.29it/s]\u001b[A\n","1506it [11:37,  4.30it/s]\u001b[A\n","1507it [11:37,  4.30it/s]\u001b[A\n","1508it [11:37,  4.31it/s]\u001b[A\n","1509it [11:37,  4.32it/s]\u001b[A\n","1510it [11:37,  4.33it/s]\u001b[A\n","1511it [11:38,  4.33it/s]\u001b[A\n","1512it [11:38,  4.33it/s]\u001b[A\n","1513it [11:38,  4.34it/s]\u001b[A\n","1514it [11:38,  4.33it/s]\u001b[A\n","1515it [11:39,  4.33it/s]\u001b[A\n","1516it [11:39,  4.33it/s]\u001b[A\n","1517it [11:39,  4.33it/s]\u001b[A\n","1518it [11:39,  4.34it/s]\u001b[A\n","1519it [11:40,  4.34it/s]\u001b[A\n","1520it [11:40,  4.34it/s]\u001b[A\n","1521it [11:40,  4.34it/s]\u001b[A\n","1522it [11:40,  4.34it/s]\u001b[A\n","1523it [11:40,  4.34it/s]\u001b[A\n","1524it [11:41,  4.34it/s]\u001b[A\n","1525it [11:41,  4.34it/s]\u001b[A\n","1526it [11:41,  4.34it/s]\u001b[A\n","1527it [11:41,  4.34it/s]\u001b[A\n","1528it [11:42,  4.34it/s]\u001b[A\n","1529it [11:42,  4.34it/s]\u001b[A\n","1530it [11:42,  4.34it/s]\u001b[A\n","1531it [11:42,  4.34it/s]\u001b[A\n","1532it [11:43,  4.34it/s]\u001b[A\n","1533it [11:43,  4.34it/s]\u001b[A\n","1534it [11:43,  4.34it/s]\u001b[A\n","1535it [11:43,  4.34it/s]\u001b[A\n","1536it [11:43,  4.34it/s]\u001b[A\n","1537it [11:44,  4.33it/s]\u001b[A\n","1538it [11:44,  4.34it/s]\u001b[A\n","1539it [11:44,  4.34it/s]\u001b[A\n","1540it [11:44,  4.34it/s]\u001b[A\n","1541it [11:45,  4.34it/s]\u001b[A\n","1542it [11:45,  4.34it/s]\u001b[A\n","1543it [11:45,  4.34it/s]\u001b[A\n","1544it [11:45,  4.34it/s]\u001b[A\n","1545it [11:46,  4.34it/s]\u001b[A\n","1546it [11:46,  4.34it/s]\u001b[A\n","1547it [11:46,  4.34it/s]\u001b[A\n","1548it [11:46,  4.34it/s]\u001b[A\n","1549it [11:46,  4.34it/s]\u001b[A\n","1550it [11:47,  4.34it/s]\u001b[A\n","1551it [11:47,  4.34it/s]\u001b[A\n","1552it [11:47,  4.34it/s]\u001b[A\n","1553it [11:47,  4.34it/s]\u001b[A\n","1554it [11:48,  4.33it/s]\u001b[A\n","1555it [11:48,  4.33it/s]\u001b[A\n","1556it [11:48,  4.33it/s]\u001b[A\n","1557it [11:48,  4.33it/s]\u001b[A\n","1558it [11:49,  4.33it/s]\u001b[A\n","1559it [11:49,  4.34it/s]\u001b[A\n","1560it [11:49,  4.33it/s]\u001b[A\n","1561it [11:49,  4.33it/s]\u001b[A\n","1562it [11:49,  4.33it/s]\u001b[A\n","1563it [11:50,  4.33it/s]\u001b[A\n","1564it [11:50,  4.33it/s]\u001b[A\n","1565it [11:50,  4.33it/s]\u001b[A\n","1566it [11:50,  4.33it/s]\u001b[A\n","1567it [11:51,  4.33it/s]\u001b[A\n","1568it [11:51,  4.33it/s]\u001b[A\n","1569it [11:51,  4.34it/s]\u001b[A\n","1570it [11:51,  4.34it/s]\u001b[A\n","1571it [11:52,  4.34it/s]\u001b[A\n","1572it [11:52,  4.34it/s]\u001b[A\n","1573it [11:52,  4.34it/s]\u001b[A\n","1574it [11:52,  4.34it/s]\u001b[A\n","1575it [11:52,  4.34it/s]\u001b[A\n","1576it [11:53,  4.34it/s]\u001b[A\n","1577it [11:53,  4.34it/s]\u001b[A\n","1578it [11:53,  4.34it/s]\u001b[A\n","1579it [11:53,  4.34it/s]\u001b[A\n","1580it [11:54,  4.34it/s]\u001b[A\n","1581it [11:54,  4.34it/s]\u001b[A\n","1582it [11:54,  4.34it/s]\u001b[A\n","1583it [11:54,  4.34it/s]\u001b[A\n","1584it [11:55,  4.34it/s]\u001b[A\n","1585it [11:55,  4.34it/s]\u001b[A\n","1586it [11:55,  4.34it/s]\u001b[A\n","1587it [11:55,  4.34it/s]\u001b[A\n","1588it [11:55,  4.34it/s]\u001b[A\n","1589it [11:56,  4.34it/s]\u001b[A\n","1590it [11:56,  4.34it/s]\u001b[A\n","1591it [11:56,  4.34it/s]\u001b[A\n","1592it [11:56,  4.34it/s]\u001b[A\n","1593it [11:57,  4.34it/s]\u001b[A\n","1594it [11:57,  4.33it/s]\u001b[A\n","1595it [11:57,  4.33it/s]\u001b[A\n","1596it [11:57,  4.34it/s]\u001b[A\n","1597it [11:58,  4.34it/s]\u001b[A\n","1598it [11:58,  4.34it/s]\u001b[A\n","1599it [11:58,  4.34it/s]\u001b[A\n","1600it [11:58,  4.34it/s]\u001b[A\n","1601it [11:58,  4.33it/s]\u001b[A\n","1602it [11:59,  4.33it/s]\u001b[A\n","1603it [11:59,  4.33it/s]\u001b[A\n","1604it [11:59,  4.33it/s]\u001b[A\n","1605it [11:59,  4.33it/s]\u001b[A\n","1606it [12:00,  4.33it/s]\u001b[A\n","1607it [12:00,  4.33it/s]\u001b[A\n","1608it [12:00,  4.34it/s]\u001b[A\n","1609it [12:00,  4.34it/s]\u001b[A\n","1610it [12:01,  4.34it/s]\u001b[A\n","1611it [12:01,  4.34it/s]\u001b[A\n","1612it [12:01,  4.34it/s]\u001b[A\n","1613it [12:01,  4.34it/s]\u001b[A\n","1614it [12:01,  4.34it/s]\u001b[A\n","1615it [12:02,  4.34it/s]\u001b[A\n","1616it [12:02,  4.34it/s]\u001b[A\n","1617it [12:02,  4.34it/s]\u001b[A\n","1618it [12:02,  4.34it/s]\u001b[A\n","1619it [12:03,  4.34it/s]\u001b[A\n","1620it [12:03,  4.34it/s]\u001b[A\n","1621it [12:03,  4.34it/s]\u001b[A\n","1622it [12:03,  4.34it/s]\u001b[A\n","1623it [12:03,  4.34it/s]\u001b[A\n","1624it [12:04,  4.34it/s]\u001b[A\n","1625it [12:04,  4.34it/s]\u001b[A\n","1626it [12:04,  4.34it/s]\u001b[A\n","1627it [12:04,  4.34it/s]\u001b[A\n","1628it [12:05,  4.34it/s]\u001b[A\n","1629it [12:05,  4.34it/s]\u001b[A\n","1630it [12:05,  4.34it/s]\u001b[A\n","1631it [12:05,  4.34it/s]\u001b[A\n","1632it [12:06,  4.34it/s]\u001b[A\n","1633it [12:06,  4.33it/s]\u001b[A\n","1634it [12:06,  4.33it/s]\u001b[A\n","1635it [12:06,  4.34it/s]\u001b[A\n","1636it [12:06,  4.34it/s]\u001b[A\n","1637it [12:07,  4.34it/s]\u001b[A\n","1638it [12:07,  4.34it/s]\u001b[A\n","1639it [12:07,  4.34it/s]\u001b[A\n","1640it [12:07,  4.34it/s]\u001b[A\n","1641it [12:08,  4.34it/s]\u001b[A\n","1642it [12:08,  4.34it/s]\u001b[A\n","1643it [12:08,  4.34it/s]\u001b[A\n","1644it [12:08,  4.33it/s]\u001b[A\n","1645it [12:09,  4.34it/s]\u001b[A\n","1646it [12:09,  4.34it/s]\u001b[A\n","1647it [12:09,  4.34it/s]\u001b[A\n","1648it [12:09,  4.34it/s]\u001b[A\n","1649it [12:09,  4.34it/s]\u001b[A\n","1650it [12:10,  4.33it/s]\u001b[A\n","1651it [12:10,  4.33it/s]\u001b[A\n","1652it [12:10,  4.33it/s]\u001b[A\n","1653it [12:10,  4.34it/s]\u001b[A\n","1654it [12:11,  4.33it/s]\u001b[A\n","1655it [12:11,  4.33it/s]\u001b[A\n","1656it [12:11,  4.34it/s]\u001b[A\n","1657it [12:11,  4.34it/s]\u001b[A\n","1658it [12:12,  4.34it/s]\u001b[A\n","1659it [12:12,  4.34it/s]\u001b[A\n","1660it [12:12,  4.34it/s]\u001b[A\n","1661it [12:12,  4.34it/s]\u001b[A\n","1662it [12:12,  4.34it/s]\u001b[A\n","1663it [12:13,  4.34it/s]\u001b[A\n","1664it [12:13,  4.34it/s]\u001b[A\n","Epoch: 100% 249/250 [13:36<00:01,  1.67s/it]\n","1666it [12:55, 12.78s/it]\u001b[A\n","1667it [12:55,  9.01s/it]\u001b[A\n","1668it [12:56,  6.38s/it]\u001b[A\n","1669it [12:56,  4.53s/it]\u001b[A\n","1670it [12:56,  3.24s/it]\u001b[A\n","1671it [12:56,  2.34s/it]\u001b[A\n","1672it [12:57,  1.71s/it]\u001b[A\n","1673it [12:57,  1.26s/it]\u001b[A\n","1674it [12:57,  1.05it/s]\u001b[A\n","1675it [12:57,  1.36it/s]\u001b[A\n","1676it [12:58,  1.71it/s]\u001b[A\n","1677it [12:58,  2.09it/s]\u001b[A\n","1678it [12:58,  2.47it/s]\u001b[A\n","1679it [12:58,  2.84it/s]\u001b[A\n","1680it [12:58,  3.17it/s]\u001b[A\n","1681it [12:59,  3.45it/s]\u001b[A\n","1682it [12:59,  3.67it/s]\u001b[A\n","1683it [12:59,  3.85it/s]\u001b[A\n","1684it [12:59,  3.98it/s]\u001b[A\n","1685it [13:00,  4.08it/s]\u001b[A\n","1686it [13:00,  4.16it/s]\u001b[A\n","1687it [13:00,  4.21it/s]\u001b[A\n","1688it [13:00,  4.25it/s]\u001b[A\n","1689it [13:01,  4.28it/s]\u001b[A\n","1690it [13:01,  4.29it/s]\u001b[A\n","1691it [13:01,  4.30it/s]\u001b[A\n","1692it [13:01,  4.31it/s]\u001b[A\n","1693it [13:01,  4.32it/s]\u001b[A\n","1694it [13:02,  4.32it/s]\u001b[A\n","1695it [13:02,  4.33it/s]\u001b[A\n","1696it [13:02,  4.33it/s]\u001b[A\n","1697it [13:02,  4.34it/s]\u001b[A\n","1698it [13:03,  4.34it/s]\u001b[A\n","1699it [13:03,  4.34it/s]\u001b[A\n","1700it [13:03,  4.34it/s]\u001b[A\n","1701it [13:03,  4.34it/s]\u001b[A\n","1702it [13:04,  4.34it/s]\u001b[A\n","1703it [13:04,  4.34it/s]\u001b[A\n","1704it [13:04,  4.33it/s]\u001b[A\n","1705it [13:04,  4.34it/s]\u001b[A\n","1706it [13:04,  4.34it/s]\u001b[A\n","1707it [13:05,  4.34it/s]\u001b[A\n","1708it [13:05,  4.33it/s]\u001b[A\n","1709it [13:05,  4.34it/s]\u001b[A\n","1710it [13:05,  4.34it/s]\u001b[A\n","1711it [13:06,  4.34it/s]\u001b[A\n","1712it [13:06,  4.34it/s]\u001b[A\n","1713it [13:06,  4.34it/s]\u001b[A\n","1714it [13:06,  4.34it/s]\u001b[A\n","1715it [13:07,  4.34it/s]\u001b[A\n","1716it [13:07,  4.34it/s]\u001b[A\n","1717it [13:07,  4.34it/s]\u001b[A\n","1718it [13:07,  4.34it/s]\u001b[A\n","1719it [13:07,  4.34it/s]\u001b[A\n","1720it [13:08,  4.34it/s]\u001b[A\n","1721it [13:08,  4.34it/s]\u001b[A\n","1722it [13:08,  4.34it/s]\u001b[A\n","1723it [13:08,  4.33it/s]\u001b[A\n","1724it [13:09,  4.33it/s]\u001b[A\n","1725it [13:09,  4.34it/s]\u001b[A\n","1726it [13:09,  4.34it/s]\u001b[A\n","1727it [13:09,  4.34it/s]\u001b[A\n","1728it [13:10,  4.33it/s]\u001b[A\n","1729it [13:10,  4.34it/s]\u001b[A\n","1730it [13:10,  4.34it/s]\u001b[A\n","1731it [13:10,  4.34it/s]\u001b[A\n","1732it [13:10,  4.34it/s]\u001b[A\n","1733it [13:11,  4.34it/s]\u001b[A\n","1734it [13:11,  4.34it/s]\u001b[A\n","1735it [13:11,  4.34it/s]\u001b[A\n","1736it [13:11,  4.34it/s]\u001b[A\n","1737it [13:12,  4.34it/s]\u001b[A\n","1738it [13:12,  4.34it/s]\u001b[A\n","1739it [13:12,  4.34it/s]\u001b[A\n","1740it [13:12,  4.34it/s]\u001b[A\n","1741it [13:13,  4.33it/s]\u001b[A\n","1742it [13:13,  4.34it/s]\u001b[A\n","1743it [13:13,  4.34it/s]\u001b[A\n","1744it [13:13,  4.34it/s]\u001b[A\n","1745it [13:13,  4.34it/s]\u001b[A\n","1746it [13:14,  4.34it/s]\u001b[A\n","1747it [13:14,  4.34it/s]\u001b[A\n","1748it [13:14,  4.34it/s]\u001b[A\n","1749it [13:14,  4.34it/s]\u001b[A\n","1750it [13:15,  4.33it/s]\u001b[A\n","1751it [13:15,  4.33it/s]\u001b[A\n","1752it [13:15,  4.33it/s]\u001b[A\n","1753it [13:15,  4.34it/s]\u001b[A\n","1754it [13:16,  4.34it/s]\u001b[A\n","1755it [13:16,  4.34it/s]\u001b[A\n","1756it [13:16,  4.34it/s]\u001b[A\n","1757it [13:16,  4.33it/s]\u001b[A\n","1758it [13:16,  4.33it/s]\u001b[A\n","1759it [13:17,  4.33it/s]\u001b[A\n","1760it [13:17,  4.33it/s]\u001b[A\n","1761it [13:17,  4.33it/s]\u001b[A\n","1762it [13:17,  4.33it/s]\u001b[A\n","1763it [13:18,  4.33it/s]\u001b[A\n","1764it [13:18,  4.34it/s]\u001b[A\n","1765it [13:18,  4.34it/s]\u001b[A\n","1766it [13:18,  4.34it/s]\u001b[A\n","1767it [13:19,  4.34it/s]\u001b[A\n","1768it [13:19,  4.34it/s]\u001b[A\n","1769it [13:19,  4.34it/s]\u001b[A\n","1770it [13:19,  4.34it/s]\u001b[A\n","1771it [13:19,  4.33it/s]\u001b[A\n","1772it [13:20,  4.34it/s]\u001b[A\n","1773it [13:20,  4.34it/s]\u001b[A\n","1774it [13:20,  4.34it/s]\u001b[A\n","1775it [13:20,  4.34it/s]\u001b[A\n","1776it [13:21,  4.34it/s]\u001b[A\n","1777it [13:21,  4.34it/s]\u001b[A\n","1778it [13:21,  4.34it/s]\u001b[A\n","1779it [13:21,  4.34it/s]\u001b[A\n","1780it [13:22,  4.34it/s]\u001b[A\n","1781it [13:22,  4.33it/s]\u001b[A\n","1782it [13:22,  4.34it/s]\u001b[A\n","1783it [13:22,  4.34it/s]\u001b[A\n","1784it [13:22,  4.34it/s]\u001b[A\n","1785it [13:23,  4.34it/s]\u001b[A\n","1786it [13:23,  4.34it/s]\u001b[A\n","1787it [13:23,  4.34it/s]\u001b[A\n","1788it [13:23,  4.34it/s]\u001b[A\n","1789it [13:24,  4.34it/s]\u001b[A\n","1790it [13:24,  4.34it/s]\u001b[A\n","1791it [13:24,  4.34it/s]\u001b[A\n","1792it [13:24,  4.34it/s]\u001b[A\n","1793it [13:25,  4.34it/s]\u001b[A\n","1794it [13:25,  4.34it/s]\u001b[A\n","1795it [13:25,  4.34it/s]\u001b[A\n","1796it [13:25,  4.34it/s]\u001b[A\n","1797it [13:25,  4.34it/s]\u001b[A\n","1798it [13:26,  4.34it/s]\u001b[A\n","1799it [13:26,  4.34it/s]\u001b[A\n","1800it [13:26,  4.34it/s]\u001b[A\n","1801it [13:26,  4.33it/s]\u001b[A\n","1802it [13:27,  4.33it/s]\u001b[A\n","1803it [13:27,  4.33it/s]\u001b[A\n","1804it [13:27,  4.33it/s]\u001b[A\n","1805it [13:27,  4.33it/s]\u001b[A\n","1806it [13:28,  4.34it/s]\u001b[A\n","1807it [13:28,  4.34it/s]\u001b[A\n","1808it [13:28,  4.34it/s]\u001b[A\n","1809it [13:28,  4.34it/s]\u001b[A\n","1810it [13:28,  4.33it/s]\u001b[A\n","1811it [13:29,  4.33it/s]\u001b[A\n","1812it [13:29,  4.33it/s]\u001b[A\n","1813it [13:29,  4.34it/s]\u001b[A\n","1814it [13:29,  4.34it/s]\u001b[A\n","1815it [13:30,  4.34it/s]\u001b[A\n","1816it [13:30,  4.34it/s]\u001b[A\n","1817it [13:30,  4.34it/s]\u001b[A\n","1818it [13:30,  4.34it/s]\u001b[A\n","1819it [13:31,  4.34it/s]\u001b[A\n","1820it [13:31,  4.34it/s]\u001b[A\n","1821it [13:31,  4.34it/s]\u001b[A\n","1822it [13:31,  4.34it/s]\u001b[A\n","1823it [13:31,  4.34it/s]\u001b[A\n","1824it [13:32,  4.34it/s]\u001b[A\n","1825it [13:32,  4.34it/s]\u001b[A\n","1826it [13:32,  4.34it/s]\u001b[A\n","1827it [13:32,  4.34it/s]\u001b[A\n","1828it [13:33,  4.34it/s]\u001b[A\n","1829it [13:33,  4.34it/s]\u001b[A\n","1830it [13:33,  4.34it/s]\u001b[A\n","1831it [13:33,  4.34it/s]\u001b[A\n","1832it [13:34,  4.34it/s]\u001b[A\n","1833it [13:34,  4.34it/s]\u001b[A\n","1834it [13:34,  4.34it/s]\u001b[A\n","1835it [13:34,  4.34it/s]\u001b[A\n","1836it [13:34,  4.34it/s]\u001b[A\n","1837it [13:35,  4.34it/s]\u001b[A\n","1838it [13:35,  4.34it/s]\u001b[A\n","1839it [13:35,  4.34it/s]\u001b[A\n","1840it [13:35,  4.34it/s]\u001b[A\n","1841it [13:36,  4.34it/s]\u001b[A\n","1842it [13:36,  4.34it/s]\u001b[A\n","1843it [13:36,  4.34it/s]\u001b[A\n","1844it [13:36,  4.34it/s]\u001b[A\n","1845it [13:37,  4.34it/s]\u001b[A\n","1846it [13:37,  4.34it/s]\u001b[A\n","1847it [13:37,  4.34it/s]\u001b[A\n","1848it [13:37,  4.34it/s]\u001b[A\n","1849it [13:37,  4.34it/s]\u001b[A\n","Epoch: 100% 250/250 [14:20<00:00,  3.44s/it]\n","03/03/2022 05:21:16 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/03/2022 05:21:31 - INFO - __main__ -   *** Validate ***\n","\n","1851it [13:52,  4.48s/it]\u001b[A\n","1852it [13:52,  3.20s/it]\u001b[A\n","1853it [13:53,  2.31s/it]\u001b[A\n","1854it [13:53,  1.69s/it]\u001b[A\n","1855it [13:53,  1.25s/it]\u001b[A\n","1856it [13:53,  1.06it/s]\u001b[A\n","1857it [13:53,  1.37it/s]\u001b[A\n","1858it [13:54,  1.72it/s]\u001b[A\n","1859it [13:54,  2.10it/s]\u001b[A\n","1860it [13:54,  2.49it/s]\u001b[A\n","1861it [13:54,  2.85it/s]\u001b[A\n","1862it [13:55,  3.18it/s]\u001b[A\n","1863it [13:55,  3.45it/s]\u001b[A\n","1864it [13:55,  3.68it/s]\u001b[A\n","1865it [13:55,  3.85it/s]\u001b[A\n","1866it [13:56,  3.99it/s]\u001b[A\n","1867it [13:56,  4.08it/s]\u001b[A\n","1868it [13:56,  4.15it/s]\u001b[A\n","1869it [13:56,  4.21it/s]\u001b[A\n","1870it [13:56,  4.24it/s]\u001b[A\n","1871it [13:57,  4.27it/s]\u001b[A\n","1872it [13:57,  4.29it/s]\u001b[A\n","1873it [13:57,  4.30it/s]\u001b[A\n","1874it [13:57,  4.31it/s]\u001b[A\n","1875it [13:58,  4.32it/s]\u001b[A\n","1876it [13:58,  4.33it/s]\u001b[A\n","1877it [13:58,  4.33it/s]\u001b[A\n","1878it [13:58,  4.33it/s]\u001b[A\n","1879it [13:59,  4.33it/s]\u001b[A\n","1880it [13:59,  4.33it/s]\u001b[A\n","1881it [13:59,  4.33it/s]\u001b[A\n","1882it [13:59,  4.33it/s]\u001b[A\n","1883it [13:59,  4.33it/s]\u001b[A\n","1884it [14:00,  4.33it/s]\u001b[A\n","1885it [14:00,  4.33it/s]\u001b[A\n","1886it [14:00,  4.33it/s]\u001b[A\n","1887it [14:00,  4.33it/s]\u001b[A\n","1888it [14:01,  4.33it/s]\u001b[A\n","1889it [14:01,  4.33it/s]\u001b[A\n","1890it [14:01,  4.33it/s]\u001b[A\n","1891it [14:01,  4.33it/s]\u001b[A\n","1892it [14:02,  4.33it/s]\u001b[A\n","1893it [14:02,  4.34it/s]\u001b[A\n","1894it [14:02,  4.33it/s]\u001b[A\n","1895it [14:02,  4.34it/s]\u001b[A\n","1896it [14:02,  4.34it/s]\u001b[A\n","1897it [14:03,  4.33it/s]\u001b[A\n","1898it [14:03,  4.33it/s]\u001b[A\n","1899it [14:03,  4.33it/s]\u001b[A\n","1900it [14:03,  4.33it/s]\u001b[A\n","1901it [14:04,  4.33it/s]\u001b[A\n","1902it [14:04,  4.33it/s]\u001b[A\n","1903it [14:04,  4.34it/s]\u001b[A\n","1904it [14:04,  4.34it/s]\u001b[A\n","1905it [14:05,  4.34it/s]\u001b[A\n","1906it [14:05,  4.34it/s]\u001b[A\n","1907it [14:05,  4.34it/s]\u001b[A\n","1908it [14:05,  4.34it/s]\u001b[A\n","1909it [14:05,  4.34it/s]\u001b[A\n","1910it [14:06,  4.33it/s]\u001b[A\n","1911it [14:06,  4.33it/s]\u001b[A\n","1912it [14:06,  4.33it/s]\u001b[A\n","1913it [14:06,  4.33it/s]\u001b[A\n","1914it [14:07,  4.33it/s]\u001b[A\n","1915it [14:07,  4.33it/s]\u001b[A\n","1916it [14:07,  4.33it/s]\u001b[A\n","1917it [14:07,  4.33it/s]\u001b[A\n","1918it [14:08,  4.33it/s]\u001b[A\n","1919it [14:08,  4.33it/s]\u001b[A\n","1920it [14:08,  4.33it/s]\u001b[A\n","1921it [14:08,  4.33it/s]\u001b[A\n","1922it [14:08,  4.33it/s]\u001b[A\n","1923it [14:09,  4.34it/s]\u001b[A\n","1924it [14:09,  4.34it/s]\u001b[A\n","1925it [14:09,  4.33it/s]\u001b[A\n","1926it [14:09,  4.33it/s]\u001b[A\n","1927it [14:10,  4.33it/s]\u001b[A\n","1928it [14:10,  4.33it/s]\u001b[A\n","1929it [14:10,  4.33it/s]\u001b[A\n","1930it [14:10,  4.33it/s]\u001b[A\n","1931it [14:11,  4.33it/s]\u001b[A\n","1932it [14:11,  4.33it/s]\u001b[A\n","1933it [14:11,  4.33it/s]\u001b[A\n","1934it [14:11,  4.33it/s]\u001b[A\n","1935it [14:11,  4.33it/s]\u001b[A\n","1936it [14:12,  4.33it/s]\u001b[A\n","1937it [14:12,  4.33it/s]\u001b[A\n","1938it [14:12,  4.33it/s]\u001b[A\n","1939it [14:12,  4.34it/s]\u001b[A\n","1940it [14:13,  4.34it/s]\u001b[A\n","1941it [14:13,  4.34it/s]\u001b[A\n","1942it [14:13,  4.34it/s]\u001b[A\n","1943it [14:13,  4.34it/s]\u001b[A\n","1944it [14:14,  4.34it/s]\u001b[A\n","1945it [14:14,  4.34it/s]\u001b[A\n","1946it [14:14,  4.34it/s]\u001b[A\n","1947it [14:14,  4.34it/s]\u001b[A\n","1948it [14:14,  4.34it/s]\u001b[A\n","1949it [14:15,  4.34it/s]\u001b[A\n","1950it [14:15,  4.34it/s]\u001b[A\n","1951it [14:15,  4.33it/s]\u001b[A\n","1952it [14:15,  4.34it/s]\u001b[A\n","1953it [14:16,  4.34it/s]\u001b[A\n","1954it [14:16,  4.34it/s]\u001b[A\n","1955it [14:16,  4.34it/s]\u001b[A\n","1956it [14:16,  4.34it/s]\u001b[A\n","1957it [14:17,  4.34it/s]\u001b[A\n","1958it [14:17,  4.34it/s]\u001b[A\n","1959it [14:17,  4.33it/s]\u001b[A\n","1960it [14:17,  4.33it/s]\u001b[A\n","1961it [14:17,  4.33it/s]\u001b[A\n","1962it [14:18,  4.33it/s]\u001b[A\n","1963it [14:18,  4.34it/s]\u001b[A\n","1964it [14:18,  4.34it/s]\u001b[A\n","1965it [14:18,  4.34it/s]\u001b[A\n","1966it [14:19,  4.34it/s]\u001b[A\n","1967it [14:19,  4.34it/s]\u001b[A\n","1968it [14:19,  4.34it/s]\u001b[A\n","1969it [14:19,  4.34it/s]\u001b[A\n","1970it [14:20,  4.34it/s]\u001b[A\n","1971it [14:20,  4.34it/s]\u001b[A\n","1972it [14:20,  4.34it/s]\u001b[A\n","1973it [14:20,  4.34it/s]\u001b[A\n","1974it [14:20,  4.34it/s]\u001b[A\n","1975it [14:21,  4.34it/s]\u001b[A\n","1976it [14:21,  4.34it/s]\u001b[A\n","1977it [14:21,  4.33it/s]\u001b[A\n","1978it [14:21,  4.33it/s]\u001b[A\n","1979it [14:22,  4.33it/s]\u001b[A\n","1980it [14:22,  4.33it/s]\u001b[A\n","1981it [14:22,  4.33it/s]\u001b[A\n","1982it [14:22,  4.33it/s]\u001b[A\n","1983it [14:23,  4.33it/s]\u001b[A\n","1984it [14:23,  4.33it/s]\u001b[A\n","1985it [14:23,  4.34it/s]\u001b[A\n","1986it [14:23,  4.34it/s]\u001b[A\n","1987it [14:23,  4.34it/s]\u001b[A\n","1988it [14:24,  4.34it/s]\u001b[A\n","1989it [14:24,  4.34it/s]\u001b[A\n","1990it [14:24,  4.34it/s]\u001b[A\n","1991it [14:24,  4.34it/s]\u001b[A\n","1992it [14:25,  4.34it/s]\u001b[A\n","1993it [14:25,  4.34it/s]\u001b[A\n","1994it [14:25,  4.34it/s]\u001b[A\n","1995it [14:25,  4.34it/s]\u001b[A\n","1996it [14:26,  4.34it/s]\u001b[A\n","1997it [14:26,  4.34it/s]\u001b[A\n","1998it [14:26,  4.34it/s]\u001b[A\n","1999it [14:26,  4.34it/s]\u001b[A\n","2000it [14:26,  4.33it/s]\u001b[A\n","2001it [14:27,  4.33it/s]\u001b[A\n","2002it [14:27,  4.34it/s]\u001b[A\n","2003it [14:27,  4.34it/s]\u001b[A\n","2004it [14:27,  4.34it/s]\u001b[A\n","2005it [14:28,  4.34it/s]\u001b[A\n","2006it [14:28,  4.34it/s]\u001b[A\n","2007it [14:28,  4.34it/s]\u001b[A\n","2008it [14:28,  4.34it/s]\u001b[A\n","2009it [14:29,  4.34it/s]\u001b[A\n","2010it [14:29,  4.34it/s]\u001b[A\n","2011it [14:29,  4.34it/s]\u001b[A\n","2012it [14:29,  4.34it/s]\u001b[A\n","2013it [14:29,  4.34it/s]\u001b[A\n","2014it [14:30,  4.34it/s]\u001b[A\n","2015it [14:30,  4.34it/s]\u001b[A\n","2016it [14:30,  4.34it/s]\u001b[A\n","2017it [14:30,  4.34it/s]\u001b[A\n","2018it [14:31,  4.34it/s]\u001b[A\n","2019it [14:31,  4.34it/s]\u001b[A\n","2020it [14:31,  4.34it/s]\u001b[A\n","2021it [14:31,  4.34it/s]\u001b[A\n","2022it [14:32,  4.34it/s]\u001b[A\n","2023it [14:32,  4.34it/s]\u001b[A\n","2024it [14:32,  4.34it/s]\u001b[A\n","2025it [14:32,  4.34it/s]\u001b[A\n","2026it [14:32,  4.34it/s]\u001b[A\n","2027it [14:33,  4.33it/s]\u001b[A\n","2028it [14:33,  4.33it/s]\u001b[A\n","2029it [14:33,  4.33it/s]\u001b[A\n","2030it [14:33,  4.33it/s]\u001b[A\n","2031it [14:34,  4.34it/s]\u001b[A\n","2032it [14:34,  4.34it/s]\u001b[A\n","2033it [14:34,  4.34it/s]\u001b[A\n","2034it [14:34,  4.34it/s]\u001b[A\n","2035it [14:35,  4.34it/s]\u001b[A03/03/2022 05:22:13 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/03/2022 05:22:13 - INFO - __main__ -     eval_loss = 0.9362032413482666\n","03/03/2022 05:22:13 - INFO - __main__ -     eval_auroc = 0.9481249451637268\n","03/03/2022 05:22:13 - INFO - __main__ -     eval_recall = 0.7599999904632568\n","03/03/2022 05:22:13 - INFO - __main__ -     eval_f1 = 0.7450980544090271\n","03/03/2022 05:22:13 - INFO - root -   *** Test ***\n","\n","2036it [14:35,  4.23it/s]\u001b[A\n","2037it [14:35,  4.26it/s]\u001b[A\n","2038it [14:35,  4.28it/s]\u001b[A\n","2039it [14:35,  4.30it/s]\u001b[A\n","2040it [14:36,  4.31it/s]\u001b[A\n","2041it [14:36,  4.31it/s]\u001b[A\n","2042it [14:36,  4.32it/s]\u001b[A\n","2043it [14:36,  4.33it/s]\u001b[A\n","2044it [14:37,  4.33it/s]\u001b[A\n","2045it [14:37,  4.33it/s]\u001b[A\n","2046it [14:37,  4.33it/s]\u001b[A\n","2047it [14:37,  4.34it/s]\u001b[A\n","2048it [14:38,  4.34it/s]\u001b[A\n","2049it [14:38,  4.34it/s]\u001b[A\n","2050it [14:38,  4.34it/s]\u001b[A\n","2051it [14:38,  4.34it/s]\u001b[A\n","2052it [14:38,  4.33it/s]\u001b[A\n","2053it [14:39,  4.33it/s]\u001b[A\n","2054it [14:39,  4.33it/s]\u001b[A\n","2055it [14:39,  4.34it/s]\u001b[A\n","2056it [14:39,  4.34it/s]\u001b[A\n","2057it [14:40,  4.34it/s]\u001b[A\n","2058it [14:40,  4.34it/s]\u001b[A\n","2059it [14:40,  4.34it/s]\u001b[A\n","2060it [14:40,  4.34it/s]\u001b[A\n","2061it [14:41,  4.34it/s]\u001b[A\n","2062it [14:41,  4.33it/s]\u001b[A\n","2063it [14:41,  4.32it/s]\u001b[A\n","2064it [14:41,  4.33it/s]\u001b[A\n","2065it [14:41,  4.33it/s]\u001b[A\n","2066it [14:42,  4.33it/s]\u001b[A\n","2067it [14:42,  4.34it/s]\u001b[A\n","2068it [14:42,  4.34it/s]\u001b[A\n","2069it [14:42,  4.34it/s]\u001b[A\n","2070it [14:43,  4.34it/s]\u001b[A\n","2071it [14:43,  4.34it/s]\u001b[A\n","2072it [14:43,  4.34it/s]\u001b[A\n","2073it [14:43,  4.34it/s]\u001b[A\n","2074it [14:44,  4.34it/s]\u001b[A\n","2075it [14:44,  4.34it/s]\u001b[A\n","2076it [14:44,  4.34it/s]\u001b[A\n","2077it [14:44,  4.34it/s]\u001b[A\n","2078it [14:44,  4.34it/s]\u001b[A\n","2079it [14:45,  4.34it/s]\u001b[A\n","2080it [14:45,  4.34it/s]\u001b[A\n","2081it [14:45,  4.34it/s]\u001b[A\n","2082it [14:45,  4.34it/s]\u001b[A\n","2083it [14:46,  4.33it/s]\u001b[A\n","2084it [14:46,  4.33it/s]\u001b[A\n","2085it [14:46,  4.34it/s]\u001b[A\n","2086it [14:46,  4.34it/s]\u001b[A\n","2087it [14:47,  4.34it/s]\u001b[A\n","2088it [14:47,  4.34it/s]\u001b[A\n","2089it [14:47,  4.34it/s]\u001b[A\n","2090it [14:47,  4.34it/s]\u001b[A\n","2091it [14:47,  4.34it/s]\u001b[A\n","2092it [14:48,  4.34it/s]\u001b[A\n","2093it [14:48,  4.33it/s]\u001b[A\n","2094it [14:48,  4.34it/s]\u001b[A\n","2095it [14:48,  4.34it/s]\u001b[A\n","2096it [14:49,  4.34it/s]\u001b[A\n","2097it [14:49,  4.34it/s]\u001b[A\n","2098it [14:49,  4.34it/s]\u001b[A\n","2099it [14:49,  4.34it/s]\u001b[A\n","2100it [14:50,  4.34it/s]\u001b[A\n","2101it [14:50,  4.34it/s]\u001b[A\n","2102it [14:50,  4.34it/s]\u001b[A\n","2103it [14:50,  4.34it/s]\u001b[A\n","2104it [14:50,  4.34it/s]\u001b[A\n","2105it [14:51,  4.34it/s]\u001b[A\n","2106it [14:51,  4.34it/s]\u001b[A\n","2107it [14:51,  4.34it/s]\u001b[A\n","2108it [14:51,  4.34it/s]\u001b[A\n","2109it [14:52,  4.34it/s]\u001b[A\n","2110it [14:52,  4.34it/s]\u001b[A\n","2111it [14:52,  4.34it/s]\u001b[A\n","2112it [14:52,  4.34it/s]\u001b[A\n","2113it [14:53,  4.34it/s]\u001b[A\n","2114it [14:53,  4.34it/s]\u001b[A\n","2115it [14:53,  4.34it/s]\u001b[A\n","2116it [14:53,  4.34it/s]\u001b[A\n","2117it [14:53,  4.34it/s]\u001b[A\n","2118it [14:54,  4.33it/s]\u001b[A\n","2119it [14:54,  4.33it/s]\u001b[A\n","2120it [14:54,  4.34it/s]\u001b[A\n","2121it [14:54,  4.34it/s]\u001b[A\n","2122it [14:55,  4.34it/s]\u001b[A\n","2123it [14:55,  4.34it/s]\u001b[A\n","2124it [14:55,  4.34it/s]\u001b[A\n","2125it [14:55,  4.34it/s]\u001b[A\n","2126it [14:56,  4.34it/s]\u001b[A\n","2127it [14:56,  4.34it/s]\u001b[A\n","2128it [14:56,  4.34it/s]\u001b[A\n","2129it [14:56,  4.34it/s]\u001b[A\n","2130it [14:56,  4.34it/s]\u001b[A\n","2131it [14:57,  4.34it/s]\u001b[A\n","2132it [14:57,  4.34it/s]\u001b[A\n","2133it [14:57,  4.34it/s]\u001b[A\n","2134it [14:57,  4.34it/s]\u001b[A\n","2135it [14:58,  4.34it/s]\u001b[A\n","2136it [14:58,  4.34it/s]\u001b[A\n","2137it [14:58,  4.34it/s]\u001b[A\n","2138it [14:58,  4.33it/s]\u001b[A\n","2139it [14:59,  4.34it/s]\u001b[A\n","2140it [14:59,  4.33it/s]\u001b[A\n","2141it [14:59,  4.33it/s]\u001b[A\n","2142it [14:59,  4.34it/s]\u001b[A\n","2143it [14:59,  4.34it/s]\u001b[A\n","2144it [15:00,  4.34it/s]\u001b[A\n","2145it [15:00,  4.34it/s]\u001b[A\n","2146it [15:00,  4.34it/s]\u001b[A\n","2147it [15:00,  4.34it/s]\u001b[A\n","2148it [15:01,  4.34it/s]\u001b[A\n","2149it [15:01,  4.34it/s]\u001b[A\n","2150it [15:01,  4.34it/s]\u001b[A\n","2151it [15:01,  4.34it/s]\u001b[A\n","2152it [15:02,  4.34it/s]\u001b[A\n","2153it [15:02,  4.34it/s]\u001b[A\n","2154it [15:02,  4.34it/s]\u001b[A\n","2155it [15:02,  4.34it/s]\u001b[A\n","2156it [15:02,  4.34it/s]\u001b[A\n","2157it [15:03,  4.34it/s]\u001b[A\n","2158it [15:03,  4.34it/s]\u001b[A\n","2159it [15:03,  4.34it/s]\u001b[A\n","2160it [15:03,  4.34it/s]\u001b[A\n","2161it [15:04,  4.34it/s]\u001b[A\n","2162it [15:04,  4.34it/s]\u001b[A\n","2163it [15:04,  4.34it/s]\u001b[A\n","2164it [15:04,  4.34it/s]\u001b[A\n","2165it [15:04,  4.34it/s]\u001b[A\n","2166it [15:05,  4.34it/s]\u001b[A\n","2167it [15:05,  4.34it/s]\u001b[A\n","2168it [15:05,  4.34it/s]\u001b[A\n","2169it [15:05,  4.34it/s]\u001b[A\n","2170it [15:06,  4.34it/s]\u001b[A\n","2171it [15:06,  4.33it/s]\u001b[A\n","2172it [15:06,  4.33it/s]\u001b[A\n","2173it [15:06,  4.33it/s]\u001b[A\n","2174it [15:07,  4.34it/s]\u001b[A\n","2175it [15:07,  4.34it/s]\u001b[A\n","2176it [15:07,  4.34it/s]\u001b[A\n","2177it [15:07,  4.33it/s]\u001b[A\n","2178it [15:07,  4.33it/s]\u001b[A\n","2179it [15:08,  4.34it/s]\u001b[A\n","2180it [15:08,  4.34it/s]\u001b[A\n","2181it [15:08,  4.34it/s]\u001b[A\n","2182it [15:08,  4.33it/s]\u001b[A\n","2183it [15:09,  4.34it/s]\u001b[A\n","2184it [15:09,  4.34it/s]\u001b[A\n","2185it [15:09,  4.34it/s]\u001b[A\n","2186it [15:09,  4.34it/s]\u001b[A\n","2187it [15:10,  4.34it/s]\u001b[A\n","2188it [15:10,  4.34it/s]\u001b[A\n","2189it [15:10,  4.34it/s]\u001b[A\n","2190it [15:10,  4.34it/s]\u001b[A\n","2191it [15:10,  4.34it/s]\u001b[A\n","2192it [15:11,  4.34it/s]\u001b[A\n","2193it [15:11,  4.34it/s]\u001b[A\n","2194it [15:11,  4.34it/s]\u001b[A\n","2195it [15:11,  4.34it/s]\u001b[A\n","2196it [15:12,  4.34it/s]\u001b[A\n","2197it [15:12,  4.34it/s]\u001b[A\n","2198it [15:12,  4.34it/s]\u001b[A\n","2199it [15:12,  4.34it/s]\u001b[A\n","2200it [15:13,  4.34it/s]\u001b[A\n","2201it [15:13,  4.34it/s]\u001b[A\n","2202it [15:13,  4.34it/s]\u001b[A\n","2203it [15:13,  4.34it/s]\u001b[A\n","2204it [15:13,  4.34it/s]\u001b[A\n","2205it [15:14,  4.34it/s]\u001b[A\n","2206it [15:14,  4.34it/s]\u001b[A\n","2207it [15:14,  4.34it/s]\u001b[A\n","2208it [15:14,  4.34it/s]\u001b[A\n","2209it [15:15,  4.34it/s]\u001b[A\n","2210it [15:15,  4.34it/s]\u001b[A\n","2211it [15:15,  4.34it/s]\u001b[A\n","2212it [15:15,  4.34it/s]\u001b[A\n","2213it [15:16,  4.34it/s]\u001b[A\n","2214it [15:16,  4.34it/s]\u001b[A\n","2215it [15:16,  4.34it/s]\u001b[A\n","2216it [15:16,  4.34it/s]\u001b[A\n","2217it [15:16,  4.34it/s]\u001b[A\n","2218it [15:17,  4.33it/s]\u001b[A\n","2219it [15:17,  4.33it/s]\u001b[A\n","2220it [15:17,  4.34it/s]\u001b[A\n","2221it [15:17,  4.34it/s]\u001b[A\n","2222it [15:18,  4.34it/s]\u001b[A\n","2223it [15:18,  4.34it/s]\u001b[A\n","2224it [15:18,  4.34it/s]\u001b[A\n","2225it [15:18,  4.34it/s]\u001b[A\n","2226it [15:19,  4.34it/s]\u001b[A\n","2227it [15:19,  4.34it/s]\u001b[A\n","2228it [15:19,  4.33it/s]\u001b[A\n","2229it [15:19,  4.34it/s]\u001b[A\n","2230it [15:19,  4.34it/s]\u001b[A\n","2231it [15:20,  4.34it/s]\u001b[A\n","2232it [15:20,  4.34it/s]\u001b[A\n","2233it [15:20,  4.34it/s]\u001b[A\n","2234it [15:20,  4.34it/s]\u001b[A\n","2235it [15:21,  4.33it/s]\u001b[A\n","2236it [15:21,  4.34it/s]\u001b[A\n","2237it [15:21,  4.34it/s]\u001b[A\n","2238it [15:21,  4.33it/s]\u001b[A\n","2239it [15:22,  4.33it/s]\u001b[A\n","2240it [15:22,  4.33it/s]\u001b[A\n","2241it [15:22,  4.33it/s]\u001b[A\n","2242it [15:22,  4.34it/s]\u001b[A\n","2243it [15:22,  4.34it/s]\u001b[A\n","2244it [15:23,  4.34it/s]\u001b[A\n","2245it [15:23,  4.34it/s]\u001b[A\n","2246it [15:23,  4.34it/s]\u001b[A\n","2247it [15:23,  4.34it/s]\u001b[A\n","2248it [15:24,  4.33it/s]\u001b[A\n","2249it [15:24,  4.34it/s]\u001b[A\n","2250it [15:24,  4.34it/s]\u001b[A\n","2251it [15:24,  4.34it/s]\u001b[A\n","2252it [15:25,  4.34it/s]\u001b[A\n","2253it [15:25,  4.34it/s]\u001b[A\n","2254it [15:25,  4.34it/s]\u001b[A\n","2255it [15:25,  4.34it/s]\u001b[A\n","2256it [15:25,  4.34it/s]\u001b[A\n","2257it [15:26,  4.34it/s]\u001b[A\n","2258it [15:26,  4.34it/s]\u001b[A\n","2259it [15:26,  4.34it/s]\u001b[A\n","2260it [15:26,  4.34it/s]\u001b[A\n","2261it [15:27,  4.34it/s]\u001b[A\n","2262it [15:27,  4.33it/s]\u001b[A\n","2263it [15:27,  4.33it/s]\u001b[A\n","2264it [15:27,  4.33it/s]\u001b[A\n","2265it [15:28,  4.33it/s]\u001b[A\n","2266it [15:28,  4.33it/s]\u001b[A\n","2267it [15:28,  4.34it/s]\u001b[A\n","2268it [15:28,  4.33it/s]\u001b[A\n","2269it [15:28,  4.33it/s]\u001b[A\n","2270it [15:29,  4.33it/s]\u001b[A\n","2271it [15:29,  4.33it/s]\u001b[A\n","2272it [15:29,  4.33it/s]\u001b[A\n","2273it [15:29,  4.34it/s]\u001b[A\n","2274it [15:30,  4.34it/s]\u001b[A\n","2275it [15:30,  4.34it/s]\u001b[A\n","2276it [15:30,  4.34it/s]\u001b[A\n","2277it [15:30,  4.34it/s]\u001b[A\n","2278it [15:31,  4.34it/s]\u001b[A\n","2279it [15:31,  4.34it/s]\u001b[A\n","2280it [15:31,  4.34it/s]\u001b[A\n","2281it [15:31,  4.34it/s]\u001b[A\n","2282it [15:31,  4.34it/s]\u001b[A\n","2283it [15:32,  4.34it/s]\u001b[A\n","2284it [15:32,  4.34it/s]\u001b[A\n","2285it [15:32,  4.34it/s]\u001b[A\n","2286it [15:32,  4.34it/s]\u001b[A\n","2287it [15:33,  4.34it/s]\u001b[A\n","2288it [15:33,  4.34it/s]\u001b[A\n","2289it [15:33,  4.34it/s]\u001b[A\n","2290it [15:33,  4.34it/s]\u001b[A\n","2291it [15:34,  4.34it/s]\u001b[A\n","2292it [15:34,  4.34it/s]\u001b[A\n","2293it [15:34,  4.34it/s]\u001b[A\n","2294it [15:34,  4.34it/s]\u001b[A\n","2295it [15:34,  4.34it/s]\u001b[A\n","2296it [15:35,  4.34it/s]\u001b[A\n","2297it [15:35,  4.34it/s]\u001b[A\n","2298it [15:35,  4.34it/s]\u001b[A\n","2299it [15:35,  4.34it/s]\u001b[A\n","2300it [15:36,  4.34it/s]\u001b[A\n","2301it [15:36,  4.33it/s]\u001b[A\n","2302it [15:36,  4.33it/s]\u001b[A\n","2303it [15:36,  4.34it/s]\u001b[A\n","2304it [15:37,  4.34it/s]\u001b[A\n","2305it [15:37,  4.33it/s]\u001b[A\n","2306it [15:37,  4.33it/s]\u001b[A\n","2307it [15:37,  4.34it/s]\u001b[A\n","2308it [15:37,  4.34it/s]\u001b[A\n","2309it [15:38,  4.34it/s]\u001b[A\n","2310it [15:38,  4.34it/s]\u001b[A\n","2311it [15:38,  4.34it/s]\u001b[A\n","2312it [15:38,  4.34it/s]\u001b[A\n","2313it [15:39,  4.34it/s]\u001b[A\n","2314it [15:39,  4.33it/s]\u001b[A\n","2315it [15:39,  4.33it/s]\u001b[A\n","2316it [15:39,  4.34it/s]\u001b[A\n","2317it [15:40,  4.34it/s]\u001b[A\n","2318it [15:40,  4.34it/s]\u001b[A\n","2319it [15:40,  4.34it/s]\u001b[A\n","2320it [15:40,  4.34it/s]\u001b[A\n","2321it [15:40,  4.34it/s]\u001b[A\n","2322it [15:41,  4.34it/s]\u001b[A\n","2323it [15:41,  4.34it/s]\u001b[A\n","2324it [15:41,  4.34it/s]\u001b[A\n","2325it [15:41,  4.34it/s]\u001b[A\n","2326it [15:42,  4.34it/s]\u001b[A\n","2327it [15:42,  4.34it/s]\u001b[A\n","2328it [15:42,  4.34it/s]\u001b[A\n","2329it [15:42,  4.34it/s]\u001b[A\n","2330it [15:43,  4.34it/s]\u001b[A\n","2331it [15:43,  4.34it/s]\u001b[A\n","2332it [15:43,  4.34it/s]\u001b[A\n","2333it [15:43,  4.34it/s]\u001b[A\n","2334it [15:43,  4.34it/s]\u001b[A\n","2335it [15:44,  4.34it/s]\u001b[A\n","2336it [15:44,  4.34it/s]\u001b[A\n","2337it [15:44,  4.34it/s]\u001b[A\n","2338it [15:44,  4.34it/s]\u001b[A\n","2339it [15:45,  4.34it/s]\u001b[A\n","2340it [15:45,  4.34it/s]\u001b[A\n","2341it [15:45,  4.34it/s]\u001b[A\n","2342it [15:45,  4.34it/s]\u001b[A\n","2343it [15:46,  4.34it/s]\u001b[A\n","2344it [15:46,  4.34it/s]\u001b[A\n","2345it [15:46,  4.34it/s]\u001b[A\n","2346it [15:46,  4.34it/s]\u001b[A\n","2347it [15:46,  4.34it/s]\u001b[A\n","2348it [15:47,  4.34it/s]\u001b[A\n","2349it [15:47,  4.34it/s]\u001b[A\n","2350it [15:47,  4.34it/s]\u001b[A\n","2351it [15:47,  4.34it/s]\u001b[A\n","2352it [15:48,  4.34it/s]\u001b[A\n","2353it [15:48,  4.34it/s]\u001b[A\n","2354it [15:48,  4.34it/s]\u001b[A03/03/2022 05:23:27 - INFO - __main__ -   ***** Test results spoilers *****\n","03/03/2022 05:23:27 - INFO - __main__ -     eval_loss = 1.2973353862762451\n","03/03/2022 05:23:27 - INFO - __main__ -     eval_auroc = 0.9113700985908508\n","03/03/2022 05:23:27 - INFO - __main__ -     eval_recall = 0.625\n","03/03/2022 05:23:27 - INFO - __main__ -     eval_f1 = 0.5263157486915588\n","03/03/2022 05:23:27 - INFO - filelock -   Lock 139631531263568 acquired on log.lock\n","03/03/2022 05:23:27 - INFO - filelock -   Lock 139631531263568 released on log.lock\n","2354it [15:48,  2.48it/s]\n"]}],"source":["\n","!source env/bin/activate; bash run_multiple.sh"]},{"cell_type":"code","source":["#yes_no: 89.5 (3.3) -> TEMPLATE=\"*cls**sent_0*._Spoiler?*mask*.*sep+*\" MAPPING=\"{0:'No',1:'Yes'}\"\n","!source env/bin/activate; python tools/gather_result.py --condition \"{'tag': 'relevant', 'task_name': 'spoilers', 'few_shot_type': 'prompt-demo'}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2FhTELXRFXq","executionInfo":{"status":"ok","timestamp":1650274099480,"user_tz":420,"elapsed":2639,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"}},"outputId":"69e4b400-59b7-47d2-8a5b-1fa77226fcb7"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["21-21: best dev (0.85511) test (0.87201) test2 (0.79139) | total trials: 2\n","    | gradient_accumulation_steps: 1 | learning_rate: 2e-05 | per_device_train_batch_size: 4 | eval_steps: 100 | max_steps: 1000 \n","mean +- std: 87.2 (0.0) (median 87.2)second metric: 79.1 (0.0) (median 79.1)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70548,"status":"ok","timestamp":1646136583679,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"},"user_tz":480},"id":"xzB-ceTHXuqW","outputId":"a38c3e87-d96e-426f-fa44-72cb98752e0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["spoilers-16-42-train\n","0it [00:00, ?it/s]| GABI USING HACKS\n","32it [00:00, 41.33it/s]\n","spoilers-16-42-dev\n","0it [00:00, ?it/s]| NO\n","185it [00:04, 42.41it/s]\n","spoilers-16-13-train\n","0it [00:00, ?it/s]| its Joe titan\n","32it [00:00, 42.19it/s]\n","spoilers-16-13-dev\n","0it [00:00, ?it/s]| Cart titan X ???\n","185it [00:04, 42.07it/s]\n","spoilers-16-21-train\n","0it [00:00, ?it/s]| gabi sniper ez\n","32it [00:00, 41.84it/s]\n","spoilers-16-21-dev\n","0it [00:00, ?it/s]| Guys, why goth mikasa was shown?\n","185it [00:04, 43.03it/s]\n","spoilers-16-87-train\n","0it [00:00, ?it/s]| Well for manga readers like me\n","32it [00:00, 42.66it/s]\n","spoilers-16-87-dev\n","0it [00:00, ?it/s]| Right?\n","185it [00:04, 42.70it/s]\n","spoilers-16-100-train\n","0it [00:00, ?it/s]| got monkey fall in this ep\n","32it [00:00, 42.71it/s]\n","spoilers-16-100-dev\n","0it [00:00, ?it/s]| if only the beast titan animation was at a higher frame rate\n","185it [00:04, 42.99it/s]\n","spoilers-16-42-train\n","0it [00:00, ?it/s]| GABI USING HACKS\n","32it [00:00, 41.09it/s]\n","spoilers-16-42-dev\n","0it [00:00, ?it/s]| NO\n","185it [00:04, 42.97it/s]\n","spoilers-16-42-test\n","0it [00:00, ?it/s]| eren's really battering him\n","319it [00:07, 42.49it/s]\n"]}],"source":["!source env/bin/activate; bash tools/get_sbert_embedding.sh roberta-large"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1332114,"status":"ok","timestamp":1646137915761,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"},"user_tz":480},"id":"wGcMn0t3xDzj","outputId":"03503d31-88ab-4adc-9478-99c827de4940"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/01/2022 12:09:47 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/01/2022 12:09:47 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-demo-16-42-roberta-large-28525', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar01_12-09-47_b710c6a7fbc6', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-demo-16-42-roberta-large-28525', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","03/01/2022 12:09:47 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/01/2022 12:09:47 - INFO - __main__ -   Automatically convert the template to using demonstrations.\n","03/01/2022 12:09:47 - INFO - __main__ -   | *cls**sent_0*.*mask*_a_spoiler.*sep+* => *cls**sent_0*.*mask*_a_spoiler.*sep+**sent_1*.*label_0*_a_spoiler.*sep+**sent_2*.*label_1*_a_spoiler.*sep+*\n","03/01/2022 12:09:50 - INFO - src.dataset -   Use demonstrations\n","03/01/2022 12:09:50 - INFO - src.dataset -   Label 0 to word ĠNot (1491)\n","03/01/2022 12:09:50 - INFO - src.dataset -   Label 1 to word ĠIs (1534)\n","03/01/2022 12:09:50 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/01/2022 12:09:50 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-42\n","03/01/2022 12:09:50 - INFO - filelock -   Lock 140067048414032 acquired on data/k-shot-10x/spoilers/16-42/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/01/2022 12:09:50 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-42/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/01/2022 12:09:50 - INFO - filelock -   Lock 140067048414032 released on data/k-shot-10x/spoilers/16-42/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/01/2022 12:09:50 - INFO - src.dataset -   Load embeddings (for demonstration filtering) from data/k-shot-10x/spoilers/16-42/train_sbert-roberta-large.npy\n","03/01/2022 12:09:51 - INFO - src.dataset -   Use demonstrations\n","03/01/2022 12:09:51 - INFO - src.dataset -   Label 0 to word ĠNot (1491)\n","03/01/2022 12:09:51 - INFO - src.dataset -   Label 1 to word ĠIs (1534)\n","03/01/2022 12:09:51 - INFO - src.dataset -   Total num_sample for mode dev: 16\n","03/01/2022 12:09:51 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-42\n","03/01/2022 12:09:51 - INFO - filelock -   Lock 140066785656464 acquired on data/k-shot-10x/spoilers/16-42/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/01/2022 12:09:51 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-42/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/01/2022 12:09:51 - INFO - filelock -   Lock 140066785656464 released on data/k-shot-10x/spoilers/16-42/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/01/2022 12:09:51 - INFO - src.dataset -   Load embeddings (for demonstration filtering) from data/k-shot-10x/spoilers/16-42/dev_sbert-roberta-large.npy\n","03/01/2022 12:09:58 - INFO - src.dataset -   *** Example ***\n","03/01/2022 12:09:58 - INFO - src.dataset -   guid: dev-0\n","03/01/2022 12:09:58 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 13449, 4, 50264, 10, 32007, 4, 2, 3084, 4, 1491, 10, 32007, 4, 2, 571, 10810, 31985, 364, 329, 4, 1534, 10, 32007, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[3], label_word_list=None)\n","03/01/2022 12:09:58 - INFO - src.dataset -   text: <s>NO.<mask> a spoiler.</s>No. Not a spoiler.</s>gabi sniper ez. Is a spoiler.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","03/01/2022 12:10:02 - INFO - src.dataset -   Use demonstrations\n","03/01/2022 12:10:02 - INFO - src.dataset -   Label 0 to word ĠNot (1491)\n","03/01/2022 12:10:02 - INFO - src.dataset -   Label 1 to word ĠIs (1534)\n","03/01/2022 12:10:02 - INFO - src.dataset -   Total num_sample for mode test: 16\n","03/01/2022 12:10:02 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-42\n","03/01/2022 12:10:02 - INFO - filelock -   Lock 140066798478032 acquired on data/k-shot-10x/spoilers/16-42/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/01/2022 12:10:02 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-42/cached_test_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/01/2022 12:10:02 - INFO - filelock -   Lock 140066798478032 released on data/k-shot-10x/spoilers/16-42/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/01/2022 12:10:02 - INFO - src.dataset -   Load embeddings (for demonstration filtering) from data/k-shot-10x/spoilers/16-42/test_sbert-roberta-large.npy\n","03/01/2022 12:10:15 - INFO - src.dataset -   *** Example ***\n","03/01/2022 12:10:15 - INFO - src.dataset -   guid: test-0\n","03/01/2022 12:10:15 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 8663, 18, 269, 15867, 154, 123, 4, 50264, 10, 32007, 4, 2, 3084, 4, 1491, 10, 32007, 4, 2, 133, 15345, 13515, 1253, 50, 9727, 39996, 8, 15345, 39996, 4, 1534, 10, 32007, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[8], label_word_list=None)\n","03/01/2022 12:10:15 - INFO - src.dataset -   text: <s>eren's really battering him.<mask> a spoiler.</s>No. Not a spoiler.</s>The jaw titans or cart titan and jaw titan. Is a spoiler.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/01/2022 12:10:36 - INFO - src.trainer -   ***** Running training *****\n","03/01/2022 12:10:36 - INFO - src.trainer -     Num examples = 32\n","03/01/2022 12:10:36 - INFO - src.trainer -     Num Epochs = 63\n","03/01/2022 12:10:36 - INFO - src.trainer -     Instantaneous batch size per device = 2\n","03/01/2022 12:10:36 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 2\n","03/01/2022 12:10:36 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/01/2022 12:10:36 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 6/63 [00:29<04:35,  4.83s/it]\n","  0% 0/185 [00:00<?, ?it/s]\u001b[A\n","  1% 2/185 [00:00<00:31,  5.78it/s]\u001b[A\n","  2% 3/185 [00:00<00:41,  4.43it/s]\u001b[A\n","  2% 4/185 [00:01<00:47,  3.82it/s]\u001b[A\n","  3% 5/185 [00:01<00:51,  3.48it/s]\u001b[A\n","  3% 6/185 [00:01<00:54,  3.28it/s]\u001b[A\n","  4% 7/185 [00:02<00:56,  3.15it/s]\u001b[A\n","  4% 8/185 [00:02<00:57,  3.06it/s]\u001b[A\n","  5% 9/185 [00:02<00:58,  3.00it/s]\u001b[A\n","  5% 10/185 [00:03<00:59,  2.96it/s]\u001b[A\n","  6% 11/185 [00:03<00:59,  2.92it/s]\u001b[A\n","  6% 12/185 [00:03<00:59,  2.89it/s]\u001b[A\n","  7% 13/185 [00:04<00:59,  2.89it/s]\u001b[A\n","  8% 14/185 [00:04<00:59,  2.88it/s]\u001b[A\n","  8% 15/185 [00:04<00:59,  2.88it/s]\u001b[A\n","  9% 16/185 [00:05<00:58,  2.87it/s]\u001b[A\n","  9% 17/185 [00:05<00:58,  2.87it/s]\u001b[A\n"," 10% 18/185 [00:05<00:58,  2.87it/s]\u001b[A\n"," 10% 19/185 [00:06<00:57,  2.87it/s]\u001b[A\n"," 11% 20/185 [00:06<00:57,  2.86it/s]\u001b[A\n"," 11% 21/185 [00:06<00:57,  2.87it/s]\u001b[A\n"," 12% 22/185 [00:07<00:57,  2.86it/s]\u001b[A\n"," 12% 23/185 [00:07<00:57,  2.84it/s]\u001b[A\n"," 13% 24/185 [00:08<00:56,  2.84it/s]\u001b[A\n"," 14% 25/185 [00:08<00:56,  2.85it/s]\u001b[A\n"," 14% 26/185 [00:08<00:55,  2.84it/s]\u001b[A\n"," 15% 27/185 [00:09<00:55,  2.85it/s]\u001b[A\n"," 15% 28/185 [00:09<00:55,  2.84it/s]\u001b[A\n"," 16% 29/185 [00:09<00:54,  2.84it/s]\u001b[A\n"," 16% 30/185 [00:10<00:54,  2.84it/s]\u001b[A\n"," 17% 31/185 [00:10<00:54,  2.84it/s]\u001b[A\n"," 17% 32/185 [00:10<00:53,  2.84it/s]\u001b[A\n"," 18% 33/185 [00:11<00:53,  2.84it/s]\u001b[A\n"," 18% 34/185 [00:11<00:53,  2.82it/s]\u001b[A\n"," 19% 35/185 [00:11<00:53,  2.81it/s]\u001b[A\n"," 19% 36/185 [00:12<00:52,  2.82it/s]\u001b[A\n"," 20% 37/185 [00:12<00:52,  2.82it/s]\u001b[A\n"," 21% 38/185 [00:12<00:52,  2.82it/s]\u001b[A\n"," 21% 39/185 [00:13<00:51,  2.81it/s]\u001b[A\n"," 22% 40/185 [00:13<00:51,  2.82it/s]\u001b[A\n"," 22% 41/185 [00:14<00:51,  2.81it/s]\u001b[A\n"," 23% 42/185 [00:14<00:50,  2.81it/s]\u001b[A\n"," 23% 43/185 [00:14<00:50,  2.81it/s]\u001b[A\n"," 24% 44/185 [00:15<00:50,  2.82it/s]\u001b[A\n"," 24% 45/185 [00:15<00:49,  2.81it/s]\u001b[A\n"," 25% 46/185 [00:15<00:49,  2.79it/s]\u001b[A\n"," 25% 47/185 [00:16<00:49,  2.79it/s]\u001b[A\n"," 26% 48/185 [00:16<00:48,  2.80it/s]\u001b[A\n"," 26% 49/185 [00:16<00:48,  2.80it/s]\u001b[A\n"," 27% 50/185 [00:17<00:48,  2.79it/s]\u001b[A\n"," 28% 51/185 [00:17<00:47,  2.80it/s]\u001b[A\n"," 28% 52/185 [00:17<00:47,  2.80it/s]\u001b[A\n"," 29% 53/185 [00:18<00:47,  2.79it/s]\u001b[A\n"," 29% 54/185 [00:18<00:46,  2.79it/s]\u001b[A\n"," 30% 55/185 [00:19<00:46,  2.80it/s]\u001b[A\n"," 30% 56/185 [00:19<00:46,  2.80it/s]\u001b[A\n"," 31% 57/185 [00:19<00:45,  2.79it/s]\u001b[A\n"," 31% 58/185 [00:20<00:45,  2.78it/s]\u001b[A\n"," 32% 59/185 [00:20<00:45,  2.77it/s]\u001b[A\n"," 32% 60/185 [00:20<00:45,  2.78it/s]\u001b[A\n"," 33% 61/185 [00:21<00:44,  2.77it/s]\u001b[A\n"," 34% 62/185 [00:21<00:44,  2.77it/s]\u001b[A\n"," 34% 63/185 [00:21<00:43,  2.78it/s]\u001b[A\n"," 35% 64/185 [00:22<00:43,  2.77it/s]\u001b[A\n"," 35% 65/185 [00:22<00:43,  2.77it/s]\u001b[A\n"," 36% 66/185 [00:23<00:42,  2.77it/s]\u001b[A\n"," 36% 67/185 [00:23<00:42,  2.77it/s]\u001b[A\n"," 37% 68/185 [00:23<00:42,  2.77it/s]\u001b[A\n"," 37% 69/185 [00:24<00:42,  2.76it/s]\u001b[A\n"," 38% 70/185 [00:24<00:41,  2.75it/s]\u001b[A\n"," 38% 71/185 [00:24<00:41,  2.75it/s]\u001b[A\n"," 39% 72/185 [00:25<00:41,  2.76it/s]\u001b[A\n"," 39% 73/185 [00:25<00:40,  2.75it/s]\u001b[A\n"," 40% 74/185 [00:25<00:40,  2.75it/s]\u001b[A\n"," 41% 75/185 [00:26<00:40,  2.75it/s]\u001b[A\n"," 41% 76/185 [00:26<00:39,  2.75it/s]\u001b[A\n"," 42% 77/185 [00:27<00:39,  2.75it/s]\u001b[A\n"," 42% 78/185 [00:27<00:38,  2.74it/s]\u001b[A\n"," 43% 79/185 [00:27<00:38,  2.74it/s]\u001b[A\n"," 43% 80/185 [00:28<00:38,  2.73it/s]\u001b[A\n"," 44% 81/185 [00:28<00:38,  2.71it/s]\u001b[A\n"," 44% 82/185 [00:28<00:37,  2.72it/s]\u001b[A\n"," 45% 83/185 [00:29<00:37,  2.73it/s]\u001b[A\n"," 45% 84/185 [00:29<00:37,  2.73it/s]\u001b[A\n"," 46% 85/185 [00:29<00:36,  2.73it/s]\u001b[A\n"," 46% 86/185 [00:30<00:36,  2.73it/s]\u001b[A\n"," 47% 87/185 [00:30<00:35,  2.73it/s]\u001b[A\n"," 48% 88/185 [00:31<00:35,  2.73it/s]\u001b[A\n"," 48% 89/185 [00:31<00:35,  2.72it/s]\u001b[A\n"," 49% 90/185 [00:31<00:34,  2.73it/s]\u001b[A\n"," 49% 91/185 [00:32<00:34,  2.73it/s]\u001b[A\n"," 50% 92/185 [00:32<00:34,  2.71it/s]\u001b[A\n"," 50% 93/185 [00:32<00:34,  2.70it/s]\u001b[A\n"," 51% 94/185 [00:33<00:33,  2.70it/s]\u001b[A\n"," 51% 95/185 [00:33<00:33,  2.71it/s]\u001b[A\n"," 52% 96/185 [00:34<00:32,  2.71it/s]\u001b[A\n"," 52% 97/185 [00:34<00:32,  2.70it/s]\u001b[A\n"," 53% 98/185 [00:34<00:32,  2.71it/s]\u001b[A\n"," 54% 99/185 [00:35<00:31,  2.70it/s]\u001b[A\n"," 54% 100/185 [00:35<00:31,  2.70it/s]\u001b[A\n"," 55% 101/185 [00:35<00:31,  2.71it/s]\u001b[A\n"," 55% 102/185 [00:36<00:30,  2.71it/s]\u001b[A\n"," 56% 103/185 [00:36<00:30,  2.70it/s]\u001b[A\n"," 56% 104/185 [00:36<00:30,  2.68it/s]\u001b[A\n"," 57% 105/185 [00:37<00:29,  2.68it/s]\u001b[A\n"," 57% 106/185 [00:37<00:29,  2.69it/s]\u001b[A\n"," 58% 107/185 [00:38<00:28,  2.69it/s]\u001b[A\n"," 58% 108/185 [00:38<00:28,  2.69it/s]\u001b[A\n"," 59% 109/185 [00:38<00:28,  2.69it/s]\u001b[A\n"," 59% 110/185 [00:39<00:27,  2.69it/s]\u001b[A\n"," 60% 111/185 [00:39<00:27,  2.69it/s]\u001b[A\n"," 61% 112/185 [00:39<00:27,  2.68it/s]\u001b[A\n"," 61% 113/185 [00:40<00:26,  2.68it/s]\u001b[A\n"," 62% 114/185 [00:40<00:26,  2.69it/s]\u001b[A\n"," 62% 115/185 [00:41<00:26,  2.67it/s]\u001b[A\n"," 63% 116/185 [00:41<00:25,  2.66it/s]\u001b[A\n"," 63% 117/185 [00:41<00:25,  2.66it/s]\u001b[A\n"," 64% 118/185 [00:42<00:25,  2.68it/s]\u001b[A\n"," 64% 119/185 [00:42<00:24,  2.67it/s]\u001b[A\n"," 65% 120/185 [00:42<00:24,  2.67it/s]\u001b[A\n"," 65% 121/185 [00:43<00:23,  2.67it/s]\u001b[A\n"," 66% 122/185 [00:43<00:23,  2.67it/s]\u001b[A\n"," 66% 123/185 [00:44<00:23,  2.66it/s]\u001b[A\n"," 67% 124/185 [00:44<00:22,  2.67it/s]\u001b[A\n"," 68% 125/185 [00:44<00:22,  2.66it/s]\u001b[A\n"," 68% 126/185 [00:45<00:22,  2.66it/s]\u001b[A\n"," 69% 127/185 [00:45<00:21,  2.64it/s]\u001b[A\n"," 69% 128/185 [00:45<00:21,  2.64it/s]\u001b[A\n"," 70% 129/185 [00:46<00:21,  2.64it/s]\u001b[A\n"," 70% 130/185 [00:46<00:20,  2.65it/s]\u001b[A\n"," 71% 131/185 [00:47<00:20,  2.65it/s]\u001b[A\n"," 71% 132/185 [00:47<00:20,  2.64it/s]\u001b[A\n"," 72% 133/185 [00:47<00:19,  2.64it/s]\u001b[A\n"," 72% 134/185 [00:48<00:19,  2.64it/s]\u001b[A\n"," 73% 135/185 [00:48<00:18,  2.64it/s]\u001b[A\n"," 74% 136/185 [00:49<00:18,  2.65it/s]\u001b[A\n"," 74% 137/185 [00:49<00:17,  2.67it/s]\u001b[A\n"," 75% 138/185 [00:49<00:17,  2.67it/s]\u001b[A\n"," 75% 139/185 [00:50<00:17,  2.66it/s]\u001b[A\n"," 76% 140/185 [00:50<00:16,  2.67it/s]\u001b[A\n"," 76% 141/185 [00:50<00:16,  2.68it/s]\u001b[A\n"," 77% 142/185 [00:51<00:16,  2.67it/s]\u001b[A\n"," 77% 143/185 [00:51<00:15,  2.68it/s]\u001b[A\n"," 78% 144/185 [00:51<00:15,  2.68it/s]\u001b[A\n"," 78% 145/185 [00:52<00:14,  2.68it/s]\u001b[A\n"," 79% 146/185 [00:52<00:14,  2.69it/s]\u001b[A\n"," 79% 147/185 [00:53<00:14,  2.68it/s]\u001b[A\n"," 80% 148/185 [00:53<00:13,  2.68it/s]\u001b[A\n"," 81% 149/185 [00:53<00:13,  2.68it/s]\u001b[A\n"," 81% 150/185 [00:54<00:13,  2.66it/s]\u001b[A\n"," 82% 151/185 [00:54<00:12,  2.66it/s]\u001b[A\n"," 82% 152/185 [00:54<00:12,  2.67it/s]\u001b[A\n"," 83% 153/185 [00:55<00:12,  2.66it/s]\u001b[A\n"," 83% 154/185 [00:55<00:11,  2.66it/s]\u001b[A\n"," 84% 155/185 [00:56<00:11,  2.66it/s]\u001b[A\n"," 84% 156/185 [00:56<00:10,  2.66it/s]\u001b[A\n"," 85% 157/185 [00:56<00:10,  2.66it/s]\u001b[A\n"," 85% 158/185 [00:57<00:10,  2.66it/s]\u001b[A\n"," 86% 159/185 [00:57<00:09,  2.67it/s]\u001b[A\n"," 86% 160/185 [00:57<00:09,  2.66it/s]\u001b[A\n"," 87% 161/185 [00:58<00:09,  2.65it/s]\u001b[A\n"," 88% 162/185 [00:58<00:08,  2.64it/s]\u001b[A\n"," 88% 163/185 [00:59<00:08,  2.64it/s]\u001b[A\n"," 89% 164/185 [00:59<00:07,  2.64it/s]\u001b[A\n"," 89% 165/185 [00:59<00:07,  2.65it/s]\u001b[A\n"," 90% 166/185 [01:00<00:07,  2.64it/s]\u001b[A\n"," 90% 167/185 [01:00<00:06,  2.64it/s]\u001b[A\n"," 91% 168/185 [01:01<00:06,  2.65it/s]\u001b[A\n"," 91% 169/185 [01:01<00:06,  2.64it/s]\u001b[A\n"," 92% 170/185 [01:01<00:05,  2.64it/s]\u001b[A\n"," 92% 171/185 [01:02<00:05,  2.64it/s]\u001b[A\n"," 93% 172/185 [01:02<00:04,  2.64it/s]\u001b[A\n"," 94% 173/185 [01:02<00:04,  2.63it/s]\u001b[A\n"," 94% 174/185 [01:03<00:04,  2.62it/s]\u001b[A\n"," 95% 175/185 [01:03<00:03,  2.62it/s]\u001b[A\n"," 95% 176/185 [01:04<00:03,  2.63it/s]\u001b[A\n"," 96% 177/185 [01:04<00:03,  2.63it/s]\u001b[A\n"," 96% 178/185 [01:04<00:02,  2.63it/s]\u001b[A\n"," 97% 179/185 [01:05<00:02,  2.63it/s]\u001b[A\n"," 97% 180/185 [01:05<00:01,  2.63it/s]\u001b[A\n"," 98% 181/185 [01:05<00:01,  2.63it/s]\u001b[A\n"," 98% 182/185 [01:06<00:01,  2.63it/s]\u001b[A\n"," 99% 183/185 [01:06<00:00,  2.63it/s]\u001b[A\n"," 99% 184/185 [01:07<00:00,  2.62it/s]\u001b[A\n","100% 185/185 [01:07<00:00,  2.60it/s]\u001b[A03/01/2022 12:12:14 - INFO - src.trainer -   Best dev result: 0.7644999623298645\n","Epoch:  19% 12/63 [02:12<07:19,  8.62s/it]\n","186it [01:44, 11.30s/it]             \u001b[A\n","187it [01:44,  8.03s/it]\u001b[A\n","188it [01:45,  5.74s/it]\u001b[A\n","189it [01:45,  4.14s/it]\u001b[A\n","190it [01:45,  3.02s/it]\u001b[A\n","191it [01:46,  2.23s/it]\u001b[A\n","192it [01:46,  1.68s/it]\u001b[A\n","193it [01:47,  1.30s/it]\u001b[A\n","194it [01:47,  1.03s/it]\u001b[A\n","195it [01:47,  1.19it/s]\u001b[A\n","196it [01:48,  1.41it/s]\u001b[A\n","197it [01:48,  1.61it/s]\u001b[A\n","198it [01:49,  1.80it/s]\u001b[A\n","199it [01:49,  1.97it/s]\u001b[A\n","200it [01:49,  2.10it/s]\u001b[A\n","201it [01:50,  2.21it/s]\u001b[A\n","202it [01:50,  2.28it/s]\u001b[A\n","203it [01:51,  2.34it/s]\u001b[A\n","204it [01:51,  2.38it/s]\u001b[A\n","205it [01:51,  2.41it/s]\u001b[A\n","206it [01:52,  2.44it/s]\u001b[A\n","207it [01:52,  2.45it/s]\u001b[A\n","208it [01:53,  2.44it/s]\u001b[A\n","209it [01:53,  2.44it/s]\u001b[A\n","210it [01:53,  2.45it/s]\u001b[A\n","211it [01:54,  2.47it/s]\u001b[A\n","212it [01:54,  2.47it/s]\u001b[A\n","213it [01:55,  2.48it/s]\u001b[A\n","214it [01:55,  2.48it/s]\u001b[A\n","215it [01:55,  2.47it/s]\u001b[A\n","216it [01:56,  2.48it/s]\u001b[A\n","217it [01:56,  2.48it/s]\u001b[A\n","218it [01:57,  2.48it/s]\u001b[A\n","219it [01:57,  2.46it/s]\u001b[A\n","220it [01:57,  2.45it/s]\u001b[A\n","221it [01:58,  2.46it/s]\u001b[A\n","222it [01:58,  2.47it/s]\u001b[A\n","223it [01:59,  2.47it/s]\u001b[A\n","224it [01:59,  2.47it/s]\u001b[A\n","225it [02:00,  2.47it/s]\u001b[A\n","226it [02:00,  2.47it/s]\u001b[A\n","227it [02:00,  2.48it/s]\u001b[A\n","228it [02:01,  2.48it/s]\u001b[A\n","229it [02:01,  2.47it/s]\u001b[A\n","230it [02:02,  2.47it/s]\u001b[A\n","231it [02:02,  2.45it/s]\u001b[A\n","232it [02:02,  2.46it/s]\u001b[A\n","233it [02:03,  2.46it/s]\u001b[A\n","234it [02:03,  2.47it/s]\u001b[A\n","235it [02:04,  2.48it/s]\u001b[A\n","236it [02:04,  2.47it/s]\u001b[A\n","237it [02:04,  2.47it/s]\u001b[A\n","238it [02:05,  2.47it/s]\u001b[A\n","239it [02:05,  2.47it/s]\u001b[A\n","240it [02:06,  2.47it/s]\u001b[A\n","241it [02:06,  2.47it/s]\u001b[A\n","242it [02:06,  2.46it/s]\u001b[A\n","243it [02:07,  2.45it/s]\u001b[A\n","244it [02:07,  2.46it/s]\u001b[A\n","245it [02:08,  2.47it/s]\u001b[A\n","246it [02:08,  2.47it/s]\u001b[A\n","247it [02:08,  2.47it/s]\u001b[A\n","248it [02:09,  2.47it/s]\u001b[A\n","249it [02:09,  2.48it/s]\u001b[A\n","250it [02:10,  2.47it/s]\u001b[A\n","251it [02:10,  2.47it/s]\u001b[A\n","252it [02:10,  2.47it/s]\u001b[A\n","253it [02:11,  2.47it/s]\u001b[A\n","254it [02:11,  2.46it/s]\u001b[A\n","255it [02:12,  2.45it/s]\u001b[A\n","256it [02:12,  2.45it/s]\u001b[A\n","257it [02:12,  2.46it/s]\u001b[A\n","258it [02:13,  2.46it/s]\u001b[A\n","259it [02:13,  2.47it/s]\u001b[A\n","260it [02:14,  2.47it/s]\u001b[A\n","261it [02:14,  2.48it/s]\u001b[A\n","262it [02:14,  2.48it/s]\u001b[A\n","263it [02:15,  2.49it/s]\u001b[A\n","264it [02:15,  2.48it/s]\u001b[A\n","265it [02:16,  2.47it/s]\u001b[A\n","266it [02:16,  2.46it/s]\u001b[A\n","267it [02:17,  2.47it/s]\u001b[A\n","268it [02:17,  2.48it/s]\u001b[A\n","269it [02:17,  2.48it/s]\u001b[A\n","270it [02:18,  2.48it/s]\u001b[A\n","271it [02:18,  2.48it/s]\u001b[A\n","272it [02:19,  2.48it/s]\u001b[A\n","273it [02:19,  2.48it/s]\u001b[A\n","274it [02:19,  2.49it/s]\u001b[A\n","275it [02:20,  2.50it/s]\u001b[A\n","276it [02:20,  2.49it/s]\u001b[A\n","277it [02:21,  2.49it/s]\u001b[A\n","278it [02:21,  2.48it/s]\u001b[A\n","279it [02:21,  2.49it/s]\u001b[A\n","280it [02:22,  2.49it/s]\u001b[A\n","281it [02:22,  2.49it/s]\u001b[A\n","282it [02:23,  2.50it/s]\u001b[A\n","283it [02:23,  2.49it/s]\u001b[A\n","284it [02:23,  2.50it/s]\u001b[A\n","285it [02:24,  2.49it/s]\u001b[A\n","286it [02:24,  2.50it/s]\u001b[A\n","287it [02:25,  2.51it/s]\u001b[A\n","288it [02:25,  2.50it/s]\u001b[A\n","289it [02:25,  2.48it/s]\u001b[A\n","290it [02:26,  2.49it/s]\u001b[A\n","291it [02:26,  2.49it/s]\u001b[A\n","292it [02:27,  2.50it/s]\u001b[A\n","293it [02:27,  2.50it/s]\u001b[A\n","294it [02:27,  2.50it/s]\u001b[A\n","295it [02:28,  2.51it/s]\u001b[A\n","296it [02:28,  2.51it/s]\u001b[A\n","297it [02:29,  2.50it/s]\u001b[A\n","298it [02:29,  2.51it/s]\u001b[A\n","299it [02:29,  2.51it/s]\u001b[A\n","300it [02:30,  2.49it/s]\u001b[A\n","301it [02:30,  2.49it/s]\u001b[A\n","302it [02:31,  2.49it/s]\u001b[A\n","303it [02:31,  2.50it/s]\u001b[A\n","304it [02:31,  2.51it/s]\u001b[A\n","305it [02:32,  2.50it/s]\u001b[A\n","306it [02:32,  2.51it/s]\u001b[A\n","307it [02:33,  2.51it/s]\u001b[A\n","308it [02:33,  2.52it/s]\u001b[A\n","309it [02:33,  2.52it/s]\u001b[A\n","310it [02:34,  2.52it/s]\u001b[A\n","311it [02:34,  2.52it/s]\u001b[A\n","312it [02:35,  2.50it/s]\u001b[A\n","313it [02:35,  2.50it/s]\u001b[A\n","314it [02:35,  2.50it/s]\u001b[A\n","315it [02:36,  2.51it/s]\u001b[A\n","316it [02:36,  2.51it/s]\u001b[A\n","317it [02:37,  2.51it/s]\u001b[A\n","318it [02:37,  2.52it/s]\u001b[A\n","319it [02:37,  2.52it/s]\u001b[A\n","320it [02:38,  2.52it/s]\u001b[A\n","321it [02:38,  2.52it/s]\u001b[A\n","322it [02:39,  2.52it/s]\u001b[A\n","323it [02:39,  2.52it/s]\u001b[A\n","324it [02:39,  2.51it/s]\u001b[A\n","325it [02:40,  2.51it/s]\u001b[A\n","326it [02:40,  2.52it/s]\u001b[A\n","327it [02:40,  2.52it/s]\u001b[A\n","328it [02:41,  2.52it/s]\u001b[A\n","329it [02:41,  2.52it/s]\u001b[A\n","330it [02:42,  2.52it/s]\u001b[A\n","331it [02:42,  2.53it/s]\u001b[A\n","332it [02:42,  2.53it/s]\u001b[A\n","333it [02:43,  2.53it/s]\u001b[A\n","334it [02:43,  2.53it/s]\u001b[A\n","335it [02:44,  2.51it/s]\u001b[A\n","336it [02:44,  2.52it/s]\u001b[A\n","337it [02:44,  2.51it/s]\u001b[A\n","338it [02:45,  2.51it/s]\u001b[A\n","339it [02:45,  2.52it/s]\u001b[A\n","340it [02:46,  2.52it/s]\u001b[A\n","341it [02:46,  2.52it/s]\u001b[A\n","342it [02:46,  2.52it/s]\u001b[A\n","343it [02:47,  2.52it/s]\u001b[A\n","344it [02:47,  2.53it/s]\u001b[A\n","345it [02:48,  2.52it/s]\u001b[A\n","346it [02:48,  2.52it/s]\u001b[A\n","347it [02:48,  2.50it/s]\u001b[A\n","348it [02:49,  2.51it/s]\u001b[A\n","349it [02:49,  2.52it/s]\u001b[A\n","350it [02:50,  2.52it/s]\u001b[A\n","351it [02:50,  2.52it/s]\u001b[A\n","352it [02:50,  2.52it/s]\u001b[A\n","353it [02:51,  2.52it/s]\u001b[A\n","354it [02:51,  2.52it/s]\u001b[A\n","355it [02:52,  2.52it/s]\u001b[A\n","356it [02:52,  2.52it/s]\u001b[A\n","357it [02:52,  2.52it/s]\u001b[A\n","358it [02:53,  2.51it/s]\u001b[A\n","359it [02:53,  2.50it/s]\u001b[A\n","360it [02:54,  2.50it/s]\u001b[A\n","361it [02:54,  2.51it/s]\u001b[A\n","362it [02:54,  2.51it/s]\u001b[A\n","363it [02:55,  2.51it/s]\u001b[A\n","364it [02:55,  2.52it/s]\u001b[A\n","365it [02:56,  2.52it/s]\u001b[A\n","366it [02:56,  2.52it/s]\u001b[A\n","367it [02:56,  2.52it/s]\u001b[A\n","368it [02:57,  2.53it/s]\u001b[A\n","369it [02:57,  2.52it/s]\u001b[A\n","370it [02:58,  2.50it/s]\u001b[A03/01/2022 12:14:05 - INFO - src.trainer -   Best dev result: 0.7852500081062317\n","Epoch:  29% 18/63 [04:02<07:07,  9.50s/it]\n","371it [03:36, 11.68s/it]\u001b[A\n","372it [03:36,  8.30s/it]\u001b[A\n","373it [03:36,  5.93s/it]\u001b[A\n","374it [03:37,  4.27s/it]\u001b[A\n","375it [03:37,  3.11s/it]\u001b[A\n","376it [03:38,  2.30s/it]\u001b[A\n","377it [03:38,  1.73s/it]\u001b[A\n","378it [03:38,  1.34s/it]\u001b[A\n","379it [03:39,  1.06s/it]\u001b[A\n","380it [03:39,  1.16it/s]\u001b[A\n","381it [03:40,  1.38it/s]\u001b[A\n","382it [03:40,  1.58it/s]\u001b[A\n","383it [03:40,  1.78it/s]\u001b[A\n","384it [03:41,  1.94it/s]\u001b[A\n","385it [03:41,  2.08it/s]\u001b[A\n","386it [03:42,  2.19it/s]\u001b[A\n","387it [03:42,  2.26it/s]\u001b[A\n","388it [03:42,  2.33it/s]\u001b[A\n","389it [03:43,  2.37it/s]\u001b[A\n","390it [03:43,  2.41it/s]\u001b[A\n","391it [03:44,  2.43it/s]\u001b[A\n","392it [03:44,  2.45it/s]\u001b[A\n","393it [03:44,  2.45it/s]\u001b[A\n","394it [03:45,  2.46it/s]\u001b[A\n","395it [03:45,  2.47it/s]\u001b[A\n","396it [03:46,  2.47it/s]\u001b[A\n","397it [03:46,  2.47it/s]\u001b[A\n","398it [03:47,  2.48it/s]\u001b[A\n","399it [03:47,  2.48it/s]\u001b[A\n","400it [03:47,  2.49it/s]\u001b[A\n","401it [03:48,  2.49it/s]\u001b[A\n","402it [03:48,  2.49it/s]\u001b[A\n","403it [03:49,  2.50it/s]\u001b[A\n","404it [03:49,  2.48it/s]\u001b[A\n","405it [03:49,  2.48it/s]\u001b[A\n","406it [03:50,  2.49it/s]\u001b[A\n","407it [03:50,  2.49it/s]\u001b[A\n","408it [03:51,  2.50it/s]\u001b[A\n","409it [03:51,  2.49it/s]\u001b[A\n","410it [03:51,  2.49it/s]\u001b[A\n","411it [03:52,  2.49it/s]\u001b[A\n","412it [03:52,  2.50it/s]\u001b[A\n","413it [03:53,  2.50it/s]\u001b[A\n","414it [03:53,  2.50it/s]\u001b[A\n","415it [03:53,  2.50it/s]\u001b[A\n","416it [03:54,  2.49it/s]\u001b[A\n","417it [03:54,  2.49it/s]\u001b[A\n","418it [03:55,  2.50it/s]\u001b[A\n","419it [03:55,  2.50it/s]\u001b[A\n","420it [03:55,  2.51it/s]\u001b[A\n","421it [03:56,  2.51it/s]\u001b[A\n","422it [03:56,  2.51it/s]\u001b[A\n","423it [03:57,  2.51it/s]\u001b[A\n","424it [03:57,  2.51it/s]\u001b[A\n","425it [03:57,  2.51it/s]\u001b[A\n","426it [03:58,  2.51it/s]\u001b[A\n","427it [03:58,  2.50it/s]\u001b[A\n","428it [03:59,  2.49it/s]\u001b[A\n","429it [03:59,  2.49it/s]\u001b[A\n","430it [03:59,  2.51it/s]\u001b[A\n","431it [04:00,  2.51it/s]\u001b[A\n","432it [04:00,  2.50it/s]\u001b[A\n","433it [04:01,  2.51it/s]\u001b[A\n","434it [04:01,  2.51it/s]\u001b[A\n","435it [04:01,  2.51it/s]\u001b[A\n","436it [04:02,  2.52it/s]\u001b[A\n","437it [04:02,  2.52it/s]\u001b[A\n","438it [04:02,  2.53it/s]\u001b[A\n","439it [04:03,  2.51it/s]\u001b[A\n","440it [04:03,  2.51it/s]\u001b[A\n","441it [04:04,  2.51it/s]\u001b[A\n","442it [04:04,  2.51it/s]\u001b[A\n","443it [04:04,  2.52it/s]\u001b[A\n","444it [04:05,  2.52it/s]\u001b[A\n","445it [04:05,  2.52it/s]\u001b[A\n","446it [04:06,  2.53it/s]\u001b[A\n","447it [04:06,  2.52it/s]\u001b[A\n","448it [04:06,  2.52it/s]\u001b[A\n","449it [04:07,  2.53it/s]\u001b[A\n","450it [04:07,  2.51it/s]\u001b[A\n","451it [04:08,  2.51it/s]\u001b[A\n","452it [04:08,  2.52it/s]\u001b[A\n","453it [04:08,  2.52it/s]\u001b[A\n","454it [04:09,  2.52it/s]\u001b[A\n","455it [04:09,  2.52it/s]\u001b[A\n","456it [04:10,  2.52it/s]\u001b[A\n","457it [04:10,  2.52it/s]\u001b[A\n","458it [04:10,  2.52it/s]\u001b[A\n","459it [04:11,  2.53it/s]\u001b[A\n","460it [04:11,  2.53it/s]\u001b[A\n","461it [04:12,  2.53it/s]\u001b[A\n","462it [04:12,  2.51it/s]\u001b[A\n","463it [04:12,  2.51it/s]\u001b[A\n","464it [04:13,  2.51it/s]\u001b[A\n","465it [04:13,  2.52it/s]\u001b[A\n","466it [04:14,  2.53it/s]\u001b[A\n","467it [04:14,  2.52it/s]\u001b[A\n","468it [04:14,  2.53it/s]\u001b[A\n","469it [04:15,  2.52it/s]\u001b[A\n","470it [04:15,  2.52it/s]\u001b[A\n","471it [04:16,  2.52it/s]\u001b[A\n","472it [04:16,  2.52it/s]\u001b[A\n","473it [04:16,  2.52it/s]\u001b[A\n","474it [04:17,  2.50it/s]\u001b[A\n","475it [04:17,  2.51it/s]\u001b[A\n","476it [04:18,  2.51it/s]\u001b[A\n","477it [04:18,  2.52it/s]\u001b[A\n","478it [04:18,  2.52it/s]\u001b[A\n","479it [04:19,  2.52it/s]\u001b[A\n","480it [04:19,  2.52it/s]\u001b[A\n","481it [04:20,  2.52it/s]\u001b[A\n","482it [04:20,  2.52it/s]\u001b[A\n","483it [04:20,  2.52it/s]\u001b[A\n","484it [04:21,  2.52it/s]\u001b[A\n","485it [04:21,  2.50it/s]\u001b[A\n","486it [04:22,  2.50it/s]\u001b[A\n","487it [04:22,  2.50it/s]\u001b[A\n","488it [04:22,  2.51it/s]\u001b[A\n","489it [04:23,  2.51it/s]\u001b[A\n","490it [04:23,  2.51it/s]\u001b[A\n","491it [04:24,  2.51it/s]\u001b[A\n","492it [04:24,  2.51it/s]\u001b[A\n","493it [04:24,  2.51it/s]\u001b[A\n","494it [04:25,  2.52it/s]\u001b[A\n","495it [04:25,  2.52it/s]\u001b[A\n","496it [04:26,  2.52it/s]\u001b[A\n","497it [04:26,  2.50it/s]\u001b[A\n","498it [04:26,  2.49it/s]\u001b[A\n","499it [04:27,  2.50it/s]\u001b[A\n","500it [04:27,  2.50it/s]\u001b[A\n","501it [04:28,  2.51it/s]\u001b[A\n","502it [04:28,  2.50it/s]\u001b[A\n","503it [04:28,  2.50it/s]\u001b[A\n","504it [04:29,  2.50it/s]\u001b[A\n","505it [04:29,  2.50it/s]\u001b[A\n","506it [04:30,  2.51it/s]\u001b[A\n","507it [04:30,  2.51it/s]\u001b[A\n","508it [04:30,  2.50it/s]\u001b[A\n","509it [04:31,  2.49it/s]\u001b[A\n","510it [04:31,  2.49it/s]\u001b[A\n","511it [04:32,  2.50it/s]\u001b[A\n","512it [04:32,  2.50it/s]\u001b[A\n","513it [04:32,  2.50it/s]\u001b[A\n","514it [04:33,  2.50it/s]\u001b[A\n","515it [04:33,  2.50it/s]\u001b[A\n","516it [04:34,  2.51it/s]\u001b[A\n","517it [04:34,  2.51it/s]\u001b[A\n","518it [04:34,  2.51it/s]\u001b[A\n","519it [04:35,  2.51it/s]\u001b[A\n","520it [04:35,  2.48it/s]\u001b[A\n","521it [04:36,  2.48it/s]\u001b[A\n","522it [04:36,  2.48it/s]\u001b[A\n","523it [04:36,  2.48it/s]\u001b[A\n","524it [04:37,  2.49it/s]\u001b[A\n","525it [04:37,  2.49it/s]\u001b[A\n","526it [04:38,  2.50it/s]\u001b[A\n","527it [04:38,  2.50it/s]\u001b[A\n","528it [04:38,  2.49it/s]\u001b[A\n","529it [04:39,  2.50it/s]\u001b[A\n","530it [04:39,  2.50it/s]\u001b[A\n","531it [04:40,  2.49it/s]\u001b[A\n","532it [04:40,  2.48it/s]\u001b[A\n","533it [04:40,  2.48it/s]\u001b[A\n","534it [04:41,  2.49it/s]\u001b[A\n","535it [04:41,  2.49it/s]\u001b[A\n","536it [04:42,  2.50it/s]\u001b[A\n","537it [04:42,  2.50it/s]\u001b[A\n","538it [04:42,  2.50it/s]\u001b[A\n","539it [04:43,  2.50it/s]\u001b[A\n","540it [04:43,  2.50it/s]\u001b[A\n","541it [04:44,  2.50it/s]\u001b[A\n","542it [04:44,  2.50it/s]\u001b[A\n","543it [04:44,  2.49it/s]\u001b[A\n","544it [04:45,  2.49it/s]\u001b[A\n","545it [04:45,  2.49it/s]\u001b[A\n","546it [04:46,  2.50it/s]\u001b[A\n","547it [04:46,  2.49it/s]\u001b[A\n","548it [04:46,  2.49it/s]\u001b[A\n","549it [04:47,  2.50it/s]\u001b[A\n","550it [04:47,  2.50it/s]\u001b[A\n","551it [04:48,  2.50it/s]\u001b[A\n","552it [04:48,  2.50it/s]\u001b[A\n","553it [04:48,  2.50it/s]\u001b[A\n","554it [04:49,  2.50it/s]\u001b[A\n","Epoch:  38% 24/63 [05:46<05:59,  9.22s/it]\n","556it [05:21,  9.70s/it]\u001b[A\n","557it [05:21,  6.91s/it]\u001b[A\n","558it [05:21,  4.96s/it]\u001b[A\n","559it [05:22,  3.59s/it]\u001b[A\n","560it [05:22,  2.64s/it]\u001b[A\n","561it [05:23,  1.96s/it]\u001b[A\n","562it [05:23,  1.50s/it]\u001b[A\n","563it [05:23,  1.17s/it]\u001b[A\n","564it [05:24,  1.07it/s]\u001b[A\n","565it [05:24,  1.29it/s]\u001b[A\n","566it [05:25,  1.50it/s]\u001b[A\n","567it [05:25,  1.70it/s]\u001b[A\n","568it [05:25,  1.89it/s]\u001b[A\n","569it [05:26,  2.04it/s]\u001b[A\n","570it [05:26,  2.16it/s]\u001b[A\n","571it [05:27,  2.25it/s]\u001b[A\n","572it [05:27,  2.32it/s]\u001b[A\n","573it [05:27,  2.37it/s]\u001b[A\n","574it [05:28,  2.41it/s]\u001b[A\n","575it [05:28,  2.44it/s]\u001b[A\n","576it [05:29,  2.46it/s]\u001b[A\n","577it [05:29,  2.47it/s]\u001b[A\n","578it [05:29,  2.46it/s]\u001b[A\n","579it [05:30,  2.47it/s]\u001b[A\n","580it [05:30,  2.48it/s]\u001b[A\n","581it [05:31,  2.48it/s]\u001b[A\n","582it [05:31,  2.49it/s]\u001b[A\n","583it [05:31,  2.49it/s]\u001b[A\n","584it [05:32,  2.50it/s]\u001b[A\n","585it [05:32,  2.50it/s]\u001b[A\n","586it [05:33,  2.50it/s]\u001b[A\n","587it [05:33,  2.50it/s]\u001b[A\n","588it [05:33,  2.50it/s]\u001b[A\n","589it [05:34,  2.49it/s]\u001b[A\n","590it [05:34,  2.48it/s]\u001b[A\n","591it [05:35,  2.49it/s]\u001b[A\n","592it [05:35,  2.50it/s]\u001b[A\n","593it [05:35,  2.49it/s]\u001b[A\n","594it [05:36,  2.49it/s]\u001b[A\n","595it [05:36,  2.49it/s]\u001b[A\n","596it [05:37,  2.49it/s]\u001b[A\n","597it [05:37,  2.50it/s]\u001b[A\n","598it [05:37,  2.50it/s]\u001b[A\n","599it [05:38,  2.50it/s]\u001b[A\n","600it [05:38,  2.50it/s]\u001b[A\n","601it [05:39,  2.48it/s]\u001b[A\n","602it [05:39,  2.48it/s]\u001b[A\n","603it [05:39,  2.49it/s]\u001b[A\n","604it [05:40,  2.49it/s]\u001b[A\n","605it [05:40,  2.50it/s]\u001b[A\n","606it [05:41,  2.49it/s]\u001b[A\n","607it [05:41,  2.49it/s]\u001b[A\n","608it [05:41,  2.49it/s]\u001b[A\n","609it [05:42,  2.49it/s]\u001b[A\n","610it [05:42,  2.51it/s]\u001b[A\n","611it [05:43,  2.50it/s]\u001b[A\n","612it [05:43,  2.49it/s]\u001b[A\n","613it [05:43,  2.48it/s]\u001b[A\n","614it [05:44,  2.48it/s]\u001b[A\n","615it [05:44,  2.48it/s]\u001b[A\n","616it [05:45,  2.48it/s]\u001b[A\n","617it [05:45,  2.49it/s]\u001b[A\n","618it [05:45,  2.50it/s]\u001b[A\n","619it [05:46,  2.49it/s]\u001b[A\n","620it [05:46,  2.50it/s]\u001b[A\n","621it [05:47,  2.50it/s]\u001b[A\n","622it [05:47,  2.50it/s]\u001b[A\n","623it [05:47,  2.50it/s]\u001b[A\n","624it [05:48,  2.49it/s]\u001b[A\n","625it [05:48,  2.48it/s]\u001b[A\n","626it [05:49,  2.49it/s]\u001b[A\n","627it [05:49,  2.49it/s]\u001b[A\n","628it [05:49,  2.49it/s]\u001b[A\n","629it [05:50,  2.49it/s]\u001b[A\n","630it [05:50,  2.50it/s]\u001b[A\n","631it [05:51,  2.50it/s]\u001b[A\n","632it [05:51,  2.50it/s]\u001b[A\n","633it [05:51,  2.50it/s]\u001b[A\n","634it [05:52,  2.50it/s]\u001b[A\n","635it [05:52,  2.49it/s]\u001b[A\n","636it [05:53,  2.48it/s]\u001b[A\n","637it [05:53,  2.49it/s]\u001b[A\n","638it [05:53,  2.49it/s]\u001b[A\n","639it [05:54,  2.49it/s]\u001b[A\n","640it [05:54,  2.50it/s]\u001b[A\n","641it [05:55,  2.50it/s]\u001b[A\n","642it [05:55,  2.50it/s]\u001b[A\n","643it [05:55,  2.50it/s]\u001b[A\n","644it [05:56,  2.50it/s]\u001b[A\n","645it [05:56,  2.50it/s]\u001b[A\n","646it [05:57,  2.50it/s]\u001b[A\n","647it [05:57,  2.49it/s]\u001b[A\n","648it [05:57,  2.49it/s]\u001b[A\n","649it [05:58,  2.50it/s]\u001b[A\n","650it [05:58,  2.51it/s]\u001b[A\n","651it [05:59,  2.51it/s]\u001b[A\n","652it [05:59,  2.51it/s]\u001b[A\n","653it [05:59,  2.50it/s]\u001b[A\n","654it [06:00,  2.50it/s]\u001b[A\n","655it [06:00,  2.50it/s]\u001b[A\n","656it [06:01,  2.50it/s]\u001b[A\n","657it [06:01,  2.51it/s]\u001b[A\n","658it [06:01,  2.50it/s]\u001b[A\n","659it [06:02,  2.48it/s]\u001b[A\n","660it [06:02,  2.49it/s]\u001b[A\n","661it [06:03,  2.49it/s]\u001b[A\n","662it [06:03,  2.50it/s]\u001b[A\n","663it [06:03,  2.49it/s]\u001b[A\n","664it [06:04,  2.49it/s]\u001b[A\n","665it [06:04,  2.50it/s]\u001b[A\n","666it [06:05,  2.50it/s]\u001b[A\n","667it [06:05,  2.50it/s]\u001b[A\n","668it [06:05,  2.50it/s]\u001b[A\n","669it [06:06,  2.51it/s]\u001b[A\n","670it [06:06,  2.49it/s]\u001b[A\n","671it [06:07,  2.48it/s]\u001b[A\n","672it [06:07,  2.49it/s]\u001b[A\n","673it [06:07,  2.50it/s]\u001b[A\n","674it [06:08,  2.50it/s]\u001b[A\n","675it [06:08,  2.50it/s]\u001b[A\n","676it [06:09,  2.50it/s]\u001b[A\n","677it [06:09,  2.50it/s]\u001b[A\n","678it [06:09,  2.51it/s]\u001b[A\n","679it [06:10,  2.50it/s]\u001b[A\n","680it [06:10,  2.51it/s]\u001b[A\n","681it [06:11,  2.50it/s]\u001b[A\n","682it [06:11,  2.48it/s]\u001b[A\n","683it [06:11,  2.48it/s]\u001b[A\n","684it [06:12,  2.49it/s]\u001b[A\n","685it [06:12,  2.49it/s]\u001b[A\n","686it [06:13,  2.49it/s]\u001b[A\n","687it [06:13,  2.49it/s]\u001b[A\n","688it [06:13,  2.50it/s]\u001b[A\n","689it [06:14,  2.50it/s]\u001b[A\n","690it [06:14,  2.50it/s]\u001b[A\n","691it [06:15,  2.50it/s]\u001b[A\n","692it [06:15,  2.50it/s]\u001b[A\n","693it [06:15,  2.49it/s]\u001b[A\n","694it [06:16,  2.48it/s]\u001b[A\n","695it [06:16,  2.49it/s]\u001b[A\n","696it [06:17,  2.49it/s]\u001b[A\n","697it [06:17,  2.49it/s]\u001b[A\n","698it [06:17,  2.50it/s]\u001b[A\n","699it [06:18,  2.50it/s]\u001b[A\n","700it [06:18,  2.50it/s]\u001b[A\n","701it [06:19,  2.51it/s]\u001b[A\n","702it [06:19,  2.51it/s]\u001b[A\n","703it [06:19,  2.50it/s]\u001b[A\n","704it [06:20,  2.51it/s]\u001b[A\n","705it [06:20,  2.49it/s]\u001b[A\n","706it [06:21,  2.49it/s]\u001b[A\n","707it [06:21,  2.50it/s]\u001b[A\n","708it [06:21,  2.49it/s]\u001b[A\n","709it [06:22,  2.50it/s]\u001b[A\n","710it [06:22,  2.50it/s]\u001b[A\n","711it [06:23,  2.50it/s]\u001b[A\n","712it [06:23,  2.50it/s]\u001b[A\n","713it [06:23,  2.50it/s]\u001b[A\n","714it [06:24,  2.50it/s]\u001b[A\n","715it [06:24,  2.50it/s]\u001b[A\n","716it [06:25,  2.49it/s]\u001b[A\n","717it [06:25,  2.48it/s]\u001b[A\n","718it [06:26,  2.49it/s]\u001b[A\n","719it [06:26,  2.49it/s]\u001b[A\n","720it [06:26,  2.50it/s]\u001b[A\n","721it [06:27,  2.50it/s]\u001b[A\n","722it [06:27,  2.50it/s]\u001b[A\n","723it [06:28,  2.50it/s]\u001b[A\n","724it [06:28,  2.50it/s]\u001b[A\n","725it [06:28,  2.51it/s]\u001b[A\n","726it [06:29,  2.50it/s]\u001b[A\n","727it [06:29,  2.50it/s]\u001b[A\n","728it [06:30,  2.49it/s]\u001b[A\n","729it [06:30,  2.48it/s]\u001b[A\n","730it [06:30,  2.49it/s]\u001b[A\n","731it [06:31,  2.49it/s]\u001b[A\n","732it [06:31,  2.49it/s]\u001b[A\n","733it [06:32,  2.49it/s]\u001b[A\n","734it [06:32,  2.50it/s]\u001b[A\n","735it [06:32,  2.50it/s]\u001b[A\n","736it [06:33,  2.50it/s]\u001b[A\n","737it [06:33,  2.50it/s]\u001b[A\n","738it [06:34,  2.50it/s]\u001b[A\n","739it [06:34,  2.50it/s]\u001b[A\n","Epoch:  49% 31/63 [07:35<04:13,  7.92s/it]\n","741it [07:06,  9.68s/it]\u001b[A\n","742it [07:06,  6.90s/it]\u001b[A\n","743it [07:06,  4.95s/it]\u001b[A\n","744it [07:07,  3.58s/it]\u001b[A\n","745it [07:07,  2.63s/it]\u001b[A\n","746it [07:08,  1.96s/it]\u001b[A\n","747it [07:08,  1.49s/it]\u001b[A\n","748it [07:08,  1.16s/it]\u001b[A\n","749it [07:09,  1.07it/s]\u001b[A\n","750it [07:09,  1.29it/s]\u001b[A\n","751it [07:10,  1.51it/s]\u001b[A\n","752it [07:10,  1.71it/s]\u001b[A\n","753it [07:10,  1.89it/s]\u001b[A\n","754it [07:11,  2.04it/s]\u001b[A\n","755it [07:11,  2.16it/s]\u001b[A\n","756it [07:12,  2.26it/s]\u001b[A\n","757it [07:12,  2.33it/s]\u001b[A\n","758it [07:12,  2.38it/s]\u001b[A\n","759it [07:13,  2.41it/s]\u001b[A\n","760it [07:13,  2.45it/s]\u001b[A\n","761it [07:14,  2.47it/s]\u001b[A\n","762it [07:14,  2.48it/s]\u001b[A\n","763it [07:14,  2.48it/s]\u001b[A\n","764it [07:15,  2.48it/s]\u001b[A\n","765it [07:15,  2.49it/s]\u001b[A\n","766it [07:16,  2.50it/s]\u001b[A\n","767it [07:16,  2.50it/s]\u001b[A\n","768it [07:16,  2.51it/s]\u001b[A\n","769it [07:17,  2.51it/s]\u001b[A\n","770it [07:17,  2.51it/s]\u001b[A\n","771it [07:18,  2.52it/s]\u001b[A\n","772it [07:18,  2.52it/s]\u001b[A\n","773it [07:18,  2.52it/s]\u001b[A\n","774it [07:19,  2.50it/s]\u001b[A\n","775it [07:19,  2.50it/s]\u001b[A\n","776it [07:20,  2.50it/s]\u001b[A\n","777it [07:20,  2.50it/s]\u001b[A\n","778it [07:20,  2.51it/s]\u001b[A\n","779it [07:21,  2.50it/s]\u001b[A\n","780it [07:21,  2.50it/s]\u001b[A\n","781it [07:22,  2.51it/s]\u001b[A\n","782it [07:22,  2.51it/s]\u001b[A\n","783it [07:22,  2.51it/s]\u001b[A\n","784it [07:23,  2.51it/s]\u001b[A\n","785it [07:23,  2.51it/s]\u001b[A\n","786it [07:24,  2.49it/s]\u001b[A\n","787it [07:24,  2.49it/s]\u001b[A\n","788it [07:24,  2.50it/s]\u001b[A\n","789it [07:25,  2.50it/s]\u001b[A\n","790it [07:25,  2.51it/s]\u001b[A\n","791it [07:26,  2.51it/s]\u001b[A\n","792it [07:26,  2.51it/s]\u001b[A\n","793it [07:26,  2.51it/s]\u001b[A\n","794it [07:27,  2.51it/s]\u001b[A\n","795it [07:27,  2.52it/s]\u001b[A\n","796it [07:28,  2.52it/s]\u001b[A\n","797it [07:28,  2.51it/s]\u001b[A\n","798it [07:28,  2.50it/s]\u001b[A\n","799it [07:29,  2.50it/s]\u001b[A\n","800it [07:29,  2.50it/s]\u001b[A\n","801it [07:30,  2.51it/s]\u001b[A\n","802it [07:30,  2.50it/s]\u001b[A\n","803it [07:30,  2.50it/s]\u001b[A\n","804it [07:31,  2.51it/s]\u001b[A\n","805it [07:31,  2.51it/s]\u001b[A\n","806it [07:32,  2.51it/s]\u001b[A\n","807it [07:32,  2.51it/s]\u001b[A\n","808it [07:32,  2.51it/s]\u001b[A\n","809it [07:33,  2.50it/s]\u001b[A\n","810it [07:33,  2.49it/s]\u001b[A\n","811it [07:34,  2.50it/s]\u001b[A\n","812it [07:34,  2.50it/s]\u001b[A\n","813it [07:34,  2.50it/s]\u001b[A\n","814it [07:35,  2.51it/s]\u001b[A\n","815it [07:35,  2.51it/s]\u001b[A\n","816it [07:36,  2.51it/s]\u001b[A\n","817it [07:36,  2.51it/s]\u001b[A\n","818it [07:36,  2.51it/s]\u001b[A\n","819it [07:37,  2.51it/s]\u001b[A\n","820it [07:37,  2.51it/s]\u001b[A\n","821it [07:38,  2.50it/s]\u001b[A\n","822it [07:38,  2.50it/s]\u001b[A\n","823it [07:38,  2.51it/s]\u001b[A\n","824it [07:39,  2.51it/s]\u001b[A\n","825it [07:39,  2.51it/s]\u001b[A\n","826it [07:40,  2.52it/s]\u001b[A\n","827it [07:40,  2.52it/s]\u001b[A\n","828it [07:40,  2.51it/s]\u001b[A\n","829it [07:41,  2.51it/s]\u001b[A\n","830it [07:41,  2.51it/s]\u001b[A\n","831it [07:42,  2.52it/s]\u001b[A\n","832it [07:42,  2.50it/s]\u001b[A\n","833it [07:42,  2.49it/s]\u001b[A\n","834it [07:43,  2.51it/s]\u001b[A\n","835it [07:43,  2.51it/s]\u001b[A\n","836it [07:44,  2.51it/s]\u001b[A\n","837it [07:44,  2.51it/s]\u001b[A\n","838it [07:44,  2.51it/s]\u001b[A\n","839it [07:45,  2.51it/s]\u001b[A\n","840it [07:45,  2.51it/s]\u001b[A\n","841it [07:46,  2.52it/s]\u001b[A\n","842it [07:46,  2.52it/s]\u001b[A\n","843it [07:46,  2.51it/s]\u001b[A\n","844it [07:47,  2.50it/s]\u001b[A\n","845it [07:47,  2.50it/s]\u001b[A\n","846it [07:48,  2.51it/s]\u001b[A\n","847it [07:48,  2.51it/s]\u001b[A\n","848it [07:48,  2.51it/s]\u001b[A\n","849it [07:49,  2.51it/s]\u001b[A\n","850it [07:49,  2.51it/s]\u001b[A\n","851it [07:50,  2.51it/s]\u001b[A\n","852it [07:50,  2.51it/s]\u001b[A\n","853it [07:50,  2.51it/s]\u001b[A\n","854it [07:51,  2.51it/s]\u001b[A\n","855it [07:51,  2.50it/s]\u001b[A\n","856it [07:52,  2.49it/s]\u001b[A\n","857it [07:52,  2.49it/s]\u001b[A\n","858it [07:52,  2.51it/s]\u001b[A\n","859it [07:53,  2.51it/s]\u001b[A\n","860it [07:53,  2.51it/s]\u001b[A\n","861it [07:54,  2.52it/s]\u001b[A\n","862it [07:54,  2.51it/s]\u001b[A\n","863it [07:54,  2.52it/s]\u001b[A\n","864it [07:55,  2.51it/s]\u001b[A\n","865it [07:55,  2.51it/s]\u001b[A\n","866it [07:56,  2.51it/s]\u001b[A\n","867it [07:56,  2.49it/s]\u001b[A\n","868it [07:56,  2.50it/s]\u001b[A\n","869it [07:57,  2.50it/s]\u001b[A\n","870it [07:57,  2.51it/s]\u001b[A\n","871it [07:58,  2.51it/s]\u001b[A\n","872it [07:58,  2.50it/s]\u001b[A\n","873it [07:58,  2.51it/s]\u001b[A\n","874it [07:59,  2.51it/s]\u001b[A\n","875it [07:59,  2.51it/s]\u001b[A\n","876it [07:59,  2.52it/s]\u001b[A\n","877it [08:00,  2.51it/s]\u001b[A\n","878it [08:00,  2.50it/s]\u001b[A\n","879it [08:01,  2.49it/s]\u001b[A\n","880it [08:01,  2.50it/s]\u001b[A\n","881it [08:01,  2.51it/s]\u001b[A\n","882it [08:02,  2.50it/s]\u001b[A\n","883it [08:02,  2.51it/s]\u001b[A\n","884it [08:03,  2.51it/s]\u001b[A\n","885it [08:03,  2.51it/s]\u001b[A\n","886it [08:03,  2.51it/s]\u001b[A\n","887it [08:04,  2.51it/s]\u001b[A\n","888it [08:04,  2.51it/s]\u001b[A\n","889it [08:05,  2.51it/s]\u001b[A\n","890it [08:05,  2.49it/s]\u001b[A\n","891it [08:05,  2.49it/s]\u001b[A\n","892it [08:06,  2.49it/s]\u001b[A\n","893it [08:06,  2.49it/s]\u001b[A\n","894it [08:07,  2.50it/s]\u001b[A\n","895it [08:07,  2.51it/s]\u001b[A\n","896it [08:07,  2.51it/s]\u001b[A\n","897it [08:08,  2.51it/s]\u001b[A\n","898it [08:08,  2.51it/s]\u001b[A\n","899it [08:09,  2.52it/s]\u001b[A\n","900it [08:09,  2.51it/s]\u001b[A\n","901it [08:09,  2.50it/s]\u001b[A\n","902it [08:10,  2.49it/s]\u001b[A\n","903it [08:10,  2.48it/s]\u001b[A\n","904it [08:11,  2.49it/s]\u001b[A\n","905it [08:11,  2.50it/s]\u001b[A\n","906it [08:11,  2.50it/s]\u001b[A\n","907it [08:12,  2.50it/s]\u001b[A\n","908it [08:12,  2.50it/s]\u001b[A\n","909it [08:13,  2.51it/s]\u001b[A\n","910it [08:13,  2.51it/s]\u001b[A\n","911it [08:13,  2.51it/s]\u001b[A\n","912it [08:14,  2.51it/s]\u001b[A\n","913it [08:14,  2.50it/s]\u001b[A\n","914it [08:15,  2.50it/s]\u001b[A\n","915it [08:15,  2.50it/s]\u001b[A\n","916it [08:15,  2.51it/s]\u001b[A\n","917it [08:16,  2.51it/s]\u001b[A\n","918it [08:16,  2.51it/s]\u001b[A\n","919it [08:17,  2.51it/s]\u001b[A\n","920it [08:17,  2.51it/s]\u001b[A\n","921it [08:17,  2.51it/s]\u001b[A\n","922it [08:18,  2.51it/s]\u001b[A\n","923it [08:18,  2.51it/s]\u001b[A\n","924it [08:19,  2.51it/s]\u001b[A\n","Epoch:  59% 37/63 [09:18<03:54,  9.02s/it]\n","926it [08:50,  9.69s/it]\u001b[A\n","927it [08:51,  6.90s/it]\u001b[A\n","928it [08:51,  4.95s/it]\u001b[A\n","929it [08:52,  3.59s/it]\u001b[A\n","930it [08:52,  2.63s/it]\u001b[A\n","931it [08:52,  1.96s/it]\u001b[A\n","932it [08:53,  1.49s/it]\u001b[A\n","933it [08:53,  1.16s/it]\u001b[A\n","934it [08:54,  1.07it/s]\u001b[A\n","935it [08:54,  1.29it/s]\u001b[A\n","936it [08:54,  1.51it/s]\u001b[A\n","937it [08:55,  1.70it/s]\u001b[A\n","938it [08:55,  1.89it/s]\u001b[A\n","939it [08:56,  2.04it/s]\u001b[A\n","940it [08:56,  2.17it/s]\u001b[A\n","941it [08:56,  2.26it/s]\u001b[A\n","942it [08:57,  2.32it/s]\u001b[A\n","943it [08:57,  2.38it/s]\u001b[A\n","944it [08:58,  2.42it/s]\u001b[A\n","945it [08:58,  2.44it/s]\u001b[A\n","946it [08:58,  2.47it/s]\u001b[A\n","947it [08:59,  2.47it/s]\u001b[A\n","948it [08:59,  2.46it/s]\u001b[A\n","949it [09:00,  2.47it/s]\u001b[A\n","950it [09:00,  2.48it/s]\u001b[A\n","951it [09:00,  2.48it/s]\u001b[A\n","952it [09:01,  2.49it/s]\u001b[A\n","953it [09:01,  2.50it/s]\u001b[A\n","954it [09:02,  2.49it/s]\u001b[A\n","955it [09:02,  2.49it/s]\u001b[A\n","956it [09:02,  2.50it/s]\u001b[A\n","957it [09:03,  2.50it/s]\u001b[A\n","958it [09:03,  2.51it/s]\u001b[A\n","959it [09:04,  2.49it/s]\u001b[A\n","960it [09:04,  2.49it/s]\u001b[A\n","961it [09:04,  2.49it/s]\u001b[A\n","962it [09:05,  2.50it/s]\u001b[A\n","963it [09:05,  2.50it/s]\u001b[A\n","964it [09:06,  2.50it/s]\u001b[A\n","965it [09:06,  2.50it/s]\u001b[A\n","966it [09:06,  2.49it/s]\u001b[A\n","967it [09:07,  2.50it/s]\u001b[A\n","968it [09:07,  2.50it/s]\u001b[A\n","969it [09:08,  2.51it/s]\u001b[A\n","970it [09:08,  2.50it/s]\u001b[A\n","971it [09:08,  2.48it/s]\u001b[A\n","972it [09:09,  2.48it/s]\u001b[A\n","973it [09:09,  2.48it/s]\u001b[A\n","974it [09:10,  2.48it/s]\u001b[A\n","975it [09:10,  2.48it/s]\u001b[A\n","976it [09:10,  2.49it/s]\u001b[A\n","977it [09:11,  2.49it/s]\u001b[A\n","978it [09:11,  2.49it/s]\u001b[A\n","979it [09:12,  2.50it/s]\u001b[A\n","980it [09:12,  2.51it/s]\u001b[A\n","981it [09:12,  2.50it/s]\u001b[A\n","982it [09:13,  2.49it/s]\u001b[A\n","983it [09:13,  2.48it/s]\u001b[A\n","984it [09:14,  2.48it/s]\u001b[A\n","985it [09:14,  2.48it/s]\u001b[A\n","986it [09:14,  2.49it/s]\u001b[A\n","987it [09:15,  2.49it/s]\u001b[A\n","988it [09:15,  2.49it/s]\u001b[A\n","989it [09:16,  2.49it/s]\u001b[A\n","990it [09:16,  2.50it/s]\u001b[A\n","991it [09:16,  2.50it/s]\u001b[A\n","992it [09:17,  2.50it/s]\u001b[A\n","993it [09:17,  2.50it/s]\u001b[A\n","994it [09:18,  2.49it/s]\u001b[A\n","995it [09:18,  2.48it/s]\u001b[A\n","996it [09:18,  2.49it/s]\u001b[A\n","997it [09:19,  2.49it/s]\u001b[A\n","998it [09:19,  2.49it/s]\u001b[A\n","999it [09:20,  2.48it/s]\u001b[A\n","1000it [09:20,  2.49it/s]\u001b[A\n","1001it [09:20,  2.49it/s]\u001b[A\n","1002it [09:21,  2.49it/s]\u001b[A\n","1003it [09:21,  2.50it/s]\u001b[A\n","1004it [09:22,  2.50it/s]\u001b[A\n","1005it [09:22,  2.49it/s]\u001b[A\n","1006it [09:22,  2.49it/s]\u001b[A\n","1007it [09:23,  2.48it/s]\u001b[A\n","1008it [09:23,  2.50it/s]\u001b[A\n","1009it [09:24,  2.49it/s]\u001b[A\n","1010it [09:24,  2.49it/s]\u001b[A\n","1011it [09:24,  2.50it/s]\u001b[A\n","1012it [09:25,  2.50it/s]\u001b[A\n","1013it [09:25,  2.49it/s]\u001b[A\n","1014it [09:26,  2.49it/s]\u001b[A\n","1015it [09:26,  2.50it/s]\u001b[A\n","1016it [09:27,  2.50it/s]\u001b[A\n","1017it [09:27,  2.48it/s]\u001b[A\n","1018it [09:27,  2.47it/s]\u001b[A\n","1019it [09:28,  2.48it/s]\u001b[A\n","1020it [09:28,  2.49it/s]\u001b[A\n","1021it [09:29,  2.49it/s]\u001b[A\n","1022it [09:29,  2.49it/s]\u001b[A\n","1023it [09:29,  2.50it/s]\u001b[A\n","1024it [09:30,  2.49it/s]\u001b[A\n","1025it [09:30,  2.49it/s]\u001b[A\n","1026it [09:31,  2.50it/s]\u001b[A\n","1027it [09:31,  2.50it/s]\u001b[A\n","1028it [09:31,  2.49it/s]\u001b[A\n","1029it [09:32,  2.47it/s]\u001b[A\n","1030it [09:32,  2.48it/s]\u001b[A\n","1031it [09:33,  2.48it/s]\u001b[A\n","1032it [09:33,  2.49it/s]\u001b[A\n","1033it [09:33,  2.49it/s]\u001b[A\n","1034it [09:34,  2.49it/s]\u001b[A\n","1035it [09:34,  2.49it/s]\u001b[A\n","1036it [09:35,  2.49it/s]\u001b[A\n","1037it [09:35,  2.49it/s]\u001b[A\n","1038it [09:35,  2.50it/s]\u001b[A\n","1039it [09:36,  2.49it/s]\u001b[A\n","1040it [09:36,  2.48it/s]\u001b[A\n","1041it [09:37,  2.47it/s]\u001b[A\n","1042it [09:37,  2.47it/s]\u001b[A\n","1043it [09:37,  2.49it/s]\u001b[A\n","1044it [09:38,  2.49it/s]\u001b[A\n","1045it [09:38,  2.49it/s]\u001b[A\n","1046it [09:39,  2.49it/s]\u001b[A\n","1047it [09:39,  2.48it/s]\u001b[A\n","1048it [09:39,  2.49it/s]\u001b[A\n","1049it [09:40,  2.49it/s]\u001b[A\n","1050it [09:40,  2.50it/s]\u001b[A\n","1051it [09:41,  2.49it/s]\u001b[A\n","1052it [09:41,  2.46it/s]\u001b[A\n","1053it [09:41,  2.47it/s]\u001b[A\n","1054it [09:42,  2.48it/s]\u001b[A\n","1055it [09:42,  2.48it/s]\u001b[A\n","1056it [09:43,  2.48it/s]\u001b[A\n","1057it [09:43,  2.48it/s]\u001b[A\n","1058it [09:43,  2.49it/s]\u001b[A\n","1059it [09:44,  2.49it/s]\u001b[A\n","1060it [09:44,  2.49it/s]\u001b[A\n","1061it [09:45,  2.49it/s]\u001b[A\n","1062it [09:45,  2.49it/s]\u001b[A\n","1063it [09:45,  2.48it/s]\u001b[A\n","1064it [09:46,  2.48it/s]\u001b[A\n","1065it [09:46,  2.48it/s]\u001b[A\n","1066it [09:47,  2.49it/s]\u001b[A\n","1067it [09:47,  2.49it/s]\u001b[A\n","1068it [09:47,  2.49it/s]\u001b[A\n","1069it [09:48,  2.49it/s]\u001b[A\n","1070it [09:48,  2.48it/s]\u001b[A\n","1071it [09:49,  2.49it/s]\u001b[A\n","1072it [09:49,  2.50it/s]\u001b[A\n","1073it [09:49,  2.49it/s]\u001b[A\n","1074it [09:50,  2.49it/s]\u001b[A\n","1075it [09:50,  2.47it/s]\u001b[A\n","1076it [09:51,  2.47it/s]\u001b[A\n","1077it [09:51,  2.48it/s]\u001b[A\n","1078it [09:51,  2.47it/s]\u001b[A\n","1079it [09:52,  2.48it/s]\u001b[A\n","1080it [09:52,  2.48it/s]\u001b[A\n","1081it [09:53,  2.49it/s]\u001b[A\n","1082it [09:53,  2.49it/s]\u001b[A\n","1083it [09:53,  2.48it/s]\u001b[A\n","1084it [09:54,  2.49it/s]\u001b[A\n","1085it [09:54,  2.49it/s]\u001b[A\n","1086it [09:55,  2.49it/s]\u001b[A\n","1087it [09:55,  2.48it/s]\u001b[A\n","1088it [09:55,  2.48it/s]\u001b[A\n","1089it [09:56,  2.49it/s]\u001b[A\n","1090it [09:56,  2.49it/s]\u001b[A\n","1091it [09:57,  2.48it/s]\u001b[A\n","1092it [09:57,  2.49it/s]\u001b[A\n","1093it [09:57,  2.49it/s]\u001b[A\n","1094it [09:58,  2.49it/s]\u001b[A\n","1095it [09:58,  2.49it/s]\u001b[A\n","1096it [09:59,  2.49it/s]\u001b[A\n","1097it [09:59,  2.49it/s]\u001b[A\n","1098it [09:59,  2.48it/s]\u001b[A\n","1099it [10:00,  2.47it/s]\u001b[A\n","1100it [10:00,  2.48it/s]\u001b[A\n","1101it [10:01,  2.49it/s]\u001b[A\n","1102it [10:01,  2.49it/s]\u001b[A\n","1103it [10:02,  2.48it/s]\u001b[A\n","1104it [10:02,  2.49it/s]\u001b[A\n","1105it [10:02,  2.49it/s]\u001b[A\n","1106it [10:03,  2.49it/s]\u001b[A\n","1107it [10:03,  2.49it/s]\u001b[A\n","1108it [10:04,  2.49it/s]\u001b[A\n","1109it [10:04,  2.48it/s]\u001b[A\n","Epoch:  68% 43/63 [11:02<03:03,  9.19s/it]\n","1111it [10:36,  9.72s/it]\u001b[A\n","1112it [10:36,  6.92s/it]\u001b[A\n","1113it [10:37,  4.97s/it]\u001b[A\n","1114it [10:37,  3.60s/it]\u001b[A\n","1115it [10:37,  2.64s/it]\u001b[A\n","1116it [10:38,  1.97s/it]\u001b[A\n","1117it [10:38,  1.50s/it]\u001b[A\n","1118it [10:39,  1.17s/it]\u001b[A\n","1119it [10:39,  1.07it/s]\u001b[A\n","1120it [10:39,  1.29it/s]\u001b[A\n","1121it [10:40,  1.51it/s]\u001b[A\n","1122it [10:40,  1.70it/s]\u001b[A\n","1123it [10:41,  1.89it/s]\u001b[A\n","1124it [10:41,  2.04it/s]\u001b[A\n","1125it [10:41,  2.16it/s]\u001b[A\n","1126it [10:42,  2.25it/s]\u001b[A\n","1127it [10:42,  2.32it/s]\u001b[A\n","1128it [10:43,  2.37it/s]\u001b[A\n","1129it [10:43,  2.41it/s]\u001b[A\n","1130it [10:43,  2.44it/s]\u001b[A\n","1131it [10:44,  2.46it/s]\u001b[A\n","1132it [10:44,  2.47it/s]\u001b[A\n","1133it [10:45,  2.47it/s]\u001b[A\n","1134it [10:45,  2.48it/s]\u001b[A\n","1135it [10:45,  2.49it/s]\u001b[A\n","1136it [10:46,  2.50it/s]\u001b[A\n","1137it [10:46,  2.50it/s]\u001b[A\n","1138it [10:47,  2.50it/s]\u001b[A\n","1139it [10:47,  2.50it/s]\u001b[A\n","1140it [10:47,  2.51it/s]\u001b[A\n","1141it [10:48,  2.51it/s]\u001b[A\n","1142it [10:48,  2.51it/s]\u001b[A\n","1143it [10:49,  2.52it/s]\u001b[A\n","1144it [10:49,  2.50it/s]\u001b[A\n","1145it [10:49,  2.49it/s]\u001b[A\n","1146it [10:50,  2.49it/s]\u001b[A\n","1147it [10:50,  2.50it/s]\u001b[A\n","1148it [10:51,  2.50it/s]\u001b[A\n","1149it [10:51,  2.50it/s]\u001b[A\n","1150it [10:51,  2.51it/s]\u001b[A\n","1151it [10:52,  2.51it/s]\u001b[A\n","1152it [10:52,  2.51it/s]\u001b[A\n","1153it [10:53,  2.52it/s]\u001b[A\n","1154it [10:53,  2.52it/s]\u001b[A\n","1155it [10:53,  2.51it/s]\u001b[A\n","1156it [10:54,  2.50it/s]\u001b[A\n","1157it [10:54,  2.50it/s]\u001b[A\n","1158it [10:55,  2.50it/s]\u001b[A\n","1159it [10:55,  2.50it/s]\u001b[A\n","1160it [10:55,  2.51it/s]\u001b[A\n","1161it [10:56,  2.51it/s]\u001b[A\n","1162it [10:56,  2.51it/s]\u001b[A\n","1163it [10:57,  2.51it/s]\u001b[A\n","1164it [10:57,  2.51it/s]\u001b[A\n","1165it [10:57,  2.52it/s]\u001b[A\n","1166it [10:58,  2.52it/s]\u001b[A\n","1167it [10:58,  2.50it/s]\u001b[A\n","1168it [10:59,  2.50it/s]\u001b[A\n","1169it [10:59,  2.50it/s]\u001b[A\n","1170it [10:59,  2.50it/s]\u001b[A\n","1171it [11:00,  2.51it/s]\u001b[A\n","1172it [11:00,  2.50it/s]\u001b[A\n","1173it [11:01,  2.51it/s]\u001b[A\n","1174it [11:01,  2.51it/s]\u001b[A\n","1175it [11:01,  2.51it/s]\u001b[A\n","1176it [11:02,  2.52it/s]\u001b[A\n","1177it [11:02,  2.52it/s]\u001b[A\n","1178it [11:03,  2.52it/s]\u001b[A\n","1179it [11:03,  2.50it/s]\u001b[A\n","1180it [11:03,  2.50it/s]\u001b[A\n","1181it [11:04,  2.51it/s]\u001b[A\n","1182it [11:04,  2.51it/s]\u001b[A\n","1183it [11:05,  2.51it/s]\u001b[A\n","1184it [11:05,  2.51it/s]\u001b[A\n","1185it [11:05,  2.51it/s]\u001b[A\n","1186it [11:06,  2.51it/s]\u001b[A\n","1187it [11:06,  2.52it/s]\u001b[A\n","1188it [11:07,  2.52it/s]\u001b[A\n","1189it [11:07,  2.52it/s]\u001b[A\n","1190it [11:07,  2.51it/s]\u001b[A\n","1191it [11:08,  2.50it/s]\u001b[A\n","1192it [11:08,  2.50it/s]\u001b[A\n","1193it [11:09,  2.51it/s]\u001b[A\n","1194it [11:09,  2.51it/s]\u001b[A\n","1195it [11:09,  2.51it/s]\u001b[A\n","1196it [11:10,  2.51it/s]\u001b[A\n","1197it [11:10,  2.52it/s]\u001b[A\n","1198it [11:11,  2.51it/s]\u001b[A\n","1199it [11:11,  2.51it/s]\u001b[A\n","1200it [11:11,  2.52it/s]\u001b[A\n","1201it [11:12,  2.51it/s]\u001b[A\n","1202it [11:12,  2.51it/s]\u001b[A\n","1203it [11:13,  2.50it/s]\u001b[A\n","1204it [11:13,  2.51it/s]\u001b[A\n","1205it [11:13,  2.51it/s]\u001b[A\n","1206it [11:14,  2.51it/s]\u001b[A\n","1207it [11:14,  2.52it/s]\u001b[A\n","1208it [11:14,  2.51it/s]\u001b[A\n","1209it [11:15,  2.51it/s]\u001b[A\n","1210it [11:15,  2.51it/s]\u001b[A\n","1211it [11:16,  2.51it/s]\u001b[A\n","1212it [11:16,  2.52it/s]\u001b[A\n","1213it [11:16,  2.51it/s]\u001b[A\n","1214it [11:17,  2.50it/s]\u001b[A\n","1215it [11:17,  2.50it/s]\u001b[A\n","1216it [11:18,  2.51it/s]\u001b[A\n","1217it [11:18,  2.52it/s]\u001b[A\n","1218it [11:18,  2.51it/s]\u001b[A\n","1219it [11:19,  2.51it/s]\u001b[A\n","1220it [11:19,  2.51it/s]\u001b[A\n","1221it [11:20,  2.51it/s]\u001b[A\n","1222it [11:20,  2.51it/s]\u001b[A\n","1223it [11:20,  2.51it/s]\u001b[A\n","1224it [11:21,  2.51it/s]\u001b[A\n","1225it [11:21,  2.50it/s]\u001b[A\n","1226it [11:22,  2.49it/s]\u001b[A\n","1227it [11:22,  2.50it/s]\u001b[A\n","1228it [11:22,  2.51it/s]\u001b[A\n","1229it [11:23,  2.51it/s]\u001b[A\n","1230it [11:23,  2.52it/s]\u001b[A\n","1231it [11:24,  2.52it/s]\u001b[A\n","1232it [11:24,  2.52it/s]\u001b[A\n","1233it [11:24,  2.52it/s]\u001b[A\n","1234it [11:25,  2.52it/s]\u001b[A\n","1235it [11:25,  2.52it/s]\u001b[A\n","1236it [11:26,  2.52it/s]\u001b[A\n","1237it [11:26,  2.50it/s]\u001b[A\n","1238it [11:26,  2.50it/s]\u001b[A\n","1239it [11:27,  2.50it/s]\u001b[A\n","1240it [11:27,  2.51it/s]\u001b[A\n","1241it [11:28,  2.51it/s]\u001b[A\n","1242it [11:28,  2.51it/s]\u001b[A\n","1243it [11:28,  2.51it/s]\u001b[A\n","1244it [11:29,  2.52it/s]\u001b[A\n","1245it [11:29,  2.51it/s]\u001b[A\n","1246it [11:30,  2.52it/s]\u001b[A\n","1247it [11:30,  2.52it/s]\u001b[A\n","1248it [11:30,  2.52it/s]\u001b[A\n","1249it [11:31,  2.51it/s]\u001b[A\n","1250it [11:31,  2.50it/s]\u001b[A\n","1251it [11:32,  2.51it/s]\u001b[A\n","1252it [11:32,  2.51it/s]\u001b[A\n","1253it [11:32,  2.51it/s]\u001b[A\n","1254it [11:33,  2.52it/s]\u001b[A\n","1255it [11:33,  2.52it/s]\u001b[A\n","1256it [11:34,  2.52it/s]\u001b[A\n","1257it [11:34,  2.53it/s]\u001b[A\n","1258it [11:34,  2.52it/s]\u001b[A\n","1259it [11:35,  2.53it/s]\u001b[A\n","1260it [11:35,  2.51it/s]\u001b[A\n","1261it [11:36,  2.50it/s]\u001b[A\n","1262it [11:36,  2.50it/s]\u001b[A\n","1263it [11:36,  2.51it/s]\u001b[A\n","1264it [11:37,  2.51it/s]\u001b[A\n","1265it [11:37,  2.51it/s]\u001b[A\n","1266it [11:38,  2.52it/s]\u001b[A\n","1267it [11:38,  2.52it/s]\u001b[A\n","1268it [11:38,  2.51it/s]\u001b[A\n","1269it [11:39,  2.52it/s]\u001b[A\n","1270it [11:39,  2.52it/s]\u001b[A\n","1271it [11:40,  2.51it/s]\u001b[A\n","1272it [11:40,  2.50it/s]\u001b[A\n","1273it [11:40,  2.50it/s]\u001b[A\n","1274it [11:41,  2.51it/s]\u001b[A\n","1275it [11:41,  2.52it/s]\u001b[A\n","1276it [11:42,  2.51it/s]\u001b[A\n","1277it [11:42,  2.52it/s]\u001b[A\n","1278it [11:42,  2.52it/s]\u001b[A\n","1279it [11:43,  2.52it/s]\u001b[A\n","1280it [11:43,  2.52it/s]\u001b[A\n","1281it [11:44,  2.52it/s]\u001b[A\n","1282it [11:44,  2.53it/s]\u001b[A\n","1283it [11:44,  2.50it/s]\u001b[A\n","1284it [11:45,  2.50it/s]\u001b[A\n","1285it [11:45,  2.50it/s]\u001b[A\n","1286it [11:46,  2.51it/s]\u001b[A\n","1287it [11:46,  2.51it/s]\u001b[A\n","1288it [11:46,  2.51it/s]\u001b[A\n","1289it [11:47,  2.52it/s]\u001b[A\n","1290it [11:47,  2.52it/s]\u001b[A\n","1291it [11:48,  2.51it/s]\u001b[A\n","1292it [11:48,  2.52it/s]\u001b[A\n","1293it [11:48,  2.52it/s]\u001b[A\n","1294it [11:49,  2.51it/s]\u001b[A\n","1295it [11:49,  2.49it/s]\u001b[A03/01/2022 12:22:56 - INFO - src.trainer -   Best dev result: 0.7853749990463257\n","Epoch:  78% 49/63 [12:54<02:14,  9.62s/it]\n","1296it [12:28, 12.07s/it]\u001b[A\n","1297it [12:29,  8.58s/it]\u001b[A\n","1298it [12:29,  6.13s/it]\u001b[A\n","1299it [12:30,  4.41s/it]\u001b[A\n","1300it [12:30,  3.21s/it]\u001b[A\n","1301it [12:30,  2.37s/it]\u001b[A\n","1302it [12:31,  1.78s/it]\u001b[A\n","1303it [12:31,  1.37s/it]\u001b[A\n","1304it [12:32,  1.08s/it]\u001b[A\n","1305it [12:32,  1.14it/s]\u001b[A\n","1306it [12:33,  1.35it/s]\u001b[A\n","1307it [12:33,  1.55it/s]\u001b[A\n","1308it [12:33,  1.75it/s]\u001b[A\n","1309it [12:34,  1.92it/s]\u001b[A\n","1310it [12:34,  2.06it/s]\u001b[A\n","1311it [12:35,  2.16it/s]\u001b[A\n","1312it [12:35,  2.24it/s]\u001b[A\n","1313it [12:35,  2.31it/s]\u001b[A\n","1314it [12:36,  2.35it/s]\u001b[A\n","1315it [12:36,  2.39it/s]\u001b[A\n","1316it [12:37,  2.41it/s]\u001b[A\n","1317it [12:37,  2.43it/s]\u001b[A\n","1318it [12:37,  2.42it/s]\u001b[A\n","1319it [12:38,  2.44it/s]\u001b[A\n","1320it [12:38,  2.45it/s]\u001b[A\n","1321it [12:39,  2.46it/s]\u001b[A\n","1322it [12:39,  2.47it/s]\u001b[A\n","1323it [12:39,  2.47it/s]\u001b[A\n","1324it [12:40,  2.47it/s]\u001b[A\n","1325it [12:40,  2.47it/s]\u001b[A\n","1326it [12:41,  2.48it/s]\u001b[A\n","1327it [12:41,  2.49it/s]\u001b[A\n","1328it [12:41,  2.49it/s]\u001b[A\n","1329it [12:42,  2.48it/s]\u001b[A\n","1330it [12:42,  2.48it/s]\u001b[A\n","1331it [12:43,  2.48it/s]\u001b[A\n","1332it [12:43,  2.49it/s]\u001b[A\n","1333it [12:43,  2.49it/s]\u001b[A\n","1334it [12:44,  2.49it/s]\u001b[A\n","1335it [12:44,  2.49it/s]\u001b[A\n","1336it [12:45,  2.49it/s]\u001b[A\n","1337it [12:45,  2.50it/s]\u001b[A\n","1338it [12:45,  2.50it/s]\u001b[A\n","1339it [12:46,  2.50it/s]\u001b[A\n","1340it [12:46,  2.50it/s]\u001b[A\n","1341it [12:47,  2.49it/s]\u001b[A\n","1342it [12:47,  2.48it/s]\u001b[A\n","1343it [12:47,  2.49it/s]\u001b[A\n","1344it [12:48,  2.50it/s]\u001b[A\n","1345it [12:48,  2.50it/s]\u001b[A\n","1346it [12:49,  2.50it/s]\u001b[A\n","1347it [12:49,  2.50it/s]\u001b[A\n","1348it [12:49,  2.51it/s]\u001b[A\n","1349it [12:50,  2.51it/s]\u001b[A\n","1350it [12:50,  2.52it/s]\u001b[A\n","1351it [12:51,  2.51it/s]\u001b[A\n","1352it [12:51,  2.50it/s]\u001b[A\n","1353it [12:51,  2.50it/s]\u001b[A\n","1354it [12:52,  2.51it/s]\u001b[A\n","1355it [12:52,  2.51it/s]\u001b[A\n","1356it [12:53,  2.51it/s]\u001b[A\n","1357it [12:53,  2.51it/s]\u001b[A\n","1358it [12:53,  2.51it/s]\u001b[A\n","1359it [12:54,  2.51it/s]\u001b[A\n","1360it [12:54,  2.51it/s]\u001b[A\n","1361it [12:55,  2.53it/s]\u001b[A\n","1362it [12:55,  2.53it/s]\u001b[A\n","1363it [12:55,  2.53it/s]\u001b[A\n","1364it [12:56,  2.51it/s]\u001b[A\n","1365it [12:56,  2.51it/s]\u001b[A\n","1366it [12:57,  2.52it/s]\u001b[A\n","1367it [12:57,  2.52it/s]\u001b[A\n","1368it [12:57,  2.53it/s]\u001b[A\n","1369it [12:58,  2.52it/s]\u001b[A\n","1370it [12:58,  2.53it/s]\u001b[A\n","1371it [12:59,  2.53it/s]\u001b[A\n","1372it [12:59,  2.53it/s]\u001b[A\n","1373it [12:59,  2.54it/s]\u001b[A\n","1374it [13:00,  2.54it/s]\u001b[A\n","1375it [13:00,  2.53it/s]\u001b[A\n","1376it [13:01,  2.52it/s]\u001b[A\n","1377it [13:01,  2.52it/s]\u001b[A\n","1378it [13:01,  2.54it/s]\u001b[A\n","1379it [13:02,  2.53it/s]\u001b[A\n","1380it [13:02,  2.53it/s]\u001b[A\n","1381it [13:03,  2.53it/s]\u001b[A\n","1382it [13:03,  2.53it/s]\u001b[A\n","1383it [13:03,  2.53it/s]\u001b[A\n","1384it [13:04,  2.53it/s]\u001b[A\n","1385it [13:04,  2.54it/s]\u001b[A\n","1386it [13:05,  2.54it/s]\u001b[A\n","1387it [13:05,  2.52it/s]\u001b[A\n","1388it [13:05,  2.52it/s]\u001b[A\n","1389it [13:06,  2.53it/s]\u001b[A\n","1390it [13:06,  2.54it/s]\u001b[A\n","1391it [13:07,  2.53it/s]\u001b[A\n","1392it [13:07,  2.54it/s]\u001b[A\n","1393it [13:07,  2.54it/s]\u001b[A\n","1394it [13:08,  2.53it/s]\u001b[A\n","1395it [13:08,  2.53it/s]\u001b[A\n","1396it [13:08,  2.53it/s]\u001b[A\n","1397it [13:09,  2.53it/s]\u001b[A\n","1398it [13:09,  2.53it/s]\u001b[A\n","1399it [13:10,  2.51it/s]\u001b[A\n","1400it [13:10,  2.52it/s]\u001b[A\n","1401it [13:10,  2.52it/s]\u001b[A\n","1402it [13:11,  2.53it/s]\u001b[A\n","1403it [13:11,  2.53it/s]\u001b[A\n","1404it [13:12,  2.53it/s]\u001b[A\n","1405it [13:12,  2.53it/s]\u001b[A\n","1406it [13:12,  2.54it/s]\u001b[A\n","1407it [13:13,  2.54it/s]\u001b[A\n","1408it [13:13,  2.53it/s]\u001b[A\n","1409it [13:14,  2.53it/s]\u001b[A\n","1410it [13:14,  2.52it/s]\u001b[A\n","1411it [13:14,  2.51it/s]\u001b[A\n","1412it [13:15,  2.52it/s]\u001b[A\n","1413it [13:15,  2.53it/s]\u001b[A\n","1414it [13:16,  2.53it/s]\u001b[A\n","1415it [13:16,  2.53it/s]\u001b[A\n","1416it [13:16,  2.53it/s]\u001b[A\n","1417it [13:17,  2.53it/s]\u001b[A\n","1418it [13:17,  2.53it/s]\u001b[A\n","1419it [13:18,  2.53it/s]\u001b[A\n","1420it [13:18,  2.54it/s]\u001b[A\n","1421it [13:18,  2.53it/s]\u001b[A\n","1422it [13:19,  2.51it/s]\u001b[A\n","1423it [13:19,  2.51it/s]\u001b[A\n","1424it [13:20,  2.51it/s]\u001b[A\n","1425it [13:20,  2.51it/s]\u001b[A\n","1426it [13:20,  2.51it/s]\u001b[A\n","1427it [13:21,  2.51it/s]\u001b[A\n","1428it [13:21,  2.51it/s]\u001b[A\n","1429it [13:22,  2.52it/s]\u001b[A\n","1430it [13:22,  2.51it/s]\u001b[A\n","1431it [13:22,  2.52it/s]\u001b[A\n","1432it [13:23,  2.52it/s]\u001b[A\n","1433it [13:23,  2.52it/s]\u001b[A\n","1434it [13:24,  2.51it/s]\u001b[A\n","1435it [13:24,  2.51it/s]\u001b[A\n","1436it [13:24,  2.51it/s]\u001b[A\n","1437it [13:25,  2.51it/s]\u001b[A\n","1438it [13:25,  2.51it/s]\u001b[A\n","1439it [13:26,  2.51it/s]\u001b[A\n","1440it [13:26,  2.51it/s]\u001b[A\n","1441it [13:26,  2.52it/s]\u001b[A\n","1442it [13:27,  2.52it/s]\u001b[A\n","1443it [13:27,  2.52it/s]\u001b[A\n","1444it [13:28,  2.52it/s]\u001b[A\n","1445it [13:28,  2.49it/s]\u001b[A\n","1446it [13:28,  2.49it/s]\u001b[A\n","1447it [13:29,  2.49it/s]\u001b[A\n","1448it [13:29,  2.50it/s]\u001b[A\n","1449it [13:30,  2.50it/s]\u001b[A\n","1450it [13:30,  2.49it/s]\u001b[A\n","1451it [13:30,  2.49it/s]\u001b[A\n","1452it [13:31,  2.50it/s]\u001b[A\n","1453it [13:31,  2.50it/s]\u001b[A\n","1454it [13:32,  2.51it/s]\u001b[A\n","1455it [13:32,  2.51it/s]\u001b[A\n","1456it [13:32,  2.50it/s]\u001b[A\n","1457it [13:33,  2.48it/s]\u001b[A\n","1458it [13:33,  2.49it/s]\u001b[A\n","1459it [13:34,  2.49it/s]\u001b[A\n","1460it [13:34,  2.49it/s]\u001b[A\n","1461it [13:34,  2.50it/s]\u001b[A\n","1462it [13:35,  2.50it/s]\u001b[A\n","1463it [13:35,  2.50it/s]\u001b[A\n","1464it [13:36,  2.50it/s]\u001b[A\n","1465it [13:36,  2.50it/s]\u001b[A\n","1466it [13:36,  2.50it/s]\u001b[A\n","1467it [13:37,  2.50it/s]\u001b[A\n","1468it [13:37,  2.48it/s]\u001b[A\n","1469it [13:38,  2.48it/s]\u001b[A\n","1470it [13:38,  2.48it/s]\u001b[A\n","1471it [13:38,  2.49it/s]\u001b[A\n","1472it [13:39,  2.49it/s]\u001b[A\n","1473it [13:39,  2.49it/s]\u001b[A\n","1474it [13:40,  2.49it/s]\u001b[A\n","1475it [13:40,  2.49it/s]\u001b[A\n","1476it [13:40,  2.50it/s]\u001b[A\n","1477it [13:41,  2.50it/s]\u001b[A\n","1478it [13:41,  2.50it/s]\u001b[A\n","1479it [13:42,  2.49it/s]\u001b[A\n","Epoch:  89% 56/63 [14:42<00:55,  7.97s/it]\n","1481it [14:13,  9.73s/it]\u001b[A\n","1482it [14:14,  6.93s/it]\u001b[A\n","1483it [14:14,  4.97s/it]\u001b[A\n","1484it [14:15,  3.60s/it]\u001b[A\n","1485it [14:15,  2.64s/it]\u001b[A\n","1486it [14:15,  1.97s/it]\u001b[A\n","1487it [14:16,  1.50s/it]\u001b[A\n","1488it [14:16,  1.17s/it]\u001b[A\n","1489it [14:17,  1.07it/s]\u001b[A\n","1490it [14:17,  1.29it/s]\u001b[A\n","1491it [14:17,  1.51it/s]\u001b[A\n","1492it [14:18,  1.70it/s]\u001b[A\n","1493it [14:18,  1.89it/s]\u001b[A\n","1494it [14:19,  2.04it/s]\u001b[A\n","1495it [14:19,  2.16it/s]\u001b[A\n","1496it [14:19,  2.25it/s]\u001b[A\n","1497it [14:20,  2.32it/s]\u001b[A\n","1498it [14:20,  2.37it/s]\u001b[A\n","1499it [14:21,  2.41it/s]\u001b[A\n","1500it [14:21,  2.44it/s]\u001b[A\n","1501it [14:21,  2.47it/s]\u001b[A\n","1502it [14:22,  2.48it/s]\u001b[A\n","1503it [14:22,  2.47it/s]\u001b[A\n","1504it [14:23,  2.48it/s]\u001b[A\n","1505it [14:23,  2.48it/s]\u001b[A\n","1506it [14:23,  2.50it/s]\u001b[A\n","1507it [14:24,  2.50it/s]\u001b[A\n","1508it [14:24,  2.50it/s]\u001b[A\n","1509it [14:25,  2.51it/s]\u001b[A\n","1510it [14:25,  2.51it/s]\u001b[A\n","1511it [14:25,  2.51it/s]\u001b[A\n","1512it [14:26,  2.52it/s]\u001b[A\n","1513it [14:26,  2.52it/s]\u001b[A\n","1514it [14:27,  2.51it/s]\u001b[A\n","1515it [14:27,  2.50it/s]\u001b[A\n","1516it [14:27,  2.51it/s]\u001b[A\n","1517it [14:28,  2.51it/s]\u001b[A\n","1518it [14:28,  2.51it/s]\u001b[A\n","1519it [14:29,  2.51it/s]\u001b[A\n","1520it [14:29,  2.51it/s]\u001b[A\n","1521it [14:29,  2.51it/s]\u001b[A\n","1522it [14:30,  2.52it/s]\u001b[A\n","1523it [14:30,  2.51it/s]\u001b[A\n","1524it [14:31,  2.52it/s]\u001b[A\n","1525it [14:31,  2.51it/s]\u001b[A\n","1526it [14:31,  2.49it/s]\u001b[A\n","1527it [14:32,  2.50it/s]\u001b[A\n","1528it [14:32,  2.50it/s]\u001b[A\n","1529it [14:33,  2.51it/s]\u001b[A\n","1530it [14:33,  2.51it/s]\u001b[A\n","1531it [14:33,  2.52it/s]\u001b[A\n","1532it [14:34,  2.52it/s]\u001b[A\n","1533it [14:34,  2.52it/s]\u001b[A\n","1534it [14:35,  2.52it/s]\u001b[A\n","1535it [14:35,  2.52it/s]\u001b[A\n","1536it [14:35,  2.52it/s]\u001b[A\n","1537it [14:36,  2.51it/s]\u001b[A\n","1538it [14:36,  2.50it/s]\u001b[A\n","1539it [14:37,  2.50it/s]\u001b[A\n","1540it [14:37,  2.50it/s]\u001b[A\n","1541it [14:37,  2.51it/s]\u001b[A\n","1542it [14:38,  2.50it/s]\u001b[A\n","1543it [14:38,  2.51it/s]\u001b[A\n","1544it [14:39,  2.51it/s]\u001b[A\n","1545it [14:39,  2.52it/s]\u001b[A\n","1546it [14:39,  2.52it/s]\u001b[A\n","1547it [14:40,  2.52it/s]\u001b[A\n","1548it [14:40,  2.52it/s]\u001b[A\n","1549it [14:41,  2.50it/s]\u001b[A\n","1550it [14:41,  2.50it/s]\u001b[A\n","1551it [14:41,  2.50it/s]\u001b[A\n","1552it [14:42,  2.51it/s]\u001b[A\n","1553it [14:42,  2.51it/s]\u001b[A\n","1554it [14:43,  2.51it/s]\u001b[A\n","1555it [14:43,  2.51it/s]\u001b[A\n","1556it [14:43,  2.52it/s]\u001b[A\n","1557it [14:44,  2.52it/s]\u001b[A\n","1558it [14:44,  2.52it/s]\u001b[A\n","1559it [14:45,  2.52it/s]\u001b[A\n","1560it [14:45,  2.51it/s]\u001b[A\n","1561it [14:45,  2.50it/s]\u001b[A\n","1562it [14:46,  2.50it/s]\u001b[A\n","1563it [14:46,  2.51it/s]\u001b[A\n","1564it [14:47,  2.52it/s]\u001b[A\n","1565it [14:47,  2.51it/s]\u001b[A\n","1566it [14:47,  2.52it/s]\u001b[A\n","1567it [14:48,  2.52it/s]\u001b[A\n","1568it [14:48,  2.52it/s]\u001b[A\n","1569it [14:49,  2.52it/s]\u001b[A\n","1570it [14:49,  2.51it/s]\u001b[A\n","1571it [14:49,  2.52it/s]\u001b[A\n","1572it [14:50,  2.51it/s]\u001b[A\n","1573it [14:50,  2.50it/s]\u001b[A\n","1574it [14:51,  2.50it/s]\u001b[A\n","1575it [14:51,  2.51it/s]\u001b[A\n","1576it [14:51,  2.52it/s]\u001b[A\n","1577it [14:52,  2.52it/s]\u001b[A\n","1578it [14:52,  2.52it/s]\u001b[A\n","1579it [14:53,  2.52it/s]\u001b[A\n","1580it [14:53,  2.51it/s]\u001b[A\n","1581it [14:53,  2.52it/s]\u001b[A\n","1582it [14:54,  2.53it/s]\u001b[A\n","1583it [14:54,  2.52it/s]\u001b[A\n","1584it [14:55,  2.50it/s]\u001b[A\n","1585it [14:55,  2.50it/s]\u001b[A\n","1586it [14:55,  2.51it/s]\u001b[A\n","1587it [14:56,  2.52it/s]\u001b[A\n","1588it [14:56,  2.51it/s]\u001b[A\n","1589it [14:57,  2.51it/s]\u001b[A\n","1590it [14:57,  2.52it/s]\u001b[A\n","1591it [14:57,  2.52it/s]\u001b[A\n","1592it [14:58,  2.52it/s]\u001b[A\n","1593it [14:58,  2.52it/s]\u001b[A\n","1594it [14:59,  2.53it/s]\u001b[A\n","1595it [14:59,  2.51it/s]\u001b[A\n","1596it [14:59,  2.50it/s]\u001b[A\n","1597it [15:00,  2.51it/s]\u001b[A\n","1598it [15:00,  2.51it/s]\u001b[A\n","1599it [15:01,  2.52it/s]\u001b[A\n","1600it [15:01,  2.51it/s]\u001b[A\n","1601it [15:01,  2.51it/s]\u001b[A\n","1602it [15:02,  2.52it/s]\u001b[A\n","1603it [15:02,  2.52it/s]\u001b[A\n","1604it [15:02,  2.52it/s]\u001b[A\n","1605it [15:03,  2.52it/s]\u001b[A\n","1606it [15:03,  2.51it/s]\u001b[A\n","1607it [15:04,  2.50it/s]\u001b[A\n","1608it [15:04,  2.51it/s]\u001b[A\n","1609it [15:04,  2.51it/s]\u001b[A\n","1610it [15:05,  2.51it/s]\u001b[A\n","1611it [15:05,  2.51it/s]\u001b[A\n","1612it [15:06,  2.52it/s]\u001b[A\n","1613it [15:06,  2.52it/s]\u001b[A\n","1614it [15:06,  2.52it/s]\u001b[A\n","1615it [15:07,  2.52it/s]\u001b[A\n","1616it [15:07,  2.52it/s]\u001b[A\n","1617it [15:08,  2.52it/s]\u001b[A\n","1618it [15:08,  2.51it/s]\u001b[A\n","1619it [15:08,  2.50it/s]\u001b[A\n","1620it [15:09,  2.51it/s]\u001b[A\n","1621it [15:09,  2.51it/s]\u001b[A\n","1622it [15:10,  2.51it/s]\u001b[A\n","1623it [15:10,  2.52it/s]\u001b[A\n","1624it [15:10,  2.52it/s]\u001b[A\n","1625it [15:11,  2.52it/s]\u001b[A\n","1626it [15:11,  2.52it/s]\u001b[A\n","1627it [15:12,  2.53it/s]\u001b[A\n","1628it [15:12,  2.53it/s]\u001b[A\n","1629it [15:12,  2.53it/s]\u001b[A\n","1630it [15:13,  2.51it/s]\u001b[A\n","1631it [15:13,  2.50it/s]\u001b[A\n","1632it [15:14,  2.50it/s]\u001b[A\n","1633it [15:14,  2.51it/s]\u001b[A\n","1634it [15:14,  2.51it/s]\u001b[A\n","1635it [15:15,  2.51it/s]\u001b[A\n","1636it [15:15,  2.51it/s]\u001b[A\n","1637it [15:16,  2.52it/s]\u001b[A\n","1638it [15:16,  2.51it/s]\u001b[A\n","1639it [15:16,  2.52it/s]\u001b[A\n","1640it [15:17,  2.52it/s]\u001b[A\n","1641it [15:17,  2.51it/s]\u001b[A\n","1642it [15:18,  2.50it/s]\u001b[A\n","1643it [15:18,  2.50it/s]\u001b[A\n","1644it [15:18,  2.51it/s]\u001b[A\n","1645it [15:19,  2.52it/s]\u001b[A\n","1646it [15:19,  2.52it/s]\u001b[A\n","1647it [15:20,  2.52it/s]\u001b[A\n","1648it [15:20,  2.52it/s]\u001b[A\n","1649it [15:20,  2.52it/s]\u001b[A\n","1650it [15:21,  2.52it/s]\u001b[A\n","1651it [15:21,  2.52it/s]\u001b[A\n","1652it [15:22,  2.52it/s]\u001b[A\n","1653it [15:22,  2.51it/s]\u001b[A\n","1654it [15:22,  2.51it/s]\u001b[A\n","1655it [15:23,  2.51it/s]\u001b[A\n","1656it [15:23,  2.52it/s]\u001b[A\n","1657it [15:24,  2.52it/s]\u001b[A\n","1658it [15:24,  2.51it/s]\u001b[A\n","1659it [15:24,  2.52it/s]\u001b[A\n","1660it [15:25,  2.52it/s]\u001b[A\n","1661it [15:25,  2.52it/s]\u001b[A\n","1662it [15:26,  2.53it/s]\u001b[A\n","1663it [15:26,  2.52it/s]\u001b[A\n","1664it [15:26,  2.52it/s]\u001b[A\n","Epoch:  98% 62/63 [16:26<00:09,  9.02s/it]\n","1666it [15:58,  9.67s/it]\u001b[A\n","1667it [15:58,  6.89s/it]\u001b[A\n","1668it [15:59,  4.94s/it]\u001b[A\n","1669it [15:59,  3.58s/it]\u001b[A\n","1670it [16:00,  2.63s/it]\u001b[A\n","1671it [16:00,  1.96s/it]\u001b[A\n","1672it [16:00,  1.49s/it]\u001b[A\n","1673it [16:01,  1.16s/it]\u001b[A\n","1674it [16:01,  1.07it/s]\u001b[A\n","1675it [16:02,  1.29it/s]\u001b[A\n","1676it [16:02,  1.51it/s]\u001b[A\n","1677it [16:02,  1.71it/s]\u001b[A\n","1678it [16:03,  1.90it/s]\u001b[A\n","1679it [16:03,  2.05it/s]\u001b[A\n","1680it [16:04,  2.16it/s]\u001b[A\n","1681it [16:04,  2.26it/s]\u001b[A\n","1682it [16:04,  2.32it/s]\u001b[A\n","1683it [16:05,  2.38it/s]\u001b[A\n","1684it [16:05,  2.42it/s]\u001b[A\n","1685it [16:06,  2.44it/s]\u001b[A\n","1686it [16:06,  2.47it/s]\u001b[A\n","1687it [16:06,  2.47it/s]\u001b[A\n","1688it [16:07,  2.47it/s]\u001b[A\n","1689it [16:07,  2.48it/s]\u001b[A\n","1690it [16:08,  2.48it/s]\u001b[A\n","1691it [16:08,  2.50it/s]\u001b[A\n","1692it [16:08,  2.50it/s]\u001b[A\n","1693it [16:09,  2.50it/s]\u001b[A\n","1694it [16:09,  2.50it/s]\u001b[A\n","1695it [16:10,  2.50it/s]\u001b[A\n","1696it [16:10,  2.51it/s]\u001b[A\n","1697it [16:10,  2.51it/s]\u001b[A\n","1698it [16:11,  2.52it/s]\u001b[A\n","1699it [16:11,  2.50it/s]\u001b[A\n","1700it [16:12,  2.49it/s]\u001b[A\n","1701it [16:12,  2.49it/s]\u001b[A\n","1702it [16:12,  2.50it/s]\u001b[A\n","1703it [16:13,  2.50it/s]\u001b[A\n","1704it [16:13,  2.50it/s]\u001b[A\n","1705it [16:14,  2.50it/s]\u001b[A\n","1706it [16:14,  2.50it/s]\u001b[A\n","1707it [16:14,  2.50it/s]\u001b[A\n","1708it [16:15,  2.50it/s]\u001b[A\n","1709it [16:15,  2.51it/s]\u001b[A\n","1710it [16:16,  2.50it/s]\u001b[A\n","1711it [16:16,  2.48it/s]\u001b[A\n","1712it [16:16,  2.48it/s]\u001b[A\n","1713it [16:17,  2.49it/s]\u001b[A\n","1714it [16:17,  2.49it/s]\u001b[A\n","1715it [16:18,  2.50it/s]\u001b[A\n","1716it [16:18,  2.50it/s]\u001b[A\n","1717it [16:18,  2.50it/s]\u001b[A\n","1718it [16:19,  2.51it/s]\u001b[A\n","1719it [16:19,  2.50it/s]\u001b[A\n","1720it [16:20,  2.51it/s]\u001b[A\n","1721it [16:20,  2.51it/s]\u001b[A\n","1722it [16:20,  2.49it/s]\u001b[A\n","1723it [16:21,  2.49it/s]\u001b[A\n","1724it [16:21,  2.49it/s]\u001b[A\n","1725it [16:22,  2.50it/s]\u001b[A\n","1726it [16:22,  2.49it/s]\u001b[A\n","1727it [16:22,  2.49it/s]\u001b[A\n","1728it [16:23,  2.50it/s]\u001b[A\n","1729it [16:23,  2.50it/s]\u001b[A\n","1730it [16:24,  2.50it/s]\u001b[A\n","1731it [16:24,  2.50it/s]\u001b[A\n","1732it [16:24,  2.50it/s]\u001b[A\n","1733it [16:25,  2.50it/s]\u001b[A\n","1734it [16:25,  2.48it/s]\u001b[A\n","1735it [16:26,  2.48it/s]\u001b[A\n","1736it [16:26,  2.48it/s]\u001b[A\n","1737it [16:26,  2.49it/s]\u001b[A\n","1738it [16:27,  2.49it/s]\u001b[A\n","1739it [16:27,  2.49it/s]\u001b[A\n","1740it [16:28,  2.50it/s]\u001b[A\n","1741it [16:28,  2.50it/s]\u001b[A\n","1742it [16:28,  2.50it/s]\u001b[A\n","1743it [16:29,  2.51it/s]\u001b[A\n","1744it [16:29,  2.50it/s]\u001b[A\n","1745it [16:30,  2.49it/s]\u001b[A\n","1746it [16:30,  2.48it/s]\u001b[A\n","1747it [16:30,  2.48it/s]\u001b[A\n","1748it [16:31,  2.49it/s]\u001b[A\n","1749it [16:31,  2.50it/s]\u001b[A\n","1750it [16:32,  2.49it/s]\u001b[A\n","1751it [16:32,  2.49it/s]\u001b[A\n","1752it [16:32,  2.49it/s]\u001b[A\n","1753it [16:33,  2.49it/s]\u001b[A\n","1754it [16:33,  2.50it/s]\u001b[A\n","1755it [16:34,  2.50it/s]\u001b[A\n","1756it [16:34,  2.50it/s]\u001b[A\n","1757it [16:34,  2.48it/s]\u001b[A\n","1758it [16:35,  2.48it/s]\u001b[A\n","1759it [16:35,  2.48it/s]\u001b[A\n","1760it [16:36,  2.49it/s]\u001b[A\n","1761it [16:36,  2.50it/s]\u001b[A\n","1762it [16:37,  2.50it/s]\u001b[A\n","1763it [16:37,  2.49it/s]\u001b[A\n","1764it [16:37,  2.49it/s]\u001b[A\n","1765it [16:38,  2.49it/s]\u001b[A\n","1766it [16:38,  2.50it/s]\u001b[A\n","1767it [16:39,  2.50it/s]\u001b[A\n","1768it [16:39,  2.49it/s]\u001b[A\n","1769it [16:39,  2.47it/s]\u001b[A\n","1770it [16:40,  2.48it/s]\u001b[A\n","1771it [16:40,  2.48it/s]\u001b[A\n","1772it [16:41,  2.49it/s]\u001b[A\n","1773it [16:41,  2.49it/s]\u001b[A\n","1774it [16:41,  2.49it/s]\u001b[A\n","1775it [16:42,  2.49it/s]\u001b[A\n","1776it [16:42,  2.49it/s]\u001b[A\n","1777it [16:43,  2.49it/s]\u001b[A\n","1778it [16:43,  2.50it/s]\u001b[A\n","1779it [16:43,  2.49it/s]\u001b[A\n","1780it [16:44,  2.48it/s]\u001b[A\n","1781it [16:44,  2.47it/s]\u001b[A\n","1782it [16:45,  2.48it/s]\u001b[A\n","1783it [16:45,  2.49it/s]\u001b[A\n","1784it [16:45,  2.49it/s]\u001b[A\n","1785it [16:46,  2.50it/s]\u001b[A\n","1786it [16:46,  2.50it/s]\u001b[A\n","1787it [16:47,  2.49it/s]\u001b[A\n","1788it [16:47,  2.50it/s]\u001b[A\n","1789it [16:47,  2.50it/s]\u001b[A\n","1790it [16:48,  2.50it/s]\u001b[A\n","1791it [16:48,  2.50it/s]\u001b[A\n","1792it [16:49,  2.48it/s]\u001b[A\n","1793it [16:49,  2.48it/s]\u001b[A\n","1794it [16:49,  2.49it/s]\u001b[A\n","1795it [16:50,  2.49it/s]\u001b[A\n","1796it [16:50,  2.49it/s]\u001b[A\n","1797it [16:51,  2.49it/s]\u001b[A\n","1798it [16:51,  2.49it/s]\u001b[A\n","1799it [16:51,  2.50it/s]\u001b[A\n","1800it [16:52,  2.50it/s]\u001b[A\n","1801it [16:52,  2.50it/s]\u001b[A\n","1802it [16:53,  2.50it/s]\u001b[A\n","1803it [16:53,  2.49it/s]\u001b[A\n","1804it [16:53,  2.48it/s]\u001b[A\n","1805it [16:54,  2.48it/s]\u001b[A\n","1806it [16:54,  2.49it/s]\u001b[A\n","1807it [16:55,  2.49it/s]\u001b[A\n","1808it [16:55,  2.49it/s]\u001b[A\n","1809it [16:55,  2.49it/s]\u001b[A\n","1810it [16:56,  2.50it/s]\u001b[A\n","1811it [16:56,  2.50it/s]\u001b[A\n","1812it [16:57,  2.50it/s]\u001b[A\n","1813it [16:57,  2.50it/s]\u001b[A\n","1814it [16:57,  2.50it/s]\u001b[A\n","1815it [16:58,  2.48it/s]\u001b[A\n","1816it [16:58,  2.48it/s]\u001b[A\n","1817it [16:59,  2.49it/s]\u001b[A\n","1818it [16:59,  2.48it/s]\u001b[A\n","1819it [16:59,  2.48it/s]\u001b[A\n","1820it [17:00,  2.48it/s]\u001b[A\n","1821it [17:00,  2.48it/s]\u001b[A\n","1822it [17:01,  2.49it/s]\u001b[A\n","1823it [17:01,  2.49it/s]\u001b[A\n","1824it [17:01,  2.49it/s]\u001b[A\n","1825it [17:02,  2.49it/s]\u001b[A\n","1826it [17:02,  2.48it/s]\u001b[A\n","1827it [17:03,  2.48it/s]\u001b[A\n","1828it [17:03,  2.48it/s]\u001b[A\n","1829it [17:03,  2.48it/s]\u001b[A\n","1830it [17:04,  2.49it/s]\u001b[A\n","1831it [17:04,  2.49it/s]\u001b[A\n","1832it [17:05,  2.49it/s]\u001b[A\n","1833it [17:05,  2.49it/s]\u001b[A\n","1834it [17:05,  2.49it/s]\u001b[A\n","1835it [17:06,  2.49it/s]\u001b[A\n","1836it [17:06,  2.49it/s]\u001b[A\n","1837it [17:07,  2.50it/s]\u001b[A\n","1838it [17:07,  2.48it/s]\u001b[A\n","1839it [17:07,  2.47it/s]\u001b[A\n","1840it [17:08,  2.48it/s]\u001b[A\n","1841it [17:08,  2.48it/s]\u001b[A\n","1842it [17:09,  2.48it/s]\u001b[A\n","1843it [17:09,  2.48it/s]\u001b[A\n","1844it [17:09,  2.49it/s]\u001b[A\n","1845it [17:10,  2.49it/s]\u001b[A\n","1846it [17:10,  2.49it/s]\u001b[A\n","1847it [17:11,  2.49it/s]\u001b[A\n","1848it [17:11,  2.49it/s]\u001b[A\n","1849it [17:11,  2.49it/s]\u001b[A\n","Epoch:  98% 62/63 [17:43<00:17, 17.15s/it]\n","03/01/2022 12:28:19 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/01/2022 12:28:31 - INFO - __main__ -   *** Validate ***\n","\n","1851it [17:25,  4.14s/it]\u001b[A\n","1852it [17:25,  3.01s/it]\u001b[A\n","1853it [17:25,  2.23s/it]\u001b[A\n","1854it [17:26,  1.67s/it]\u001b[A\n","1855it [17:26,  1.28s/it]\u001b[A\n","1856it [17:27,  1.02s/it]\u001b[A\n","1857it [17:27,  1.21it/s]\u001b[A\n","1858it [17:27,  1.44it/s]\u001b[A\n","1859it [17:28,  1.66it/s]\u001b[A\n","1860it [17:28,  1.87it/s]\u001b[A\n","1861it [17:29,  2.01it/s]\u001b[A\n","1862it [17:29,  2.16it/s]\u001b[A\n","1863it [17:29,  2.27it/s]\u001b[A\n","1864it [17:30,  2.35it/s]\u001b[A\n","1865it [17:30,  2.42it/s]\u001b[A\n","1866it [17:31,  2.44it/s]\u001b[A\n","1867it [17:31,  2.49it/s]\u001b[A\n","1868it [17:31,  2.51it/s]\u001b[A\n","1869it [17:32,  2.52it/s]\u001b[A\n","1870it [17:32,  2.54it/s]\u001b[A\n","1871it [17:32,  2.54it/s]\u001b[A\n","1872it [17:33,  2.53it/s]\u001b[A\n","1873it [17:33,  2.53it/s]\u001b[A\n","1874it [17:34,  2.52it/s]\u001b[A\n","1875it [17:34,  2.52it/s]\u001b[A\n","1876it [17:34,  2.52it/s]\u001b[A\n","1877it [17:35,  2.51it/s]\u001b[A\n","1878it [17:35,  2.52it/s]\u001b[A\n","1879it [17:36,  2.51it/s]\u001b[A\n","1880it [17:36,  2.51it/s]\u001b[A\n","1881it [17:36,  2.51it/s]\u001b[A\n","1882it [17:37,  2.51it/s]\u001b[A\n","1883it [17:37,  2.50it/s]\u001b[A\n","1884it [17:38,  2.49it/s]\u001b[A\n","1885it [17:38,  2.47it/s]\u001b[A\n","1886it [17:38,  2.47it/s]\u001b[A\n","1887it [17:39,  2.48it/s]\u001b[A\n","1888it [17:39,  2.47it/s]\u001b[A\n","1889it [17:40,  2.47it/s]\u001b[A\n","1890it [17:40,  2.46it/s]\u001b[A\n","1891it [17:41,  2.46it/s]\u001b[A\n","1892it [17:41,  2.47it/s]\u001b[A\n","1893it [17:41,  2.46it/s]\u001b[A\n","1894it [17:42,  2.46it/s]\u001b[A\n","1895it [17:42,  2.46it/s]\u001b[A\n","1896it [17:43,  2.44it/s]\u001b[A\n","1897it [17:43,  2.43it/s]\u001b[A\n","1898it [17:43,  2.43it/s]\u001b[A\n","1899it [17:44,  2.42it/s]\u001b[A\n","1900it [17:44,  2.43it/s]\u001b[A\n","1901it [17:45,  2.42it/s]\u001b[A\n","1902it [17:45,  2.42it/s]\u001b[A\n","1903it [17:45,  2.41it/s]\u001b[A\n","1904it [17:46,  2.41it/s]\u001b[A\n","1905it [17:46,  2.41it/s]\u001b[A\n","1906it [17:47,  2.41it/s]\u001b[A\n","1907it [17:47,  2.39it/s]\u001b[A\n","1908it [17:48,  2.37it/s]\u001b[A\n","1909it [17:48,  2.37it/s]\u001b[A\n","1910it [17:48,  2.37it/s]\u001b[A\n","1911it [17:49,  2.37it/s]\u001b[A\n","1912it [17:49,  2.37it/s]\u001b[A\n","1913it [17:50,  2.38it/s]\u001b[A\n","1914it [17:50,  2.37it/s]\u001b[A\n","1915it [17:51,  2.37it/s]\u001b[A\n","1916it [17:51,  2.37it/s]\u001b[A\n","1917it [17:51,  2.37it/s]\u001b[A\n","1918it [17:52,  2.38it/s]\u001b[A\n","1919it [17:52,  2.35it/s]\u001b[A\n","1920it [17:53,  2.35it/s]\u001b[A\n","1921it [17:53,  2.35it/s]\u001b[A\n","1922it [17:53,  2.36it/s]\u001b[A\n","1923it [17:54,  2.36it/s]\u001b[A\n","1924it [17:54,  2.36it/s]\u001b[A\n","1925it [17:55,  2.37it/s]\u001b[A\n","1926it [17:55,  2.37it/s]\u001b[A\n","1927it [17:56,  2.38it/s]\u001b[A\n","1928it [17:56,  2.38it/s]\u001b[A\n","1929it [17:56,  2.38it/s]\u001b[A\n","1930it [17:57,  2.38it/s]\u001b[A\n","1931it [17:57,  2.37it/s]\u001b[A\n","1932it [17:58,  2.38it/s]\u001b[A\n","1933it [17:58,  2.39it/s]\u001b[A\n","1934it [17:59,  2.40it/s]\u001b[A\n","1935it [17:59,  2.40it/s]\u001b[A\n","1936it [17:59,  2.40it/s]\u001b[A\n","1937it [18:00,  2.41it/s]\u001b[A\n","1938it [18:00,  2.41it/s]\u001b[A\n","1939it [18:01,  2.42it/s]\u001b[A\n","1940it [18:01,  2.43it/s]\u001b[A\n","1941it [18:01,  2.43it/s]\u001b[A\n","1942it [18:02,  2.42it/s]\u001b[A\n","1943it [18:02,  2.41it/s]\u001b[A\n","1944it [18:03,  2.43it/s]\u001b[A\n","1945it [18:03,  2.44it/s]\u001b[A\n","1946it [18:03,  2.45it/s]\u001b[A\n","1947it [18:04,  2.45it/s]\u001b[A\n","1948it [18:04,  2.46it/s]\u001b[A\n","1949it [18:05,  2.46it/s]\u001b[A\n","1950it [18:05,  2.46it/s]\u001b[A\n","1951it [18:05,  2.47it/s]\u001b[A\n","1952it [18:06,  2.47it/s]\u001b[A\n","1953it [18:06,  2.47it/s]\u001b[A\n","1954it [18:07,  2.45it/s]\u001b[A\n","1955it [18:07,  2.45it/s]\u001b[A\n","1956it [18:08,  2.46it/s]\u001b[A\n","1957it [18:08,  2.47it/s]\u001b[A\n","1958it [18:08,  2.47it/s]\u001b[A\n","1959it [18:09,  2.47it/s]\u001b[A\n","1960it [18:09,  2.48it/s]\u001b[A\n","1961it [18:10,  1.46it/s]\u001b[A\n","1962it [18:11,  1.68it/s]\u001b[A\n","1963it [18:11,  1.84it/s]\u001b[A\n","1964it [18:12,  2.02it/s]\u001b[A\n","1965it [18:12,  2.12it/s]\u001b[A\n","1966it [18:12,  2.22it/s]\u001b[A\n","1967it [18:13,  2.32it/s]\u001b[A\n","1968it [18:13,  2.35it/s]\u001b[A\n","1969it [18:14,  2.42it/s]\u001b[A\n","1970it [18:14,  2.43it/s]\u001b[A\n","1971it [18:14,  2.46it/s]\u001b[A\n","1972it [18:15,  2.49it/s]\u001b[A\n","1973it [18:15,  2.48it/s]\u001b[A\n","1974it [18:16,  2.51it/s]\u001b[A\n","1975it [18:16,  2.51it/s]\u001b[A\n","1976it [18:16,  2.51it/s]\u001b[A\n","1977it [18:17,  2.51it/s]\u001b[A\n","1978it [18:17,  2.50it/s]\u001b[A\n","1979it [18:18,  2.52it/s]\u001b[A\n","1980it [18:18,  2.52it/s]\u001b[A\n","1981it [18:18,  2.52it/s]\u001b[A\n","1982it [18:19,  2.53it/s]\u001b[A\n","1983it [18:19,  2.52it/s]\u001b[A\n","1984it [18:20,  2.54it/s]\u001b[A\n","1985it [18:20,  2.53it/s]\u001b[A\n","1986it [18:20,  2.53it/s]\u001b[A\n","1987it [18:21,  2.54it/s]\u001b[A\n","1988it [18:21,  2.52it/s]\u001b[A\n","1989it [18:22,  2.52it/s]\u001b[A\n","1990it [18:22,  2.52it/s]\u001b[A\n","1991it [18:22,  2.53it/s]\u001b[A\n","1992it [18:23,  2.54it/s]\u001b[A\n","1993it [18:23,  2.53it/s]\u001b[A\n","1994it [18:24,  2.54it/s]\u001b[A\n","1995it [18:24,  2.54it/s]\u001b[A\n","1996it [18:24,  2.54it/s]\u001b[A\n","1997it [18:25,  2.56it/s]\u001b[A\n","1998it [18:25,  2.55it/s]\u001b[A\n","1999it [18:26,  2.55it/s]\u001b[A\n","2000it [18:26,  2.54it/s]\u001b[A\n","2001it [18:26,  2.52it/s]\u001b[A\n","2002it [18:27,  2.53it/s]\u001b[A\n","2003it [18:27,  2.53it/s]\u001b[A\n","2004it [18:27,  2.54it/s]\u001b[A\n","2005it [18:28,  2.54it/s]\u001b[A\n","2006it [18:28,  2.54it/s]\u001b[A\n","2007it [18:29,  2.55it/s]\u001b[A\n","2008it [18:29,  2.55it/s]\u001b[A\n","2009it [18:29,  2.55it/s]\u001b[A\n","2010it [18:30,  2.55it/s]\u001b[A\n","2011it [18:30,  2.54it/s]\u001b[A\n","2012it [18:31,  2.54it/s]\u001b[A\n","2013it [18:31,  2.54it/s]\u001b[A\n","2014it [18:31,  2.54it/s]\u001b[A\n","2015it [18:32,  2.55it/s]\u001b[A\n","2016it [18:32,  2.55it/s]\u001b[A\n","2017it [18:33,  2.55it/s]\u001b[A\n","2018it [18:33,  2.56it/s]\u001b[A\n","2019it [18:33,  2.55it/s]\u001b[A\n","2020it [18:34,  2.56it/s]\u001b[A\n","2021it [18:34,  2.55it/s]\u001b[A\n","2022it [18:35,  2.56it/s]\u001b[A\n","2023it [18:35,  2.54it/s]\u001b[A\n","2024it [18:35,  2.53it/s]\u001b[A\n","2025it [18:36,  2.54it/s]\u001b[A\n","2026it [18:36,  2.55it/s]\u001b[A\n","2027it [18:37,  2.55it/s]\u001b[A\n","2028it [18:37,  2.55it/s]\u001b[A\n","2029it [18:37,  2.55it/s]\u001b[A\n","2030it [18:38,  2.56it/s]\u001b[A\n","2031it [18:38,  2.55it/s]\u001b[A\n","2032it [18:38,  2.55it/s]\u001b[A\n","2033it [18:39,  2.56it/s]\u001b[A\n","2034it [18:39,  2.54it/s]\u001b[A\n","2035it [18:40,  2.53it/s]\u001b[A03/01/2022 12:29:47 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/01/2022 12:29:47 - INFO - __main__ -     eval_loss = 1.6975188255310059\n","03/01/2022 12:29:47 - INFO - __main__ -     eval_auroc = tensor(0.7854)\n","03/01/2022 12:29:47 - INFO - __main__ -     eval_recall = tensor(0.5200)\n","03/01/2022 12:29:47 - INFO - __main__ -     eval_f1 = tensor(0.4062)\n","03/01/2022 12:29:47 - INFO - root -   *** Test ***\n","\n","2036it [18:40,  2.53it/s]\u001b[A\n","2037it [18:40,  2.54it/s]\u001b[A\n","2038it [18:41,  2.53it/s]\u001b[A\n","2039it [18:41,  2.55it/s]\u001b[A\n","2040it [18:42,  2.54it/s]\u001b[A\n","2041it [18:42,  2.54it/s]\u001b[A\n","2042it [18:42,  2.55it/s]\u001b[A\n","2043it [18:43,  2.54it/s]\u001b[A\n","2044it [18:43,  2.55it/s]\u001b[A\n","2045it [18:44,  2.55it/s]\u001b[A\n","2046it [18:44,  2.54it/s]\u001b[A\n","2047it [18:44,  2.54it/s]\u001b[A\n","2048it [18:45,  2.54it/s]\u001b[A\n","2049it [18:45,  2.55it/s]\u001b[A\n","2050it [18:46,  2.55it/s]\u001b[A\n","2051it [18:46,  2.55it/s]\u001b[A\n","2052it [18:46,  2.54it/s]\u001b[A\n","2053it [18:47,  2.54it/s]\u001b[A\n","2054it [18:47,  2.54it/s]\u001b[A\n","2055it [18:48,  2.52it/s]\u001b[A\n","2056it [18:48,  2.52it/s]\u001b[A\n","2057it [18:48,  2.54it/s]\u001b[A\n","2058it [18:49,  2.53it/s]\u001b[A\n","2059it [18:49,  2.54it/s]\u001b[A\n","2060it [18:50,  2.54it/s]\u001b[A\n","2061it [18:50,  2.53it/s]\u001b[A\n","2062it [18:50,  2.53it/s]\u001b[A\n","2063it [18:51,  2.53it/s]\u001b[A\n","2064it [18:51,  2.53it/s]\u001b[A\n","2065it [18:51,  2.53it/s]\u001b[A\n","2066it [18:52,  2.53it/s]\u001b[A\n","2067it [18:52,  2.53it/s]\u001b[A\n","2068it [18:53,  2.53it/s]\u001b[A\n","2069it [18:53,  2.53it/s]\u001b[A\n","2070it [18:53,  2.52it/s]\u001b[A\n","2071it [18:54,  2.53it/s]\u001b[A\n","2072it [18:54,  2.52it/s]\u001b[A\n","2073it [18:55,  2.53it/s]\u001b[A\n","2074it [18:55,  2.52it/s]\u001b[A\n","2075it [18:55,  2.51it/s]\u001b[A\n","2076it [18:56,  2.52it/s]\u001b[A\n","2077it [18:56,  2.51it/s]\u001b[A\n","2078it [18:57,  2.51it/s]\u001b[A\n","2079it [18:57,  2.52it/s]\u001b[A\n","2080it [18:57,  2.51it/s]\u001b[A\n","2081it [18:58,  2.50it/s]\u001b[A\n","2082it [18:58,  2.50it/s]\u001b[A\n","2083it [18:59,  2.50it/s]\u001b[A\n","2084it [18:59,  2.51it/s]\u001b[A\n","2085it [18:59,  2.51it/s]\u001b[A\n","2086it [19:00,  2.51it/s]\u001b[A\n","2087it [19:00,  2.51it/s]\u001b[A\n","2088it [19:01,  2.51it/s]\u001b[A\n","2089it [19:01,  2.51it/s]\u001b[A\n","2090it [19:01,  2.50it/s]\u001b[A\n","2091it [19:02,  2.50it/s]\u001b[A\n","2092it [19:02,  2.50it/s]\u001b[A\n","2093it [19:03,  2.50it/s]\u001b[A\n","2094it [19:03,  2.50it/s]\u001b[A\n","2095it [19:03,  2.48it/s]\u001b[A\n","2096it [19:04,  2.49it/s]\u001b[A\n","2097it [19:04,  2.49it/s]\u001b[A\n","2098it [19:05,  2.49it/s]\u001b[A\n","2099it [19:05,  2.50it/s]\u001b[A\n","2100it [19:05,  2.49it/s]\u001b[A\n","2101it [19:06,  2.49it/s]\u001b[A\n","2102it [19:06,  2.49it/s]\u001b[A\n","2103it [19:07,  2.48it/s]\u001b[A\n","2104it [19:07,  2.49it/s]\u001b[A\n","2105it [19:07,  2.49it/s]\u001b[A\n","2106it [19:08,  2.50it/s]\u001b[A\n","2107it [19:08,  2.49it/s]\u001b[A\n","2108it [19:09,  2.49it/s]\u001b[A\n","2109it [19:09,  2.49it/s]\u001b[A\n","2110it [19:09,  2.49it/s]\u001b[A\n","2111it [19:10,  2.49it/s]\u001b[A\n","2112it [19:10,  2.48it/s]\u001b[A\n","2113it [19:11,  2.49it/s]\u001b[A\n","2114it [19:11,  2.48it/s]\u001b[A\n","2115it [19:11,  2.47it/s]\u001b[A\n","2116it [19:12,  2.48it/s]\u001b[A\n","2117it [19:12,  2.48it/s]\u001b[A\n","2118it [19:13,  2.48it/s]\u001b[A\n","2119it [19:13,  2.48it/s]\u001b[A\n","2120it [19:14,  2.48it/s]\u001b[A\n","2121it [19:14,  2.47it/s]\u001b[A\n","2122it [19:14,  2.48it/s]\u001b[A\n","2123it [19:15,  2.48it/s]\u001b[A\n","2124it [19:15,  2.48it/s]\u001b[A\n","2125it [19:16,  2.49it/s]\u001b[A\n","2126it [19:16,  2.48it/s]\u001b[A\n","2127it [19:16,  2.48it/s]\u001b[A\n","2128it [19:17,  2.48it/s]\u001b[A\n","2129it [19:17,  2.48it/s]\u001b[A\n","2130it [19:18,  2.48it/s]\u001b[A\n","2131it [19:18,  2.48it/s]\u001b[A\n","2132it [19:18,  2.47it/s]\u001b[A\n","2133it [19:19,  2.48it/s]\u001b[A\n","2134it [19:19,  2.48it/s]\u001b[A\n","2135it [19:20,  2.47it/s]\u001b[A\n","2136it [19:20,  2.48it/s]\u001b[A\n","2137it [19:20,  2.49it/s]\u001b[A\n","2138it [19:21,  2.48it/s]\u001b[A\n","2139it [19:21,  2.49it/s]\u001b[A\n","2140it [19:22,  2.48it/s]\u001b[A\n","2141it [19:22,  2.48it/s]\u001b[A\n","2142it [19:22,  2.48it/s]\u001b[A\n","2143it [19:23,  2.49it/s]\u001b[A\n","2144it [19:23,  2.49it/s]\u001b[A\n","2145it [19:24,  2.49it/s]\u001b[A\n","2146it [19:24,  2.48it/s]\u001b[A\n","2147it [19:24,  2.48it/s]\u001b[A\n","2148it [19:25,  2.49it/s]\u001b[A\n","2149it [19:25,  2.49it/s]\u001b[A\n","2150it [19:26,  2.50it/s]\u001b[A\n","2151it [19:26,  2.49it/s]\u001b[A\n","2152it [19:26,  2.49it/s]\u001b[A\n","2153it [19:27,  2.49it/s]\u001b[A\n","2154it [19:27,  2.49it/s]\u001b[A\n","2155it [19:28,  2.49it/s]\u001b[A\n","2156it [19:28,  2.49it/s]\u001b[A\n","2157it [19:28,  2.49it/s]\u001b[A\n","2158it [19:29,  2.49it/s]\u001b[A\n","2159it [19:29,  2.49it/s]\u001b[A\n","2160it [19:30,  2.49it/s]\u001b[A\n","2161it [19:30,  2.48it/s]\u001b[A\n","2162it [19:30,  2.49it/s]\u001b[A\n","2163it [19:31,  2.49it/s]\u001b[A\n","2164it [19:31,  2.49it/s]\u001b[A\n","2165it [19:32,  2.50it/s]\u001b[A\n","2166it [19:32,  2.50it/s]\u001b[A\n","2167it [19:32,  2.50it/s]\u001b[A\n","2168it [19:33,  2.50it/s]\u001b[A\n","2169it [19:33,  2.50it/s]\u001b[A\n","2170it [19:34,  2.50it/s]\u001b[A\n","2171it [19:34,  2.49it/s]\u001b[A\n","2172it [19:34,  2.49it/s]\u001b[A\n","2173it [19:35,  2.49it/s]\u001b[A\n","2174it [19:35,  2.50it/s]\u001b[A\n","2175it [19:36,  2.49it/s]\u001b[A\n","2176it [19:36,  2.49it/s]\u001b[A\n","2177it [19:36,  2.49it/s]\u001b[A\n","2178it [19:37,  2.50it/s]\u001b[A\n","2179it [19:37,  2.51it/s]\u001b[A\n","2180it [19:38,  2.50it/s]\u001b[A\n","2181it [19:38,  2.50it/s]\u001b[A\n","2182it [19:38,  2.51it/s]\u001b[A\n","2183it [19:39,  2.50it/s]\u001b[A\n","2184it [19:39,  2.51it/s]\u001b[A\n","2185it [19:40,  2.51it/s]\u001b[A\n","2186it [19:40,  2.51it/s]\u001b[A\n","2187it [19:40,  2.51it/s]\u001b[A\n","2188it [19:41,  2.51it/s]\u001b[A\n","2189it [19:41,  2.51it/s]\u001b[A\n","2190it [19:42,  2.51it/s]\u001b[A\n","2191it [19:42,  2.50it/s]\u001b[A\n","2192it [19:42,  2.50it/s]\u001b[A\n","2193it [19:43,  2.51it/s]\u001b[A\n","2194it [19:43,  2.50it/s]\u001b[A\n","2195it [19:44,  2.49it/s]\u001b[A\n","2196it [19:44,  2.50it/s]\u001b[A\n","2197it [19:44,  2.51it/s]\u001b[A\n","2198it [19:45,  2.51it/s]\u001b[A\n","2199it [19:45,  2.51it/s]\u001b[A\n","2200it [19:46,  2.51it/s]\u001b[A\n","2201it [19:46,  2.50it/s]\u001b[A\n","2202it [19:46,  2.52it/s]\u001b[A\n","2203it [19:47,  2.51it/s]\u001b[A\n","2204it [19:47,  2.51it/s]\u001b[A\n","2205it [19:48,  2.51it/s]\u001b[A\n","2206it [19:48,  2.51it/s]\u001b[A\n","2207it [19:48,  2.51it/s]\u001b[A\n","2208it [19:49,  2.52it/s]\u001b[A\n","2209it [19:49,  2.52it/s]\u001b[A\n","2210it [19:50,  2.51it/s]\u001b[A\n","2211it [19:50,  2.51it/s]\u001b[A\n","2212it [19:50,  2.52it/s]\u001b[A\n","2213it [19:51,  2.51it/s]\u001b[A\n","2214it [19:51,  2.51it/s]\u001b[A\n","2215it [19:52,  2.51it/s]\u001b[A\n","2216it [19:52,  2.51it/s]\u001b[A\n","2217it [19:52,  2.51it/s]\u001b[A\n","2218it [19:53,  2.51it/s]\u001b[A\n","2219it [19:53,  2.51it/s]\u001b[A\n","2220it [19:54,  2.52it/s]\u001b[A\n","2221it [19:54,  2.50it/s]\u001b[A\n","2222it [19:54,  2.51it/s]\u001b[A\n","2223it [19:55,  2.51it/s]\u001b[A\n","2224it [19:55,  2.51it/s]\u001b[A\n","2225it [19:56,  2.51it/s]\u001b[A\n","2226it [19:56,  2.52it/s]\u001b[A\n","2227it [19:56,  2.52it/s]\u001b[A\n","2228it [19:57,  2.52it/s]\u001b[A\n","2229it [19:57,  2.52it/s]\u001b[A\n","2230it [19:58,  2.52it/s]\u001b[A\n","2231it [19:58,  2.51it/s]\u001b[A\n","2232it [19:58,  2.52it/s]\u001b[A\n","2233it [19:59,  2.52it/s]\u001b[A\n","2234it [19:59,  2.51it/s]\u001b[A\n","2235it [20:00,  2.51it/s]\u001b[A\n","2236it [20:00,  2.52it/s]\u001b[A\n","2237it [20:00,  2.52it/s]\u001b[A\n","2238it [20:01,  2.52it/s]\u001b[A\n","2239it [20:01,  2.52it/s]\u001b[A\n","2240it [20:02,  2.52it/s]\u001b[A\n","2241it [20:02,  2.51it/s]\u001b[A\n","2242it [20:02,  2.52it/s]\u001b[A\n","2243it [20:03,  2.52it/s]\u001b[A\n","2244it [20:03,  2.52it/s]\u001b[A\n","2245it [20:03,  2.52it/s]\u001b[A\n","2246it [20:04,  2.52it/s]\u001b[A\n","2247it [20:04,  2.52it/s]\u001b[A\n","2248it [20:05,  2.53it/s]\u001b[A\n","2249it [20:05,  2.53it/s]\u001b[A\n","2250it [20:05,  2.52it/s]\u001b[A\n","2251it [20:06,  2.52it/s]\u001b[A\n","2252it [20:06,  2.52it/s]\u001b[A\n","2253it [20:07,  2.53it/s]\u001b[A\n","2254it [20:07,  2.52it/s]\u001b[A\n","2255it [20:07,  2.51it/s]\u001b[A\n","2256it [20:08,  2.53it/s]\u001b[A\n","2257it [20:08,  2.53it/s]\u001b[A\n","2258it [20:09,  2.53it/s]\u001b[A\n","2259it [20:09,  2.53it/s]\u001b[A\n","2260it [20:09,  2.53it/s]\u001b[A\n","2261it [20:10,  2.52it/s]\u001b[A\n","2262it [20:10,  2.53it/s]\u001b[A\n","2263it [20:11,  2.52it/s]\u001b[A\n","2264it [20:11,  2.52it/s]\u001b[A\n","2265it [20:11,  2.52it/s]\u001b[A\n","2266it [20:12,  2.52it/s]\u001b[A\n","2267it [20:12,  2.52it/s]\u001b[A\n","2268it [20:13,  2.53it/s]\u001b[A\n","2269it [20:13,  2.53it/s]\u001b[A\n","2270it [20:13,  2.53it/s]\u001b[A\n","2271it [20:14,  2.52it/s]\u001b[A\n","2272it [20:14,  2.53it/s]\u001b[A\n","2273it [20:15,  2.52it/s]\u001b[A\n","2274it [20:15,  2.52it/s]\u001b[A\n","2275it [20:15,  2.51it/s]\u001b[A\n","2276it [20:16,  2.52it/s]\u001b[A\n","2277it [20:16,  2.51it/s]\u001b[A\n","2278it [20:17,  2.52it/s]\u001b[A\n","2279it [20:17,  2.52it/s]\u001b[A\n","2280it [20:17,  2.52it/s]\u001b[A\n","2281it [20:18,  2.51it/s]\u001b[A\n","2282it [20:18,  2.52it/s]\u001b[A\n","2283it [20:19,  2.52it/s]\u001b[A\n","2284it [20:19,  2.53it/s]\u001b[A\n","2285it [20:19,  2.52it/s]\u001b[A\n","2286it [20:20,  2.51it/s]\u001b[A\n","2287it [20:20,  2.52it/s]\u001b[A\n","2288it [20:21,  2.52it/s]\u001b[A\n","2289it [20:21,  2.52it/s]\u001b[A\n","2290it [20:21,  2.52it/s]\u001b[A\n","2291it [20:22,  2.51it/s]\u001b[A\n","2292it [20:22,  2.52it/s]\u001b[A\n","2293it [20:23,  2.51it/s]\u001b[A\n","2294it [20:23,  2.51it/s]\u001b[A\n","2295it [20:23,  2.51it/s]\u001b[A\n","2296it [20:24,  2.51it/s]\u001b[A\n","2297it [20:24,  2.52it/s]\u001b[A\n","2298it [20:25,  2.51it/s]\u001b[A\n","2299it [20:25,  2.51it/s]\u001b[A\n","2300it [20:25,  2.51it/s]\u001b[A\n","2301it [20:26,  2.51it/s]\u001b[A\n","2302it [20:26,  2.51it/s]\u001b[A\n","2303it [20:27,  2.52it/s]\u001b[A\n","2304it [20:27,  2.52it/s]\u001b[A\n","2305it [20:27,  2.52it/s]\u001b[A\n","2306it [20:28,  2.51it/s]\u001b[A\n","2307it [20:28,  2.51it/s]\u001b[A\n","2308it [20:29,  2.51it/s]\u001b[A\n","2309it [20:29,  2.51it/s]\u001b[A\n","2310it [20:29,  2.51it/s]\u001b[A\n","2311it [20:30,  2.51it/s]\u001b[A\n","2312it [20:30,  2.52it/s]\u001b[A\n","2313it [20:31,  2.51it/s]\u001b[A\n","2314it [20:31,  2.51it/s]\u001b[A\n","2315it [20:31,  2.51it/s]\u001b[A\n","2316it [20:32,  2.51it/s]\u001b[A\n","2317it [20:32,  2.52it/s]\u001b[A\n","2318it [20:32,  2.52it/s]\u001b[A\n","2319it [20:33,  2.52it/s]\u001b[A\n","2320it [20:33,  2.52it/s]\u001b[A\n","2321it [20:34,  2.51it/s]\u001b[A\n","2322it [20:34,  2.52it/s]\u001b[A\n","2323it [20:34,  2.51it/s]\u001b[A\n","2324it [20:35,  2.52it/s]\u001b[A\n","2325it [20:35,  2.52it/s]\u001b[A\n","2326it [20:36,  2.52it/s]\u001b[A\n","2327it [20:36,  2.52it/s]\u001b[A\n","2328it [20:36,  2.51it/s]\u001b[A\n","2329it [20:37,  2.52it/s]\u001b[A\n","2330it [20:37,  2.52it/s]\u001b[A\n","2331it [20:38,  2.51it/s]\u001b[A\n","2332it [20:38,  2.51it/s]\u001b[A\n","2333it [20:38,  2.51it/s]\u001b[A\n","2334it [20:39,  2.50it/s]\u001b[A\n","2335it [20:39,  2.50it/s]\u001b[A\n","2336it [20:40,  2.51it/s]\u001b[A\n","2337it [20:40,  2.51it/s]\u001b[A\n","2338it [20:40,  2.51it/s]\u001b[A\n","2339it [20:41,  2.51it/s]\u001b[A\n","2340it [20:41,  2.50it/s]\u001b[A\n","2341it [20:42,  2.50it/s]\u001b[A\n","2342it [20:42,  2.51it/s]\u001b[A\n","2343it [20:42,  2.52it/s]\u001b[A\n","2344it [20:43,  2.52it/s]\u001b[A\n","2345it [20:43,  2.52it/s]\u001b[A\n","2346it [20:44,  2.51it/s]\u001b[A\n","2347it [20:44,  2.52it/s]\u001b[A\n","2348it [20:44,  2.52it/s]\u001b[A\n","2349it [20:45,  2.52it/s]\u001b[A\n","2350it [20:45,  2.51it/s]\u001b[A\n","2351it [20:46,  2.51it/s]\u001b[A\n","2352it [20:46,  2.52it/s]\u001b[A\n","2353it [20:46,  2.52it/s]\u001b[A\n","2354it [20:47,  2.50it/s]\u001b[A03/01/2022 12:31:54 - INFO - __main__ -   ***** Test results spoilers *****\n","03/01/2022 12:31:54 - INFO - __main__ -     eval_loss = 1.6639256477355957\n","03/01/2022 12:31:54 - INFO - __main__ -     eval_auroc = tensor(0.7816)\n","03/01/2022 12:31:54 - INFO - __main__ -     eval_recall = tensor(0.5417)\n","03/01/2022 12:31:54 - INFO - __main__ -     eval_f1 = tensor(0.2921)\n","03/01/2022 12:31:54 - INFO - filelock -   Lock 140066844025808 acquired on log.lock\n","03/01/2022 12:31:54 - INFO - filelock -   Lock 140066844025808 released on log.lock\n","2354it [20:47,  1.89it/s]\n"]}],"source":["!source env/bin/activate; TAG=handcrafted_10x TYPE=prompt-demo TASK=spoilers BS=8 LR=2e-5 SEED=42 MODEL=roberta-large bash run_experiment.sh \"--demo_filter --demo_filter_model sbert-roberta-large\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sq1oOn6FzHjy","executionInfo":{"status":"ok","timestamp":1646472613861,"user_tz":480,"elapsed":8259872,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"}},"outputId":"fa82992b-fa2e-40cf-e0b1-4a7010a1fdb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: 100% 1.20k/1.20k [00:00<00:00, 1.15MB/s]\n","Downloading: 100% 11.4G/11.4G [04:55<00:00, 38.6MB/s]\n","Downloading: 100% 792k/792k [00:00<00:00, 11.1MB/s]\n","# spoilers 16 13 100\n","| dataset examples\n","| {'label': 0, 'text': ['fuck him']}\n","| {'label': 1, 'text': ['AAAAAAA eyy levi vs zeke alrdy happening djnjkfbje,rkfker im not ready']}\n","\n","| mapping\n","| {0: 'No', 1: 'Yes'}\n","####### example #######\n","Fuck him<extra_id_0> No<extra_id_1>\n"," ⁇ <extra_id_0> No<extra_id_1>\n","LES GOO<extra_id_0> No<extra_id_1>\n","####### example #######\n","\n","100% 18/18 [17:22<00:00, 57.90s/it]\n","####### generated results #######\n","--------------\n","score: -1.2218170166015625\n","generated ids [32099, 5, 32098, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>.</s>\n","--------------\n","score: -2.7679529190063477\n","generated ids [32099, 5, 32098, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>!</s>\n","--------------\n","score: -2.8754196166992188\n","generated ids [32099, 58, 32098, 5, 1]\n","generated text <extra_id_0>?<extra_id_1>.</s>\n","--------------\n","score: -3.019397735595703\n","generated ids [32099, 55, 32098, 55, 1]\n","generated text <extra_id_0>!<extra_id_1>!</s>\n","--------------\n","score: -3.4200124740600586\n","generated ids [32099, 55, 32098, 5, 1]\n","generated text <extra_id_0>!<extra_id_1>.</s>\n","--------------\n","score: -3.474440574645996\n","generated ids [32099, 5, 32098, 58, 1]\n","generated text <extra_id_0>.<extra_id_1>?</s>\n","--------------\n","score: -3.635866165161133\n","generated ids [32099, 58, 32098, 55, 1]\n","generated text <extra_id_0>?<extra_id_1>!</s>\n","--------------\n","score: -4.3214521408081055\n","generated ids [32099, 6, 32098, 5, 1]\n","generated text <extra_id_0>,<extra_id_1>.</s>\n","--------------\n","score: -4.337130546569824\n","generated ids [32099, 58, 32098, 58, 1]\n","generated text <extra_id_0>?<extra_id_1>?</s>\n","--------------\n","score: -4.798468589782715\n","generated ids [32099, 6, 32098, 58, 1]\n","generated text <extra_id_0>,<extra_id_1>?</s>\n","--------------\n","score: -4.940351486206055\n","generated ids [32099, 11, 32098, 5, 1]\n","generated text <extra_id_0>▁and<extra_id_1>.</s>\n","--------------\n","score: -4.966713905334473\n","generated ids [32099, 233, 32098, 5, 1]\n","generated text <extra_id_0>...<extra_id_1>.</s>\n","--------------\n","score: -5.033829689025879\n","generated ids [32099, 55, 32098, 58, 1]\n","generated text <extra_id_0>!<extra_id_1>?</s>\n","--------------\n","score: -5.11971378326416\n","generated ids [32099, 6, 32098, 55, 1]\n","generated text <extra_id_0>,<extra_id_1>!</s>\n","--------------\n","score: -5.342216491699219\n","generated ids [32099, 10, 32098, 5, 1]\n","generated text <extra_id_0>:<extra_id_1>.</s>\n","--------------\n","score: -5.442136764526367\n","generated ids [32099, 233, 32098, 55, 1]\n","generated text <extra_id_0>...<extra_id_1>!</s>\n","--------------\n","score: -5.5059356689453125\n","generated ids [32099, 16, 32098, 5, 1]\n","generated text <extra_id_0>▁in<extra_id_1>.</s>\n","--------------\n","score: -5.6271467208862305\n","generated ids [32099, 11, 32098, 55, 1]\n","generated text <extra_id_0>▁and<extra_id_1>!</s>\n","--------------\n","score: -5.8178815841674805\n","generated ids [32099, 21, 32098, 5, 1]\n","generated text <extra_id_0>▁for<extra_id_1>.</s>\n","--------------\n","score: -5.975133895874023\n","generated ids [32099, 1603, 32098, 55, 1]\n","generated text <extra_id_0>!!<extra_id_1>!</s>\n","--------------\n","score: -6.019323348999023\n","generated ids [32099, 233, 32098, 58, 1]\n","generated text <extra_id_0>...<extra_id_1>?</s>\n","--------------\n","score: -6.238743782043457\n","generated ids [32099, 2824, 32098, 5, 1]\n","generated text <extra_id_0>....<extra_id_1>.</s>\n","--------------\n","score: -6.2390594482421875\n","generated ids [32099, 12, 32098, 5, 1]\n","generated text <extra_id_0>▁to<extra_id_1>.</s>\n","--------------\n","score: -6.239649772644043\n","generated ids [32099, 3158, 32098, 55, 1]\n","generated text <extra_id_0>!!!<extra_id_1>!</s>\n","--------------\n","score: -6.289035797119141\n","generated ids [32099, 10, 32098, 55, 1]\n","generated text <extra_id_0>:<extra_id_1>!</s>\n","--------------\n","score: -6.341745376586914\n","generated ids [32099, 5, 32098, 2824, 1]\n","generated text <extra_id_0>.<extra_id_1>....</s>\n","--------------\n","score: -6.414777755737305\n","generated ids [32099, 28, 32098, 5, 1]\n","generated text <extra_id_0>▁with<extra_id_1>.</s>\n","--------------\n","score: -6.573809623718262\n","generated ids [32099, 16, 32098, 55, 1]\n","generated text <extra_id_0>▁in<extra_id_1>!</s>\n","--------------\n","score: -6.582045555114746\n","generated ids [32099, 30, 32098, 5, 1]\n","generated text <extra_id_0>▁on<extra_id_1>.</s>\n","--------------\n","score: -6.631983757019043\n","generated ids [32099, 18, 32098, 5, 1]\n","generated text <extra_id_0>-<extra_id_1>.</s>\n","--------------\n","score: -6.642685890197754\n","generated ids [32099, 44, 32098, 5, 1]\n","generated text <extra_id_0>▁at<extra_id_1>.</s>\n","--------------\n","score: -6.704753875732422\n","generated ids [32099, 1603, 32098, 5, 1]\n","generated text <extra_id_0>!!<extra_id_1>.</s>\n","--------------\n","score: -6.739200592041016\n","generated ids [32099, 21, 32098, 55, 1]\n","generated text <extra_id_0>▁for<extra_id_1>!</s>\n","--------------\n","score: -6.7506303787231445\n","generated ids [32099, 137, 32098, 5, 1]\n","generated text <extra_id_0>).<extra_id_1>.</s>\n","--------------\n","score: -6.762882232666016\n","generated ids [32099, 121, 32098, 5, 1]\n","generated text <extra_id_0>\"<extra_id_1>.</s>\n","--------------\n","score: -6.7899885177612305\n","generated ids [32099, 2824, 32098, 55, 1]\n","generated text <extra_id_0>....<extra_id_1>!</s>\n","--------------\n","score: -6.814227104187012\n","generated ids [32099, 42, 32098, 5, 1]\n","generated text <extra_id_0>▁or<extra_id_1>.</s>\n","--------------\n","score: -6.915645599365234\n","generated ids [32099, 13, 32098, 5, 1]\n","generated text <extra_id_0>▁of<extra_id_1>.</s>\n","--------------\n","score: -6.935544967651367\n","generated ids [32099, 45, 32098, 5, 1]\n","generated text <extra_id_0>▁from<extra_id_1>.</s>\n","--------------\n","score: -6.9377031326293945\n","generated ids [32099, 535, 32098, 5, 1]\n","generated text <extra_id_0>.\"<extra_id_1>.</s>\n","--------------\n","score: -6.960565567016602\n","generated ids [32099, 57, 32098, 5, 1]\n","generated text <extra_id_0>▁by<extra_id_1>.</s>\n","--------------\n","score: -6.972816467285156\n","generated ids [32099, 19, 32098, 5, 1]\n","generated text <extra_id_0>▁is<extra_id_1>.</s>\n","--------------\n","score: -7.025813102722168\n","generated ids [32099, 27, 32098, 5, 1]\n","generated text <extra_id_0>▁I<extra_id_1>.</s>\n","--------------\n","score: -7.075336456298828\n","generated ids [32099, 18, 32098, 55, 1]\n","generated text <extra_id_0>-<extra_id_1>!</s>\n","--------------\n","score: -7.115358352661133\n","generated ids [32099, 3158, 32098, 5, 1]\n","generated text <extra_id_0>!!!<extra_id_1>.</s>\n","--------------\n","score: -7.1540117263793945\n","generated ids [32099, 12, 32098, 55, 1]\n","generated text <extra_id_0>▁to<extra_id_1>!</s>\n","--------------\n","score: -7.245495796203613\n","generated ids [32099, 30, 32098, 55, 1]\n","generated text <extra_id_0>▁on<extra_id_1>!</s>\n","--------------\n","score: -7.247825622558594\n","generated ids [32099, 5, 32098, 7, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>s.</s>\n","--------------\n","score: -7.253409385681152\n","generated ids [32099, 8665, 32098, 5, 1]\n","generated text <extra_id_0>???<extra_id_1>.</s>\n","--------------\n","score: -7.3152923583984375\n","generated ids [32099, 117, 32098, 5, 1]\n","generated text <extra_id_0>;<extra_id_1>.</s>\n","--------------\n","score: -7.340390205383301\n","generated ids [32099, 28, 32098, 55, 1]\n","generated text <extra_id_0>▁with<extra_id_1>!</s>\n","--------------\n","score: -7.3477373123168945\n","generated ids [32099, 9374, 32098, 5, 1]\n","generated text <extra_id_0>.....<extra_id_1>.</s>\n","--------------\n","score: -7.355802536010742\n","generated ids [32099, 8, 32098, 5, 1]\n","generated text <extra_id_0>▁the<extra_id_1>.</s>\n","--------------\n","score: -7.374383926391602\n","generated ids [32099, 61, 32098, 5, 1]\n","generated text <extra_id_0>)<extra_id_1>.</s>\n","--------------\n","score: -7.384424209594727\n","generated ids [32099, 8546, 32098, 5, 1]\n","generated text <extra_id_0>??<extra_id_1>.</s>\n","--------------\n","score: -7.403497695922852\n","generated ids [32099, 68, 32098, 5, 1]\n","generated text <extra_id_0>▁but<extra_id_1>.</s>\n","--------------\n","score: -7.445364952087402\n","generated ids [32099, 42, 32098, 58, 1]\n","generated text <extra_id_0>▁or<extra_id_1>?</s>\n","--------------\n","score: -7.476717948913574\n","generated ids [32099, 5, 32098, 9374, 1]\n","generated text <extra_id_0>.<extra_id_1>.....</s>\n","--------------\n","score: -7.48173713684082\n","generated ids [32099, 10769, 32098, 55, 1]\n","generated text <extra_id_0>?!<extra_id_1>!</s>\n","--------------\n","score: -7.491970062255859\n","generated ids [32099, 38, 32098, 5, 1]\n","generated text <extra_id_0>▁as<extra_id_1>.</s>\n","--------------\n","score: -7.511699676513672\n","generated ids [32099, 8665, 32098, 55, 1]\n","generated text <extra_id_0>???<extra_id_1>!</s>\n","--------------\n","score: -7.525190353393555\n","generated ids [32099, 7, 32098, 5, 1]\n","generated text <extra_id_0>s<extra_id_1>.</s>\n","--------------\n","score: -7.536100387573242\n","generated ids [32099, 78, 32098, 5, 1]\n","generated text <extra_id_0>▁so<extra_id_1>.</s>\n","--------------\n","score: -7.545064926147461\n","generated ids [32099, 5, 32098, 6, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>,.</s>\n","--------------\n","score: -7.572956085205078\n","generated ids [32099, 3274, 32098, 5, 1]\n","generated text <extra_id_0>▁=<extra_id_1>.</s>\n","--------------\n","score: -7.604872703552246\n","generated ids [32099, 5, 32098, 233, 1]\n","generated text <extra_id_0>.<extra_id_1>...</s>\n","--------------\n","score: -7.613193511962891\n","generated ids [32099, 11, 32098, 58, 1]\n","generated text <extra_id_0>▁and<extra_id_1>?</s>\n","--------------\n","score: -7.617954254150391\n","generated ids [32099, 121, 32098, 55, 1]\n","generated text <extra_id_0>\"<extra_id_1>!</s>\n","--------------\n","score: -7.625491142272949\n","generated ids [32099, 5, 32098, 535, 1]\n","generated text <extra_id_0>.<extra_id_1>.\"</s>\n","--------------\n","score: -7.663120269775391\n","generated ids [32099, 42, 32098, 55, 1]\n","generated text <extra_id_0>▁or<extra_id_1>!</s>\n","--------------\n","score: -7.667414665222168\n","generated ids [32099, 8665, 32098, 58, 1]\n","generated text <extra_id_0>???<extra_id_1>?</s>\n","--------------\n","score: -7.674284934997559\n","generated ids [32099, 58, 32098, 2824, 1]\n","generated text <extra_id_0>?<extra_id_1>....</s>\n","--------------\n","score: -7.677645683288574\n","generated ids [32099, 1603, 32098, 58, 1]\n","generated text <extra_id_0>!!<extra_id_1>?</s>\n","--------------\n","score: -7.690956115722656\n","generated ids [32099, 55, 32098, 2824, 1]\n","generated text <extra_id_0>!<extra_id_1>....</s>\n","--------------\n","score: -7.708894729614258\n","generated ids [32099, 2, 32098, 5, 1]\n","generated text <extra_id_0><unk><extra_id_1>.</s>\n","--------------\n","score: -7.729979515075684\n","generated ids [32099, 12887, 32098, 55, 1]\n","generated text <extra_id_0>!!!!<extra_id_1>!</s>\n","--------------\n","score: -7.7471818923950195\n","generated ids [32099, 55, 32098, 1603, 1]\n","generated text <extra_id_0>!<extra_id_1>!!</s>\n","--------------\n","score: -7.7602128982543945\n","generated ids [32099, 5, 32098, 1603, 1]\n","generated text <extra_id_0>.<extra_id_1>!!</s>\n","--------------\n","score: -7.761103630065918\n","generated ids [32099, 8546, 32098, 55, 1]\n","generated text <extra_id_0>??<extra_id_1>!</s>\n","--------------\n","score: -7.771567344665527\n","generated ids [32099, 465, 32098, 5, 1]\n","generated text <extra_id_0>▁No<extra_id_1>.</s>\n","--------------\n","score: -7.783194541931152\n","generated ids [32099, 44, 32098, 55, 1]\n","generated text <extra_id_0>▁at<extra_id_1>!</s>\n","--------------\n","score: -7.792331695556641\n","generated ids [32099, 31, 32098, 5, 1]\n","generated text <extra_id_0>'<extra_id_1>.</s>\n","--------------\n","score: -7.797260284423828\n","generated ids [32099, 18, 32098, 58, 1]\n","generated text <extra_id_0>-<extra_id_1>?</s>\n","--------------\n","score: -7.797460556030273\n","generated ids [32099, 2824, 32098, 58, 1]\n","generated text <extra_id_0>....<extra_id_1>?</s>\n","--------------\n","score: -7.801627159118652\n","generated ids [32099, 150, 32098, 5, 1]\n","generated text <extra_id_0>▁no<extra_id_1>.</s>\n","--------------\n","score: -7.821159362792969\n","generated ids [32099, 10, 32098, 58, 1]\n","generated text <extra_id_0>:<extra_id_1>?</s>\n","--------------\n","score: -7.826973915100098\n","generated ids [32099, 13, 32098, 55, 1]\n","generated text <extra_id_0>▁of<extra_id_1>!</s>\n","--------------\n","score: -7.829840660095215\n","generated ids [32099, 16, 32098, 58, 1]\n","generated text <extra_id_0>▁in<extra_id_1>?</s>\n","--------------\n","score: -7.8384599685668945\n","generated ids [32099, 8546, 32098, 58, 1]\n","generated text <extra_id_0>??<extra_id_1>?</s>\n","--------------\n","score: -7.845475196838379\n","generated ids [32099, 5, 32098, 2049, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁thanks.</s>\n","--------------\n","score: -7.874835014343262\n","generated ids [32099, 27, 32098, 55, 1]\n","generated text <extra_id_0>▁I<extra_id_1>!</s>\n","--------------\n","score: -7.918957710266113\n","generated ids [32099, 9374, 32098, 55, 1]\n","generated text <extra_id_0>.....<extra_id_1>!</s>\n","--------------\n","score: -7.946876525878906\n","generated ids [32099, 5, 32098, 754, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁please.</s>\n","--------------\n","score: -8.219295501708984\n","generated ids [32099, 7, 5, 32098, 5, 1]\n","generated text <extra_id_0>s.<extra_id_1>.</s>\n","--------------\n","score: -8.297714233398438\n","generated ids [32099, 5, 32098, 55, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>!.</s>\n","--------------\n","score: -8.356426239013672\n","generated ids [32099, 5, 32098, 34, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁it.</s>\n","--------------\n","score: -8.465116500854492\n","generated ids [32099, 5, 32098, 7, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>s!</s>\n","--------------\n","score: -8.533140182495117\n","generated ids [32099, 5, 32098, 6, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>,!</s>\n","--------------\n","score: -8.596519470214844\n","generated ids [32099, 5, 32098, 233, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>...!</s>\n","--------------\n","score: -8.641142845153809\n","generated ids [32099, 58, 32098, 1603, 1]\n","generated text <extra_id_0>?<extra_id_1>!!</s>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls**sent_0*.*mask*.*sep+*\n","*cls**sent_0*.*mask*!*sep+*\n","*cls**sent_0*?*mask*.*sep+*\n","*cls**sent_0*!*mask*!*sep+*\n","*cls**sent_0*!*mask*.*sep+*\n","*cls**sent_0*.*mask*?*sep+*\n","*cls**sent_0*?*mask*!*sep+*\n","*cls**sent_0*,*mask*.*sep+*\n","*cls**sent_0*?*mask*?*sep+*\n","*cls**sent_0*,*mask*?*sep+*\n","*cls**sent_0*▁and*mask*.*sep+*\n","*cls**sent_0*...*mask*.*sep+*\n","*cls**sent_0*!*mask*?*sep+*\n","*cls**sent_0*,*mask*!*sep+*\n","*cls**sent_0*:*mask*.*sep+*\n","*cls**sent_0*...*mask*!*sep+*\n","*cls**sent_0*▁in*mask*.*sep+*\n","*cls**sent_0*▁and*mask*!*sep+*\n","*cls**sent_0*▁for*mask*.*sep+*\n","*cls**sent_0*!!*mask*!*sep+*\n","*cls**sent_0*...*mask*?*sep+*\n","*cls**sent_0*....*mask*.*sep+*\n","*cls**sent_0*▁to*mask*.*sep+*\n","*cls**sent_0*!!!*mask*!*sep+*\n","*cls**sent_0*:*mask*!*sep+*\n","*cls**sent_0*.*mask*....*sep+*\n","*cls**sent_0*▁with*mask*.*sep+*\n","*cls**sent_0*▁in*mask*!*sep+*\n","*cls**sent_0*▁on*mask*.*sep+*\n","*cls**sent_0*-*mask*.*sep+*\n","*cls**sent_0*▁at*mask*.*sep+*\n","*cls**sent_0*!!*mask*.*sep+*\n","*cls**sent_0*▁for*mask*!*sep+*\n","*cls**sent_0*).*mask*.*sep+*\n","*cls**sent_0*\"*mask*.*sep+*\n","*cls**sent_0*....*mask*!*sep+*\n","*cls**sent_0*▁or*mask*.*sep+*\n","*cls**sent_0*▁of*mask*.*sep+*\n","*cls**sent_0*▁from*mask*.*sep+*\n","*cls**sent_0*.\"*mask*.*sep+*\n","*cls**sent_0*▁by*mask*.*sep+*\n","*cls**sent_0*▁is*mask*.*sep+*\n","*cls**sent_0*▁I*mask*.*sep+*\n","*cls**sent_0*-*mask*!*sep+*\n","*cls**sent_0*!!!*mask*.*sep+*\n","*cls**sent_0*▁to*mask*!*sep+*\n","*cls**sent_0*▁on*mask*!*sep+*\n","*cls**sent_0*.*mask*s.*sep+*\n","*cls**sent_0*???*mask*.*sep+*\n","*cls**sent_0*;*mask*.*sep+*\n","####### generated templates #######\n","\n","####### example #######\n",".<extra_id_0> No<extra_id_1> Fuck him\n",".<extra_id_0> No<extra_id_1>  ⁇ \n",".<extra_id_0> No<extra_id_1> LES GOO\n","####### example #######\n","\n","100% 18/18 [18:21<00:00, 61.17s/it]\n","####### generated results #######\n","--------------\n","score: -3.468428611755371\n","generated ids [32099, 5, 32098, 5, 32097]\n","generated text <extra_id_0>.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -4.382051467895508\n","generated ids [32099, 5, 32098, 55, 32097]\n","generated text <extra_id_0>.<extra_id_1>!<extra_id_2>\n","--------------\n","score: -4.522876739501953\n","generated ids [32099, 5, 32098, 6, 32097]\n","generated text <extra_id_0>.<extra_id_1>,<extra_id_2>\n","--------------\n","score: -5.192413330078125\n","generated ids [32099, 3359, 32098, 55, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>!<extra_id_2>\n","--------------\n","score: -5.329358100891113\n","generated ids [32099, 465, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁No.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.348462104797363\n","generated ids [32099, 2163, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.611668586730957\n","generated ids [32099, 275, 32098, 5, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.642195701599121\n","generated ids [32099, 3359, 32098, 5, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.966270446777344\n","generated ids [32099, 5, 32098, 58, 32097]\n","generated text <extra_id_0>.<extra_id_1>?<extra_id_2>\n","--------------\n","score: -6.026120185852051\n","generated ids [32099, 299, 32098, 5, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.117805480957031\n","generated ids [32099, 2163, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.206509590148926\n","generated ids [32099, 465, 32098, 5, 32097]\n","generated text <extra_id_0>▁No<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.217319488525391\n","generated ids [32099, 2163, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.23952579498291\n","generated ids [32099, 275, 32098, 55, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.257397651672363\n","generated ids [32099, 275, 32098, 6, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.295951843261719\n","generated ids [32099, 1300, 32098, 5, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.347886085510254\n","generated ids [32099, 2490, 32098, 5, 32097]\n","generated text <extra_id_0>▁><extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.3704328536987305\n","generated ids [32099, 264, 32098, 5, 32097]\n","generated text <extra_id_0>▁So<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.414079189300537\n","generated ids [32099, 37, 32098, 5, 32097]\n","generated text <extra_id_0>▁The<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.41858434677124\n","generated ids [32099, 96, 32098, 5, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.424838066101074\n","generated ids [32099, 71, 32098, 5, 32097]\n","generated text <extra_id_0>▁A<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.4695892333984375\n","generated ids [32099, 1682, 32098, 5, 32097]\n","generated text <extra_id_0>▁2.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.490856647491455\n","generated ids [32099, 1877, 32098, 5, 32097]\n","generated text <extra_id_0>▁3.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.5012054443359375\n","generated ids [32099, 5, 32098, 233, 32097]\n","generated text <extra_id_0>.<extra_id_1>...<extra_id_2>\n","--------------\n","score: -6.601796627044678\n","generated ids [32099, 1429, 32098, 5, 32097]\n","generated text <extra_id_0>▁*<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.652213096618652\n","generated ids [32099, 299, 32098, 55, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.709785461425781\n","generated ids [32099, 2853, 32098, 5, 32097]\n","generated text <extra_id_0>▁4.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.740492820739746\n","generated ids [32099, 287, 5, 32098, 5, 32097]\n","generated text <extra_id_0>com.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.899898529052734\n","generated ids [32099, 3594, 32098, 5, 32097]\n","generated text <extra_id_0>▁5.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.9389543533325195\n","generated ids [32099, 1697, 32098, 5, 32097]\n","generated text <extra_id_0>▁•<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.951129913330078\n","generated ids [32099, 61, 32098, 5, 32097]\n","generated text <extra_id_0>)<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.971027374267578\n","generated ids [32099, 71, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁A.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.988164901733398\n","generated ids [32099, 465, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁No,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.003595352172852\n","generated ids [32099, 2163, 6, 32098, 6, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.016085624694824\n","generated ids [32099, 20510, 32098, 5, 32097]\n","generated text <extra_id_0>▁Absolutely<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.077771186828613\n","generated ids [32099, 6, 32098, 5, 32097]\n","generated text <extra_id_0>,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.120598793029785\n","generated ids [32099, 1333, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thanks.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.151937484741211\n","generated ids [32099, 1142, 32098, 5, 32097]\n","generated text <extra_id_0>▁Just<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.197295188903809\n","generated ids [32099, 9710, 32098, 5, 32097]\n","generated text <extra_id_0>▁Ah<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.208718299865723\n","generated ids [32099, 1300, 32098, 5, 1682, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>.▁2.<extra_id_2>\n","--------------\n","score: -7.257999420166016\n","generated ids [32099, 9710, 32098, 55, 32097]\n","generated text <extra_id_0>▁Ah<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.258688926696777\n","generated ids [32099, 299, 32098, 6, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.264352798461914\n","generated ids [32099, 11419, 32098, 55, 32097]\n","generated text <extra_id_0>▁Hell<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.310789108276367\n","generated ids [32099, 1562, 25, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thank▁you.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.310934543609619\n","generated ids [32099, 86, 32098, 5, 32097]\n","generated text <extra_id_0>▁In<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.329959869384766\n","generated ids [32099, 96, 32098, 55, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.339268684387207\n","generated ids [32099, 100, 19, 32098, 5, 32097]\n","generated text <extra_id_0>▁This▁is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.349923610687256\n","generated ids [32099, 209, 32098, 5, 32097]\n","generated text <extra_id_0>▁1<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.362039566040039\n","generated ids [32099, 2163, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁Yes!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.364221572875977\n","generated ids [32099, 27, 32098, 5, 32097]\n","generated text <extra_id_0>▁I<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.365718364715576\n","generated ids [32099, 14125, 32098, 5, 32097]\n","generated text <extra_id_0>......<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.366961479187012\n","generated ids [32099, 58, 32098, 5, 32097]\n","generated text <extra_id_0>?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.3727922439575195\n","generated ids [32099, 71, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁A:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.377326965332031\n","generated ids [32099, 11475, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yeah.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.395620822906494\n","generated ids [32099, 3359, 32098, 6, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.3990631103515625\n","generated ids [32099, 11, 32098, 5, 32097]\n","generated text <extra_id_0>▁and<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.409787654876709\n","generated ids [32099, 156, 32098, 6, 32097]\n","generated text <extra_id_0>▁If<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.412896156311035\n","generated ids [32099, 96, 32098, 6, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.414542198181152\n","generated ids [32099, 204, 32098, 5, 32097]\n","generated text <extra_id_0>▁2<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.4288129806518555\n","generated ids [32099, 264, 32098, 55, 32097]\n","generated text <extra_id_0>▁So<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.495062828063965\n","generated ids [32099, 11342, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Sorry.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.5057196617126465\n","generated ids [32099, 5835, 32098, 5, 32097]\n","generated text <extra_id_0>▁9.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.506656169891357\n","generated ids [32099, 2, 32098, 5, 32097]\n","generated text <extra_id_0><unk><extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.511106014251709\n","generated ids [32099, 5, 32098, 1603, 32097]\n","generated text <extra_id_0>.<extra_id_1>!!<extra_id_2>\n","--------------\n","score: -7.5140862464904785\n","generated ids [32099, 2163, 32098, 2163, 32097]\n","generated text <extra_id_0>▁Yes<extra_id_1>▁Yes<extra_id_2>\n","--------------\n","score: -7.553779602050781\n","generated ids [32099, 499, 32098, 5, 32097]\n","generated text <extra_id_0>▁My<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.561332702636719\n","generated ids [32099, 465, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁No!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.5696892738342285\n","generated ids [32099, 4357, 32098, 5, 32097]\n","generated text <extra_id_0>▁6.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.578649520874023\n","generated ids [32099, 2163, 11, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes▁and<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.5876312255859375\n","generated ids [32099, 1682, 32098, 5, 1877, 32097]\n","generated text <extra_id_0>▁2.<extra_id_1>.▁3.<extra_id_2>\n","--------------\n","score: -7.5923261642456055\n","generated ids [32099, 1429, 32098, 6, 32097]\n","generated text <extra_id_0>▁*<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.597675323486328\n","generated ids [32099, 955, 32098, 5, 32097]\n","generated text <extra_id_0>▁Or<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.607219219207764\n","generated ids [32099, 2490, 32098, 6, 32097]\n","generated text <extra_id_0>▁><extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.64619255065918\n","generated ids [32099, 4306, 32098, 5, 32097]\n","generated text <extra_id_0>▁7.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.6617326736450195\n","generated ids [32099, 1820, 32098, 5, 32097]\n","generated text <extra_id_0>▁|<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.669842720031738\n","generated ids [32099, 264, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁So,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.67164421081543\n","generated ids [32099, 27, 43, 32098, 5, 32097]\n","generated text <extra_id_0>▁I▁have<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.6726508140563965\n","generated ids [32099, 5, 32098, 11, 32097]\n","generated text <extra_id_0>.<extra_id_1>▁and<extra_id_2>\n","--------------\n","score: -7.6905927658081055\n","generated ids [32099, 1300, 32098, 6, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.699089050292969\n","generated ids [32099, 27, 317, 32098, 5, 32097]\n","generated text <extra_id_0>▁I▁think<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.705231666564941\n","generated ids [32099, 5, 32098, 2824, 32097]\n","generated text <extra_id_0>.<extra_id_1>....<extra_id_2>\n","--------------\n","score: -7.713710784912109\n","generated ids [32099, 290, 19, 32098, 5, 32097]\n","generated text <extra_id_0>▁There▁is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.715217590332031\n","generated ids [32099, 419, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁Re:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.720565319061279\n","generated ids [32099, 220, 32098, 5, 32097]\n","generated text <extra_id_0>▁3<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.750735282897949\n","generated ids [32099, 1626, 1024, 32098, 5, 32097]\n","generated text <extra_id_0>▁Haha<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.773403167724609\n","generated ids [32099, 1333, 55, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thanks!<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.804340362548828\n","generated ids [32099, 1682, 32098, 6, 32097]\n","generated text <extra_id_0>▁2.<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.8329010009765625\n","generated ids [32099, 2163, 6, 32098, 55, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.838263034820557\n","generated ids [32099, 314, 32098, 5, 32097]\n","generated text <extra_id_0>▁4<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.846686363220215\n","generated ids [32099, 5, 58, 32098, 5, 32097]\n","generated text <extra_id_0>.?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.874466896057129\n","generated ids [32099, 94, 19, 32098, 5, 32097]\n","generated text <extra_id_0>▁It▁is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.92708158493042\n","generated ids [32099, 287, 5, 32098, 6, 32097]\n","generated text <extra_id_0>com.<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.936504364013672\n","generated ids [32099, 1615, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁Why?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.938315391540527\n","generated ids [32099, 27, 7, 34, 32098, 58, 32097]\n","generated text <extra_id_0>▁Is▁it<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.951229095458984\n","generated ids [32099, 287, 5, 32098, 55, 32097]\n","generated text <extra_id_0>com.<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.95488166809082\n","generated ids [32099, 363, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁What?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.983869552612305\n","generated ids [32099, 27, 7, 32098, 5, 32097]\n","generated text <extra_id_0>▁Is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.9927978515625\n","generated ids [32099, 3359, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Oh,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -8.079388618469238\n","generated ids [32099, 5, 32098, 7, 5, 32097]\n","generated text <extra_id_0>.<extra_id_1>s.<extra_id_2>\n","--------------\n","score: -8.410041809082031\n","generated ids [32099, 1562, 25, 55, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thank▁you!<extra_id_1>.<extra_id_2>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls*.*mask*.*+sent_0**sep+*\n","*cls*.*mask*!*+sent_0**sep+*\n","*cls*.*mask*,*+sent_0**sep+*\n","*cls*▁Oh*mask*!*+sent_0**sep+*\n","*cls*▁No.*mask*.*+sent_0**sep+*\n","*cls*▁Yes.*mask*.*+sent_0**sep+*\n","*cls*▁And*mask*.*+sent_0**sep+*\n","*cls*▁Oh*mask*.*+sent_0**sep+*\n","*cls*.*mask*?*+sent_0**sep+*\n","*cls*▁But*mask*.*+sent_0**sep+*\n","*cls*▁Yes*mask*.*+sent_0**sep+*\n","*cls*▁No*mask*.*+sent_0**sep+*\n","*cls*▁Yes,*mask*.*+sent_0**sep+*\n","*cls*▁And*mask*!*+sent_0**sep+*\n","*cls*▁And*mask*,*+sent_0**sep+*\n","*cls*▁1.*mask*.*+sent_0**sep+*\n","*cls*▁>*mask*.*+sent_0**sep+*\n","*cls*▁So*mask*.*+sent_0**sep+*\n","*cls*▁The*mask*.*+sent_0**sep+*\n","*cls*▁\"*mask*.*+sent_0**sep+*\n","*cls*▁A*mask*.*+sent_0**sep+*\n","*cls*▁2.*mask*.*+sent_0**sep+*\n","*cls*▁3.*mask*.*+sent_0**sep+*\n","*cls*.*mask*...*+sent_0**sep+*\n","*cls*▁**mask*.*+sent_0**sep+*\n","*cls*▁But*mask*!*+sent_0**sep+*\n","*cls*▁4.*mask*.*+sent_0**sep+*\n","*cls*com.*mask*.*+sent_0**sep+*\n","*cls*▁5.*mask*.*+sent_0**sep+*\n","*cls*▁•*mask*.*+sent_0**sep+*\n","*cls*)*mask*.*+sent_0**sep+*\n","*cls*▁A.*mask*.*+sent_0**sep+*\n","*cls*▁No,*mask*.*+sent_0**sep+*\n","*cls*▁Yes,*mask*,*+sent_0**sep+*\n","*cls*▁Absolutely*mask*.*+sent_0**sep+*\n","*cls*,*mask*.*+sent_0**sep+*\n","*cls*▁Thanks.*mask*.*+sent_0**sep+*\n","*cls*▁Just*mask*.*+sent_0**sep+*\n","*cls*▁Ah*mask*.*+sent_0**sep+*\n","*cls*▁1.*mask*.▁2.*+sent_0**sep+*\n","*cls*▁Ah*mask*!*+sent_0**sep+*\n","*cls*▁But*mask*,*+sent_0**sep+*\n","*cls*▁Hell*mask*!*+sent_0**sep+*\n","*cls*▁Thank▁you.*mask*.*+sent_0**sep+*\n","*cls*▁In*mask*.*+sent_0**sep+*\n","*cls*▁\"*mask*!*+sent_0**sep+*\n","*cls*▁This▁is*mask*.*+sent_0**sep+*\n","*cls*▁1*mask*.*+sent_0**sep+*\n","*cls*▁Yes!*mask*!*+sent_0**sep+*\n","*cls*▁I*mask*.*+sent_0**sep+*\n","####### generated templates #######\n","\n","# spoilers 16 21 100\n","| dataset examples\n","| {'label': 1, 'text': ['Bye bye levi']}\n","| {'label': 0, 'text': ['HE MADE THE OST?']}\n","\n","| mapping\n","| {0: 'No', 1: 'Yes'}\n","####### example #######\n","Bye bye levi<extra_id_0> Yes<extra_id_1>\n","I like how they added this lil transformation effect on the hange-eren scene<extra_id_0> Yes<extra_id_1>\n","I have a big feeling they'll use youseebiggirl in Warhammer's transformation reveal next ep<extra_id_0> Yes<extra_id_1>\n","####### example #######\n","\n","100% 18/18 [09:45<00:00, 32.51s/it]\n","####### generated results #######\n","--------------\n","score: -0.8004055023193359\n","generated ids [32099, 5, 32098, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>.</s>\n","--------------\n","score: -2.630514144897461\n","generated ids [32099, 5, 32098, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>!</s>\n","--------------\n","score: -3.08660888671875\n","generated ids [32099, 58, 32098, 5, 1]\n","generated text <extra_id_0>?<extra_id_1>.</s>\n","--------------\n","score: -3.286663055419922\n","generated ids [32099, 5, 32098, 58, 1]\n","generated text <extra_id_0>.<extra_id_1>?</s>\n","--------------\n","score: -3.4201440811157227\n","generated ids [32099, 55, 32098, 55, 1]\n","generated text <extra_id_0>!<extra_id_1>!</s>\n","--------------\n","score: -3.6519298553466797\n","generated ids [32099, 55, 32098, 5, 1]\n","generated text <extra_id_0>!<extra_id_1>.</s>\n","--------------\n","score: -3.72481632232666\n","generated ids [32099, 6, 32098, 5, 1]\n","generated text <extra_id_0>,<extra_id_1>.</s>\n","--------------\n","score: -4.158753395080566\n","generated ids [32099, 58, 32098, 55, 1]\n","generated text <extra_id_0>?<extra_id_1>!</s>\n","--------------\n","score: -4.429693222045898\n","generated ids [32099, 233, 32098, 5, 1]\n","generated text <extra_id_0>...<extra_id_1>.</s>\n","--------------\n","score: -4.758957862854004\n","generated ids [32099, 6, 32098, 58, 1]\n","generated text <extra_id_0>,<extra_id_1>?</s>\n","--------------\n","score: -4.819576263427734\n","generated ids [32099, 6, 32098, 55, 1]\n","generated text <extra_id_0>,<extra_id_1>!</s>\n","--------------\n","score: -5.115126609802246\n","generated ids [32099, 58, 32098, 58, 1]\n","generated text <extra_id_0>?<extra_id_1>?</s>\n","--------------\n","score: -5.190045356750488\n","generated ids [32099, 10, 32098, 5, 1]\n","generated text <extra_id_0>:<extra_id_1>.</s>\n","--------------\n","score: -5.200260162353516\n","generated ids [32099, 233, 32098, 55, 1]\n","generated text <extra_id_0>...<extra_id_1>!</s>\n","--------------\n","score: -5.354143142700195\n","generated ids [32099, 11, 32098, 5, 1]\n","generated text <extra_id_0>▁and<extra_id_1>.</s>\n","--------------\n","score: -5.494646072387695\n","generated ids [32099, 55, 32098, 58, 1]\n","generated text <extra_id_0>!<extra_id_1>?</s>\n","--------------\n","score: -5.572105407714844\n","generated ids [32099, 16, 32098, 5, 1]\n","generated text <extra_id_0>▁in<extra_id_1>.</s>\n","--------------\n","score: -6.017142295837402\n","generated ids [32099, 2824, 32098, 5, 1]\n","generated text <extra_id_0>....<extra_id_1>.</s>\n","--------------\n","score: -6.025769233703613\n","generated ids [32099, 5, 32098, 2824, 1]\n","generated text <extra_id_0>.<extra_id_1>....</s>\n","--------------\n","score: -6.0927534103393555\n","generated ids [32099, 233, 32098, 58, 1]\n","generated text <extra_id_0>...<extra_id_1>?</s>\n","--------------\n","score: -6.343471527099609\n","generated ids [32099, 11, 32098, 55, 1]\n","generated text <extra_id_0>▁and<extra_id_1>!</s>\n","--------------\n","score: -6.43168830871582\n","generated ids [32099, 21, 32098, 5, 1]\n","generated text <extra_id_0>▁for<extra_id_1>.</s>\n","--------------\n","score: -6.436247825622559\n","generated ids [32099, 10, 32098, 55, 1]\n","generated text <extra_id_0>:<extra_id_1>!</s>\n","--------------\n","score: -6.467538833618164\n","generated ids [32099, 1603, 32098, 55, 1]\n","generated text <extra_id_0>!!<extra_id_1>!</s>\n","--------------\n","score: -6.5040693283081055\n","generated ids [32099, 28, 32098, 5, 1]\n","generated text <extra_id_0>▁with<extra_id_1>.</s>\n","--------------\n","score: -6.731142044067383\n","generated ids [32099, 30, 32098, 5, 1]\n","generated text <extra_id_0>▁on<extra_id_1>.</s>\n","--------------\n","score: -6.757250785827637\n","generated ids [32099, 12, 32098, 5, 1]\n","generated text <extra_id_0>▁to<extra_id_1>.</s>\n","--------------\n","score: -6.780303001403809\n","generated ids [32099, 13, 32098, 5, 1]\n","generated text <extra_id_0>▁of<extra_id_1>.</s>\n","--------------\n","score: -6.796121597290039\n","generated ids [32099, 44, 32098, 5, 1]\n","generated text <extra_id_0>▁at<extra_id_1>.</s>\n","--------------\n","score: -6.819663047790527\n","generated ids [32099, 2824, 32098, 55, 1]\n","generated text <extra_id_0>....<extra_id_1>!</s>\n","--------------\n","score: -6.864810943603516\n","generated ids [32099, 5, 32098, 535, 1]\n","generated text <extra_id_0>.<extra_id_1>.\"</s>\n","--------------\n","score: -6.886845588684082\n","generated ids [32099, 18, 32098, 5, 1]\n","generated text <extra_id_0>-<extra_id_1>.</s>\n","--------------\n","score: -6.911285400390625\n","generated ids [32099, 3158, 32098, 55, 1]\n","generated text <extra_id_0>!!!<extra_id_1>!</s>\n","--------------\n","score: -6.941834449768066\n","generated ids [32099, 117, 32098, 5, 1]\n","generated text <extra_id_0>;<extra_id_1>.</s>\n","--------------\n","score: -7.000020980834961\n","generated ids [32099, 535, 32098, 5, 1]\n","generated text <extra_id_0>.\"<extra_id_1>.</s>\n","--------------\n","score: -7.0332231521606445\n","generated ids [32099, 1603, 32098, 5, 1]\n","generated text <extra_id_0>!!<extra_id_1>.</s>\n","--------------\n","score: -7.102847099304199\n","generated ids [32099, 16, 32098, 55, 1]\n","generated text <extra_id_0>▁in<extra_id_1>!</s>\n","--------------\n","score: -7.118269920349121\n","generated ids [32099, 42, 32098, 5, 1]\n","generated text <extra_id_0>▁or<extra_id_1>.</s>\n","--------------\n","score: -7.20118522644043\n","generated ids [32099, 57, 32098, 5, 1]\n","generated text <extra_id_0>▁by<extra_id_1>.</s>\n","--------------\n","score: -7.223993301391602\n","generated ids [32099, 5, 32098, 9374, 1]\n","generated text <extra_id_0>.<extra_id_1>.....</s>\n","--------------\n","score: -7.233685493469238\n","generated ids [32099, 137, 32098, 5, 1]\n","generated text <extra_id_0>).<extra_id_1>.</s>\n","--------------\n","score: -7.277712821960449\n","generated ids [32099, 27, 32098, 5, 1]\n","generated text <extra_id_0>▁I<extra_id_1>.</s>\n","--------------\n","score: -7.303854942321777\n","generated ids [32099, 9374, 32098, 5, 1]\n","generated text <extra_id_0>.....<extra_id_1>.</s>\n","--------------\n","score: -7.315460205078125\n","generated ids [32099, 121, 32098, 5, 1]\n","generated text <extra_id_0>\"<extra_id_1>.</s>\n","--------------\n","score: -7.385013580322266\n","generated ids [32099, 5, 32098, 6, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>,.</s>\n","--------------\n","score: -7.39251708984375\n","generated ids [32099, 5, 32098, 233, 1]\n","generated text <extra_id_0>.<extra_id_1>...</s>\n","--------------\n","score: -7.421843528747559\n","generated ids [32099, 45, 32098, 5, 1]\n","generated text <extra_id_0>▁from<extra_id_1>.</s>\n","--------------\n","score: -7.477888107299805\n","generated ids [32099, 19, 32098, 5, 1]\n","generated text <extra_id_0>▁is<extra_id_1>.</s>\n","--------------\n","score: -7.516793251037598\n","generated ids [32099, 5, 32098, 2049, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁thanks.</s>\n","--------------\n","score: -7.547698020935059\n","generated ids [32099, 5, 32098, 7, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>s.</s>\n","--------------\n","score: -7.553890228271484\n","generated ids [32099, 8, 32098, 5, 1]\n","generated text <extra_id_0>▁the<extra_id_1>.</s>\n","--------------\n","score: -7.5735273361206055\n","generated ids [32099, 68, 32098, 5, 1]\n","generated text <extra_id_0>▁but<extra_id_1>.</s>\n","--------------\n","score: -7.591833114624023\n","generated ids [32099, 8665, 32098, 5, 1]\n","generated text <extra_id_0>???<extra_id_1>.</s>\n","--------------\n","score: -7.638945579528809\n","generated ids [32099, 3158, 32098, 5, 1]\n","generated text <extra_id_0>!!!<extra_id_1>.</s>\n","--------------\n","score: -7.667603492736816\n","generated ids [32099, 38, 32098, 5, 1]\n","generated text <extra_id_0>▁as<extra_id_1>.</s>\n","--------------\n","score: -7.68515682220459\n","generated ids [32099, 5, 32098, 1603, 1]\n","generated text <extra_id_0>.<extra_id_1>!!</s>\n","--------------\n","score: -7.700209617614746\n","generated ids [32099, 18, 32098, 55, 1]\n","generated text <extra_id_0>-<extra_id_1>!</s>\n","--------------\n","score: -7.734344482421875\n","generated ids [32099, 2, 32098, 5, 1]\n","generated text <extra_id_0><unk><extra_id_1>.</s>\n","--------------\n","score: -7.7347307205200195\n","generated ids [32099, 8546, 32098, 5, 1]\n","generated text <extra_id_0>??<extra_id_1>.</s>\n","--------------\n","score: -7.740113258361816\n","generated ids [32099, 28, 32098, 55, 1]\n","generated text <extra_id_0>▁with<extra_id_1>!</s>\n","--------------\n","score: -7.744266510009766\n","generated ids [32099, 61, 32098, 5, 1]\n","generated text <extra_id_0>)<extra_id_1>.</s>\n","--------------\n","score: -7.746522903442383\n","generated ids [32099, 3274, 32098, 5, 1]\n","generated text <extra_id_0>▁=<extra_id_1>.</s>\n","--------------\n","score: -7.760991096496582\n","generated ids [32099, 21, 32098, 55, 1]\n","generated text <extra_id_0>▁for<extra_id_1>!</s>\n","--------------\n","score: -7.819910049438477\n","generated ids [32099, 30, 32098, 55, 1]\n","generated text <extra_id_0>▁on<extra_id_1>!</s>\n","--------------\n","score: -7.867542266845703\n","generated ids [32099, 1636, 32098, 5, 1]\n","generated text <extra_id_0>▁--<extra_id_1>.</s>\n","--------------\n","score: -7.885918617248535\n","generated ids [32099, 5, 32098, 754, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁please.</s>\n","--------------\n","score: -7.993420600891113\n","generated ids [32099, 465, 32098, 5, 1]\n","generated text <extra_id_0>▁No<extra_id_1>.</s>\n","--------------\n","score: -8.003801345825195\n","generated ids [32099, 42, 32098, 58, 1]\n","generated text <extra_id_0>▁or<extra_id_1>?</s>\n","--------------\n","score: -8.01433277130127\n","generated ids [32099, 13, 32098, 55, 1]\n","generated text <extra_id_0>▁of<extra_id_1>!</s>\n","--------------\n","score: -8.020755767822266\n","generated ids [32099, 55, 32098, 2824, 1]\n","generated text <extra_id_0>!<extra_id_1>....</s>\n","--------------\n","score: -8.023070335388184\n","generated ids [32099, 2824, 32098, 58, 1]\n","generated text <extra_id_0>....<extra_id_1>?</s>\n","--------------\n","score: -8.035253524780273\n","generated ids [32099, 10769, 32098, 55, 1]\n","generated text <extra_id_0>?!<extra_id_1>!</s>\n","--------------\n","score: -8.054129600524902\n","generated ids [32099, 7, 32098, 5, 1]\n","generated text <extra_id_0>s<extra_id_1>.</s>\n","--------------\n","score: -8.066472053527832\n","generated ids [32099, 12, 32098, 55, 1]\n","generated text <extra_id_0>▁to<extra_id_1>!</s>\n","--------------\n","score: -8.067536354064941\n","generated ids [32099, 8665, 32098, 55, 1]\n","generated text <extra_id_0>???<extra_id_1>!</s>\n","--------------\n","score: -8.075971603393555\n","generated ids [32099, 58, 32098, 2824, 1]\n","generated text <extra_id_0>?<extra_id_1>....</s>\n","--------------\n","score: -8.07655143737793\n","generated ids [32099, 5, 32098, 137, 1]\n","generated text <extra_id_0>.<extra_id_1>).</s>\n","--------------\n","score: -8.087708473205566\n","generated ids [32099, 31, 32098, 5, 1]\n","generated text <extra_id_0>'<extra_id_1>.</s>\n","--------------\n","score: -8.112724304199219\n","generated ids [32099, 14125, 32098, 5, 1]\n","generated text <extra_id_0>......<extra_id_1>.</s>\n","--------------\n","score: -8.116482734680176\n","generated ids [32099, 9374, 32098, 55, 1]\n","generated text <extra_id_0>.....<extra_id_1>!</s>\n","--------------\n","score: -8.179637908935547\n","generated ids [32099, 78, 32098, 5, 1]\n","generated text <extra_id_0>▁so<extra_id_1>.</s>\n","--------------\n","score: -8.190052032470703\n","generated ids [32099, 42, 32098, 55, 1]\n","generated text <extra_id_0>▁or<extra_id_1>!</s>\n","--------------\n","score: -8.194441795349121\n","generated ids [32099, 1820, 32098, 5, 1]\n","generated text <extra_id_0>▁|<extra_id_1>.</s>\n","--------------\n","score: -8.194554328918457\n","generated ids [32099, 117, 32098, 55, 1]\n","generated text <extra_id_0>;<extra_id_1>!</s>\n","--------------\n","score: -8.19771671295166\n","generated ids [32099, 114, 32098, 5, 1]\n","generated text <extra_id_0>▁like<extra_id_1>.</s>\n","--------------\n","score: -8.214324951171875\n","generated ids [32099, 55, 32098, 1603, 1]\n","generated text <extra_id_0>!<extra_id_1>!!</s>\n","--------------\n","score: -8.297650337219238\n","generated ids [32099, 10, 32098, 58, 1]\n","generated text <extra_id_0>:<extra_id_1>?</s>\n","--------------\n","score: -8.312176704406738\n","generated ids [32099, 2163, 32098, 5, 1]\n","generated text <extra_id_0>▁Yes<extra_id_1>.</s>\n","--------------\n","score: -8.325129508972168\n","generated ids [32099, 37, 32098, 5, 1]\n","generated text <extra_id_0>▁The<extra_id_1>.</s>\n","--------------\n","score: -8.332531929016113\n","generated ids [32099, 16, 32098, 58, 1]\n","generated text <extra_id_0>▁in<extra_id_1>?</s>\n","--------------\n","score: -8.336019515991211\n","generated ids [32099, 8546, 32098, 55, 1]\n","generated text <extra_id_0>??<extra_id_1>!</s>\n","--------------\n","score: -8.362360000610352\n","generated ids [32099, 121, 32098, 55, 1]\n","generated text <extra_id_0>\"<extra_id_1>!</s>\n","--------------\n","score: -8.44446086883545\n","generated ids [32099, 5, 32098, 6, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>,!</s>\n","--------------\n","score: -8.489535331726074\n","generated ids [32099, 5, 32098, 233, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>...!</s>\n","--------------\n","score: -8.5028657913208\n","generated ids [32099, 5, 465, 32098, 5, 1]\n","generated text <extra_id_0>.▁No<extra_id_1>.</s>\n","--------------\n","score: -8.544998168945312\n","generated ids [32099, 5, 3359, 32098, 5, 1]\n","generated text <extra_id_0>.▁Oh<extra_id_1>.</s>\n","--------------\n","score: -8.617036819458008\n","generated ids [32099, 7, 5, 32098, 5, 1]\n","generated text <extra_id_0>s.<extra_id_1>.</s>\n","--------------\n","score: -8.667612075805664\n","generated ids [32099, 5, 32098, 34, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁it.</s>\n","--------------\n","score: -8.740901947021484\n","generated ids [32099, 5, 32098, 2049, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>▁thanks!</s>\n","--------------\n","score: -8.803901672363281\n","generated ids [32099, 5, 32098, 754, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>▁please!</s>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls**sent_0*.*mask*.*sep+*\n","*cls**sent_0*.*mask*!*sep+*\n","*cls**sent_0*?*mask*.*sep+*\n","*cls**sent_0*.*mask*?*sep+*\n","*cls**sent_0*!*mask*!*sep+*\n","*cls**sent_0*!*mask*.*sep+*\n","*cls**sent_0*,*mask*.*sep+*\n","*cls**sent_0*?*mask*!*sep+*\n","*cls**sent_0*...*mask*.*sep+*\n","*cls**sent_0*,*mask*?*sep+*\n","*cls**sent_0*,*mask*!*sep+*\n","*cls**sent_0*?*mask*?*sep+*\n","*cls**sent_0*:*mask*.*sep+*\n","*cls**sent_0*...*mask*!*sep+*\n","*cls**sent_0*▁and*mask*.*sep+*\n","*cls**sent_0*!*mask*?*sep+*\n","*cls**sent_0*▁in*mask*.*sep+*\n","*cls**sent_0*....*mask*.*sep+*\n","*cls**sent_0*.*mask*....*sep+*\n","*cls**sent_0*...*mask*?*sep+*\n","*cls**sent_0*▁and*mask*!*sep+*\n","*cls**sent_0*▁for*mask*.*sep+*\n","*cls**sent_0*:*mask*!*sep+*\n","*cls**sent_0*!!*mask*!*sep+*\n","*cls**sent_0*▁with*mask*.*sep+*\n","*cls**sent_0*▁on*mask*.*sep+*\n","*cls**sent_0*▁to*mask*.*sep+*\n","*cls**sent_0*▁of*mask*.*sep+*\n","*cls**sent_0*▁at*mask*.*sep+*\n","*cls**sent_0*....*mask*!*sep+*\n","*cls**sent_0*.*mask*.\"*sep+*\n","*cls**sent_0*-*mask*.*sep+*\n","*cls**sent_0*!!!*mask*!*sep+*\n","*cls**sent_0*;*mask*.*sep+*\n","*cls**sent_0*.\"*mask*.*sep+*\n","*cls**sent_0*!!*mask*.*sep+*\n","*cls**sent_0*▁in*mask*!*sep+*\n","*cls**sent_0*▁or*mask*.*sep+*\n","*cls**sent_0*▁by*mask*.*sep+*\n","*cls**sent_0*.*mask*.....*sep+*\n","*cls**sent_0*).*mask*.*sep+*\n","*cls**sent_0*▁I*mask*.*sep+*\n","*cls**sent_0*.....*mask*.*sep+*\n","*cls**sent_0*\"*mask*.*sep+*\n","*cls**sent_0*.*mask*,.*sep+*\n","*cls**sent_0*.*mask*...*sep+*\n","*cls**sent_0*▁from*mask*.*sep+*\n","*cls**sent_0*▁is*mask*.*sep+*\n","*cls**sent_0*.*mask*▁thanks.*sep+*\n","*cls**sent_0*.*mask*s.*sep+*\n","####### generated templates #######\n","\n","####### example #######\n",".<extra_id_0> Yes<extra_id_1> Bye bye levi\n",".<extra_id_0> Yes<extra_id_1> I like how they added this lil transformation effect on the hange-eren scene\n",".<extra_id_0> Yes<extra_id_1> I have a big feeling they'll use youseebiggirl in Warhammer's transformation reveal next ep\n","####### example #######\n","\n","100% 18/18 [10:36<00:00, 35.35s/it]\n","####### generated results #######\n","--------------\n","score: -3.5701661109924316\n","generated ids [32099, 5, 32098, 5, 32097]\n","generated text <extra_id_0>.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -4.428466320037842\n","generated ids [32099, 5, 32098, 6, 32097]\n","generated text <extra_id_0>.<extra_id_1>,<extra_id_2>\n","--------------\n","score: -4.558627605438232\n","generated ids [32099, 5, 32098, 55, 32097]\n","generated text <extra_id_0>.<extra_id_1>!<extra_id_2>\n","--------------\n","score: -4.804282188415527\n","generated ids [32099, 3359, 32098, 55, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>!<extra_id_2>\n","--------------\n","score: -4.998427391052246\n","generated ids [32099, 2163, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.15877628326416\n","generated ids [32099, 465, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁No.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.1742753982543945\n","generated ids [32099, 3359, 32098, 5, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.223573684692383\n","generated ids [32099, 275, 32098, 5, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.640048980712891\n","generated ids [32099, 299, 32098, 5, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.85680627822876\n","generated ids [32099, 275, 32098, 6, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>,<extra_id_2>\n","--------------\n","score: -5.98029899597168\n","generated ids [32099, 2163, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.043021202087402\n","generated ids [32099, 264, 32098, 5, 32097]\n","generated text <extra_id_0>▁So<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.057297706604004\n","generated ids [32099, 275, 32098, 55, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.063112258911133\n","generated ids [32099, 2163, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.139072895050049\n","generated ids [32099, 5, 32098, 58, 32097]\n","generated text <extra_id_0>.<extra_id_1>?<extra_id_2>\n","--------------\n","score: -6.231776714324951\n","generated ids [32099, 71, 32098, 5, 32097]\n","generated text <extra_id_0>▁A<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.23884391784668\n","generated ids [32099, 465, 32098, 5, 32097]\n","generated text <extra_id_0>▁No<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.288205146789551\n","generated ids [32099, 2490, 32098, 5, 32097]\n","generated text <extra_id_0>▁><extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.321242332458496\n","generated ids [32099, 37, 32098, 5, 32097]\n","generated text <extra_id_0>▁The<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.340621471405029\n","generated ids [32099, 96, 32098, 5, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.4551897048950195\n","generated ids [32099, 1300, 32098, 5, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.455878257751465\n","generated ids [32099, 299, 32098, 55, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.545542240142822\n","generated ids [32099, 5, 32098, 233, 32097]\n","generated text <extra_id_0>.<extra_id_1>...<extra_id_2>\n","--------------\n","score: -6.585536003112793\n","generated ids [32099, 1877, 32098, 5, 32097]\n","generated text <extra_id_0>▁3.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.595745086669922\n","generated ids [32099, 3359, 32098, 6, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.638272762298584\n","generated ids [32099, 1682, 32098, 5, 32097]\n","generated text <extra_id_0>▁2.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.640939235687256\n","generated ids [32099, 2163, 6, 32098, 6, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.660170555114746\n","generated ids [32099, 11475, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yeah.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.732241153717041\n","generated ids [32099, 299, 32098, 6, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.74924373626709\n","generated ids [32099, 2853, 32098, 5, 32097]\n","generated text <extra_id_0>▁4.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.761051177978516\n","generated ids [32099, 9710, 32098, 5, 32097]\n","generated text <extra_id_0>▁Ah<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.784671783447266\n","generated ids [32099, 465, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁No,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.804208755493164\n","generated ids [32099, 1429, 32098, 5, 32097]\n","generated text <extra_id_0>▁*<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.88568639755249\n","generated ids [32099, 3594, 32098, 5, 32097]\n","generated text <extra_id_0>▁5.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.904314041137695\n","generated ids [32099, 1142, 32098, 5, 32097]\n","generated text <extra_id_0>▁Just<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.904477119445801\n","generated ids [32099, 61, 32098, 5, 32097]\n","generated text <extra_id_0>)<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.948476791381836\n","generated ids [32099, 71, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁A.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.97824764251709\n","generated ids [32099, 1333, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thanks.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.98313045501709\n","generated ids [32099, 9710, 32098, 55, 32097]\n","generated text <extra_id_0>▁Ah<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.984208106994629\n","generated ids [32099, 11419, 32098, 55, 32097]\n","generated text <extra_id_0>▁Hell<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.008966445922852\n","generated ids [32099, 96, 32098, 6, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.04068660736084\n","generated ids [32099, 20510, 32098, 5, 32097]\n","generated text <extra_id_0>▁Absolutely<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.071423530578613\n","generated ids [32099, 2163, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁Yes!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.096182823181152\n","generated ids [32099, 1697, 32098, 5, 32097]\n","generated text <extra_id_0>▁•<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.129288673400879\n","generated ids [32099, 27, 32098, 5, 32097]\n","generated text <extra_id_0>▁I<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.181836128234863\n","generated ids [32099, 2163, 11, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes▁and<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.184045791625977\n","generated ids [32099, 287, 5, 32098, 5, 32097]\n","generated text <extra_id_0>com.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.23045539855957\n","generated ids [32099, 1562, 25, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thank▁you.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.236337184906006\n","generated ids [32099, 6, 32098, 5, 32097]\n","generated text <extra_id_0>,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.243233680725098\n","generated ids [32099, 3359, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Oh,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.249622821807861\n","generated ids [32099, 2490, 32098, 6, 32097]\n","generated text <extra_id_0>▁><extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.260406494140625\n","generated ids [32099, 96, 32098, 55, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.266408920288086\n","generated ids [32099, 264, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁So,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.275856971740723\n","generated ids [32099, 264, 32098, 55, 32097]\n","generated text <extra_id_0>▁So<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.284600257873535\n","generated ids [32099, 3359, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Oh.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.317241668701172\n","generated ids [32099, 499, 32098, 5, 32097]\n","generated text <extra_id_0>▁My<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.323020935058594\n","generated ids [32099, 465, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁No!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.323193550109863\n","generated ids [32099, 11, 32098, 5, 32097]\n","generated text <extra_id_0>▁and<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.331000328063965\n","generated ids [32099, 71, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁A:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.349117279052734\n","generated ids [32099, 1804, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Good.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.349144458770752\n","generated ids [32099, 264, 32098, 6, 32097]\n","generated text <extra_id_0>▁So<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.355927467346191\n","generated ids [32099, 11475, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yeah,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.356243133544922\n","generated ids [32099, 100, 19, 32098, 5, 32097]\n","generated text <extra_id_0>▁This▁is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.358414173126221\n","generated ids [32099, 86, 32098, 5, 32097]\n","generated text <extra_id_0>▁In<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.3685503005981445\n","generated ids [32099, 955, 32098, 5, 32097]\n","generated text <extra_id_0>▁Or<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.375679016113281\n","generated ids [32099, 3359, 6, 32098, 55, 32097]\n","generated text <extra_id_0>▁Oh,<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.391992568969727\n","generated ids [32099, 58, 32098, 5, 32097]\n","generated text <extra_id_0>?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.39561653137207\n","generated ids [32099, 363, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁What?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.3961076736450195\n","generated ids [32099, 11475, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yeah<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.40788459777832\n","generated ids [32099, 1429, 32098, 6, 32097]\n","generated text <extra_id_0>▁*<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.426548957824707\n","generated ids [32099, 5835, 32098, 5, 32097]\n","generated text <extra_id_0>▁9.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.427635192871094\n","generated ids [32099, 11342, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Sorry.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.430843353271484\n","generated ids [32099, 27, 317, 32098, 5, 32097]\n","generated text <extra_id_0>▁I▁think<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.457248687744141\n","generated ids [32099, 465, 6, 32098, 6, 32097]\n","generated text <extra_id_0>▁No,<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.463981628417969\n","generated ids [32099, 1300, 32098, 5, 1682, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>.▁2.<extra_id_2>\n","--------------\n","score: -7.4696760177612305\n","generated ids [32099, 156, 32098, 6, 32097]\n","generated text <extra_id_0>▁If<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.4756364822387695\n","generated ids [32099, 9758, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Wow.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.47684907913208\n","generated ids [32099, 209, 32098, 5, 32097]\n","generated text <extra_id_0>▁1<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.486079692840576\n","generated ids [32099, 14125, 32098, 5, 32097]\n","generated text <extra_id_0>......<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.492000579833984\n","generated ids [32099, 411, 32098, 5, 32097]\n","generated text <extra_id_0>▁O<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.494736671447754\n","generated ids [32099, 2163, 32098, 2163, 32097]\n","generated text <extra_id_0>▁Yes<extra_id_1>▁Yes<extra_id_2>\n","--------------\n","score: -7.49606990814209\n","generated ids [32099, 9710, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Ah,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.535770416259766\n","generated ids [32099, 1548, 32098, 5, 32097]\n","generated text <extra_id_0>▁Well<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.554400444030762\n","generated ids [32099, 1548, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Well,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.557475566864014\n","generated ids [32099, 4357, 32098, 5, 32097]\n","generated text <extra_id_0>▁6.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.571306228637695\n","generated ids [32099, 412, 107, 32098, 5, 32097]\n","generated text <extra_id_0>▁Uh<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.576242446899414\n","generated ids [32099, 465, 32098, 465, 32097]\n","generated text <extra_id_0>▁No<extra_id_1>▁No<extra_id_2>\n","--------------\n","score: -7.5816826820373535\n","generated ids [32099, 1300, 32098, 6, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.590207099914551\n","generated ids [32099, 11291, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁Really?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.602311134338379\n","generated ids [32099, 204, 32098, 5, 32097]\n","generated text <extra_id_0>▁2<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.622204780578613\n","generated ids [32099, 1626, 1024, 32098, 5, 32097]\n","generated text <extra_id_0>▁Haha<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.638465881347656\n","generated ids [32099, 419, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁Re:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.654026985168457\n","generated ids [32099, 1333, 55, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thanks!<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.671467304229736\n","generated ids [32099, 4848, 32098, 5, 32097]\n","generated text <extra_id_0>▁8.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.6902055740356445\n","generated ids [32099, 2163, 6, 32098, 55, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.734724044799805\n","generated ids [32099, 363, 58, 32098, 55, 32097]\n","generated text <extra_id_0>▁What?<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.751291275024414\n","generated ids [32099, 27, 7, 34, 32098, 58, 32097]\n","generated text <extra_id_0>▁Is▁it<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.768131256103516\n","generated ids [32099, 27, 43, 32098, 5, 32097]\n","generated text <extra_id_0>▁I▁have<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.7692084312438965\n","generated ids [32099, 5, 58, 32098, 5, 32097]\n","generated text <extra_id_0>.?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.780916213989258\n","generated ids [32099, 27, 7, 32098, 5, 32097]\n","generated text <extra_id_0>▁Is<extra_id_1>.<extra_id_2>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls*.*mask*.*+sent_0**sep+*\n","*cls*.*mask*,*+sent_0**sep+*\n","*cls*.*mask*!*+sent_0**sep+*\n","*cls*▁Oh*mask*!*+sent_0**sep+*\n","*cls*▁Yes.*mask*.*+sent_0**sep+*\n","*cls*▁No.*mask*.*+sent_0**sep+*\n","*cls*▁Oh*mask*.*+sent_0**sep+*\n","*cls*▁And*mask*.*+sent_0**sep+*\n","*cls*▁But*mask*.*+sent_0**sep+*\n","*cls*▁And*mask*,*+sent_0**sep+*\n","*cls*▁Yes,*mask*.*+sent_0**sep+*\n","*cls*▁So*mask*.*+sent_0**sep+*\n","*cls*▁And*mask*!*+sent_0**sep+*\n","*cls*▁Yes*mask*.*+sent_0**sep+*\n","*cls*.*mask*?*+sent_0**sep+*\n","*cls*▁A*mask*.*+sent_0**sep+*\n","*cls*▁No*mask*.*+sent_0**sep+*\n","*cls*▁>*mask*.*+sent_0**sep+*\n","*cls*▁The*mask*.*+sent_0**sep+*\n","*cls*▁\"*mask*.*+sent_0**sep+*\n","*cls*▁1.*mask*.*+sent_0**sep+*\n","*cls*▁But*mask*!*+sent_0**sep+*\n","*cls*.*mask*...*+sent_0**sep+*\n","*cls*▁3.*mask*.*+sent_0**sep+*\n","*cls*▁Oh*mask*,*+sent_0**sep+*\n","*cls*▁2.*mask*.*+sent_0**sep+*\n","*cls*▁Yes,*mask*,*+sent_0**sep+*\n","*cls*▁Yeah.*mask*.*+sent_0**sep+*\n","*cls*▁But*mask*,*+sent_0**sep+*\n","*cls*▁4.*mask*.*+sent_0**sep+*\n","*cls*▁Ah*mask*.*+sent_0**sep+*\n","*cls*▁No,*mask*.*+sent_0**sep+*\n","*cls*▁**mask*.*+sent_0**sep+*\n","*cls*▁5.*mask*.*+sent_0**sep+*\n","*cls*▁Just*mask*.*+sent_0**sep+*\n","*cls*)*mask*.*+sent_0**sep+*\n","*cls*▁A.*mask*.*+sent_0**sep+*\n","*cls*▁Thanks.*mask*.*+sent_0**sep+*\n","*cls*▁Ah*mask*!*+sent_0**sep+*\n","*cls*▁Hell*mask*!*+sent_0**sep+*\n","*cls*▁\"*mask*,*+sent_0**sep+*\n","*cls*▁Absolutely*mask*.*+sent_0**sep+*\n","*cls*▁Yes!*mask*!*+sent_0**sep+*\n","*cls*▁•*mask*.*+sent_0**sep+*\n","*cls*▁I*mask*.*+sent_0**sep+*\n","*cls*▁Yes▁and*mask*.*+sent_0**sep+*\n","*cls*com.*mask*.*+sent_0**sep+*\n","*cls*▁Thank▁you.*mask*.*+sent_0**sep+*\n","*cls*,*mask*.*+sent_0**sep+*\n","*cls*▁Oh,*mask*.*+sent_0**sep+*\n","####### generated templates #######\n","\n","# spoilers 16 42 100\n","| dataset examples\n","| {'label': 0, 'text': ['YALLL HEARD THAT']}\n","| {'label': 1, 'text': ['RIP meat lover:SNOsashasmug: \\U0001f972 😭']}\n","\n","| mapping\n","| {0: 'No', 1: 'Yes'}\n","####### example #######\n","YALLL HEARD THAT<extra_id_0> No<extra_id_1>\n","YE<extra_id_0> No<extra_id_1>\n","F<extra_id_0> No<extra_id_1>\n","####### example #######\n","\n","100% 18/18 [08:49<00:00, 29.43s/it]\n","####### generated results #######\n","--------------\n","score: -0.9131555557250977\n","generated ids [32099, 5, 32098, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>.</s>\n","--------------\n","score: -2.686647415161133\n","generated ids [32099, 5, 32098, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>!</s>\n","--------------\n","score: -2.7004566192626953\n","generated ids [32099, 58, 32098, 5, 1]\n","generated text <extra_id_0>?<extra_id_1>.</s>\n","--------------\n","score: -3.1137313842773438\n","generated ids [32099, 55, 32098, 55, 1]\n","generated text <extra_id_0>!<extra_id_1>!</s>\n","--------------\n","score: -3.3128252029418945\n","generated ids [32099, 5, 32098, 58, 1]\n","generated text <extra_id_0>.<extra_id_1>?</s>\n","--------------\n","score: -3.61818790435791\n","generated ids [32099, 58, 32098, 55, 1]\n","generated text <extra_id_0>?<extra_id_1>!</s>\n","--------------\n","score: -3.6439409255981445\n","generated ids [32099, 55, 32098, 5, 1]\n","generated text <extra_id_0>!<extra_id_1>.</s>\n","--------------\n","score: -3.652939796447754\n","generated ids [32099, 6, 32098, 5, 1]\n","generated text <extra_id_0>,<extra_id_1>.</s>\n","--------------\n","score: -4.421653747558594\n","generated ids [32099, 58, 32098, 58, 1]\n","generated text <extra_id_0>?<extra_id_1>?</s>\n","--------------\n","score: -4.566596984863281\n","generated ids [32099, 6, 32098, 58, 1]\n","generated text <extra_id_0>,<extra_id_1>?</s>\n","--------------\n","score: -4.576594352722168\n","generated ids [32099, 6, 32098, 55, 1]\n","generated text <extra_id_0>,<extra_id_1>!</s>\n","--------------\n","score: -4.694146156311035\n","generated ids [32099, 11, 32098, 5, 1]\n","generated text <extra_id_0>▁and<extra_id_1>.</s>\n","--------------\n","score: -4.825272560119629\n","generated ids [32099, 233, 32098, 5, 1]\n","generated text <extra_id_0>...<extra_id_1>.</s>\n","--------------\n","score: -5.12603759765625\n","generated ids [32099, 10, 32098, 5, 1]\n","generated text <extra_id_0>:<extra_id_1>.</s>\n","--------------\n","score: -5.142118453979492\n","generated ids [32099, 16, 32098, 5, 1]\n","generated text <extra_id_0>▁in<extra_id_1>.</s>\n","--------------\n","score: -5.2335662841796875\n","generated ids [32099, 55, 32098, 58, 1]\n","generated text <extra_id_0>!<extra_id_1>?</s>\n","--------------\n","score: -5.388507843017578\n","generated ids [32099, 233, 32098, 55, 1]\n","generated text <extra_id_0>...<extra_id_1>!</s>\n","--------------\n","score: -5.658219337463379\n","generated ids [32099, 11, 32098, 55, 1]\n","generated text <extra_id_0>▁and<extra_id_1>!</s>\n","--------------\n","score: -5.678433418273926\n","generated ids [32099, 21, 32098, 5, 1]\n","generated text <extra_id_0>▁for<extra_id_1>.</s>\n","--------------\n","score: -5.8658857345581055\n","generated ids [32099, 28, 32098, 5, 1]\n","generated text <extra_id_0>▁with<extra_id_1>.</s>\n","--------------\n","score: -6.134047508239746\n","generated ids [32099, 2824, 32098, 5, 1]\n","generated text <extra_id_0>....<extra_id_1>.</s>\n","--------------\n","score: -6.142531394958496\n","generated ids [32099, 5, 32098, 2824, 1]\n","generated text <extra_id_0>.<extra_id_1>....</s>\n","--------------\n","score: -6.161696434020996\n","generated ids [32099, 233, 32098, 58, 1]\n","generated text <extra_id_0>...<extra_id_1>?</s>\n","--------------\n","score: -6.21162223815918\n","generated ids [32099, 12, 32098, 5, 1]\n","generated text <extra_id_0>▁to<extra_id_1>.</s>\n","--------------\n","score: -6.23008918762207\n","generated ids [32099, 30, 32098, 5, 1]\n","generated text <extra_id_0>▁on<extra_id_1>.</s>\n","--------------\n","score: -6.257928848266602\n","generated ids [32099, 10, 32098, 55, 1]\n","generated text <extra_id_0>:<extra_id_1>!</s>\n","--------------\n","score: -6.3523712158203125\n","generated ids [32099, 44, 32098, 5, 1]\n","generated text <extra_id_0>▁at<extra_id_1>.</s>\n","--------------\n","score: -6.382565498352051\n","generated ids [32099, 1603, 32098, 55, 1]\n","generated text <extra_id_0>!!<extra_id_1>!</s>\n","--------------\n","score: -6.391301155090332\n","generated ids [32099, 13, 32098, 5, 1]\n","generated text <extra_id_0>▁of<extra_id_1>.</s>\n","--------------\n","score: -6.6176300048828125\n","generated ids [32099, 3158, 32098, 55, 1]\n","generated text <extra_id_0>!!!<extra_id_1>!</s>\n","--------------\n","score: -6.618108749389648\n","generated ids [32099, 45, 32098, 5, 1]\n","generated text <extra_id_0>▁from<extra_id_1>.</s>\n","--------------\n","score: -6.632024765014648\n","generated ids [32099, 16, 32098, 55, 1]\n","generated text <extra_id_0>▁in<extra_id_1>!</s>\n","--------------\n","score: -6.634521484375\n","generated ids [32099, 42, 32098, 5, 1]\n","generated text <extra_id_0>▁or<extra_id_1>.</s>\n","--------------\n","score: -6.728665351867676\n","generated ids [32099, 18, 32098, 5, 1]\n","generated text <extra_id_0>-<extra_id_1>.</s>\n","--------------\n","score: -6.760052680969238\n","generated ids [32099, 2824, 32098, 55, 1]\n","generated text <extra_id_0>....<extra_id_1>!</s>\n","--------------\n","score: -6.793418884277344\n","generated ids [32099, 5, 32098, 535, 1]\n","generated text <extra_id_0>.<extra_id_1>.\"</s>\n","--------------\n","score: -6.830297470092773\n","generated ids [32099, 57, 32098, 5, 1]\n","generated text <extra_id_0>▁by<extra_id_1>.</s>\n","--------------\n","score: -6.89273738861084\n","generated ids [32099, 19, 32098, 5, 1]\n","generated text <extra_id_0>▁is<extra_id_1>.</s>\n","--------------\n","score: -6.898865699768066\n","generated ids [32099, 117, 32098, 5, 1]\n","generated text <extra_id_0>;<extra_id_1>.</s>\n","--------------\n","score: -6.966718673706055\n","generated ids [32099, 535, 32098, 5, 1]\n","generated text <extra_id_0>.\"<extra_id_1>.</s>\n","--------------\n","score: -6.990129470825195\n","generated ids [32099, 21, 32098, 55, 1]\n","generated text <extra_id_0>▁for<extra_id_1>!</s>\n","--------------\n","score: -7.03920841217041\n","generated ids [32099, 38, 32098, 5, 1]\n","generated text <extra_id_0>▁as<extra_id_1>.</s>\n","--------------\n","score: -7.0913591384887695\n","generated ids [32099, 28, 32098, 55, 1]\n","generated text <extra_id_0>▁with<extra_id_1>!</s>\n","--------------\n","score: -7.172523498535156\n","generated ids [32099, 1603, 32098, 5, 1]\n","generated text <extra_id_0>!!<extra_id_1>.</s>\n","--------------\n","score: -7.243227958679199\n","generated ids [32099, 68, 32098, 5, 1]\n","generated text <extra_id_0>▁but<extra_id_1>.</s>\n","--------------\n","score: -7.268623352050781\n","generated ids [32099, 137, 32098, 5, 1]\n","generated text <extra_id_0>).<extra_id_1>.</s>\n","--------------\n","score: -7.29815673828125\n","generated ids [32099, 30, 32098, 55, 1]\n","generated text <extra_id_0>▁on<extra_id_1>!</s>\n","--------------\n","score: -7.3099870681762695\n","generated ids [32099, 42, 32098, 58, 1]\n","generated text <extra_id_0>▁or<extra_id_1>?</s>\n","--------------\n","score: -7.337607383728027\n","generated ids [32099, 18, 32098, 55, 1]\n","generated text <extra_id_0>-<extra_id_1>!</s>\n","--------------\n","score: -7.359162330627441\n","generated ids [32099, 5, 32098, 9374, 1]\n","generated text <extra_id_0>.<extra_id_1>.....</s>\n","--------------\n","score: -7.38722038269043\n","generated ids [32099, 7, 32098, 5, 1]\n","generated text <extra_id_0>s<extra_id_1>.</s>\n","--------------\n","score: -7.3948163986206055\n","generated ids [32099, 121, 32098, 5, 1]\n","generated text <extra_id_0>\"<extra_id_1>.</s>\n","--------------\n","score: -7.415566444396973\n","generated ids [32099, 27, 32098, 5, 1]\n","generated text <extra_id_0>▁I<extra_id_1>.</s>\n","--------------\n","score: -7.43605899810791\n","generated ids [32099, 9374, 32098, 5, 1]\n","generated text <extra_id_0>.....<extra_id_1>.</s>\n","--------------\n","score: -7.455588340759277\n","generated ids [32099, 8, 32098, 5, 1]\n","generated text <extra_id_0>▁the<extra_id_1>.</s>\n","--------------\n","score: -7.461864471435547\n","generated ids [32099, 114, 32098, 5, 1]\n","generated text <extra_id_0>▁like<extra_id_1>.</s>\n","--------------\n","score: -7.48069953918457\n","generated ids [32099, 5, 32098, 233, 1]\n","generated text <extra_id_0>.<extra_id_1>...</s>\n","--------------\n","score: -7.4892730712890625\n","generated ids [32099, 8546, 32098, 5, 1]\n","generated text <extra_id_0>??<extra_id_1>.</s>\n","--------------\n","score: -7.500550270080566\n","generated ids [32099, 78, 32098, 5, 1]\n","generated text <extra_id_0>▁so<extra_id_1>.</s>\n","--------------\n","score: -7.5493364334106445\n","generated ids [32099, 11, 32098, 58, 1]\n","generated text <extra_id_0>▁and<extra_id_1>?</s>\n","--------------\n","score: -7.55413818359375\n","generated ids [32099, 3158, 32098, 5, 1]\n","generated text <extra_id_0>!!!<extra_id_1>.</s>\n","--------------\n","score: -7.579814910888672\n","generated ids [32099, 16, 32098, 58, 1]\n","generated text <extra_id_0>▁in<extra_id_1>?</s>\n","--------------\n","score: -7.580550193786621\n","generated ids [32099, 12, 32098, 55, 1]\n","generated text <extra_id_0>▁to<extra_id_1>!</s>\n","--------------\n","score: -7.585704803466797\n","generated ids [32099, 5, 32098, 1603, 1]\n","generated text <extra_id_0>.<extra_id_1>!!</s>\n","--------------\n","score: -7.600142478942871\n","generated ids [32099, 5, 32098, 6, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>,.</s>\n","--------------\n","score: -7.632437705993652\n","generated ids [32099, 58, 32098, 2824, 1]\n","generated text <extra_id_0>?<extra_id_1>....</s>\n","--------------\n","score: -7.667403221130371\n","generated ids [32099, 8665, 32098, 5, 1]\n","generated text <extra_id_0>???<extra_id_1>.</s>\n","--------------\n","score: -7.6680097579956055\n","generated ids [32099, 13, 32098, 55, 1]\n","generated text <extra_id_0>▁of<extra_id_1>!</s>\n","--------------\n","score: -7.729367256164551\n","generated ids [32099, 10769, 32098, 55, 1]\n","generated text <extra_id_0>?!<extra_id_1>!</s>\n","--------------\n","score: -7.78029727935791\n","generated ids [32099, 42, 32098, 55, 1]\n","generated text <extra_id_0>▁or<extra_id_1>!</s>\n","--------------\n","score: -7.829705238342285\n","generated ids [32099, 55, 32098, 1603, 1]\n","generated text <extra_id_0>!<extra_id_1>!!</s>\n","--------------\n","score: -7.904252052307129\n","generated ids [32099, 21, 32098, 58, 1]\n","generated text <extra_id_0>▁for<extra_id_1>?</s>\n","--------------\n","score: -7.921117782592773\n","generated ids [32099, 44, 32098, 55, 1]\n","generated text <extra_id_0>▁at<extra_id_1>!</s>\n","--------------\n","score: -7.941287994384766\n","generated ids [32099, 55, 32098, 2824, 1]\n","generated text <extra_id_0>!<extra_id_1>....</s>\n","--------------\n","score: -7.942286491394043\n","generated ids [32099, 10, 32098, 58, 1]\n","generated text <extra_id_0>:<extra_id_1>?</s>\n","--------------\n","score: -7.943966865539551\n","generated ids [32099, 2824, 32098, 58, 1]\n","generated text <extra_id_0>....<extra_id_1>?</s>\n","--------------\n","score: -7.954756736755371\n","generated ids [32099, 8546, 32098, 55, 1]\n","generated text <extra_id_0>??<extra_id_1>!</s>\n","--------------\n","score: -7.972097396850586\n","generated ids [32099, 8665, 32098, 55, 1]\n","generated text <extra_id_0>???<extra_id_1>!</s>\n","--------------\n","score: -7.983391761779785\n","generated ids [32099, 3274, 32098, 5, 1]\n","generated text <extra_id_0>▁=<extra_id_1>.</s>\n","--------------\n","score: -8.013651847839355\n","generated ids [32099, 117, 32098, 55, 1]\n","generated text <extra_id_0>;<extra_id_1>!</s>\n","--------------\n","score: -8.033109664916992\n","generated ids [32099, 5, 32098, 137, 1]\n","generated text <extra_id_0>.<extra_id_1>).</s>\n","--------------\n","score: -8.03442668914795\n","generated ids [32099, 5, 32098, 7, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>s.</s>\n","--------------\n","score: -8.039811134338379\n","generated ids [32099, 45, 32098, 55, 1]\n","generated text <extra_id_0>▁from<extra_id_1>!</s>\n","--------------\n","score: -8.072026252746582\n","generated ids [32099, 61, 32098, 5, 1]\n","generated text <extra_id_0>)<extra_id_1>.</s>\n","--------------\n","score: -8.072526931762695\n","generated ids [32099, 9374, 32098, 55, 1]\n","generated text <extra_id_0>.....<extra_id_1>!</s>\n","--------------\n","score: -8.085626602172852\n","generated ids [32099, 12887, 32098, 55, 1]\n","generated text <extra_id_0>!!!!<extra_id_1>!</s>\n","--------------\n","score: -8.115190505981445\n","generated ids [32099, 28, 32098, 58, 1]\n","generated text <extra_id_0>▁with<extra_id_1>?</s>\n","--------------\n","score: -8.122429847717285\n","generated ids [32099, 5, 32098, 2049, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁thanks.</s>\n","--------------\n","score: -8.131511688232422\n","generated ids [32099, 24, 32098, 5, 1]\n","generated text <extra_id_0>▁that<extra_id_1>.</s>\n","--------------\n","score: -8.175690650939941\n","generated ids [32099, 18, 32098, 58, 1]\n","generated text <extra_id_0>-<extra_id_1>?</s>\n","--------------\n","score: -8.184216499328613\n","generated ids [32099, 5, 32098, 754, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁please.</s>\n","--------------\n","score: -8.194158554077148\n","generated ids [32099, 57, 32098, 55, 1]\n","generated text <extra_id_0>▁by<extra_id_1>!</s>\n","--------------\n","score: -8.197918891906738\n","generated ids [32099, 150, 32098, 5, 1]\n","generated text <extra_id_0>▁no<extra_id_1>.</s>\n","--------------\n","score: -8.1997709274292\n","generated ids [32099, 2, 32098, 5, 1]\n","generated text <extra_id_0><unk><extra_id_1>.</s>\n","--------------\n","score: -8.226398468017578\n","generated ids [32099, 31, 32098, 5, 1]\n","generated text <extra_id_0>'<extra_id_1>.</s>\n","--------------\n","score: -8.238625526428223\n","generated ids [32099, 1636, 32098, 5, 1]\n","generated text <extra_id_0>▁--<extra_id_1>.</s>\n","--------------\n","score: -8.25525951385498\n","generated ids [32099, 47, 32098, 5, 1]\n","generated text <extra_id_0>▁was<extra_id_1>.</s>\n","--------------\n","score: -8.3694429397583\n","generated ids [32099, 7, 5, 32098, 5, 1]\n","generated text <extra_id_0>s.<extra_id_1>.</s>\n","--------------\n","score: -8.463895797729492\n","generated ids [32099, 396, 5, 32098, 5, 1]\n","generated text <extra_id_0>▁too.<extra_id_1>.</s>\n","--------------\n","score: -8.699732780456543\n","generated ids [32099, 5, 32098, 6, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>,!</s>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls**sent_0*.*mask*.*sep+*\n","*cls**sent_0*.*mask*!*sep+*\n","*cls**sent_0*?*mask*.*sep+*\n","*cls**sent_0*!*mask*!*sep+*\n","*cls**sent_0*.*mask*?*sep+*\n","*cls**sent_0*?*mask*!*sep+*\n","*cls**sent_0*!*mask*.*sep+*\n","*cls**sent_0*,*mask*.*sep+*\n","*cls**sent_0*?*mask*?*sep+*\n","*cls**sent_0*,*mask*?*sep+*\n","*cls**sent_0*,*mask*!*sep+*\n","*cls**sent_0*▁and*mask*.*sep+*\n","*cls**sent_0*...*mask*.*sep+*\n","*cls**sent_0*:*mask*.*sep+*\n","*cls**sent_0*▁in*mask*.*sep+*\n","*cls**sent_0*!*mask*?*sep+*\n","*cls**sent_0*...*mask*!*sep+*\n","*cls**sent_0*▁and*mask*!*sep+*\n","*cls**sent_0*▁for*mask*.*sep+*\n","*cls**sent_0*▁with*mask*.*sep+*\n","*cls**sent_0*....*mask*.*sep+*\n","*cls**sent_0*.*mask*....*sep+*\n","*cls**sent_0*...*mask*?*sep+*\n","*cls**sent_0*▁to*mask*.*sep+*\n","*cls**sent_0*▁on*mask*.*sep+*\n","*cls**sent_0*:*mask*!*sep+*\n","*cls**sent_0*▁at*mask*.*sep+*\n","*cls**sent_0*!!*mask*!*sep+*\n","*cls**sent_0*▁of*mask*.*sep+*\n","*cls**sent_0*!!!*mask*!*sep+*\n","*cls**sent_0*▁from*mask*.*sep+*\n","*cls**sent_0*▁in*mask*!*sep+*\n","*cls**sent_0*▁or*mask*.*sep+*\n","*cls**sent_0*-*mask*.*sep+*\n","*cls**sent_0*....*mask*!*sep+*\n","*cls**sent_0*.*mask*.\"*sep+*\n","*cls**sent_0*▁by*mask*.*sep+*\n","*cls**sent_0*▁is*mask*.*sep+*\n","*cls**sent_0*;*mask*.*sep+*\n","*cls**sent_0*.\"*mask*.*sep+*\n","*cls**sent_0*▁for*mask*!*sep+*\n","*cls**sent_0*▁as*mask*.*sep+*\n","*cls**sent_0*▁with*mask*!*sep+*\n","*cls**sent_0*!!*mask*.*sep+*\n","*cls**sent_0*▁but*mask*.*sep+*\n","*cls**sent_0*).*mask*.*sep+*\n","*cls**sent_0*▁on*mask*!*sep+*\n","*cls**sent_0*▁or*mask*?*sep+*\n","*cls**sent_0*-*mask*!*sep+*\n","*cls**sent_0*.*mask*.....*sep+*\n","####### generated templates #######\n","\n","####### example #######\n",".<extra_id_0> No<extra_id_1> YALLL HEARD THAT\n",".<extra_id_0> No<extra_id_1> YE\n",".<extra_id_0> No<extra_id_1> F\n","####### example #######\n","\n","100% 18/18 [09:27<00:00, 31.54s/it]\n","####### generated results #######\n","--------------\n","score: -3.1787896156311035\n","generated ids [32099, 5, 32098, 5, 32097]\n","generated text <extra_id_0>.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -4.314711570739746\n","generated ids [32099, 5, 32098, 55, 32097]\n","generated text <extra_id_0>.<extra_id_1>!<extra_id_2>\n","--------------\n","score: -4.566702365875244\n","generated ids [32099, 5, 32098, 6, 32097]\n","generated text <extra_id_0>.<extra_id_1>,<extra_id_2>\n","--------------\n","score: -4.767760276794434\n","generated ids [32099, 2163, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -4.961633682250977\n","generated ids [32099, 465, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁No.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.123135566711426\n","generated ids [32099, 3359, 32098, 55, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>!<extra_id_2>\n","--------------\n","score: -5.354979515075684\n","generated ids [32099, 275, 32098, 5, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.570303916931152\n","generated ids [32099, 3359, 32098, 5, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.591729164123535\n","generated ids [32099, 2163, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.671388626098633\n","generated ids [32099, 299, 32098, 5, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.72918176651001\n","generated ids [32099, 1300, 32098, 5, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.736873626708984\n","generated ids [32099, 2163, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.936792373657227\n","generated ids [32099, 1682, 32098, 5, 32097]\n","generated text <extra_id_0>▁2.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.972280502319336\n","generated ids [32099, 1877, 32098, 5, 32097]\n","generated text <extra_id_0>▁3.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.9763641357421875\n","generated ids [32099, 465, 32098, 5, 32097]\n","generated text <extra_id_0>▁No<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.0200700759887695\n","generated ids [32099, 71, 32098, 5, 32097]\n","generated text <extra_id_0>▁A<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.0317840576171875\n","generated ids [32099, 2490, 32098, 5, 32097]\n","generated text <extra_id_0>▁><extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.060752868652344\n","generated ids [32099, 96, 32098, 5, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.11216926574707\n","generated ids [32099, 2853, 32098, 5, 32097]\n","generated text <extra_id_0>▁4.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.150001525878906\n","generated ids [32099, 275, 32098, 55, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.174498558044434\n","generated ids [32099, 5, 32098, 58, 32097]\n","generated text <extra_id_0>.<extra_id_1>?<extra_id_2>\n","--------------\n","score: -6.188490867614746\n","generated ids [32099, 37, 32098, 5, 32097]\n","generated text <extra_id_0>▁The<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.298456192016602\n","generated ids [32099, 3594, 32098, 5, 32097]\n","generated text <extra_id_0>▁5.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.346295356750488\n","generated ids [32099, 264, 32098, 5, 32097]\n","generated text <extra_id_0>▁So<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.362606525421143\n","generated ids [32099, 275, 32098, 6, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.379090309143066\n","generated ids [32099, 5, 32098, 233, 32097]\n","generated text <extra_id_0>.<extra_id_1>...<extra_id_2>\n","--------------\n","score: -6.39781379699707\n","generated ids [32099, 299, 32098, 55, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.47227668762207\n","generated ids [32099, 71, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁A.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.520773887634277\n","generated ids [32099, 1429, 32098, 5, 32097]\n","generated text <extra_id_0>▁*<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.571139335632324\n","generated ids [32099, 1697, 32098, 5, 32097]\n","generated text <extra_id_0>▁•<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.606266021728516\n","generated ids [32099, 61, 32098, 5, 32097]\n","generated text <extra_id_0>)<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.725414276123047\n","generated ids [32099, 71, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁A:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.7419610023498535\n","generated ids [32099, 209, 32098, 5, 32097]\n","generated text <extra_id_0>▁1<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.773951530456543\n","generated ids [32099, 5835, 32098, 5, 32097]\n","generated text <extra_id_0>▁9.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.7819929122924805\n","generated ids [32099, 465, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁No,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.839984893798828\n","generated ids [32099, 1333, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thanks.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.855424880981445\n","generated ids [32099, 204, 32098, 5, 32097]\n","generated text <extra_id_0>▁2<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.899952411651611\n","generated ids [32099, 6, 32098, 5, 32097]\n","generated text <extra_id_0>,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.903718948364258\n","generated ids [32099, 2163, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁Yes!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.937386989593506\n","generated ids [32099, 86, 32098, 5, 32097]\n","generated text <extra_id_0>▁In<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.940522193908691\n","generated ids [32099, 58, 32098, 5, 32097]\n","generated text <extra_id_0>?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.94389533996582\n","generated ids [32099, 2163, 6, 32098, 6, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.954066276550293\n","generated ids [32099, 4357, 32098, 5, 32097]\n","generated text <extra_id_0>▁6.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.958919525146484\n","generated ids [32099, 9710, 32098, 5, 32097]\n","generated text <extra_id_0>▁Ah<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.972485542297363\n","generated ids [32099, 2163, 11, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes▁and<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.009743690490723\n","generated ids [32099, 20510, 32098, 5, 32097]\n","generated text <extra_id_0>▁Absolutely<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.013106346130371\n","generated ids [32099, 4848, 32098, 5, 32097]\n","generated text <extra_id_0>▁8.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.032957077026367\n","generated ids [32099, 96, 32098, 55, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.03409481048584\n","generated ids [32099, 220, 32098, 5, 32097]\n","generated text <extra_id_0>▁3<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.051141738891602\n","generated ids [32099, 4306, 32098, 5, 32097]\n","generated text <extra_id_0>▁7.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.051177978515625\n","generated ids [32099, 287, 5, 32098, 5, 32097]\n","generated text <extra_id_0>com.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.072941780090332\n","generated ids [32099, 9710, 32098, 55, 32097]\n","generated text <extra_id_0>▁Ah<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.0929365158081055\n","generated ids [32099, 11, 32098, 5, 32097]\n","generated text <extra_id_0>▁and<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.121556282043457\n","generated ids [32099, 1562, 25, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thank▁you.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.14640998840332\n","generated ids [32099, 299, 32098, 6, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.148902893066406\n","generated ids [32099, 314, 32098, 5, 32097]\n","generated text <extra_id_0>▁4<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.160234451293945\n","generated ids [32099, 5477, 32098, 5, 32097]\n","generated text <extra_id_0>▁10.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.171546936035156\n","generated ids [32099, 27, 32098, 5, 32097]\n","generated text <extra_id_0>▁I<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.182836532592773\n","generated ids [32099, 465, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁No!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.202622413635254\n","generated ids [32099, 11475, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yeah.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.23354434967041\n","generated ids [32099, 14125, 32098, 5, 32097]\n","generated text <extra_id_0>......<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.263845443725586\n","generated ids [32099, 1142, 32098, 5, 32097]\n","generated text <extra_id_0>▁Just<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.278659820556641\n","generated ids [32099, 11419, 32098, 55, 32097]\n","generated text <extra_id_0>▁Hell<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.287906169891357\n","generated ids [32099, 96, 32098, 6, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.292318344116211\n","generated ids [32099, 1300, 32098, 5, 1682, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>.▁2.<extra_id_2>\n","--------------\n","score: -7.293074131011963\n","generated ids [32099, 305, 32098, 5, 32097]\n","generated text <extra_id_0>▁5<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.355151176452637\n","generated ids [32099, 156, 32098, 6, 32097]\n","generated text <extra_id_0>▁If<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.363207817077637\n","generated ids [32099, 411, 32098, 5, 32097]\n","generated text <extra_id_0>▁O<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.3716511726379395\n","generated ids [32099, 3359, 32098, 6, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.372425556182861\n","generated ids [32099, 2163, 32098, 2163, 32097]\n","generated text <extra_id_0>▁Yes<extra_id_1>▁Yes<extra_id_2>\n","--------------\n","score: -7.382988452911377\n","generated ids [32099, 1820, 32098, 5, 32097]\n","generated text <extra_id_0>▁|<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.385481834411621\n","generated ids [32099, 27, 7, 32098, 5, 32097]\n","generated text <extra_id_0>▁Is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.393003463745117\n","generated ids [32099, 955, 32098, 5, 32097]\n","generated text <extra_id_0>▁Or<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.393949508666992\n","generated ids [32099, 1804, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Good.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.406272888183594\n","generated ids [32099, 2, 32098, 5, 32097]\n","generated text <extra_id_0><unk><extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.4079694747924805\n","generated ids [32099, 1593, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁Q:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.431077003479004\n","generated ids [32099, 335, 32098, 5, 32097]\n","generated text <extra_id_0>▁10<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.431720733642578\n","generated ids [32099, 31, 32098, 5, 32097]\n","generated text <extra_id_0>'<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.452448844909668\n","generated ids [32099, 2163, 6, 32098, 55, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.461620330810547\n","generated ids [32099, 27, 317, 32098, 5, 32097]\n","generated text <extra_id_0>▁I▁think<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.469228744506836\n","generated ids [32099, 100, 19, 32098, 5, 32097]\n","generated text <extra_id_0>▁This▁is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.472079277038574\n","generated ids [32099, 499, 32098, 5, 32097]\n","generated text <extra_id_0>▁My<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.47821044921875\n","generated ids [32099, 264, 32098, 55, 32097]\n","generated text <extra_id_0>▁So<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.486699104309082\n","generated ids [32099, 5, 58, 32098, 5, 32097]\n","generated text <extra_id_0>.?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.503593444824219\n","generated ids [32099, 419, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁Re:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.522242546081543\n","generated ids [32099, 363, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁What?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.5224714279174805\n","generated ids [32099, 1300, 32098, 55, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.544200897216797\n","generated ids [32099, 505, 32098, 5, 32097]\n","generated text <extra_id_0>▁8<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.546428680419922\n","generated ids [32099, 1300, 32098, 6, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.547218322753906\n","generated ids [32099, 1333, 55, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thanks!<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.553128242492676\n","generated ids [32099, 2490, 32098, 6, 32097]\n","generated text <extra_id_0>▁><extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.554305076599121\n","generated ids [32099, 1615, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁Why?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.558199882507324\n","generated ids [32099, 11342, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Sorry.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.643718719482422\n","generated ids [32099, 27, 7, 34, 32098, 58, 32097]\n","generated text <extra_id_0>▁Is▁it<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.678577423095703\n","generated ids [32099, 1682, 32098, 5, 1877, 32097]\n","generated text <extra_id_0>▁2.<extra_id_1>.▁3.<extra_id_2>\n","--------------\n","score: -7.721428871154785\n","generated ids [32099, 264, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁So,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.731943607330322\n","generated ids [32099, 465, 32098, 465, 32097]\n","generated text <extra_id_0>▁No<extra_id_1>▁No<extra_id_2>\n","--------------\n","score: -7.739652633666992\n","generated ids [32099, 290, 19, 32098, 5, 32097]\n","generated text <extra_id_0>▁There▁is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.759004592895508\n","generated ids [32099, 27, 43, 32098, 5, 32097]\n","generated text <extra_id_0>▁I▁have<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.801900863647461\n","generated ids [32099, 465, 5, 32098, 55, 32097]\n","generated text <extra_id_0>▁No.<extra_id_1>!<extra_id_2>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls*.*mask*.*+sent_0**sep+*\n","*cls*.*mask*!*+sent_0**sep+*\n","*cls*.*mask*,*+sent_0**sep+*\n","*cls*▁Yes.*mask*.*+sent_0**sep+*\n","*cls*▁No.*mask*.*+sent_0**sep+*\n","*cls*▁Oh*mask*!*+sent_0**sep+*\n","*cls*▁And*mask*.*+sent_0**sep+*\n","*cls*▁Oh*mask*.*+sent_0**sep+*\n","*cls*▁Yes*mask*.*+sent_0**sep+*\n","*cls*▁But*mask*.*+sent_0**sep+*\n","*cls*▁1.*mask*.*+sent_0**sep+*\n","*cls*▁Yes,*mask*.*+sent_0**sep+*\n","*cls*▁2.*mask*.*+sent_0**sep+*\n","*cls*▁3.*mask*.*+sent_0**sep+*\n","*cls*▁No*mask*.*+sent_0**sep+*\n","*cls*▁A*mask*.*+sent_0**sep+*\n","*cls*▁>*mask*.*+sent_0**sep+*\n","*cls*▁\"*mask*.*+sent_0**sep+*\n","*cls*▁4.*mask*.*+sent_0**sep+*\n","*cls*▁And*mask*!*+sent_0**sep+*\n","*cls*.*mask*?*+sent_0**sep+*\n","*cls*▁The*mask*.*+sent_0**sep+*\n","*cls*▁5.*mask*.*+sent_0**sep+*\n","*cls*▁So*mask*.*+sent_0**sep+*\n","*cls*▁And*mask*,*+sent_0**sep+*\n","*cls*.*mask*...*+sent_0**sep+*\n","*cls*▁But*mask*!*+sent_0**sep+*\n","*cls*▁A.*mask*.*+sent_0**sep+*\n","*cls*▁**mask*.*+sent_0**sep+*\n","*cls*▁•*mask*.*+sent_0**sep+*\n","*cls*)*mask*.*+sent_0**sep+*\n","*cls*▁A:*mask*.*+sent_0**sep+*\n","*cls*▁1*mask*.*+sent_0**sep+*\n","*cls*▁9.*mask*.*+sent_0**sep+*\n","*cls*▁No,*mask*.*+sent_0**sep+*\n","*cls*▁Thanks.*mask*.*+sent_0**sep+*\n","*cls*▁2*mask*.*+sent_0**sep+*\n","*cls*,*mask*.*+sent_0**sep+*\n","*cls*▁Yes!*mask*!*+sent_0**sep+*\n","*cls*▁In*mask*.*+sent_0**sep+*\n","*cls*?*mask*.*+sent_0**sep+*\n","*cls*▁Yes,*mask*,*+sent_0**sep+*\n","*cls*▁6.*mask*.*+sent_0**sep+*\n","*cls*▁Ah*mask*.*+sent_0**sep+*\n","*cls*▁Yes▁and*mask*.*+sent_0**sep+*\n","*cls*▁Absolutely*mask*.*+sent_0**sep+*\n","*cls*▁8.*mask*.*+sent_0**sep+*\n","*cls*▁\"*mask*!*+sent_0**sep+*\n","*cls*▁3*mask*.*+sent_0**sep+*\n","*cls*▁7.*mask*.*+sent_0**sep+*\n","####### generated templates #######\n","\n","# spoilers 16 87 100\n","| dataset examples\n","| {'label': 0, 'text': ['EVEN MORE THAN THE PREVIOUS']}\n","| {'label': 1, 'text': ['Mikasa is stronger then him so he argues her']}\n","\n","| mapping\n","| {0: 'No', 1: 'Yes'}\n","####### example #######\n","EVEN MORE THAN THE PREVIOUS<extra_id_0> No<extra_id_1>\n","The Chad himself<extra_id_0> No<extra_id_1>\n","OHH OKI THANK U SM<extra_id_0> No<extra_id_1>\n","####### example #######\n","\n","100% 18/18 [13:02<00:00, 43.47s/it]\n","####### generated results #######\n","--------------\n","score: -0.816253662109375\n","generated ids [32099, 5, 32098, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>.</s>\n","--------------\n","score: -2.446578025817871\n","generated ids [32099, 5, 32098, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>!</s>\n","--------------\n","score: -2.878347396850586\n","generated ids [32099, 58, 32098, 5, 1]\n","generated text <extra_id_0>?<extra_id_1>.</s>\n","--------------\n","score: -3.052236557006836\n","generated ids [32099, 5, 32098, 58, 1]\n","generated text <extra_id_0>.<extra_id_1>?</s>\n","--------------\n","score: -3.2280235290527344\n","generated ids [32099, 55, 32098, 55, 1]\n","generated text <extra_id_0>!<extra_id_1>!</s>\n","--------------\n","score: -3.541851043701172\n","generated ids [32099, 55, 32098, 5, 1]\n","generated text <extra_id_0>!<extra_id_1>.</s>\n","--------------\n","score: -3.8884220123291016\n","generated ids [32099, 6, 32098, 5, 1]\n","generated text <extra_id_0>,<extra_id_1>.</s>\n","--------------\n","score: -3.906158447265625\n","generated ids [32099, 58, 32098, 55, 1]\n","generated text <extra_id_0>?<extra_id_1>!</s>\n","--------------\n","score: -4.8266401290893555\n","generated ids [32099, 6, 32098, 58, 1]\n","generated text <extra_id_0>,<extra_id_1>?</s>\n","--------------\n","score: -4.855776786804199\n","generated ids [32099, 58, 32098, 58, 1]\n","generated text <extra_id_0>?<extra_id_1>?</s>\n","--------------\n","score: -4.909316062927246\n","generated ids [32099, 6, 32098, 55, 1]\n","generated text <extra_id_0>,<extra_id_1>!</s>\n","--------------\n","score: -4.939001083374023\n","generated ids [32099, 233, 32098, 5, 1]\n","generated text <extra_id_0>...<extra_id_1>.</s>\n","--------------\n","score: -5.281832695007324\n","generated ids [32099, 55, 32098, 58, 1]\n","generated text <extra_id_0>!<extra_id_1>?</s>\n","--------------\n","score: -5.505188941955566\n","generated ids [32099, 11, 32098, 5, 1]\n","generated text <extra_id_0>▁and<extra_id_1>.</s>\n","--------------\n","score: -5.607095718383789\n","generated ids [32099, 233, 32098, 55, 1]\n","generated text <extra_id_0>...<extra_id_1>!</s>\n","--------------\n","score: -5.620848655700684\n","generated ids [32099, 16, 32098, 5, 1]\n","generated text <extra_id_0>▁in<extra_id_1>.</s>\n","--------------\n","score: -5.655890464782715\n","generated ids [32099, 10, 32098, 5, 1]\n","generated text <extra_id_0>:<extra_id_1>.</s>\n","--------------\n","score: -5.980114936828613\n","generated ids [32099, 5, 32098, 2824, 1]\n","generated text <extra_id_0>.<extra_id_1>....</s>\n","--------------\n","score: -6.077554702758789\n","generated ids [32099, 21, 32098, 5, 1]\n","generated text <extra_id_0>▁for<extra_id_1>.</s>\n","--------------\n","score: -6.150687217712402\n","generated ids [32099, 2824, 32098, 5, 1]\n","generated text <extra_id_0>....<extra_id_1>.</s>\n","--------------\n","score: -6.248801231384277\n","generated ids [32099, 233, 32098, 58, 1]\n","generated text <extra_id_0>...<extra_id_1>?</s>\n","--------------\n","score: -6.3187971115112305\n","generated ids [32099, 28, 32098, 5, 1]\n","generated text <extra_id_0>▁with<extra_id_1>.</s>\n","--------------\n","score: -6.419669151306152\n","generated ids [32099, 11, 32098, 55, 1]\n","generated text <extra_id_0>▁and<extra_id_1>!</s>\n","--------------\n","score: -6.46129035949707\n","generated ids [32099, 1603, 32098, 55, 1]\n","generated text <extra_id_0>!!<extra_id_1>!</s>\n","--------------\n","score: -6.485620498657227\n","generated ids [32099, 30, 32098, 5, 1]\n","generated text <extra_id_0>▁on<extra_id_1>.</s>\n","--------------\n","score: -6.589696884155273\n","generated ids [32099, 12, 32098, 5, 1]\n","generated text <extra_id_0>▁to<extra_id_1>.</s>\n","--------------\n","score: -6.608146667480469\n","generated ids [32099, 13, 32098, 5, 1]\n","generated text <extra_id_0>▁of<extra_id_1>.</s>\n","--------------\n","score: -6.686427116394043\n","generated ids [32099, 3158, 32098, 55, 1]\n","generated text <extra_id_0>!!!<extra_id_1>!</s>\n","--------------\n","score: -6.700803756713867\n","generated ids [32099, 44, 32098, 5, 1]\n","generated text <extra_id_0>▁at<extra_id_1>.</s>\n","--------------\n","score: -6.7792558670043945\n","generated ids [32099, 10, 32098, 55, 1]\n","generated text <extra_id_0>:<extra_id_1>!</s>\n","--------------\n","score: -6.804758071899414\n","generated ids [32099, 5, 32098, 535, 1]\n","generated text <extra_id_0>.<extra_id_1>.\"</s>\n","--------------\n","score: -6.860711097717285\n","generated ids [32099, 2824, 32098, 55, 1]\n","generated text <extra_id_0>....<extra_id_1>!</s>\n","--------------\n","score: -7.003419876098633\n","generated ids [32099, 57, 32098, 5, 1]\n","generated text <extra_id_0>▁by<extra_id_1>.</s>\n","--------------\n","score: -7.020168304443359\n","generated ids [32099, 535, 32098, 5, 1]\n","generated text <extra_id_0>.\"<extra_id_1>.</s>\n","--------------\n","score: -7.0346879959106445\n","generated ids [32099, 42, 32098, 5, 1]\n","generated text <extra_id_0>▁or<extra_id_1>.</s>\n","--------------\n","score: -7.0443572998046875\n","generated ids [32099, 1603, 32098, 5, 1]\n","generated text <extra_id_0>!!<extra_id_1>.</s>\n","--------------\n","score: -7.056219100952148\n","generated ids [32099, 45, 32098, 5, 1]\n","generated text <extra_id_0>▁from<extra_id_1>.</s>\n","--------------\n","score: -7.0796003341674805\n","generated ids [32099, 18, 32098, 5, 1]\n","generated text <extra_id_0>-<extra_id_1>.</s>\n","--------------\n","score: -7.094803810119629\n","generated ids [32099, 16, 32098, 55, 1]\n","generated text <extra_id_0>▁in<extra_id_1>!</s>\n","--------------\n","score: -7.103573799133301\n","generated ids [32099, 19, 32098, 5, 1]\n","generated text <extra_id_0>▁is<extra_id_1>.</s>\n","--------------\n","score: -7.160886764526367\n","generated ids [32099, 5, 32098, 9374, 1]\n","generated text <extra_id_0>.<extra_id_1>.....</s>\n","--------------\n","score: -7.219023704528809\n","generated ids [32099, 5, 32098, 1603, 1]\n","generated text <extra_id_0>.<extra_id_1>!!</s>\n","--------------\n","score: -7.241183280944824\n","generated ids [32099, 137, 32098, 5, 1]\n","generated text <extra_id_0>).<extra_id_1>.</s>\n","--------------\n","score: -7.296947479248047\n","generated ids [32099, 7, 5, 32098, 5, 1]\n","generated text <extra_id_0>s.<extra_id_1>.</s>\n","--------------\n","score: -7.2982025146484375\n","generated ids [32099, 117, 32098, 5, 1]\n","generated text <extra_id_0>;<extra_id_1>.</s>\n","--------------\n","score: -7.314115524291992\n","generated ids [32099, 38, 32098, 5, 1]\n","generated text <extra_id_0>▁as<extra_id_1>.</s>\n","--------------\n","score: -7.338879585266113\n","generated ids [32099, 21, 32098, 55, 1]\n","generated text <extra_id_0>▁for<extra_id_1>!</s>\n","--------------\n","score: -7.353151321411133\n","generated ids [32099, 5, 32098, 233, 1]\n","generated text <extra_id_0>.<extra_id_1>...</s>\n","--------------\n","score: -7.369314193725586\n","generated ids [32099, 7, 32098, 5, 1]\n","generated text <extra_id_0>s<extra_id_1>.</s>\n","--------------\n","score: -7.392991065979004\n","generated ids [32099, 5, 32098, 6, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>,.</s>\n","--------------\n","score: -7.421160697937012\n","generated ids [32099, 5, 32098, 2049, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁thanks.</s>\n","--------------\n","score: -7.427436828613281\n","generated ids [32099, 3158, 32098, 5, 1]\n","generated text <extra_id_0>!!!<extra_id_1>.</s>\n","--------------\n","score: -7.473615646362305\n","generated ids [32099, 5, 32098, 7, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>s.</s>\n","--------------\n","score: -7.47987174987793\n","generated ids [32099, 9374, 32098, 5, 1]\n","generated text <extra_id_0>.....<extra_id_1>.</s>\n","--------------\n","score: -7.49875545501709\n","generated ids [32099, 8546, 32098, 5, 1]\n","generated text <extra_id_0>??<extra_id_1>.</s>\n","--------------\n","score: -7.536135673522949\n","generated ids [32099, 30, 32098, 55, 1]\n","generated text <extra_id_0>▁on<extra_id_1>!</s>\n","--------------\n","score: -7.551407814025879\n","generated ids [32099, 28, 32098, 55, 1]\n","generated text <extra_id_0>▁with<extra_id_1>!</s>\n","--------------\n","score: -7.566173553466797\n","generated ids [32099, 5, 32098, 754, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁please.</s>\n","--------------\n","score: -7.572251319885254\n","generated ids [32099, 121, 32098, 5, 1]\n","generated text <extra_id_0>\"<extra_id_1>.</s>\n","--------------\n","score: -7.580876350402832\n","generated ids [32099, 68, 32098, 5, 1]\n","generated text <extra_id_0>▁but<extra_id_1>.</s>\n","--------------\n","score: -7.618284225463867\n","generated ids [32099, 78, 32098, 5, 1]\n","generated text <extra_id_0>▁so<extra_id_1>.</s>\n","--------------\n","score: -7.622119903564453\n","generated ids [32099, 27, 32098, 5, 1]\n","generated text <extra_id_0>▁I<extra_id_1>.</s>\n","--------------\n","score: -7.761259078979492\n","generated ids [32099, 18, 32098, 55, 1]\n","generated text <extra_id_0>-<extra_id_1>!</s>\n","--------------\n","score: -7.767416954040527\n","generated ids [32099, 8665, 32098, 5, 1]\n","generated text <extra_id_0>???<extra_id_1>.</s>\n","--------------\n","score: -7.778316497802734\n","generated ids [32099, 55, 32098, 1603, 1]\n","generated text <extra_id_0>!<extra_id_1>!!</s>\n","--------------\n","score: -7.82216739654541\n","generated ids [32099, 58, 32098, 2824, 1]\n","generated text <extra_id_0>?<extra_id_1>....</s>\n","--------------\n","score: -7.847700119018555\n","generated ids [32099, 10769, 32098, 55, 1]\n","generated text <extra_id_0>?!<extra_id_1>!</s>\n","--------------\n","score: -7.854874610900879\n","generated ids [32099, 13, 32098, 55, 1]\n","generated text <extra_id_0>▁of<extra_id_1>!</s>\n","--------------\n","score: -7.876492500305176\n","generated ids [32099, 12, 32098, 55, 1]\n","generated text <extra_id_0>▁to<extra_id_1>!</s>\n","--------------\n","score: -7.883349418640137\n","generated ids [32099, 55, 32098, 2824, 1]\n","generated text <extra_id_0>!<extra_id_1>....</s>\n","--------------\n","score: -7.8967695236206055\n","generated ids [32099, 42, 32098, 58, 1]\n","generated text <extra_id_0>▁or<extra_id_1>?</s>\n","--------------\n","score: -7.91140079498291\n","generated ids [32099, 8, 32098, 5, 1]\n","generated text <extra_id_0>▁the<extra_id_1>.</s>\n","--------------\n","score: -7.934521675109863\n","generated ids [32099, 2824, 32098, 58, 1]\n","generated text <extra_id_0>....<extra_id_1>?</s>\n","--------------\n","score: -8.011792182922363\n","generated ids [32099, 114, 32098, 5, 1]\n","generated text <extra_id_0>▁like<extra_id_1>.</s>\n","--------------\n","score: -8.0889892578125\n","generated ids [32099, 8546, 32098, 55, 1]\n","generated text <extra_id_0>??<extra_id_1>!</s>\n","--------------\n","score: -8.092428207397461\n","generated ids [32099, 61, 32098, 5, 1]\n","generated text <extra_id_0>)<extra_id_1>.</s>\n","--------------\n","score: -8.138349533081055\n","generated ids [32099, 5, 32098, 137, 1]\n","generated text <extra_id_0>.<extra_id_1>).</s>\n","--------------\n","score: -8.138670921325684\n","generated ids [32099, 42, 32098, 55, 1]\n","generated text <extra_id_0>▁or<extra_id_1>!</s>\n","--------------\n","score: -8.15857982635498\n","generated ids [32099, 12887, 32098, 55, 1]\n","generated text <extra_id_0>!!!!<extra_id_1>!</s>\n","--------------\n","score: -8.179920196533203\n","generated ids [32099, 16, 32098, 58, 1]\n","generated text <extra_id_0>▁in<extra_id_1>?</s>\n","--------------\n","score: -8.185500144958496\n","generated ids [32099, 9374, 32098, 55, 1]\n","generated text <extra_id_0>.....<extra_id_1>!</s>\n","--------------\n","score: -8.205318450927734\n","generated ids [32099, 8665, 32098, 55, 1]\n","generated text <extra_id_0>???<extra_id_1>!</s>\n","--------------\n","score: -8.260510444641113\n","generated ids [32099, 44, 32098, 55, 1]\n","generated text <extra_id_0>▁at<extra_id_1>!</s>\n","--------------\n","score: -8.26480484008789\n","generated ids [32099, 396, 5, 32098, 5, 1]\n","generated text <extra_id_0>▁too.<extra_id_1>.</s>\n","--------------\n","score: -8.281256675720215\n","generated ids [32099, 1603, 32098, 58, 1]\n","generated text <extra_id_0>!!<extra_id_1>?</s>\n","--------------\n","score: -8.281261444091797\n","generated ids [32099, 5, 32098, 121, 1]\n","generated text <extra_id_0>.<extra_id_1>\"</s>\n","--------------\n","score: -8.347504615783691\n","generated ids [32099, 45, 32098, 55, 1]\n","generated text <extra_id_0>▁from<extra_id_1>!</s>\n","--------------\n","score: -8.353058815002441\n","generated ids [32099, 3274, 32098, 5, 1]\n","generated text <extra_id_0>▁=<extra_id_1>.</s>\n","--------------\n","score: -8.35799503326416\n","generated ids [32099, 5, 32098, 3158, 1]\n","generated text <extra_id_0>.<extra_id_1>!!!</s>\n","--------------\n","score: -8.364681243896484\n","generated ids [32099, 57, 32098, 55, 1]\n","generated text <extra_id_0>▁by<extra_id_1>!</s>\n","--------------\n","score: -8.385043144226074\n","generated ids [32099, 10769, 32098, 5, 1]\n","generated text <extra_id_0>?!<extra_id_1>.</s>\n","--------------\n","score: -8.400644302368164\n","generated ids [32099, 5, 32098, 6, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>,!</s>\n","--------------\n","score: -8.406550407409668\n","generated ids [32099, 5, 32098, 754, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>▁please!</s>\n","--------------\n","score: -8.4093599319458\n","generated ids [32099, 535, 32098, 55, 1]\n","generated text <extra_id_0>.\"<extra_id_1>!</s>\n","--------------\n","score: -8.409626960754395\n","generated ids [32099, 47, 32098, 5, 1]\n","generated text <extra_id_0>▁was<extra_id_1>.</s>\n","--------------\n","score: -8.414815902709961\n","generated ids [32099, 5, 32098, 310, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁really.</s>\n","--------------\n","score: -8.437217712402344\n","generated ids [32099, 24, 32098, 5, 1]\n","generated text <extra_id_0>▁that<extra_id_1>.</s>\n","--------------\n","score: -8.438515663146973\n","generated ids [32099, 78, 32098, 55, 1]\n","generated text <extra_id_0>▁so<extra_id_1>!</s>\n","--------------\n","score: -8.530516624450684\n","generated ids [32099, 5, 32098, 2049, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>▁thanks!</s>\n","--------------\n","score: -8.636159896850586\n","generated ids [32099, 5, 32098, 34, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁it.</s>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls**sent_0*.*mask*.*sep+*\n","*cls**sent_0*.*mask*!*sep+*\n","*cls**sent_0*?*mask*.*sep+*\n","*cls**sent_0*.*mask*?*sep+*\n","*cls**sent_0*!*mask*!*sep+*\n","*cls**sent_0*!*mask*.*sep+*\n","*cls**sent_0*,*mask*.*sep+*\n","*cls**sent_0*?*mask*!*sep+*\n","*cls**sent_0*,*mask*?*sep+*\n","*cls**sent_0*?*mask*?*sep+*\n","*cls**sent_0*,*mask*!*sep+*\n","*cls**sent_0*...*mask*.*sep+*\n","*cls**sent_0*!*mask*?*sep+*\n","*cls**sent_0*▁and*mask*.*sep+*\n","*cls**sent_0*...*mask*!*sep+*\n","*cls**sent_0*▁in*mask*.*sep+*\n","*cls**sent_0*:*mask*.*sep+*\n","*cls**sent_0*.*mask*....*sep+*\n","*cls**sent_0*▁for*mask*.*sep+*\n","*cls**sent_0*....*mask*.*sep+*\n","*cls**sent_0*...*mask*?*sep+*\n","*cls**sent_0*▁with*mask*.*sep+*\n","*cls**sent_0*▁and*mask*!*sep+*\n","*cls**sent_0*!!*mask*!*sep+*\n","*cls**sent_0*▁on*mask*.*sep+*\n","*cls**sent_0*▁to*mask*.*sep+*\n","*cls**sent_0*▁of*mask*.*sep+*\n","*cls**sent_0*!!!*mask*!*sep+*\n","*cls**sent_0*▁at*mask*.*sep+*\n","*cls**sent_0*:*mask*!*sep+*\n","*cls**sent_0*.*mask*.\"*sep+*\n","*cls**sent_0*....*mask*!*sep+*\n","*cls**sent_0*▁by*mask*.*sep+*\n","*cls**sent_0*.\"*mask*.*sep+*\n","*cls**sent_0*▁or*mask*.*sep+*\n","*cls**sent_0*!!*mask*.*sep+*\n","*cls**sent_0*▁from*mask*.*sep+*\n","*cls**sent_0*-*mask*.*sep+*\n","*cls**sent_0*▁in*mask*!*sep+*\n","*cls**sent_0*▁is*mask*.*sep+*\n","*cls**sent_0*.*mask*.....*sep+*\n","*cls**sent_0*.*mask*!!*sep+*\n","*cls**sent_0*).*mask*.*sep+*\n","*cls**sent_0*s.*mask*.*sep+*\n","*cls**sent_0*;*mask*.*sep+*\n","*cls**sent_0*▁as*mask*.*sep+*\n","*cls**sent_0*▁for*mask*!*sep+*\n","*cls**sent_0*.*mask*...*sep+*\n","*cls**sent_0*s*mask*.*sep+*\n","*cls**sent_0*.*mask*,.*sep+*\n","####### generated templates #######\n","\n","####### example #######\n",".<extra_id_0> No<extra_id_1> EVEN MORE THAN THE PREVIOUS\n",".<extra_id_0> No<extra_id_1> The Chad himself\n",".<extra_id_0> No<extra_id_1> OHH OKI THANK U SM\n","####### example #######\n","\n","100% 18/18 [14:09<00:00, 47.20s/it]\n","####### generated results #######\n","--------------\n","score: -3.2521228790283203\n","generated ids [32099, 5, 32098, 5, 32097]\n","generated text <extra_id_0>.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -4.351896286010742\n","generated ids [32099, 5, 32098, 55, 32097]\n","generated text <extra_id_0>.<extra_id_1>!<extra_id_2>\n","--------------\n","score: -4.502222537994385\n","generated ids [32099, 5, 32098, 6, 32097]\n","generated text <extra_id_0>.<extra_id_1>,<extra_id_2>\n","--------------\n","score: -5.003849983215332\n","generated ids [32099, 2163, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.165234565734863\n","generated ids [32099, 465, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁No.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.226134300231934\n","generated ids [32099, 3359, 32098, 55, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>!<extra_id_2>\n","--------------\n","score: -5.440759658813477\n","generated ids [32099, 275, 32098, 5, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.650282859802246\n","generated ids [32099, 3359, 32098, 5, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.753972053527832\n","generated ids [32099, 299, 32098, 5, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.8631181716918945\n","generated ids [32099, 2163, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.999385833740234\n","generated ids [32099, 2163, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.042203426361084\n","generated ids [32099, 5, 32098, 58, 32097]\n","generated text <extra_id_0>.<extra_id_1>?<extra_id_2>\n","--------------\n","score: -6.094542503356934\n","generated ids [32099, 1300, 32098, 5, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.0965423583984375\n","generated ids [32099, 275, 32098, 55, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.131707191467285\n","generated ids [32099, 37, 32098, 5, 32097]\n","generated text <extra_id_0>▁The<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.211221218109131\n","generated ids [32099, 275, 32098, 6, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.252669334411621\n","generated ids [32099, 1682, 32098, 5, 32097]\n","generated text <extra_id_0>▁2.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.270272254943848\n","generated ids [32099, 96, 32098, 5, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.279650688171387\n","generated ids [32099, 465, 32098, 5, 32097]\n","generated text <extra_id_0>▁No<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.327258110046387\n","generated ids [32099, 264, 32098, 5, 32097]\n","generated text <extra_id_0>▁So<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.364067554473877\n","generated ids [32099, 71, 32098, 5, 32097]\n","generated text <extra_id_0>▁A<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.3945488929748535\n","generated ids [32099, 2490, 32098, 5, 32097]\n","generated text <extra_id_0>▁><extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.400763511657715\n","generated ids [32099, 1877, 32098, 5, 32097]\n","generated text <extra_id_0>▁3.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.4082770347595215\n","generated ids [32099, 5, 32098, 233, 32097]\n","generated text <extra_id_0>.<extra_id_1>...<extra_id_2>\n","--------------\n","score: -6.440752983093262\n","generated ids [32099, 299, 32098, 55, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.597668647766113\n","generated ids [32099, 2853, 32098, 5, 32097]\n","generated text <extra_id_0>▁4.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.6229472160339355\n","generated ids [32099, 1429, 32098, 5, 32097]\n","generated text <extra_id_0>▁*<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.752823829650879\n","generated ids [32099, 3594, 32098, 5, 32097]\n","generated text <extra_id_0>▁5.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.753165245056152\n","generated ids [32099, 1333, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thanks.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.755354881286621\n","generated ids [32099, 465, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁No,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.787631988525391\n","generated ids [32099, 71, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁A.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.830107688903809\n","generated ids [32099, 71, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁A:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.872359275817871\n","generated ids [32099, 20510, 32098, 5, 32097]\n","generated text <extra_id_0>▁Absolutely<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.914437770843506\n","generated ids [32099, 1697, 32098, 5, 32097]\n","generated text <extra_id_0>▁•<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.91560173034668\n","generated ids [32099, 2163, 11, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes▁and<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.986137866973877\n","generated ids [32099, 2163, 6, 32098, 6, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.994011402130127\n","generated ids [32099, 61, 32098, 5, 32097]\n","generated text <extra_id_0>)<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.072944641113281\n","generated ids [32099, 299, 32098, 6, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.110568046569824\n","generated ids [32099, 2163, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁Yes!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.116389274597168\n","generated ids [32099, 11419, 32098, 55, 32097]\n","generated text <extra_id_0>▁Hell<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.122945785522461\n","generated ids [32099, 287, 5, 32098, 5, 32097]\n","generated text <extra_id_0>com.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.133776664733887\n","generated ids [32099, 6, 32098, 5, 32097]\n","generated text <extra_id_0>,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.145654678344727\n","generated ids [32099, 11475, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yeah.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.153777122497559\n","generated ids [32099, 1562, 25, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thank▁you.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.18690824508667\n","generated ids [32099, 14125, 32098, 5, 32097]\n","generated text <extra_id_0>......<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.203571796417236\n","generated ids [32099, 11, 32098, 5, 32097]\n","generated text <extra_id_0>▁and<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.204123497009277\n","generated ids [32099, 9710, 32098, 5, 32097]\n","generated text <extra_id_0>▁Ah<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.217297554016113\n","generated ids [32099, 156, 32098, 6, 32097]\n","generated text <extra_id_0>▁If<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.227527618408203\n","generated ids [32099, 100, 19, 32098, 5, 32097]\n","generated text <extra_id_0>▁This▁is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.238208293914795\n","generated ids [32099, 209, 32098, 5, 32097]\n","generated text <extra_id_0>▁1<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.239254951477051\n","generated ids [32099, 204, 32098, 5, 32097]\n","generated text <extra_id_0>▁2<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.26210880279541\n","generated ids [32099, 86, 32098, 5, 32097]\n","generated text <extra_id_0>▁In<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.281411170959473\n","generated ids [32099, 1333, 55, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thanks!<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.286686897277832\n","generated ids [32099, 9710, 32098, 55, 32097]\n","generated text <extra_id_0>▁Ah<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.29550838470459\n","generated ids [32099, 3359, 32098, 6, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.303328514099121\n","generated ids [32099, 465, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁No!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.321059703826904\n","generated ids [32099, 1142, 32098, 5, 32097]\n","generated text <extra_id_0>▁Just<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.323417663574219\n","generated ids [32099, 96, 32098, 55, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.324352264404297\n","generated ids [32099, 58, 32098, 5, 32097]\n","generated text <extra_id_0>?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.359673023223877\n","generated ids [32099, 27, 32098, 5, 32097]\n","generated text <extra_id_0>▁I<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.38014030456543\n","generated ids [32099, 27, 317, 32098, 5, 32097]\n","generated text <extra_id_0>▁I▁think<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.3811845779418945\n","generated ids [32099, 5835, 32098, 5, 32097]\n","generated text <extra_id_0>▁9.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.424999713897705\n","generated ids [32099, 2163, 32098, 2163, 32097]\n","generated text <extra_id_0>▁Yes<extra_id_1>▁Yes<extra_id_2>\n","--------------\n","score: -7.437482833862305\n","generated ids [32099, 4357, 32098, 5, 32097]\n","generated text <extra_id_0>▁6.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.448366641998291\n","generated ids [32099, 220, 32098, 5, 32097]\n","generated text <extra_id_0>▁3<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.462844371795654\n","generated ids [32099, 5, 58, 32098, 5, 32097]\n","generated text <extra_id_0>.?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.471517086029053\n","generated ids [32099, 5, 32098, 1603, 32097]\n","generated text <extra_id_0>.<extra_id_1>!!<extra_id_2>\n","--------------\n","score: -7.472233772277832\n","generated ids [32099, 290, 19, 32098, 5, 32097]\n","generated text <extra_id_0>▁There▁is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.47723388671875\n","generated ids [32099, 264, 32098, 55, 32097]\n","generated text <extra_id_0>▁So<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.495481491088867\n","generated ids [32099, 419, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁Re:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.51723575592041\n","generated ids [32099, 2163, 6, 32098, 55, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.528323173522949\n","generated ids [32099, 363, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁What?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.532430171966553\n","generated ids [32099, 955, 32098, 5, 32097]\n","generated text <extra_id_0>▁Or<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.535224914550781\n","generated ids [32099, 1548, 32098, 5, 32097]\n","generated text <extra_id_0>▁Well<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.536397933959961\n","generated ids [32099, 27, 43, 32098, 5, 32097]\n","generated text <extra_id_0>▁I▁have<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.536959648132324\n","generated ids [32099, 11342, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Sorry.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.550161361694336\n","generated ids [32099, 96, 32098, 6, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.587340354919434\n","generated ids [32099, 1626, 1024, 32098, 5, 32097]\n","generated text <extra_id_0>▁Haha<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.601418495178223\n","generated ids [32099, 1804, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Good.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.612641334533691\n","generated ids [32099, 11291, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁Really?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.620644569396973\n","generated ids [32099, 27, 7, 34, 32098, 58, 32097]\n","generated text <extra_id_0>▁Is▁it<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.632771015167236\n","generated ids [32099, 314, 32098, 5, 32097]\n","generated text <extra_id_0>▁4<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.636358261108398\n","generated ids [32099, 1615, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁Why?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.658487796783447\n","generated ids [32099, 156, 32098, 5, 32097]\n","generated text <extra_id_0>▁If<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.6830549240112305\n","generated ids [32099, 264, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁So,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.744362831115723\n","generated ids [32099, 1333, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁Thanks!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.754944801330566\n","generated ids [32099, 94, 19, 32098, 5, 32097]\n","generated text <extra_id_0>▁It▁is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.75660514831543\n","generated ids [32099, 1300, 32098, 5, 1682, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>.▁2.<extra_id_2>\n","--------------\n","score: -7.770820617675781\n","generated ids [32099, 27, 7, 32098, 5, 32097]\n","generated text <extra_id_0>▁Is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.779309272766113\n","generated ids [32099, 1593, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁Q:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.788116931915283\n","generated ids [32099, 1429, 32098, 6, 32097]\n","generated text <extra_id_0>▁*<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.799985408782959\n","generated ids [32099, 2490, 32098, 6, 32097]\n","generated text <extra_id_0>▁><extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.811723709106445\n","generated ids [32099, 465, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁No?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.816483497619629\n","generated ids [32099, 465, 855, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Nope.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.873363494873047\n","generated ids [32099, 3359, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Oh,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.90269660949707\n","generated ids [32099, 363, 58, 32098, 55, 32097]\n","generated text <extra_id_0>▁What?<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.9051008224487305\n","generated ids [32099, 9710, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Ah,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.933989524841309\n","generated ids [32099, 5, 32098, 11, 32097]\n","generated text <extra_id_0>.<extra_id_1>▁and<extra_id_2>\n","--------------\n","score: -7.937302589416504\n","generated ids [32099, 465, 5, 32098, 55, 32097]\n","generated text <extra_id_0>▁No.<extra_id_1>!<extra_id_2>\n","--------------\n","score: -8.190723419189453\n","generated ids [32099, 1562, 25, 55, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thank▁you!<extra_id_1>.<extra_id_2>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls*.*mask*.*+sent_0**sep+*\n","*cls*.*mask*!*+sent_0**sep+*\n","*cls*.*mask*,*+sent_0**sep+*\n","*cls*▁Yes.*mask*.*+sent_0**sep+*\n","*cls*▁No.*mask*.*+sent_0**sep+*\n","*cls*▁Oh*mask*!*+sent_0**sep+*\n","*cls*▁And*mask*.*+sent_0**sep+*\n","*cls*▁Oh*mask*.*+sent_0**sep+*\n","*cls*▁But*mask*.*+sent_0**sep+*\n","*cls*▁Yes,*mask*.*+sent_0**sep+*\n","*cls*▁Yes*mask*.*+sent_0**sep+*\n","*cls*.*mask*?*+sent_0**sep+*\n","*cls*▁1.*mask*.*+sent_0**sep+*\n","*cls*▁And*mask*!*+sent_0**sep+*\n","*cls*▁The*mask*.*+sent_0**sep+*\n","*cls*▁And*mask*,*+sent_0**sep+*\n","*cls*▁2.*mask*.*+sent_0**sep+*\n","*cls*▁\"*mask*.*+sent_0**sep+*\n","*cls*▁No*mask*.*+sent_0**sep+*\n","*cls*▁So*mask*.*+sent_0**sep+*\n","*cls*▁A*mask*.*+sent_0**sep+*\n","*cls*▁>*mask*.*+sent_0**sep+*\n","*cls*▁3.*mask*.*+sent_0**sep+*\n","*cls*.*mask*...*+sent_0**sep+*\n","*cls*▁But*mask*!*+sent_0**sep+*\n","*cls*▁4.*mask*.*+sent_0**sep+*\n","*cls*▁**mask*.*+sent_0**sep+*\n","*cls*▁5.*mask*.*+sent_0**sep+*\n","*cls*▁Thanks.*mask*.*+sent_0**sep+*\n","*cls*▁No,*mask*.*+sent_0**sep+*\n","*cls*▁A.*mask*.*+sent_0**sep+*\n","*cls*▁A:*mask*.*+sent_0**sep+*\n","*cls*▁Absolutely*mask*.*+sent_0**sep+*\n","*cls*▁•*mask*.*+sent_0**sep+*\n","*cls*▁Yes▁and*mask*.*+sent_0**sep+*\n","*cls*▁Yes,*mask*,*+sent_0**sep+*\n","*cls*)*mask*.*+sent_0**sep+*\n","*cls*▁But*mask*,*+sent_0**sep+*\n","*cls*▁Yes!*mask*!*+sent_0**sep+*\n","*cls*▁Hell*mask*!*+sent_0**sep+*\n","*cls*com.*mask*.*+sent_0**sep+*\n","*cls*,*mask*.*+sent_0**sep+*\n","*cls*▁Yeah.*mask*.*+sent_0**sep+*\n","*cls*▁Thank▁you.*mask*.*+sent_0**sep+*\n","*cls*......*mask*.*+sent_0**sep+*\n","*cls*▁and*mask*.*+sent_0**sep+*\n","*cls*▁Ah*mask*.*+sent_0**sep+*\n","*cls*▁If*mask*,*+sent_0**sep+*\n","*cls*▁This▁is*mask*.*+sent_0**sep+*\n","*cls*▁1*mask*.*+sent_0**sep+*\n","####### generated templates #######\n","\n","# spoilers 16 100 100\n","| dataset examples\n","| {'label': 1, 'text': ['Armored and attack titan fight IS THE BEST CLIP THO']}\n","| {'label': 0, 'text': ['basically after season 3 part 1 was over']}\n","\n","| mapping\n","| {0: 'No', 1: 'Yes'}\n","####### example #######\n","Armored and attack titan fight IS THE BEST CLIP THO<extra_id_0> Yes<extra_id_1>\n","AAAAAAA eyy levi vs zeke alrdy happening djnjkfbje,rkfker im not ready<extra_id_0> Yes<extra_id_1>\n","Mikasa ate armin's d-<extra_id_0> Yes<extra_id_1>\n","####### example #######\n","\n","100% 18/18 [10:18<00:00, 34.37s/it]\n","####### generated results #######\n","--------------\n","score: -0.9415483474731445\n","generated ids [32099, 5, 32098, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>.</s>\n","--------------\n","score: -2.624114990234375\n","generated ids [32099, 5, 32098, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>!</s>\n","--------------\n","score: -2.8365306854248047\n","generated ids [32099, 58, 32098, 5, 1]\n","generated text <extra_id_0>?<extra_id_1>.</s>\n","--------------\n","score: -3.0692834854125977\n","generated ids [32099, 55, 32098, 55, 1]\n","generated text <extra_id_0>!<extra_id_1>!</s>\n","--------------\n","score: -3.310248374938965\n","generated ids [32099, 5, 32098, 58, 1]\n","generated text <extra_id_0>.<extra_id_1>?</s>\n","--------------\n","score: -3.358675956726074\n","generated ids [32099, 55, 32098, 5, 1]\n","generated text <extra_id_0>!<extra_id_1>.</s>\n","--------------\n","score: -3.748983383178711\n","generated ids [32099, 6, 32098, 5, 1]\n","generated text <extra_id_0>,<extra_id_1>.</s>\n","--------------\n","score: -3.7741756439208984\n","generated ids [32099, 58, 32098, 55, 1]\n","generated text <extra_id_0>?<extra_id_1>!</s>\n","--------------\n","score: -4.541240692138672\n","generated ids [32099, 58, 32098, 58, 1]\n","generated text <extra_id_0>?<extra_id_1>?</s>\n","--------------\n","score: -4.548158645629883\n","generated ids [32099, 233, 32098, 5, 1]\n","generated text <extra_id_0>...<extra_id_1>.</s>\n","--------------\n","score: -4.570998191833496\n","generated ids [32099, 6, 32098, 55, 1]\n","generated text <extra_id_0>,<extra_id_1>!</s>\n","--------------\n","score: -4.769164085388184\n","generated ids [32099, 6, 32098, 58, 1]\n","generated text <extra_id_0>,<extra_id_1>?</s>\n","--------------\n","score: -5.029030799865723\n","generated ids [32099, 55, 32098, 58, 1]\n","generated text <extra_id_0>!<extra_id_1>?</s>\n","--------------\n","score: -5.187990188598633\n","generated ids [32099, 233, 32098, 55, 1]\n","generated text <extra_id_0>...<extra_id_1>!</s>\n","--------------\n","score: -5.195430755615234\n","generated ids [32099, 10, 32098, 5, 1]\n","generated text <extra_id_0>:<extra_id_1>.</s>\n","--------------\n","score: -5.240725517272949\n","generated ids [32099, 11, 32098, 5, 1]\n","generated text <extra_id_0>▁and<extra_id_1>.</s>\n","--------------\n","score: -5.942203521728516\n","generated ids [32099, 233, 32098, 58, 1]\n","generated text <extra_id_0>...<extra_id_1>?</s>\n","--------------\n","score: -5.989467620849609\n","generated ids [32099, 2824, 32098, 5, 1]\n","generated text <extra_id_0>....<extra_id_1>.</s>\n","--------------\n","score: -6.007105827331543\n","generated ids [32099, 11, 32098, 55, 1]\n","generated text <extra_id_0>▁and<extra_id_1>!</s>\n","--------------\n","score: -6.038914680480957\n","generated ids [32099, 1603, 32098, 55, 1]\n","generated text <extra_id_0>!!<extra_id_1>!</s>\n","--------------\n","score: -6.104120254516602\n","generated ids [32099, 16, 32098, 5, 1]\n","generated text <extra_id_0>▁in<extra_id_1>.</s>\n","--------------\n","score: -6.156587600708008\n","generated ids [32099, 10, 32098, 55, 1]\n","generated text <extra_id_0>:<extra_id_1>!</s>\n","--------------\n","score: -6.214771270751953\n","generated ids [32099, 21, 32098, 5, 1]\n","generated text <extra_id_0>▁for<extra_id_1>.</s>\n","--------------\n","score: -6.235744476318359\n","generated ids [32099, 5, 32098, 2824, 1]\n","generated text <extra_id_0>.<extra_id_1>....</s>\n","--------------\n","score: -6.250414848327637\n","generated ids [32099, 3158, 32098, 55, 1]\n","generated text <extra_id_0>!!!<extra_id_1>!</s>\n","--------------\n","score: -6.564812660217285\n","generated ids [32099, 12, 32098, 5, 1]\n","generated text <extra_id_0>▁to<extra_id_1>.</s>\n","--------------\n","score: -6.617379188537598\n","generated ids [32099, 1603, 32098, 5, 1]\n","generated text <extra_id_0>!!<extra_id_1>.</s>\n","--------------\n","score: -6.684186935424805\n","generated ids [32099, 2824, 32098, 55, 1]\n","generated text <extra_id_0>....<extra_id_1>!</s>\n","--------------\n","score: -6.717597007751465\n","generated ids [32099, 28, 32098, 5, 1]\n","generated text <extra_id_0>▁with<extra_id_1>.</s>\n","--------------\n","score: -6.727801322937012\n","generated ids [32099, 18, 32098, 5, 1]\n","generated text <extra_id_0>-<extra_id_1>.</s>\n","--------------\n","score: -6.839795112609863\n","generated ids [32099, 121, 32098, 5, 1]\n","generated text <extra_id_0>\"<extra_id_1>.</s>\n","--------------\n","score: -6.847148895263672\n","generated ids [32099, 137, 32098, 5, 1]\n","generated text <extra_id_0>).<extra_id_1>.</s>\n","--------------\n","score: -6.954499244689941\n","generated ids [32099, 3158, 32098, 5, 1]\n","generated text <extra_id_0>!!!<extra_id_1>.</s>\n","--------------\n","score: -6.966925621032715\n","generated ids [32099, 68, 32098, 5, 1]\n","generated text <extra_id_0>▁but<extra_id_1>.</s>\n","--------------\n","score: -6.990907669067383\n","generated ids [32099, 30, 32098, 5, 1]\n","generated text <extra_id_0>▁on<extra_id_1>.</s>\n","--------------\n","score: -6.994462966918945\n","generated ids [32099, 44, 32098, 5, 1]\n","generated text <extra_id_0>▁at<extra_id_1>.</s>\n","--------------\n","score: -7.006651878356934\n","generated ids [32099, 27, 32098, 5, 1]\n","generated text <extra_id_0>▁I<extra_id_1>.</s>\n","--------------\n","score: -7.012027740478516\n","generated ids [32099, 19, 32098, 5, 1]\n","generated text <extra_id_0>▁is<extra_id_1>.</s>\n","--------------\n","score: -7.024641990661621\n","generated ids [32099, 42, 32098, 5, 1]\n","generated text <extra_id_0>▁or<extra_id_1>.</s>\n","--------------\n","score: -7.026032447814941\n","generated ids [32099, 535, 32098, 5, 1]\n","generated text <extra_id_0>.\"<extra_id_1>.</s>\n","--------------\n","score: -7.044004440307617\n","generated ids [32099, 13, 32098, 5, 1]\n","generated text <extra_id_0>▁of<extra_id_1>.</s>\n","--------------\n","score: -7.055458068847656\n","generated ids [32099, 117, 32098, 5, 1]\n","generated text <extra_id_0>;<extra_id_1>.</s>\n","--------------\n","score: -7.119953155517578\n","generated ids [32099, 21, 32098, 55, 1]\n","generated text <extra_id_0>▁for<extra_id_1>!</s>\n","--------------\n","score: -7.177955627441406\n","generated ids [32099, 5, 32098, 535, 1]\n","generated text <extra_id_0>.<extra_id_1>.\"</s>\n","--------------\n","score: -7.21096134185791\n","generated ids [32099, 9374, 32098, 5, 1]\n","generated text <extra_id_0>.....<extra_id_1>.</s>\n","--------------\n","score: -7.217089653015137\n","generated ids [32099, 16, 32098, 55, 1]\n","generated text <extra_id_0>▁in<extra_id_1>!</s>\n","--------------\n","score: -7.21934700012207\n","generated ids [32099, 57, 32098, 5, 1]\n","generated text <extra_id_0>▁by<extra_id_1>.</s>\n","--------------\n","score: -7.240609169006348\n","generated ids [32099, 5, 32098, 2049, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁thanks.</s>\n","--------------\n","score: -7.241850852966309\n","generated ids [32099, 8665, 32098, 5, 1]\n","generated text <extra_id_0>???<extra_id_1>.</s>\n","--------------\n","score: -7.294920921325684\n","generated ids [32099, 61, 32098, 5, 1]\n","generated text <extra_id_0>)<extra_id_1>.</s>\n","--------------\n","score: -7.307512283325195\n","generated ids [32099, 18, 32098, 55, 1]\n","generated text <extra_id_0>-<extra_id_1>!</s>\n","--------------\n","score: -7.308137893676758\n","generated ids [32099, 8546, 32098, 5, 1]\n","generated text <extra_id_0>??<extra_id_1>.</s>\n","--------------\n","score: -7.402654647827148\n","generated ids [32099, 3274, 32098, 5, 1]\n","generated text <extra_id_0>▁=<extra_id_1>.</s>\n","--------------\n","score: -7.462602615356445\n","generated ids [32099, 78, 32098, 5, 1]\n","generated text <extra_id_0>▁so<extra_id_1>.</s>\n","--------------\n","score: -7.4687395095825195\n","generated ids [32099, 5, 32098, 9374, 1]\n","generated text <extra_id_0>.<extra_id_1>.....</s>\n","--------------\n","score: -7.497470855712891\n","generated ids [32099, 5, 32098, 233, 1]\n","generated text <extra_id_0>.<extra_id_1>...</s>\n","--------------\n","score: -7.516340255737305\n","generated ids [32099, 12, 32098, 55, 1]\n","generated text <extra_id_0>▁to<extra_id_1>!</s>\n","--------------\n","score: -7.533233642578125\n","generated ids [32099, 5, 32098, 1603, 1]\n","generated text <extra_id_0>.<extra_id_1>!!</s>\n","--------------\n","score: -7.550305366516113\n","generated ids [32099, 42, 32098, 58, 1]\n","generated text <extra_id_0>▁or<extra_id_1>?</s>\n","--------------\n","score: -7.5742387771606445\n","generated ids [32099, 45, 32098, 5, 1]\n","generated text <extra_id_0>▁from<extra_id_1>.</s>\n","--------------\n","score: -7.583676338195801\n","generated ids [32099, 10769, 32098, 55, 1]\n","generated text <extra_id_0>?!<extra_id_1>!</s>\n","--------------\n","score: -7.59775447845459\n","generated ids [32099, 2, 32098, 5, 1]\n","generated text <extra_id_0><unk><extra_id_1>.</s>\n","--------------\n","score: -7.602537155151367\n","generated ids [32099, 150, 32098, 5, 1]\n","generated text <extra_id_0>▁no<extra_id_1>.</s>\n","--------------\n","score: -7.602931022644043\n","generated ids [32099, 7, 32098, 5, 1]\n","generated text <extra_id_0>s<extra_id_1>.</s>\n","--------------\n","score: -7.613263130187988\n","generated ids [32099, 8665, 32098, 55, 1]\n","generated text <extra_id_0>???<extra_id_1>!</s>\n","--------------\n","score: -7.639212608337402\n","generated ids [32099, 465, 32098, 5, 1]\n","generated text <extra_id_0>▁No<extra_id_1>.</s>\n","--------------\n","score: -7.646109580993652\n","generated ids [32099, 38, 32098, 5, 1]\n","generated text <extra_id_0>▁as<extra_id_1>.</s>\n","--------------\n","score: -7.682086944580078\n","generated ids [32099, 30, 32098, 55, 1]\n","generated text <extra_id_0>▁on<extra_id_1>!</s>\n","--------------\n","score: -7.699624061584473\n","generated ids [32099, 5, 32098, 6, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>,.</s>\n","--------------\n","score: -7.71791934967041\n","generated ids [32099, 2824, 32098, 58, 1]\n","generated text <extra_id_0>....<extra_id_1>?</s>\n","--------------\n","score: -7.719516754150391\n","generated ids [32099, 8, 32098, 5, 1]\n","generated text <extra_id_0>▁the<extra_id_1>.</s>\n","--------------\n","score: -7.7269134521484375\n","generated ids [32099, 12887, 32098, 55, 1]\n","generated text <extra_id_0>!!!!<extra_id_1>!</s>\n","--------------\n","score: -7.728029251098633\n","generated ids [32099, 121, 32098, 55, 1]\n","generated text <extra_id_0>\"<extra_id_1>!</s>\n","--------------\n","score: -7.7470245361328125\n","generated ids [32099, 28, 32098, 55, 1]\n","generated text <extra_id_0>▁with<extra_id_1>!</s>\n","--------------\n","score: -7.76827335357666\n","generated ids [32099, 55, 32098, 1603, 1]\n","generated text <extra_id_0>!<extra_id_1>!!</s>\n","--------------\n","score: -7.786755561828613\n","generated ids [32099, 55, 32098, 2824, 1]\n","generated text <extra_id_0>!<extra_id_1>....</s>\n","--------------\n","score: -7.815352439880371\n","generated ids [32099, 8546, 32098, 55, 1]\n","generated text <extra_id_0>??<extra_id_1>!</s>\n","--------------\n","score: -7.819541931152344\n","generated ids [32099, 58, 32098, 2824, 1]\n","generated text <extra_id_0>?<extra_id_1>....</s>\n","--------------\n","score: -7.828607559204102\n","generated ids [32099, 5, 32098, 7, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>s.</s>\n","--------------\n","score: -7.831075668334961\n","generated ids [32099, 10, 32098, 58, 1]\n","generated text <extra_id_0>:<extra_id_1>?</s>\n","--------------\n","score: -7.883914947509766\n","generated ids [32099, 1603, 32098, 58, 1]\n","generated text <extra_id_0>!!<extra_id_1>?</s>\n","--------------\n","score: -7.912383079528809\n","generated ids [32099, 9374, 32098, 55, 1]\n","generated text <extra_id_0>.....<extra_id_1>!</s>\n","--------------\n","score: -7.912816047668457\n","generated ids [32099, 42, 32098, 55, 1]\n","generated text <extra_id_0>▁or<extra_id_1>!</s>\n","--------------\n","score: -7.919161796569824\n","generated ids [32099, 68, 32098, 55, 1]\n","generated text <extra_id_0>▁but<extra_id_1>!</s>\n","--------------\n","score: -7.923374176025391\n","generated ids [32099, 27, 32098, 55, 1]\n","generated text <extra_id_0>▁I<extra_id_1>!</s>\n","--------------\n","score: -7.931670188903809\n","generated ids [32099, 1636, 32098, 5, 1]\n","generated text <extra_id_0>▁--<extra_id_1>.</s>\n","--------------\n","score: -7.98898983001709\n","generated ids [32099, 1820, 32098, 5, 1]\n","generated text <extra_id_0>▁|<extra_id_1>.</s>\n","--------------\n","score: -8.002620697021484\n","generated ids [32099, 14125, 32098, 5, 1]\n","generated text <extra_id_0>......<extra_id_1>.</s>\n","--------------\n","score: -8.003936767578125\n","generated ids [32099, 13, 32098, 55, 1]\n","generated text <extra_id_0>▁of<extra_id_1>!</s>\n","--------------\n","score: -8.011685371398926\n","generated ids [32099, 114, 32098, 5, 1]\n","generated text <extra_id_0>▁like<extra_id_1>.</s>\n","--------------\n","score: -8.017827987670898\n","generated ids [32099, 31, 32098, 5, 1]\n","generated text <extra_id_0>'<extra_id_1>.</s>\n","--------------\n","score: -8.029488563537598\n","generated ids [32099, 5, 32098, 754, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>▁please.</s>\n","--------------\n","score: -8.066944122314453\n","generated ids [32099, 8665, 32098, 58, 1]\n","generated text <extra_id_0>???<extra_id_1>?</s>\n","--------------\n","score: -8.067510604858398\n","generated ids [32099, 117, 32098, 55, 1]\n","generated text <extra_id_0>;<extra_id_1>!</s>\n","--------------\n","score: -8.067588806152344\n","generated ids [32099, 10769, 32098, 5, 1]\n","generated text <extra_id_0>?!<extra_id_1>.</s>\n","--------------\n","score: -8.086091041564941\n","generated ids [32099, 2163, 32098, 5, 1]\n","generated text <extra_id_0>▁Yes<extra_id_1>.</s>\n","--------------\n","score: -8.117923736572266\n","generated ids [32099, 16497, 32098, 5, 1]\n","generated text <extra_id_0>▁lol<extra_id_1>.</s>\n","--------------\n","score: -8.124242782592773\n","generated ids [32099, 8546, 32098, 58, 1]\n","generated text <extra_id_0>??<extra_id_1>?</s>\n","--------------\n","score: -8.136626243591309\n","generated ids [32099, 1280, 32098, 5, 1]\n","generated text <extra_id_0>\".<extra_id_1>.</s>\n","--------------\n","score: -8.517455101013184\n","generated ids [32099, 5, 32098, 2049, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>▁thanks!</s>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls**sent_0*.*mask*.*sep+*\n","*cls**sent_0*.*mask*!*sep+*\n","*cls**sent_0*?*mask*.*sep+*\n","*cls**sent_0*!*mask*!*sep+*\n","*cls**sent_0*.*mask*?*sep+*\n","*cls**sent_0*!*mask*.*sep+*\n","*cls**sent_0*,*mask*.*sep+*\n","*cls**sent_0*?*mask*!*sep+*\n","*cls**sent_0*?*mask*?*sep+*\n","*cls**sent_0*...*mask*.*sep+*\n","*cls**sent_0*,*mask*!*sep+*\n","*cls**sent_0*,*mask*?*sep+*\n","*cls**sent_0*!*mask*?*sep+*\n","*cls**sent_0*...*mask*!*sep+*\n","*cls**sent_0*:*mask*.*sep+*\n","*cls**sent_0*▁and*mask*.*sep+*\n","*cls**sent_0*...*mask*?*sep+*\n","*cls**sent_0*....*mask*.*sep+*\n","*cls**sent_0*▁and*mask*!*sep+*\n","*cls**sent_0*!!*mask*!*sep+*\n","*cls**sent_0*▁in*mask*.*sep+*\n","*cls**sent_0*:*mask*!*sep+*\n","*cls**sent_0*▁for*mask*.*sep+*\n","*cls**sent_0*.*mask*....*sep+*\n","*cls**sent_0*!!!*mask*!*sep+*\n","*cls**sent_0*▁to*mask*.*sep+*\n","*cls**sent_0*!!*mask*.*sep+*\n","*cls**sent_0*....*mask*!*sep+*\n","*cls**sent_0*▁with*mask*.*sep+*\n","*cls**sent_0*-*mask*.*sep+*\n","*cls**sent_0*\"*mask*.*sep+*\n","*cls**sent_0*).*mask*.*sep+*\n","*cls**sent_0*!!!*mask*.*sep+*\n","*cls**sent_0*▁but*mask*.*sep+*\n","*cls**sent_0*▁on*mask*.*sep+*\n","*cls**sent_0*▁at*mask*.*sep+*\n","*cls**sent_0*▁I*mask*.*sep+*\n","*cls**sent_0*▁is*mask*.*sep+*\n","*cls**sent_0*▁or*mask*.*sep+*\n","*cls**sent_0*.\"*mask*.*sep+*\n","*cls**sent_0*▁of*mask*.*sep+*\n","*cls**sent_0*;*mask*.*sep+*\n","*cls**sent_0*▁for*mask*!*sep+*\n","*cls**sent_0*.*mask*.\"*sep+*\n","*cls**sent_0*.....*mask*.*sep+*\n","*cls**sent_0*▁in*mask*!*sep+*\n","*cls**sent_0*▁by*mask*.*sep+*\n","*cls**sent_0*.*mask*▁thanks.*sep+*\n","*cls**sent_0*???*mask*.*sep+*\n","*cls**sent_0*)*mask*.*sep+*\n","####### generated templates #######\n","\n","####### example #######\n",".<extra_id_0> Yes<extra_id_1> Armored and attack titan fight IS THE BEST CLIP THO\n",".<extra_id_0> Yes<extra_id_1> AAAAAAA eyy levi vs zeke alrdy happening djnjkfbje,rkfker im not ready\n",".<extra_id_0> Yes<extra_id_1> Mikasa ate armin's d-\n","####### example #######\n","\n","100% 18/18 [11:13<00:00, 37.41s/it]\n","####### generated results #######\n","--------------\n","score: -3.333495616912842\n","generated ids [32099, 5, 32098, 5, 32097]\n","generated text <extra_id_0>.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -3.985541343688965\n","generated ids [32099, 5, 32098, 55, 32097]\n","generated text <extra_id_0>.<extra_id_1>!<extra_id_2>\n","--------------\n","score: -4.322233200073242\n","generated ids [32099, 5, 32098, 6, 32097]\n","generated text <extra_id_0>.<extra_id_1>,<extra_id_2>\n","--------------\n","score: -4.805596351623535\n","generated ids [32099, 3359, 32098, 55, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>!<extra_id_2>\n","--------------\n","score: -4.944242477416992\n","generated ids [32099, 2163, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.224204063415527\n","generated ids [32099, 465, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁No.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.277992248535156\n","generated ids [32099, 3359, 32098, 5, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.486249923706055\n","generated ids [32099, 275, 32098, 5, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.545858383178711\n","generated ids [32099, 5, 32098, 58, 32097]\n","generated text <extra_id_0>.<extra_id_1>?<extra_id_2>\n","--------------\n","score: -5.876005172729492\n","generated ids [32099, 299, 32098, 5, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.8871870040893555\n","generated ids [32099, 2163, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.966524124145508\n","generated ids [32099, 275, 32098, 55, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>!<extra_id_2>\n","--------------\n","score: -5.972579002380371\n","generated ids [32099, 2163, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.0279741287231445\n","generated ids [32099, 2490, 32098, 5, 32097]\n","generated text <extra_id_0>▁><extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.135412216186523\n","generated ids [32099, 465, 32098, 5, 32097]\n","generated text <extra_id_0>▁No<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.157512187957764\n","generated ids [32099, 5, 32098, 233, 32097]\n","generated text <extra_id_0>.<extra_id_1>...<extra_id_2>\n","--------------\n","score: -6.168750286102295\n","generated ids [32099, 275, 32098, 6, 32097]\n","generated text <extra_id_0>▁And<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.274050712585449\n","generated ids [32099, 264, 32098, 5, 32097]\n","generated text <extra_id_0>▁So<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.412654876708984\n","generated ids [32099, 299, 32098, 55, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.4235334396362305\n","generated ids [32099, 1300, 32098, 5, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.464278697967529\n","generated ids [32099, 96, 32098, 5, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.465493202209473\n","generated ids [32099, 37, 32098, 5, 32097]\n","generated text <extra_id_0>▁The<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.4739580154418945\n","generated ids [32099, 71, 32098, 5, 32097]\n","generated text <extra_id_0>▁A<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.525609016418457\n","generated ids [32099, 1877, 32098, 5, 32097]\n","generated text <extra_id_0>▁3.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.548717975616455\n","generated ids [32099, 1682, 32098, 5, 32097]\n","generated text <extra_id_0>▁2.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.666958808898926\n","generated ids [32099, 1429, 32098, 5, 32097]\n","generated text <extra_id_0>▁*<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.704953670501709\n","generated ids [32099, 2853, 32098, 5, 32097]\n","generated text <extra_id_0>▁4.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.805159568786621\n","generated ids [32099, 11475, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yeah.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.835855484008789\n","generated ids [32099, 9710, 32098, 55, 32097]\n","generated text <extra_id_0>▁Ah<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.841800689697266\n","generated ids [32099, 9710, 32098, 5, 32097]\n","generated text <extra_id_0>▁Ah<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.843647003173828\n","generated ids [32099, 2163, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁Yes!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.848404884338379\n","generated ids [32099, 465, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁No,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.850714683532715\n","generated ids [32099, 1333, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thanks.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.86559534072876\n","generated ids [32099, 3594, 32098, 5, 32097]\n","generated text <extra_id_0>▁5.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.895514488220215\n","generated ids [32099, 2163, 6, 32098, 6, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.954029083251953\n","generated ids [32099, 61, 32098, 5, 32097]\n","generated text <extra_id_0>)<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.986319541931152\n","generated ids [32099, 3359, 32098, 6, 32097]\n","generated text <extra_id_0>▁Oh<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.021406173706055\n","generated ids [32099, 1142, 32098, 5, 32097]\n","generated text <extra_id_0>▁Just<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.0861663818359375\n","generated ids [32099, 20510, 32098, 5, 32097]\n","generated text <extra_id_0>▁Absolutely<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.091025352478027\n","generated ids [32099, 96, 32098, 55, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.107603073120117\n","generated ids [32099, 1697, 32098, 5, 32097]\n","generated text <extra_id_0>▁•<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.1145219802856445\n","generated ids [32099, 465, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁No!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.115487098693848\n","generated ids [32099, 11419, 32098, 55, 32097]\n","generated text <extra_id_0>▁Hell<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.13070011138916\n","generated ids [32099, 5, 32098, 1603, 32097]\n","generated text <extra_id_0>.<extra_id_1>!!<extra_id_2>\n","--------------\n","score: -7.1602783203125\n","generated ids [32099, 71, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁A.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.165858268737793\n","generated ids [32099, 287, 5, 32098, 5, 32097]\n","generated text <extra_id_0>com.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.169785976409912\n","generated ids [32099, 299, 32098, 6, 32097]\n","generated text <extra_id_0>▁But<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.187593460083008\n","generated ids [32099, 71, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁A:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.229315280914307\n","generated ids [32099, 2490, 32098, 6, 32097]\n","generated text <extra_id_0>▁><extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.233689308166504\n","generated ids [32099, 264, 32098, 55, 32097]\n","generated text <extra_id_0>▁So<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.235100746154785\n","generated ids [32099, 1562, 25, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thank▁you.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.238258361816406\n","generated ids [32099, 1626, 1024, 32098, 5, 32097]\n","generated text <extra_id_0>▁Haha<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.255300521850586\n","generated ids [32099, 1300, 32098, 5, 1682, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>.▁2.<extra_id_2>\n","--------------\n","score: -7.273305892944336\n","generated ids [32099, 3359, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Oh.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.2764201164245605\n","generated ids [32099, 96, 32098, 6, 32097]\n","generated text <extra_id_0>▁\"<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.290441989898682\n","generated ids [32099, 58, 32098, 5, 32097]\n","generated text <extra_id_0>?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.29279899597168\n","generated ids [32099, 2163, 11, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yes▁and<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.305180549621582\n","generated ids [32099, 2163, 32098, 2163, 32097]\n","generated text <extra_id_0>▁Yes<extra_id_1>▁Yes<extra_id_2>\n","--------------\n","score: -7.330589771270752\n","generated ids [32099, 14125, 32098, 5, 32097]\n","generated text <extra_id_0>......<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.359997749328613\n","generated ids [32099, 6, 32098, 5, 32097]\n","generated text <extra_id_0>,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.3646016120910645\n","generated ids [32099, 2, 32098, 5, 32097]\n","generated text <extra_id_0><unk><extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.369382381439209\n","generated ids [32099, 156, 32098, 6, 32097]\n","generated text <extra_id_0>▁If<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.380092620849609\n","generated ids [32099, 11, 32098, 5, 32097]\n","generated text <extra_id_0>▁and<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.388427734375\n","generated ids [32099, 27, 32098, 5, 32097]\n","generated text <extra_id_0>▁I<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.388826847076416\n","generated ids [32099, 465, 32098, 465, 32097]\n","generated text <extra_id_0>▁No<extra_id_1>▁No<extra_id_2>\n","--------------\n","score: -7.404290199279785\n","generated ids [32099, 11342, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Sorry.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.428982734680176\n","generated ids [32099, 1333, 55, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thanks!<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.429128646850586\n","generated ids [32099, 100, 19, 32098, 5, 32097]\n","generated text <extra_id_0>▁This▁is<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.431171417236328\n","generated ids [32099, 11475, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yeah<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.444300651550293\n","generated ids [32099, 2490, 32098, 55, 32097]\n","generated text <extra_id_0>▁><extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.4474687576293945\n","generated ids [32099, 363, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁What?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.45048713684082\n","generated ids [32099, 499, 32098, 5, 32097]\n","generated text <extra_id_0>▁My<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.460760593414307\n","generated ids [32099, 5835, 32098, 5, 32097]\n","generated text <extra_id_0>▁9.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.479018211364746\n","generated ids [32099, 5, 32098, 2824, 32097]\n","generated text <extra_id_0>.<extra_id_1>....<extra_id_2>\n","--------------\n","score: -7.4812822341918945\n","generated ids [32099, 1429, 32098, 6, 32097]\n","generated text <extra_id_0>▁*<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.507524013519287\n","generated ids [32099, 209, 32098, 5, 32097]\n","generated text <extra_id_0>▁1<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.507956504821777\n","generated ids [32099, 1682, 32098, 5, 1877, 32097]\n","generated text <extra_id_0>▁2.<extra_id_1>.▁3.<extra_id_2>\n","--------------\n","score: -7.520596027374268\n","generated ids [32099, 5, 58, 32098, 5, 32097]\n","generated text <extra_id_0>.?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.535678863525391\n","generated ids [32099, 2163, 6, 32098, 55, 32097]\n","generated text <extra_id_0>▁Yes,<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.537935733795166\n","generated ids [32099, 204, 32098, 5, 32097]\n","generated text <extra_id_0>▁2<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.553731918334961\n","generated ids [32099, 5, 32098, 11, 32097]\n","generated text <extra_id_0>.<extra_id_1>▁and<extra_id_2>\n","--------------\n","score: -7.554302215576172\n","generated ids [32099, 1300, 32098, 55, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.562067031860352\n","generated ids [32099, 363, 58, 32098, 55, 32097]\n","generated text <extra_id_0>▁What?<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.562403678894043\n","generated ids [32099, 4357, 32098, 5, 32097]\n","generated text <extra_id_0>▁6.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.564577102661133\n","generated ids [32099, 1626, 1024, 32098, 55, 32097]\n","generated text <extra_id_0>▁Haha<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.569156646728516\n","generated ids [32099, 1300, 32098, 6, 32097]\n","generated text <extra_id_0>▁1.<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.594182014465332\n","generated ids [32099, 1333, 55, 32098, 55, 32097]\n","generated text <extra_id_0>▁Thanks!<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.632523536682129\n","generated ids [32099, 27, 317, 32098, 5, 32097]\n","generated text <extra_id_0>▁I▁think<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.644831657409668\n","generated ids [32099, 4306, 32098, 5, 32097]\n","generated text <extra_id_0>▁7.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.649486541748047\n","generated ids [32099, 9758, 5, 32098, 5, 32097]\n","generated text <extra_id_0>▁Wow.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.669598579406738\n","generated ids [32099, 11291, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁Really?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.677187919616699\n","generated ids [32099, 419, 10, 32098, 5, 32097]\n","generated text <extra_id_0>▁Re:<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.720054626464844\n","generated ids [32099, 3359, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Oh,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.732203483581543\n","generated ids [32099, 11475, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Yeah,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.739287376403809\n","generated ids [32099, 9710, 6, 32098, 5, 32097]\n","generated text <extra_id_0>▁Ah,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.758618354797363\n","generated ids [32099, 1615, 58, 32098, 5, 32097]\n","generated text <extra_id_0>▁Why?<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.764182090759277\n","generated ids [32099, 3359, 6, 32098, 55, 32097]\n","generated text <extra_id_0>▁Oh,<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.8054704666137695\n","generated ids [32099, 27, 7, 34, 32098, 58, 32097]\n","generated text <extra_id_0>▁Is▁it<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.837625503540039\n","generated ids [32099, 27, 43, 32098, 5, 32097]\n","generated text <extra_id_0>▁I▁have<extra_id_1>.<extra_id_2>\n","--------------\n","score: -8.294025421142578\n","generated ids [32099, 1562, 25, 55, 32098, 5, 32097]\n","generated text <extra_id_0>▁Thank▁you!<extra_id_1>.<extra_id_2>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls*.*mask*.*+sent_0**sep+*\n","*cls*.*mask*!*+sent_0**sep+*\n","*cls*.*mask*,*+sent_0**sep+*\n","*cls*▁Oh*mask*!*+sent_0**sep+*\n","*cls*▁Yes.*mask*.*+sent_0**sep+*\n","*cls*▁No.*mask*.*+sent_0**sep+*\n","*cls*▁Oh*mask*.*+sent_0**sep+*\n","*cls*▁And*mask*.*+sent_0**sep+*\n","*cls*.*mask*?*+sent_0**sep+*\n","*cls*▁But*mask*.*+sent_0**sep+*\n","*cls*▁Yes*mask*.*+sent_0**sep+*\n","*cls*▁And*mask*!*+sent_0**sep+*\n","*cls*▁Yes,*mask*.*+sent_0**sep+*\n","*cls*▁>*mask*.*+sent_0**sep+*\n","*cls*▁No*mask*.*+sent_0**sep+*\n","*cls*.*mask*...*+sent_0**sep+*\n","*cls*▁And*mask*,*+sent_0**sep+*\n","*cls*▁So*mask*.*+sent_0**sep+*\n","*cls*▁But*mask*!*+sent_0**sep+*\n","*cls*▁1.*mask*.*+sent_0**sep+*\n","*cls*▁\"*mask*.*+sent_0**sep+*\n","*cls*▁The*mask*.*+sent_0**sep+*\n","*cls*▁A*mask*.*+sent_0**sep+*\n","*cls*▁3.*mask*.*+sent_0**sep+*\n","*cls*▁2.*mask*.*+sent_0**sep+*\n","*cls*▁**mask*.*+sent_0**sep+*\n","*cls*▁4.*mask*.*+sent_0**sep+*\n","*cls*▁Yeah.*mask*.*+sent_0**sep+*\n","*cls*▁Ah*mask*!*+sent_0**sep+*\n","*cls*▁Ah*mask*.*+sent_0**sep+*\n","*cls*▁Yes!*mask*!*+sent_0**sep+*\n","*cls*▁No,*mask*.*+sent_0**sep+*\n","*cls*▁Thanks.*mask*.*+sent_0**sep+*\n","*cls*▁5.*mask*.*+sent_0**sep+*\n","*cls*▁Yes,*mask*,*+sent_0**sep+*\n","*cls*)*mask*.*+sent_0**sep+*\n","*cls*▁Oh*mask*,*+sent_0**sep+*\n","*cls*▁Just*mask*.*+sent_0**sep+*\n","*cls*▁Absolutely*mask*.*+sent_0**sep+*\n","*cls*▁\"*mask*!*+sent_0**sep+*\n","*cls*▁•*mask*.*+sent_0**sep+*\n","*cls*▁No!*mask*!*+sent_0**sep+*\n","*cls*▁Hell*mask*!*+sent_0**sep+*\n","*cls*.*mask*!!*+sent_0**sep+*\n","*cls*▁A.*mask*.*+sent_0**sep+*\n","*cls*com.*mask*.*+sent_0**sep+*\n","*cls*▁But*mask*,*+sent_0**sep+*\n","*cls*▁A:*mask*.*+sent_0**sep+*\n","*cls*▁>*mask*,*+sent_0**sep+*\n","*cls*▁So*mask*!*+sent_0**sep+*\n","####### generated templates #######\n","\n"]}],"source":["!source env/bin/activate; python tools/generate_template.py \\\n","    --output_dir spoilers_auto_template \\\n","    --task_name spoilers \\\n","    --seed 13 21 42 87 100 \\\n","    --t5_model t5-3b \\\n","    --beam 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEg8Nol65TAU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8ea0960f-178a-47f9-f146-ce93e8c27255","executionInfo":{"status":"ok","timestamp":1646639557716,"user_tz":480,"elapsed":307865,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","43it [02:37,  1.99s/it]\u001b[A\n","44it [02:37,  1.46s/it]\u001b[A\n","45it [02:37,  1.09s/it]\u001b[A\n","46it [02:37,  1.20it/s]\u001b[A\n","47it [02:38,  1.53it/s]\u001b[A\n","Epoch:  50% 124/250 [04:00<03:28,  1.65s/it]\n","49it [03:19, 12.83s/it]\u001b[A\n","50it [03:20,  9.05s/it]\u001b[A\n","51it [03:20,  6.40s/it]\u001b[A\n","52it [03:20,  4.55s/it]\u001b[A\n","53it [03:20,  3.26s/it]\u001b[A\n","54it [03:21,  2.35s/it]\u001b[A\n","55it [03:21,  1.71s/it]\u001b[A\n","56it [03:21,  1.27s/it]\u001b[A\n","57it [03:21,  1.05it/s]\u001b[A\n","58it [03:21,  1.35it/s]\u001b[A\n","59it [03:22,  1.70it/s]\u001b[A\n","60it [03:22,  2.23it/s]\u001b[A03/07/2022 04:08:09 - INFO - src.trainer -   Best dev result: 0.9296250343322754\n","Epoch:  60% 149/250 [04:51<02:49,  1.68s/it]\n","61it [04:11, 15.07s/it]\u001b[A\n","62it [04:11, 10.62s/it]\u001b[A\n","63it [04:11,  7.50s/it]\u001b[A\n","64it [04:12,  5.32s/it]\u001b[A\n","65it [04:12,  3.79s/it]\u001b[A\n","66it [04:12,  2.73s/it]\u001b[A\n","67it [04:12,  1.98s/it]\u001b[A\n","68it [04:13,  1.45s/it]\u001b[A\n","69it [04:13,  1.09s/it]\u001b[A\n","70it [04:13,  1.20it/s]\u001b[A\n","71it [04:13,  1.54it/s]\u001b[A\n","Epoch:  70% 174/250 [05:35<02:05,  1.66s/it]\n","73it [04:55, 12.82s/it]\u001b[A\n","74it [04:55,  9.05s/it]\u001b[A\n","75it [04:55,  6.40s/it]\u001b[A\n","76it [04:56,  4.55s/it]\u001b[A\n","77it [04:56,  3.25s/it]\u001b[A\n","78it [04:56,  2.35s/it]\u001b[A\n","79it [04:56,  1.71s/it]\u001b[A\n","80it [04:57,  1.27s/it]\u001b[A\n","81it [04:57,  1.05it/s]\u001b[A\n","82it [04:57,  1.35it/s]\u001b[A\n","83it [04:57,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:19<01:24,  1.66s/it]\n","85it [05:39, 12.81s/it]\u001b[A\n","86it [05:39,  9.04s/it]\u001b[A\n","87it [05:40,  6.39s/it]\u001b[A\n","88it [05:40,  4.55s/it]\u001b[A\n","89it [05:40,  3.25s/it]\u001b[A\n","90it [05:40,  2.35s/it]\u001b[A\n","91it [05:40,  1.71s/it]\u001b[A\n","92it [05:41,  1.27s/it]\u001b[A\n","93it [05:41,  1.05it/s]\u001b[A\n","94it [05:41,  1.35it/s]\u001b[A\n","95it [05:41,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:03<00:42,  1.65s/it]\n","97it [06:23, 12.81s/it]\u001b[A\n","98it [06:23,  9.04s/it]\u001b[A\n","99it [06:24,  6.40s/it]\u001b[A\n","100it [06:24,  4.55s/it]\u001b[A\n","101it [06:24,  3.25s/it]\u001b[A\n","102it [06:24,  2.35s/it]\u001b[A\n","103it [06:25,  1.71s/it]\u001b[A\n","104it [06:25,  1.27s/it]\u001b[A\n","105it [06:25,  1.05it/s]\u001b[A\n","106it [06:25,  1.35it/s]\u001b[A\n","107it [06:26,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:48<00:01,  1.65s/it]\n","109it [07:07, 12.80s/it]\u001b[A\n","110it [07:07,  9.03s/it]\u001b[A\n","111it [07:08,  6.39s/it]\u001b[A\n","112it [07:08,  4.54s/it]\u001b[A\n","113it [07:08,  3.25s/it]\u001b[A\n","114it [07:08,  2.34s/it]\u001b[A\n","115it [07:09,  1.71s/it]\u001b[A\n","116it [07:09,  1.27s/it]\u001b[A\n","117it [07:09,  1.05it/s]\u001b[A\n","118it [07:09,  1.36it/s]\u001b[A\n","119it [07:10,  1.71it/s]\u001b[A\n","Epoch: 100% 250/250 [07:52<00:00,  1.89s/it]\n","03/07/2022 04:11:57 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 04:12:11 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:24,  4.60s/it]\u001b[A\n","122it [07:24,  3.29s/it]\u001b[A\n","123it [07:24,  2.37s/it]\u001b[A\n","124it [07:25,  1.73s/it]\u001b[A\n","125it [07:25,  1.28s/it]\u001b[A\n","126it [07:25,  1.04it/s]\u001b[A\n","127it [07:25,  1.34it/s]\u001b[A\n","128it [07:26,  1.69it/s]\u001b[A\n","129it [07:26,  2.07it/s]\u001b[A\n","130it [07:26,  2.45it/s]\u001b[A\n","131it [07:26,  2.82it/s]\u001b[A\n","132it [07:26,  3.50it/s]\u001b[A03/07/2022 04:12:13 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 04:12:13 - INFO - __main__ -     eval_loss = 3.047750949859619\n","03/07/2022 04:12:13 - INFO - __main__ -     eval_auroc = 0.9296250343322754\n","03/07/2022 04:12:13 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/07/2022 04:12:13 - INFO - __main__ -     eval_f1 = 0.4285714328289032\n","03/07/2022 04:12:13 - INFO - filelock -   Lock 140637662466320 acquired on log.lock\n","03/07/2022 04:12:13 - INFO - filelock -   Lock 140637662466320 released on log.lock\n","132it [07:26,  3.39s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 04:12:19 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 04:12:19 - INFO - __main__ -   Specify load the 74-th template: *cls*▁Oh*mask*,*+sent_0**sep+*\n","03/07/2022 04:12:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 04:12:19 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-31709', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_04-12-19_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-31709', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 04:12:19 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 04:12:23 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:12:23 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:12:23 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 04:12:23 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:12:23 - INFO - filelock -   Lock 140539317118672 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:12:23 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 04:12:23 - INFO - filelock -   Lock 140539317118672 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:12:23 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:12:23 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:12:23 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 04:12:23 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:12:23 - INFO - filelock -   Lock 140539315680336 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:12:23 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 04:12:23 - INFO - filelock -   Lock 140539315680336 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:12:23 - INFO - src.dataset -   *** Example ***\n","03/07/2022 04:12:23 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 04:12:23 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 7516, 50264, 6, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/07/2022 04:12:23 - INFO - src.dataset -   text: <s>▁Oh<mask>, Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 04:12:38 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 04:12:38 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 04:12:38 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 04:12:38 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 04:12:38 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 04:12:38 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 04:12:38 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/07/2022 04:13:22 - INFO - src.trainer -   Best dev result: 0.8514999747276306\n","Epoch:  20% 49/250 [01:32<05:40,  1.69s/it]\n","13it [00:51, 14.99s/it]            \u001b[A\n","14it [00:52, 10.56s/it]\u001b[A\n","15it [00:52,  7.46s/it]\u001b[A\n","16it [00:52,  5.29s/it]\u001b[A\n","17it [00:52,  3.77s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/07/2022 04:14:14 - INFO - src.trainer -   Best dev result: 0.8761250376701355\n","Epoch:  30% 74/250 [02:23<04:56,  1.69s/it]\n","25it [01:43, 15.01s/it]\u001b[A\n","26it [01:43, 10.58s/it]\u001b[A\n","27it [01:43,  7.47s/it]\u001b[A\n","28it [01:43,  5.30s/it]\u001b[A\n","29it [01:44,  3.78s/it]\u001b[A\n","30it [01:44,  2.72s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:44,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","36it [01:45,  2.03it/s]\u001b[A03/07/2022 04:15:06 - INFO - src.trainer -   Best dev result: 0.8948749303817749\n","Epoch:  40% 99/250 [03:15<04:14,  1.69s/it]\n","37it [02:35, 15.24s/it]\u001b[A\n","38it [02:35, 10.74s/it]\u001b[A\n","39it [02:35,  7.59s/it]\u001b[A\n","40it [02:36,  5.38s/it]\u001b[A\n","41it [02:36,  3.84s/it]\u001b[A\n","42it [02:36,  2.75s/it]\u001b[A\n","43it [02:36,  2.00s/it]\u001b[A\n","44it [02:36,  1.47s/it]\u001b[A\n","45it [02:37,  1.10s/it]\u001b[A\n","46it [02:37,  1.19it/s]\u001b[A\n","47it [02:37,  1.53it/s]\u001b[A\n","Epoch:  50% 124/250 [03:59<03:28,  1.66s/it]\n","49it [03:19, 12.87s/it]\u001b[A\n","50it [03:19,  9.08s/it]\u001b[A\n","51it [03:20,  6.43s/it]\u001b[A\n","52it [03:20,  4.57s/it]\u001b[A\n","53it [03:20,  3.27s/it]\u001b[A\n","54it [03:20,  2.36s/it]\u001b[A\n","55it [03:20,  1.72s/it]\u001b[A\n","56it [03:21,  1.27s/it]\u001b[A\n","57it [03:21,  1.04it/s]\u001b[A\n","58it [03:21,  1.35it/s]\u001b[A\n","59it [03:21,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:43<02:47,  1.66s/it]\n","61it [04:03, 12.81s/it]\u001b[A\n","62it [04:03,  9.03s/it]\u001b[A\n","63it [04:04,  6.39s/it]\u001b[A\n","64it [04:04,  4.54s/it]\u001b[A\n","65it [04:04,  3.25s/it]\u001b[A\n","66it [04:04,  2.35s/it]\u001b[A\n","67it [04:05,  1.71s/it]\u001b[A\n","68it [04:05,  1.27s/it]\u001b[A\n","69it [04:05,  1.05it/s]\u001b[A\n","70it [04:05,  1.35it/s]\u001b[A\n","71it [04:05,  1.71it/s]\u001b[A\n","Epoch:  70% 174/250 [05:27<02:05,  1.65s/it]\n","73it [04:47, 12.81s/it]\u001b[A\n","74it [04:47,  9.04s/it]\u001b[A\n","75it [04:48,  6.40s/it]\u001b[A\n","76it [04:48,  4.55s/it]\u001b[A\n","77it [04:48,  3.25s/it]\u001b[A\n","78it [04:48,  2.35s/it]\u001b[A\n","79it [04:49,  1.71s/it]\u001b[A\n","80it [04:49,  1.27s/it]\u001b[A\n","81it [04:49,  1.05it/s]\u001b[A\n","82it [04:49,  1.35it/s]\u001b[A\n","83it [04:50,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:11<01:24,  1.66s/it]\n","85it [05:31, 12.81s/it]\u001b[A\n","86it [05:32,  9.04s/it]\u001b[A\n","87it [05:32,  6.40s/it]\u001b[A\n","88it [05:32,  4.55s/it]\u001b[A\n","89it [05:32,  3.25s/it]\u001b[A\n","90it [05:33,  2.35s/it]\u001b[A\n","91it [05:33,  1.71s/it]\u001b[A\n","92it [05:33,  1.27s/it]\u001b[A\n","93it [05:33,  1.05it/s]\u001b[A\n","94it [05:33,  1.35it/s]\u001b[A\n","95it [05:34,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:56<00:43,  1.66s/it]\n","97it [06:15, 12.82s/it]\u001b[A\n","98it [06:16,  9.04s/it]\u001b[A\n","99it [06:16,  6.40s/it]\u001b[A\n","100it [06:16,  4.55s/it]\u001b[A\n","101it [06:16,  3.25s/it]\u001b[A\n","102it [06:17,  2.35s/it]\u001b[A\n","103it [06:17,  1.71s/it]\u001b[A\n","104it [06:17,  1.27s/it]\u001b[A\n","105it [06:17,  1.05it/s]\u001b[A\n","106it [06:18,  1.35it/s]\u001b[A\n","107it [06:18,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:40<00:01,  1.66s/it]\n","109it [07:00, 12.82s/it]\u001b[A\n","110it [07:00,  9.04s/it]\u001b[A\n","111it [07:00,  6.40s/it]\u001b[A\n","112it [07:00,  4.55s/it]\u001b[A\n","113it [07:01,  3.25s/it]\u001b[A\n","114it [07:01,  2.35s/it]\u001b[A\n","115it [07:01,  1.71s/it]\u001b[A\n","116it [07:01,  1.27s/it]\u001b[A\n","117it [07:01,  1.05it/s]\u001b[A\n","118it [07:02,  1.35it/s]\u001b[A\n","119it [07:02,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:44<00:00,  1.86s/it]\n","03/07/2022 04:20:22 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 04:20:37 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:16,  4.64s/it]\u001b[A\n","122it [07:17,  3.31s/it]\u001b[A\n","123it [07:17,  2.39s/it]\u001b[A\n","124it [07:17,  1.74s/it]\u001b[A\n","125it [07:17,  1.29s/it]\u001b[A\n","126it [07:18,  1.03it/s]\u001b[A\n","127it [07:18,  1.33it/s]\u001b[A\n","128it [07:18,  1.68it/s]\u001b[A\n","129it [07:18,  2.06it/s]\u001b[A\n","130it [07:19,  2.44it/s]\u001b[A\n","131it [07:19,  2.81it/s]\u001b[A\n","132it [07:19,  3.49it/s]\u001b[A03/07/2022 04:20:39 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 04:20:39 - INFO - __main__ -     eval_loss = 2.520556688308716\n","03/07/2022 04:20:39 - INFO - __main__ -     eval_auroc = 0.8948749303817749\n","03/07/2022 04:20:39 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/07/2022 04:20:39 - INFO - __main__ -     eval_f1 = 0.4680851101875305\n","03/07/2022 04:20:39 - INFO - filelock -   Lock 140539662137296 acquired on log.lock\n","03/07/2022 04:20:39 - INFO - filelock -   Lock 140539662137296 released on log.lock\n","132it [07:19,  3.33s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 04:20:45 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 04:20:45 - INFO - __main__ -   Specify load the 75-th template: *cls*▁2.*mask*.*+sent_0**sep+*\n","03/07/2022 04:20:45 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 04:20:45 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-22832', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_04-20-45_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-22832', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 04:20:45 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 04:20:48 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:20:48 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:20:48 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 04:20:48 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:20:48 - INFO - filelock -   Lock 140256055568400 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:20:48 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 04:20:48 - INFO - filelock -   Lock 140256055568400 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:20:48 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:20:48 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:20:48 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 04:20:48 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:20:48 - INFO - filelock -   Lock 140255752457232 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:20:48 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 04:20:48 - INFO - filelock -   Lock 140255752457232 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:20:48 - INFO - src.dataset -   *** Example ***\n","03/07/2022 04:20:48 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 04:20:48 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 176, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/07/2022 04:20:48 - INFO - src.dataset -   text: <s>▁2.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 04:21:04 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 04:21:04 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 04:21:04 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 04:21:04 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 04:21:04 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 04:21:04 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 04:21:04 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 04:21:48 - INFO - src.trainer -   Best dev result: 0.8541250228881836\n","Epoch:  20% 49/250 [01:32<05:39,  1.69s/it]\n","13it [00:51, 15.00s/it]            \u001b[A\n","14it [00:52, 10.57s/it]\u001b[A\n","15it [00:52,  7.47s/it]\u001b[A\n","16it [00:52,  5.30s/it]\u001b[A\n","17it [00:52,  3.78s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/07/2022 04:22:40 - INFO - src.trainer -   Best dev result: 0.8641249537467957\n","Epoch:  30% 74/250 [02:23<04:57,  1.69s/it]\n","25it [01:43, 15.02s/it]\u001b[A\n","26it [01:43, 10.58s/it]\u001b[A\n","27it [01:43,  7.48s/it]\u001b[A\n","28it [01:44,  5.30s/it]\u001b[A\n","29it [01:44,  3.78s/it]\u001b[A\n","30it [01:44,  2.72s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:44,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","36it [01:45,  2.03it/s]\u001b[A03/07/2022 04:23:32 - INFO - src.trainer -   Best dev result: 0.9008749723434448\n","Epoch:  40% 99/250 [03:15<04:14,  1.69s/it]\n","37it [02:35, 15.26s/it]\u001b[A\n","38it [02:35, 10.75s/it]\u001b[A\n","39it [02:35,  7.60s/it]\u001b[A\n","40it [02:36,  5.39s/it]\u001b[A\n","41it [02:36,  3.84s/it]\u001b[A\n","42it [02:36,  2.76s/it]\u001b[A\n","43it [02:36,  2.00s/it]\u001b[A\n","44it [02:37,  1.47s/it]\u001b[A\n","45it [02:37,  1.10s/it]\u001b[A\n","46it [02:37,  1.19it/s]\u001b[A\n","47it [02:37,  1.52it/s]\u001b[A\n","48it [02:37,  2.01it/s]\u001b[A03/07/2022 04:24:24 - INFO - src.trainer -   Best dev result: 0.9033750295639038\n","Epoch:  50% 124/250 [04:07<03:32,  1.69s/it]\n","49it [03:27, 15.20s/it]\u001b[A\n","50it [03:27, 10.71s/it]\u001b[A\n","51it [03:27,  7.57s/it]\u001b[A\n","52it [03:28,  5.37s/it]\u001b[A\n","53it [03:28,  3.83s/it]\u001b[A\n","54it [03:28,  2.75s/it]\u001b[A\n","55it [03:28,  1.99s/it]\u001b[A\n","56it [03:29,  1.46s/it]\u001b[A\n","57it [03:29,  1.09s/it]\u001b[A\n","58it [03:29,  1.20it/s]\u001b[A\n","59it [03:29,  1.53it/s]\u001b[A\n","Epoch:  60% 149/250 [04:51<02:47,  1.66s/it]\n","61it [04:11, 12.84s/it]\u001b[A\n","62it [04:11,  9.06s/it]\u001b[A\n","63it [04:12,  6.41s/it]\u001b[A\n","64it [04:12,  4.56s/it]\u001b[A\n","65it [04:12,  3.26s/it]\u001b[A\n","66it [04:12,  2.35s/it]\u001b[A\n","67it [04:12,  1.72s/it]\u001b[A\n","68it [04:13,  1.27s/it]\u001b[A\n","69it [04:13,  1.04it/s]\u001b[A\n","70it [04:13,  1.35it/s]\u001b[A\n","71it [04:13,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:35<02:05,  1.65s/it]\n","73it [04:55, 12.80s/it]\u001b[A\n","74it [04:55,  9.03s/it]\u001b[A\n","75it [04:56,  6.39s/it]\u001b[A\n","76it [04:56,  4.54s/it]\u001b[A\n","77it [04:56,  3.25s/it]\u001b[A\n","78it [04:56,  2.34s/it]\u001b[A\n","79it [04:57,  1.71s/it]\u001b[A\n","80it [04:57,  1.27s/it]\u001b[A\n","81it [04:57,  1.05it/s]\u001b[A\n","82it [04:57,  1.35it/s]\u001b[A\n","83it [04:57,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:19<01:24,  1.65s/it]\n","85it [05:39, 12.80s/it]\u001b[A\n","86it [05:39,  9.03s/it]\u001b[A\n","87it [05:40,  6.39s/it]\u001b[A\n","88it [05:40,  4.54s/it]\u001b[A\n","89it [05:40,  3.25s/it]\u001b[A\n","90it [05:40,  2.34s/it]\u001b[A\n","91it [05:41,  1.71s/it]\u001b[A\n","92it [05:41,  1.27s/it]\u001b[A\n","93it [05:41,  1.05it/s]\u001b[A\n","94it [05:41,  1.35it/s]\u001b[A\n","95it [05:41,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:03<00:43,  1.66s/it]\n","97it [06:23, 12.80s/it]\u001b[A\n","98it [06:23,  9.03s/it]\u001b[A\n","99it [06:24,  6.39s/it]\u001b[A\n","100it [06:24,  4.54s/it]\u001b[A\n","101it [06:24,  3.25s/it]\u001b[A\n","102it [06:24,  2.34s/it]\u001b[A\n","103it [06:25,  1.71s/it]\u001b[A\n","104it [06:25,  1.27s/it]\u001b[A\n","105it [06:25,  1.05it/s]\u001b[A\n","106it [06:25,  1.35it/s]\u001b[A\n","107it [06:26,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:48<00:01,  1.66s/it]\n","109it [07:07, 12.85s/it]\u001b[A\n","110it [07:08,  9.06s/it]\u001b[A\n","111it [07:08,  6.41s/it]\u001b[A\n","112it [07:08,  4.56s/it]\u001b[A\n","113it [07:08,  3.26s/it]\u001b[A\n","114it [07:09,  2.35s/it]\u001b[A\n","115it [07:09,  1.72s/it]\u001b[A\n","116it [07:09,  1.27s/it]\u001b[A\n","117it [07:09,  1.04it/s]\u001b[A\n","118it [07:10,  1.35it/s]\u001b[A\n","119it [07:10,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:52<00:00,  1.89s/it]\n","03/07/2022 04:28:56 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 04:29:10 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:24,  4.61s/it]\u001b[A\n","122it [07:24,  3.30s/it]\u001b[A\n","123it [07:25,  2.38s/it]\u001b[A\n","124it [07:25,  1.73s/it]\u001b[A\n","125it [07:25,  1.28s/it]\u001b[A\n","126it [07:25,  1.03it/s]\u001b[A\n","127it [07:26,  1.34it/s]\u001b[A\n","128it [07:26,  1.69it/s]\u001b[A\n","129it [07:26,  2.07it/s]\u001b[A\n","130it [07:26,  2.45it/s]\u001b[A\n","131it [07:27,  2.82it/s]\u001b[A\n","132it [07:27,  3.49it/s]\u001b[A03/07/2022 04:29:13 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 04:29:13 - INFO - __main__ -     eval_loss = 2.2727692127227783\n","03/07/2022 04:29:13 - INFO - __main__ -     eval_auroc = 0.9033750295639038\n","03/07/2022 04:29:13 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/07/2022 04:29:13 - INFO - __main__ -     eval_f1 = 0.47311830520629883\n","03/07/2022 04:29:13 - INFO - filelock -   Lock 140255720488400 acquired on log.lock\n","03/07/2022 04:29:13 - INFO - filelock -   Lock 140255720488400 released on log.lock\n","132it [07:27,  3.39s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 04:29:19 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 04:29:19 - INFO - __main__ -   Specify load the 76-th template: *cls*▁Yes,*mask*,*+sent_0**sep+*\n","03/07/2022 04:29:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 04:29:19 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-22633', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_04-29-19_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-22633', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 04:29:19 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 04:29:22 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:29:22 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:29:22 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 04:29:22 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:29:22 - INFO - filelock -   Lock 139853804774928 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:29:22 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 04:29:22 - INFO - filelock -   Lock 139853804774928 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:29:22 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:29:22 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:29:22 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 04:29:22 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:29:22 - INFO - filelock -   Lock 139853803338384 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:29:22 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 04:29:22 - INFO - filelock -   Lock 139853803338384 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:29:22 - INFO - src.dataset -   *** Example ***\n","03/07/2022 04:29:22 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 04:29:22 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 6, 50264, 6, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/07/2022 04:29:22 - INFO - src.dataset -   text: <s>▁Yes,<mask>, Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 04:29:37 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 04:29:37 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 04:29:37 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 04:29:37 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 04:29:37 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 04:29:37 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 04:29:37 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.93it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 04:30:22 - INFO - src.trainer -   Best dev result: 0.9227499961853027\n","Epoch:  20% 49/250 [01:32<05:38,  1.68s/it]\n","13it [00:52, 15.23s/it]            \u001b[A\n","14it [00:52, 10.73s/it]\u001b[A\n","15it [00:53,  7.58s/it]\u001b[A\n","16it [00:53,  5.38s/it]\u001b[A\n","17it [00:53,  3.83s/it]\u001b[A\n","18it [00:53,  2.75s/it]\u001b[A\n","19it [00:54,  2.00s/it]\u001b[A\n","20it [00:54,  1.47s/it]\u001b[A\n","21it [00:54,  1.10s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:55,  1.53it/s]\u001b[A\n","24it [00:55,  2.01it/s]\u001b[A03/07/2022 04:31:15 - INFO - src.trainer -   Best dev result: 0.9283750057220459\n","Epoch:  30% 74/250 [02:24<04:55,  1.68s/it]\n","25it [01:44, 15.03s/it]\u001b[A\n","26it [01:44, 10.59s/it]\u001b[A\n","27it [01:44,  7.48s/it]\u001b[A\n","28it [01:44,  5.31s/it]\u001b[A\n","29it [01:45,  3.78s/it]\u001b[A\n","30it [01:45,  2.72s/it]\u001b[A\n","31it [01:45,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:46,  1.21it/s]\u001b[A\n","35it [01:46,  1.54it/s]\u001b[A\n","36it [01:46,  2.03it/s]\u001b[A03/07/2022 04:32:06 - INFO - src.trainer -   Best dev result: 0.9290000200271606\n","Epoch:  40% 99/250 [03:16<04:15,  1.70s/it]\n","37it [02:35, 15.17s/it]\u001b[A\n","38it [02:36, 10.69s/it]\u001b[A\n","39it [02:36,  7.55s/it]\u001b[A\n","40it [02:36,  5.35s/it]\u001b[A\n","41it [02:36,  3.82s/it]\u001b[A\n","42it [02:37,  2.74s/it]\u001b[A\n","43it [02:37,  1.99s/it]\u001b[A\n","44it [02:37,  1.46s/it]\u001b[A\n","45it [02:37,  1.09s/it]\u001b[A\n","46it [02:38,  1.20it/s]\u001b[A\n","47it [02:38,  1.53it/s]\u001b[A\n","Epoch:  50% 124/250 [04:00<03:28,  1.66s/it]\n","49it [03:20, 12.84s/it]\u001b[A\n","50it [03:20,  9.06s/it]\u001b[A\n","51it [03:20,  6.41s/it]\u001b[A\n","52it [03:20,  4.56s/it]\u001b[A\n","53it [03:20,  3.26s/it]\u001b[A\n","54it [03:21,  2.35s/it]\u001b[A\n","55it [03:21,  1.71s/it]\u001b[A\n","56it [03:21,  1.27s/it]\u001b[A\n","57it [03:21,  1.04it/s]\u001b[A\n","58it [03:22,  1.35it/s]\u001b[A\n","59it [03:22,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:44<02:47,  1.66s/it]\n","61it [04:04, 12.81s/it]\u001b[A\n","62it [04:04,  9.03s/it]\u001b[A\n","63it [04:04,  6.39s/it]\u001b[A\n","64it [04:04,  4.54s/it]\u001b[A\n","65it [04:05,  3.25s/it]\u001b[A\n","66it [04:05,  2.34s/it]\u001b[A\n","67it [04:05,  1.71s/it]\u001b[A\n","68it [04:05,  1.27s/it]\u001b[A\n","69it [04:05,  1.05it/s]\u001b[A\n","70it [04:06,  1.35it/s]\u001b[A\n","71it [04:06,  1.71it/s]\u001b[A\n","Epoch:  70% 174/250 [05:28<02:05,  1.66s/it]\n","73it [04:48, 12.80s/it]\u001b[A\n","74it [04:48,  9.03s/it]\u001b[A\n","75it [04:48,  6.39s/it]\u001b[A\n","76it [04:48,  4.54s/it]\u001b[A\n","77it [04:49,  3.25s/it]\u001b[A\n","78it [04:49,  2.34s/it]\u001b[A\n","79it [04:49,  1.71s/it]\u001b[A\n","80it [04:49,  1.27s/it]\u001b[A\n","81it [04:50,  1.05it/s]\u001b[A\n","82it [04:50,  1.35it/s]\u001b[A\n","83it [04:50,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:12<01:24,  1.66s/it]\n","85it [05:32, 12.80s/it]\u001b[A\n","86it [05:32,  9.03s/it]\u001b[A\n","87it [05:32,  6.39s/it]\u001b[A\n","88it [05:32,  4.54s/it]\u001b[A\n","89it [05:33,  3.25s/it]\u001b[A\n","90it [05:33,  2.34s/it]\u001b[A\n","91it [05:33,  1.71s/it]\u001b[A\n","92it [05:33,  1.27s/it]\u001b[A\n","93it [05:34,  1.05it/s]\u001b[A\n","94it [05:34,  1.35it/s]\u001b[A\n","95it [05:34,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [06:56<00:43,  1.66s/it]\n","97it [06:16, 12.84s/it]\u001b[A\n","98it [06:16,  9.06s/it]\u001b[A\n","99it [06:16,  6.41s/it]\u001b[A\n","100it [06:17,  4.56s/it]\u001b[A\n","101it [06:17,  3.26s/it]\u001b[A\n","102it [06:17,  2.35s/it]\u001b[A\n","103it [06:17,  1.71s/it]\u001b[A\n","104it [06:18,  1.27s/it]\u001b[A\n","105it [06:18,  1.04it/s]\u001b[A\n","106it [06:18,  1.35it/s]\u001b[A\n","107it [06:18,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:40<00:01,  1.66s/it]\n","109it [07:00, 12.82s/it]\u001b[A\n","110it [07:00,  9.04s/it]\u001b[A\n","111it [07:01,  6.40s/it]\u001b[A\n","112it [07:01,  4.55s/it]\u001b[A\n","113it [07:01,  3.25s/it]\u001b[A\n","114it [07:01,  2.35s/it]\u001b[A\n","115it [07:01,  1.71s/it]\u001b[A\n","116it [07:02,  1.27s/it]\u001b[A\n","117it [07:02,  1.05it/s]\u001b[A\n","118it [07:02,  1.35it/s]\u001b[A\n","119it [07:02,  1.71it/s]\u001b[A\n","Epoch: 100% 250/250 [07:45<00:00,  1.86s/it]\n","03/07/2022 04:37:23 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 04:37:37 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:17,  4.59s/it]\u001b[A\n","122it [07:17,  3.28s/it]\u001b[A\n","123it [07:17,  2.36s/it]\u001b[A\n","124it [07:17,  1.72s/it]\u001b[A\n","125it [07:18,  1.28s/it]\u001b[A\n","126it [07:18,  1.04it/s]\u001b[A\n","127it [07:18,  1.35it/s]\u001b[A\n","128it [07:18,  1.70it/s]\u001b[A\n","129it [07:19,  2.08it/s]\u001b[A\n","130it [07:19,  2.46it/s]\u001b[A\n","131it [07:19,  2.83it/s]\u001b[A\n","132it [07:19,  3.50it/s]\u001b[A03/07/2022 04:37:39 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 04:37:39 - INFO - __main__ -     eval_loss = 2.068753719329834\n","03/07/2022 04:37:39 - INFO - __main__ -     eval_auroc = 0.9290000200271606\n","03/07/2022 04:37:39 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/07/2022 04:37:39 - INFO - __main__ -     eval_f1 = 0.5647059082984924\n","03/07/2022 04:37:39 - INFO - filelock -   Lock 139853803281872 acquired on log.lock\n","03/07/2022 04:37:39 - INFO - filelock -   Lock 139853803281872 released on log.lock\n","132it [07:19,  3.33s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 04:37:45 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 04:37:45 - INFO - __main__ -   Specify load the 77-th template: *cls*▁Yeah.*mask*.*+sent_0**sep+*\n","03/07/2022 04:37:45 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 04:37:45 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-16658', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_04-37-45_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-16658', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 04:37:45 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 04:37:48 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:37:48 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:37:48 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 04:37:48 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:37:48 - INFO - filelock -   Lock 140313782141008 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:37:48 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 04:37:48 - INFO - filelock -   Lock 140313782141008 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:37:48 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:37:48 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:37:48 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 04:37:48 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:37:48 - INFO - filelock -   Lock 140313782250128 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:37:48 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 04:37:48 - INFO - filelock -   Lock 140313782250128 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:37:48 - INFO - src.dataset -   *** Example ***\n","03/07/2022 04:37:48 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 04:37:48 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 14783, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/07/2022 04:37:48 - INFO - src.dataset -   text: <s>▁Yeah.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 04:38:04 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 04:38:04 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 04:38:04 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 04:38:04 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 04:38:04 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 04:38:04 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 04:38:04 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.52it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.11it/s]\u001b[A03/07/2022 04:38:48 - INFO - src.trainer -   Best dev result: 0.8432499766349792\n","Epoch:  20% 49/250 [01:31<05:39,  1.69s/it]\n","13it [00:51, 14.83s/it]            \u001b[A\n","14it [00:51, 10.45s/it]\u001b[A\n","15it [00:51,  7.39s/it]\u001b[A\n","16it [00:52,  5.24s/it]\u001b[A\n","17it [00:52,  3.74s/it]\u001b[A\n","18it [00:52,  2.69s/it]\u001b[A\n","19it [00:52,  1.95s/it]\u001b[A\n","20it [00:53,  1.43s/it]\u001b[A\n","21it [00:53,  1.07s/it]\u001b[A\n","22it [00:53,  1.22it/s]\u001b[A\n","23it [00:53,  1.55it/s]\u001b[A\n","24it [00:53,  2.05it/s]\u001b[A03/07/2022 04:39:40 - INFO - src.trainer -   Best dev result: 0.8740000128746033\n","Epoch:  30% 74/250 [02:22<04:58,  1.69s/it]\n","25it [01:42, 15.03s/it]\u001b[A\n","26it [01:43, 10.59s/it]\u001b[A\n","27it [01:43,  7.48s/it]\u001b[A\n","28it [01:43,  5.31s/it]\u001b[A\n","29it [01:43,  3.78s/it]\u001b[A\n","30it [01:43,  2.72s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:44,  1.45s/it]\u001b[A\n","33it [01:44,  1.08s/it]\u001b[A\n","34it [01:44,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","36it [01:45,  2.03it/s]\u001b[A03/07/2022 04:40:31 - INFO - src.trainer -   Best dev result: 0.8833750486373901\n","Epoch:  40% 99/250 [03:15<04:15,  1.69s/it]\n","37it [02:35, 15.28s/it]\u001b[A\n","38it [02:35, 10.76s/it]\u001b[A\n","39it [02:35,  7.60s/it]\u001b[A\n","40it [02:35,  5.39s/it]\u001b[A\n","41it [02:35,  3.84s/it]\u001b[A\n","42it [02:36,  2.76s/it]\u001b[A\n","43it [02:36,  2.00s/it]\u001b[A\n","44it [02:36,  1.47s/it]\u001b[A\n","45it [02:36,  1.10s/it]\u001b[A\n","46it [02:37,  1.19it/s]\u001b[A\n","47it [02:37,  1.52it/s]\u001b[A\n","Epoch:  50% 124/250 [03:59<03:29,  1.66s/it]\n","49it [03:19, 12.90s/it]\u001b[A\n","50it [03:19,  9.10s/it]\u001b[A\n","51it [03:19,  6.44s/it]\u001b[A\n","52it [03:20,  4.58s/it]\u001b[A\n","53it [03:20,  3.27s/it]\u001b[A\n","54it [03:20,  2.36s/it]\u001b[A\n","55it [03:20,  1.72s/it]\u001b[A\n","56it [03:20,  1.27s/it]\u001b[A\n","57it [03:21,  1.04it/s]\u001b[A\n","58it [03:21,  1.35it/s]\u001b[A\n","59it [03:21,  1.70it/s]\u001b[A\n","60it [03:21,  2.22it/s]\u001b[A03/07/2022 04:42:07 - INFO - src.trainer -   Best dev result: 0.8835000395774841\n","Epoch:  60% 149/250 [04:51<02:50,  1.69s/it]\n","61it [04:10, 15.07s/it]\u001b[A\n","62it [04:11, 10.62s/it]\u001b[A\n","63it [04:11,  7.50s/it]\u001b[A\n","64it [04:11,  5.32s/it]\u001b[A\n","65it [04:11,  3.79s/it]\u001b[A\n","66it [04:12,  2.73s/it]\u001b[A\n","67it [04:12,  1.98s/it]\u001b[A\n","68it [04:12,  1.45s/it]\u001b[A\n","69it [04:12,  1.09s/it]\u001b[A\n","70it [04:13,  1.21it/s]\u001b[A\n","71it [04:13,  1.54it/s]\u001b[A\n","Epoch:  70% 174/250 [05:35<02:05,  1.65s/it]\n","73it [04:54, 12.82s/it]\u001b[A\n","74it [04:55,  9.04s/it]\u001b[A\n","75it [04:55,  6.40s/it]\u001b[A\n","76it [04:55,  4.55s/it]\u001b[A\n","77it [04:55,  3.25s/it]\u001b[A\n","78it [04:56,  2.35s/it]\u001b[A\n","79it [04:56,  1.71s/it]\u001b[A\n","80it [04:56,  1.27s/it]\u001b[A\n","81it [04:56,  1.05it/s]\u001b[A\n","82it [04:57,  1.35it/s]\u001b[A\n","83it [04:57,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:19<01:24,  1.65s/it]\n","85it [05:38, 12.79s/it]\u001b[A\n","86it [05:39,  9.02s/it]\u001b[A\n","87it [05:39,  6.39s/it]\u001b[A\n","88it [05:39,  4.54s/it]\u001b[A\n","89it [05:39,  3.25s/it]\u001b[A\n","90it [05:40,  2.34s/it]\u001b[A\n","91it [05:40,  1.71s/it]\u001b[A\n","92it [05:40,  1.27s/it]\u001b[A\n","93it [05:40,  1.05it/s]\u001b[A\n","94it [05:41,  1.36it/s]\u001b[A\n","95it [05:41,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [07:03<00:43,  1.65s/it]\n","97it [06:23, 12.80s/it]\u001b[A\n","98it [06:23,  9.03s/it]\u001b[A\n","99it [06:23,  6.39s/it]\u001b[A\n","100it [06:23,  4.54s/it]\u001b[A\n","101it [06:23,  3.25s/it]\u001b[A\n","102it [06:24,  2.34s/it]\u001b[A\n","103it [06:24,  1.71s/it]\u001b[A\n","104it [06:24,  1.27s/it]\u001b[A\n","105it [06:24,  1.05it/s]\u001b[A\n","106it [06:25,  1.36it/s]\u001b[A\n","107it [06:25,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:47<00:01,  1.66s/it]\n","109it [07:07, 12.80s/it]\u001b[A\n","110it [07:07,  9.03s/it]\u001b[A\n","111it [07:07,  6.39s/it]\u001b[A\n","112it [07:07,  4.54s/it]\u001b[A\n","113it [07:07,  3.25s/it]\u001b[A\n","114it [07:08,  2.34s/it]\u001b[A\n","115it [07:08,  1.71s/it]\u001b[A\n","116it [07:08,  1.27s/it]\u001b[A\n","117it [07:08,  1.05it/s]\u001b[A\n","118it [07:09,  1.36it/s]\u001b[A\n","119it [07:09,  1.71it/s]\u001b[A\n","Epoch: 100% 250/250 [07:51<00:00,  1.89s/it]\n","03/07/2022 04:45:55 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 04:46:09 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:23,  4.64s/it]\u001b[A\n","122it [07:24,  3.32s/it]\u001b[A\n","123it [07:24,  2.39s/it]\u001b[A\n","124it [07:24,  1.74s/it]\u001b[A\n","125it [07:24,  1.29s/it]\u001b[A\n","126it [07:25,  1.03it/s]\u001b[A\n","127it [07:25,  1.33it/s]\u001b[A\n","128it [07:25,  1.68it/s]\u001b[A\n","129it [07:25,  2.06it/s]\u001b[A\n","130it [07:26,  2.45it/s]\u001b[A\n","131it [07:26,  2.81it/s]\u001b[A\n","132it [07:26,  3.49it/s]\u001b[A03/07/2022 04:46:12 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 04:46:12 - INFO - __main__ -     eval_loss = 3.427764654159546\n","03/07/2022 04:46:12 - INFO - __main__ -     eval_auroc = 0.8835000395774841\n","03/07/2022 04:46:12 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/07/2022 04:46:12 - INFO - __main__ -     eval_f1 = 0.44897961616516113\n","03/07/2022 04:46:12 - INFO - filelock -   Lock 140313860369808 acquired on log.lock\n","03/07/2022 04:46:12 - INFO - filelock -   Lock 140313860369808 released on log.lock\n","132it [07:26,  3.38s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 04:46:18 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 04:46:18 - INFO - __main__ -   Specify load the 78-th template: *cls*▁But*mask*,*+sent_0**sep+*\n","03/07/2022 04:46:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 04:46:18 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-13228', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_04-46-18_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-13228', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 04:46:18 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 04:46:21 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:46:21 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:46:21 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 04:46:21 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:46:21 - INFO - filelock -   Lock 140683716087312 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:46:21 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 04:46:21 - INFO - filelock -   Lock 140683716087312 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:46:21 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:46:21 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:46:21 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 04:46:21 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:46:21 - INFO - filelock -   Lock 140683716250640 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:46:21 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 04:46:21 - INFO - filelock -   Lock 140683716250640 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:46:21 - INFO - src.dataset -   *** Example ***\n","03/07/2022 04:46:21 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 04:46:21 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 1708, 50264, 6, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/07/2022 04:46:21 - INFO - src.dataset -   text: <s>▁But<mask>, Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 04:46:37 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 04:46:37 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 04:46:37 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 04:46:37 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 04:46:37 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 04:46:37 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 04:46:37 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.68it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.52it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 04:47:21 - INFO - src.trainer -   Best dev result: 0.890125036239624\n","Epoch:  20% 49/250 [01:31<05:39,  1.69s/it]\n","13it [00:51, 14.93s/it]            \u001b[A\n","14it [00:51, 10.52s/it]\u001b[A\n","15it [00:52,  7.43s/it]\u001b[A\n","16it [00:52,  5.27s/it]\u001b[A\n","17it [00:52,  3.76s/it]\u001b[A\n","18it [00:52,  2.70s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.55it/s]\u001b[A\n","24it [00:54,  2.04it/s]\u001b[A03/07/2022 04:48:13 - INFO - src.trainer -   Best dev result: 0.8931249976158142\n","Epoch:  30% 74/250 [02:23<04:57,  1.69s/it]\n","25it [01:43, 15.23s/it]\u001b[A\n","26it [01:44, 10.73s/it]\u001b[A\n","27it [01:44,  7.58s/it]\u001b[A\n","28it [01:44,  5.38s/it]\u001b[A\n","29it [01:44,  3.83s/it]\u001b[A\n","30it [01:44,  2.75s/it]\u001b[A\n","31it [01:45,  2.00s/it]\u001b[A\n","32it [01:45,  1.47s/it]\u001b[A\n","33it [01:45,  1.10s/it]\u001b[A\n","34it [01:45,  1.19it/s]\u001b[A\n","35it [01:46,  1.52it/s]\u001b[A\n","36it [01:46,  2.01it/s]\u001b[A03/07/2022 04:49:05 - INFO - src.trainer -   Best dev result: 0.8957499861717224\n","Epoch:  40% 99/250 [03:15<04:14,  1.68s/it]\n","37it [02:35, 15.02s/it]\u001b[A\n","38it [02:35, 10.58s/it]\u001b[A\n","39it [02:35,  7.48s/it]\u001b[A\n","40it [02:35,  5.30s/it]\u001b[A\n","41it [02:36,  3.78s/it]\u001b[A\n","42it [02:36,  2.72s/it]\u001b[A\n","43it [02:36,  1.97s/it]\u001b[A\n","44it [02:36,  1.45s/it]\u001b[A\n","45it [02:37,  1.08s/it]\u001b[A\n","46it [02:37,  1.21it/s]\u001b[A\n","47it [02:37,  1.54it/s]\u001b[A\n","48it [02:37,  2.03it/s]\u001b[A03/07/2022 04:49:56 - INFO - src.trainer -   Best dev result: 0.8967499732971191\n","Epoch:  50% 124/250 [04:08<03:31,  1.68s/it]\n","49it [03:28, 15.46s/it]\u001b[A\n","50it [03:28, 10.89s/it]\u001b[A\n","51it [03:28,  7.69s/it]\u001b[A\n","52it [03:28,  5.45s/it]\u001b[A\n","53it [03:28,  3.89s/it]\u001b[A\n","54it [03:29,  2.79s/it]\u001b[A\n","55it [03:29,  2.02s/it]\u001b[A\n","56it [03:29,  1.49s/it]\u001b[A\n","57it [03:29,  1.11s/it]\u001b[A\n","58it [03:30,  1.18it/s]\u001b[A\n","59it [03:30,  1.51it/s]\u001b[A\n","60it [03:30,  2.00it/s]\u001b[A03/07/2022 04:50:49 - INFO - src.trainer -   Best dev result: 0.8977499604225159\n","Epoch:  60% 149/250 [05:00<02:51,  1.70s/it]\n","61it [04:19, 15.22s/it]\u001b[A\n","62it [04:20, 10.72s/it]\u001b[A\n","63it [04:20,  7.57s/it]\u001b[A\n","64it [04:20,  5.37s/it]\u001b[A\n","65it [04:20,  3.83s/it]\u001b[A\n","66it [04:21,  2.75s/it]\u001b[A\n","67it [04:21,  1.99s/it]\u001b[A\n","68it [04:21,  1.47s/it]\u001b[A\n","69it [04:21,  1.09s/it]\u001b[A\n","70it [04:22,  1.20it/s]\u001b[A\n","71it [04:22,  1.53it/s]\u001b[A\n","Epoch:  70% 174/250 [05:44<02:05,  1.65s/it]\n","73it [05:04, 12.83s/it]\u001b[A\n","74it [05:04,  9.05s/it]\u001b[A\n","75it [05:04,  6.41s/it]\u001b[A\n","76it [05:04,  4.55s/it]\u001b[A\n","77it [05:04,  3.26s/it]\u001b[A\n","78it [05:05,  2.35s/it]\u001b[A\n","79it [05:05,  1.71s/it]\u001b[A\n","80it [05:05,  1.27s/it]\u001b[A\n","81it [05:05,  1.04it/s]\u001b[A\n","82it [05:06,  1.35it/s]\u001b[A\n","83it [05:06,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:28<01:24,  1.66s/it]\n","85it [05:48, 12.80s/it]\u001b[A\n","86it [05:48,  9.03s/it]\u001b[A\n","87it [05:48,  6.39s/it]\u001b[A\n","88it [05:48,  4.54s/it]\u001b[A\n","89it [05:49,  3.25s/it]\u001b[A\n","90it [05:49,  2.34s/it]\u001b[A\n","91it [05:49,  1.71s/it]\u001b[A\n","92it [05:49,  1.27s/it]\u001b[A\n","93it [05:49,  1.05it/s]\u001b[A\n","94it [05:50,  1.35it/s]\u001b[A\n","95it [05:50,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [07:12<00:42,  1.65s/it]\n","97it [06:32, 12.80s/it]\u001b[A\n","98it [06:32,  9.03s/it]\u001b[A\n","99it [06:32,  6.39s/it]\u001b[A\n","100it [06:32,  4.54s/it]\u001b[A\n","101it [06:33,  3.25s/it]\u001b[A\n","102it [06:33,  2.34s/it]\u001b[A\n","103it [06:33,  1.71s/it]\u001b[A\n","104it [06:33,  1.27s/it]\u001b[A\n","105it [06:34,  1.05it/s]\u001b[A\n","106it [06:34,  1.35it/s]\u001b[A\n","107it [06:34,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:56<00:01,  1.66s/it]\n","109it [07:16, 12.80s/it]\u001b[A\n","110it [07:16,  9.03s/it]\u001b[A\n","111it [07:16,  6.39s/it]\u001b[A\n","112it [07:16,  4.54s/it]\u001b[A\n","113it [07:17,  3.25s/it]\u001b[A\n","114it [07:17,  2.34s/it]\u001b[A\n","115it [07:17,  1.71s/it]\u001b[A\n","116it [07:17,  1.27s/it]\u001b[A\n","117it [07:18,  1.05it/s]\u001b[A\n","118it [07:18,  1.36it/s]\u001b[A\n","119it [07:18,  1.71it/s]\u001b[A\n","Epoch: 100% 250/250 [08:00<00:00,  1.92s/it]\n","03/07/2022 04:54:37 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 04:54:51 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:32,  4.58s/it]\u001b[A\n","122it [07:33,  3.28s/it]\u001b[A\n","123it [07:33,  2.36s/it]\u001b[A\n","124it [07:33,  1.72s/it]\u001b[A\n","125it [07:33,  1.28s/it]\u001b[A\n","126it [07:34,  1.04it/s]\u001b[A\n","127it [07:34,  1.35it/s]\u001b[A\n","128it [07:34,  1.70it/s]\u001b[A\n","129it [07:34,  2.08it/s]\u001b[A\n","130it [07:34,  2.46it/s]\u001b[A\n","131it [07:35,  2.82it/s]\u001b[A\n","132it [07:35,  3.50it/s]\u001b[A03/07/2022 04:54:54 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 04:54:54 - INFO - __main__ -     eval_loss = 2.193680763244629\n","03/07/2022 04:54:54 - INFO - __main__ -     eval_auroc = 0.8977499604225159\n","03/07/2022 04:54:54 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/07/2022 04:54:54 - INFO - __main__ -     eval_f1 = 0.5052631497383118\n","03/07/2022 04:54:54 - INFO - filelock -   Lock 140683716087760 acquired on log.lock\n","03/07/2022 04:54:54 - INFO - filelock -   Lock 140683716087760 released on log.lock\n","132it [07:35,  3.45s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 04:55:00 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 04:55:00 - INFO - __main__ -   Specify load the 79-th template: *cls*▁4.*mask*.*+sent_0**sep+*\n","03/07/2022 04:55:00 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 04:55:00 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-30826', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_04-55-00_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-30826', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 04:55:00 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 04:55:03 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:55:03 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:55:03 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 04:55:03 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:55:03 - INFO - filelock -   Lock 140584648550544 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:55:03 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 04:55:03 - INFO - filelock -   Lock 140584648550544 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:55:03 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 04:55:03 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 04:55:03 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 04:55:03 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 04:55:03 - INFO - filelock -   Lock 140584647192272 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:55:03 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 04:55:03 - INFO - filelock -   Lock 140584647192272 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 04:55:03 - INFO - src.dataset -   *** Example ***\n","03/07/2022 04:55:03 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 04:55:03 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 306, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/07/2022 04:55:03 - INFO - src.dataset -   text: <s>▁4.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 04:55:19 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 04:55:19 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 04:55:19 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 04:55:19 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 04:55:19 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 04:55:19 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 04:55:19 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.68it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.52it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 04:56:03 - INFO - src.trainer -   Best dev result: 0.9284999966621399\n","Epoch:  20% 49/250 [01:32<05:41,  1.70s/it]\n","13it [00:52, 15.11s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:53,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/07/2022 04:56:55 - INFO - src.trainer -   Best dev result: 0.9331249594688416\n","Epoch:  30% 74/250 [02:24<04:56,  1.69s/it]\n","25it [01:44, 15.22s/it]\u001b[A\n","26it [01:44, 10.72s/it]\u001b[A\n","27it [01:44,  7.58s/it]\u001b[A\n","28it [01:45,  5.37s/it]\u001b[A\n","29it [01:45,  3.83s/it]\u001b[A\n","30it [01:45,  2.75s/it]\u001b[A\n","31it [01:45,  2.00s/it]\u001b[A\n","32it [01:45,  1.47s/it]\u001b[A\n","33it [01:46,  1.10s/it]\u001b[A\n","34it [01:46,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","36it [01:46,  2.02it/s]\u001b[A03/07/2022 04:57:48 - INFO - src.trainer -   Best dev result: 0.9353750348091125\n","Epoch:  40% 99/250 [03:16<04:15,  1.69s/it]\n","37it [02:36, 15.11s/it]\u001b[A\n","38it [02:36, 10.65s/it]\u001b[A\n","39it [02:36,  7.52s/it]\u001b[A\n","40it [02:36,  5.34s/it]\u001b[A\n","41it [02:36,  3.80s/it]\u001b[A\n","42it [02:37,  2.73s/it]\u001b[A\n","43it [02:37,  1.98s/it]\u001b[A\n","44it [02:37,  1.46s/it]\u001b[A\n","45it [02:37,  1.09s/it]\u001b[A\n","46it [02:38,  1.20it/s]\u001b[A\n","47it [02:38,  1.53it/s]\u001b[A\n","48it [02:38,  2.02it/s]\u001b[A03/07/2022 04:58:39 - INFO - src.trainer -   Best dev result: 0.9363750219345093\n","Epoch:  50% 124/250 [04:08<03:32,  1.68s/it]\n","49it [03:28, 15.24s/it]\u001b[A\n","50it [03:28, 10.74s/it]\u001b[A\n","51it [03:28,  7.58s/it]\u001b[A\n","52it [03:28,  5.38s/it]\u001b[A\n","53it [03:29,  3.83s/it]\u001b[A\n","54it [03:29,  2.75s/it]\u001b[A\n","55it [03:29,  2.00s/it]\u001b[A\n","56it [03:29,  1.47s/it]\u001b[A\n","57it [03:29,  1.10s/it]\u001b[A\n","58it [03:30,  1.19it/s]\u001b[A\n","59it [03:30,  1.53it/s]\u001b[A\n","Epoch:  60% 149/250 [04:52<02:47,  1.66s/it]\n","61it [04:12, 12.86s/it]\u001b[A\n","62it [04:12,  9.07s/it]\u001b[A\n","63it [04:12,  6.42s/it]\u001b[A\n","64it [04:12,  4.56s/it]\u001b[A\n","65it [04:13,  3.26s/it]\u001b[A\n","66it [04:13,  2.35s/it]\u001b[A\n","67it [04:13,  1.72s/it]\u001b[A\n","68it [04:13,  1.27s/it]\u001b[A\n","69it [04:14,  1.04it/s]\u001b[A\n","70it [04:14,  1.35it/s]\u001b[A\n","71it [04:14,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:36<02:05,  1.66s/it]\n","73it [04:56, 12.80s/it]\u001b[A\n","74it [04:56,  9.03s/it]\u001b[A\n","75it [04:56,  6.39s/it]\u001b[A\n","76it [04:56,  4.54s/it]\u001b[A\n","77it [04:57,  3.25s/it]\u001b[A\n","78it [04:57,  2.34s/it]\u001b[A\n","79it [04:57,  1.71s/it]\u001b[A\n","80it [04:57,  1.27s/it]\u001b[A\n","81it [04:58,  1.05it/s]\u001b[A\n","82it [04:58,  1.35it/s]\u001b[A\n","83it [04:58,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:20<01:24,  1.67s/it]\n","85it [05:40, 12.82s/it]\u001b[A\n","86it [05:40,  9.04s/it]\u001b[A\n","87it [05:40,  6.40s/it]\u001b[A\n","88it [05:41,  4.55s/it]\u001b[A\n","89it [05:41,  3.25s/it]\u001b[A\n","90it [05:41,  2.35s/it]\u001b[A\n","91it [05:41,  1.71s/it]\u001b[A\n","92it [05:42,  1.27s/it]\u001b[A\n","93it [05:42,  1.05it/s]\u001b[A\n","94it [05:42,  1.35it/s]\u001b[A\n","95it [05:42,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:04<00:43,  1.66s/it]\n","97it [06:24, 12.82s/it]\u001b[A\n","98it [06:24,  9.05s/it]\u001b[A\n","99it [06:25,  6.40s/it]\u001b[A\n","100it [06:25,  4.55s/it]\u001b[A\n","101it [06:25,  3.25s/it]\u001b[A\n","102it [06:25,  2.35s/it]\u001b[A\n","103it [06:25,  1.71s/it]\u001b[A\n","104it [06:26,  1.27s/it]\u001b[A\n","105it [06:26,  1.05it/s]\u001b[A\n","106it [06:26,  1.35it/s]\u001b[A\n","107it [06:26,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:48<00:01,  1.66s/it]\n","109it [07:08, 12.81s/it]\u001b[A\n","110it [07:08,  9.03s/it]\u001b[A\n","111it [07:09,  6.39s/it]\u001b[A\n","112it [07:09,  4.54s/it]\u001b[A\n","113it [07:09,  3.25s/it]\u001b[A\n","114it [07:09,  2.34s/it]\u001b[A\n","115it [07:10,  1.71s/it]\u001b[A\n","116it [07:10,  1.27s/it]\u001b[A\n","117it [07:10,  1.05it/s]\u001b[A\n","118it [07:10,  1.35it/s]\u001b[A\n","119it [07:10,  1.71it/s]\u001b[A\n","120it [07:11,  2.23it/s]\u001b[A03/07/2022 05:03:12 - INFO - src.trainer -   Best dev result: 0.9367499351501465\n","Epoch: 100% 250/250 [07:59<00:00,  1.92s/it]\n","03/07/2022 05:03:18 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 05:03:33 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:32,  6.61s/it]\u001b[A\n","122it [07:32,  4.69s/it]\u001b[A\n","123it [07:32,  3.36s/it]\u001b[A\n","124it [07:32,  2.42s/it]\u001b[A\n","125it [07:32,  1.76s/it]\u001b[A\n","126it [07:33,  1.30s/it]\u001b[A\n","127it [07:33,  1.02it/s]\u001b[A\n","128it [07:33,  1.32it/s]\u001b[A\n","129it [07:33,  1.67it/s]\u001b[A\n","130it [07:34,  2.05it/s]\u001b[A\n","131it [07:34,  2.43it/s]\u001b[A\n","132it [07:34,  3.07it/s]\u001b[A03/07/2022 05:03:35 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 05:03:35 - INFO - __main__ -     eval_loss = 3.198004722595215\n","03/07/2022 05:03:35 - INFO - __main__ -     eval_auroc = 0.9367499351501465\n","03/07/2022 05:03:35 - INFO - __main__ -     eval_recall = 1.0\n","03/07/2022 05:03:35 - INFO - __main__ -     eval_f1 = 0.43478262424468994\n","03/07/2022 05:03:35 - INFO - filelock -   Lock 140584692671568 acquired on log.lock\n","03/07/2022 05:03:35 - INFO - filelock -   Lock 140584692671568 released on log.lock\n","132it [07:34,  3.44s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 05:03:42 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 05:03:42 - INFO - __main__ -   Specify load the 80-th template: *cls*▁Ah*mask*.*+sent_0**sep+*\n","03/07/2022 05:03:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 05:03:42 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-25469', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_05-03-42_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-25469', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 05:03:42 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 05:03:46 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:03:46 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:03:46 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 05:03:46 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:03:46 - INFO - filelock -   Lock 140677757834512 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:03:46 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 05:03:46 - INFO - filelock -   Lock 140677757834512 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:03:46 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:03:46 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:03:46 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 05:03:46 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:03:46 - INFO - filelock -   Lock 140677728652048 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:03:46 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 05:03:46 - INFO - filelock -   Lock 140677728652048 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:03:46 - INFO - src.dataset -   *** Example ***\n","03/07/2022 05:03:46 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 05:03:46 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 17986, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/07/2022 05:03:46 - INFO - src.dataset -   text: <s>▁Ah<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 05:04:04 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 05:04:04 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 05:04:04 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 05:04:04 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 05:04:04 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 05:04:04 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 05:04:04 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.52it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 05:04:48 - INFO - src.trainer -   Best dev result: 0.8031250238418579\n","Epoch:  20% 49/250 [01:32<05:40,  1.70s/it]\n","13it [00:51, 15.00s/it]            \u001b[A\n","14it [00:52, 10.57s/it]\u001b[A\n","15it [00:52,  7.47s/it]\u001b[A\n","16it [00:52,  5.30s/it]\u001b[A\n","17it [00:52,  3.78s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/07/2022 05:05:40 - INFO - src.trainer -   Best dev result: 0.903999924659729\n","Epoch:  30% 74/250 [02:23<04:56,  1.69s/it]\n","25it [01:43, 14.97s/it]\u001b[A\n","26it [01:43, 10.55s/it]\u001b[A\n","27it [01:43,  7.46s/it]\u001b[A\n","28it [01:43,  5.29s/it]\u001b[A\n","29it [01:44,  3.77s/it]\u001b[A\n","30it [01:44,  2.71s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:44,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","36it [01:45,  2.04it/s]\u001b[A03/07/2022 05:06:31 - INFO - src.trainer -   Best dev result: 0.9067500233650208\n","Epoch:  40% 99/250 [03:15<04:15,  1.69s/it]\n","37it [02:35, 15.27s/it]\u001b[A\n","38it [02:35, 10.76s/it]\u001b[A\n","39it [02:35,  7.60s/it]\u001b[A\n","40it [02:36,  5.39s/it]\u001b[A\n","41it [02:36,  3.84s/it]\u001b[A\n","42it [02:36,  2.76s/it]\u001b[A\n","43it [02:36,  2.00s/it]\u001b[A\n","44it [02:36,  1.47s/it]\u001b[A\n","45it [02:37,  1.10s/it]\u001b[A\n","46it [02:37,  1.19it/s]\u001b[A\n","47it [02:37,  1.52it/s]\u001b[A\n","48it [02:37,  2.01it/s]\u001b[A03/07/2022 05:07:24 - INFO - src.trainer -   Best dev result: 0.9077500700950623\n","Epoch:  50% 124/250 [04:07<03:33,  1.69s/it]\n","49it [03:27, 15.28s/it]\u001b[A\n","50it [03:27, 10.76s/it]\u001b[A\n","51it [03:28,  7.60s/it]\u001b[A\n","52it [03:28,  5.39s/it]\u001b[A\n","53it [03:28,  3.84s/it]\u001b[A\n","54it [03:28,  2.76s/it]\u001b[A\n","55it [03:28,  2.00s/it]\u001b[A\n","56it [03:29,  1.47s/it]\u001b[A\n","57it [03:29,  1.10s/it]\u001b[A\n","58it [03:29,  1.19it/s]\u001b[A\n","59it [03:29,  1.52it/s]\u001b[A\n","60it [03:30,  2.01it/s]\u001b[A03/07/2022 05:08:16 - INFO - src.trainer -   Best dev result: 0.9080000519752502\n","Epoch:  60% 149/250 [04:59<02:51,  1.69s/it]\n","61it [04:19, 15.29s/it]\u001b[A\n","62it [04:20, 10.77s/it]\u001b[A\n","63it [04:20,  7.61s/it]\u001b[A\n","64it [04:20,  5.40s/it]\u001b[A\n","65it [04:20,  3.85s/it]\u001b[A\n","66it [04:20,  2.76s/it]\u001b[A\n","67it [04:21,  2.00s/it]\u001b[A\n","68it [04:21,  1.47s/it]\u001b[A\n","69it [04:21,  1.10s/it]\u001b[A\n","70it [04:21,  1.19it/s]\u001b[A\n","71it [04:22,  1.52it/s]\u001b[A\n","72it [04:22,  2.01it/s]\u001b[A03/07/2022 05:09:08 - INFO - src.trainer -   Best dev result: 0.9084999561309814\n","Epoch:  70% 174/250 [05:52<02:08,  1.69s/it]\n","73it [05:12, 15.33s/it]\u001b[A\n","74it [05:12, 10.80s/it]\u001b[A\n","75it [05:12,  7.63s/it]\u001b[A\n","76it [05:12,  5.41s/it]\u001b[A\n","77it [05:13,  3.86s/it]\u001b[A\n","78it [05:13,  2.77s/it]\u001b[A\n","79it [05:13,  2.01s/it]\u001b[A\n","80it [05:13,  1.48s/it]\u001b[A\n","81it [05:14,  1.10s/it]\u001b[A\n","82it [05:14,  1.19it/s]\u001b[A\n","83it [05:14,  1.52it/s]\u001b[A\n","84it [05:14,  2.01it/s]\u001b[A03/07/2022 05:10:01 - INFO - src.trainer -   Best dev result: 0.9087499976158142\n","Epoch:  80% 199/250 [06:44<01:26,  1.71s/it]\n","85it [06:04, 15.33s/it]\u001b[A\n","86it [06:04, 10.80s/it]\u001b[A\n","87it [06:05,  7.63s/it]\u001b[A\n","88it [06:05,  5.41s/it]\u001b[A\n","89it [06:05,  3.86s/it]\u001b[A\n","90it [06:05,  2.77s/it]\u001b[A\n","91it [06:05,  2.01s/it]\u001b[A\n","92it [06:06,  1.47s/it]\u001b[A\n","93it [06:06,  1.10s/it]\u001b[A\n","94it [06:06,  1.19it/s]\u001b[A\n","95it [06:06,  1.52it/s]\u001b[A\n","96it [06:07,  2.01it/s]\u001b[A03/07/2022 05:10:53 - INFO - src.trainer -   Best dev result: 0.909000039100647\n","Epoch:  90% 224/250 [07:36<00:44,  1.69s/it]\n","97it [06:56, 15.27s/it]\u001b[A\n","98it [06:56, 10.76s/it]\u001b[A\n","99it [06:57,  7.60s/it]\u001b[A\n","100it [06:57,  5.39s/it]\u001b[A\n","101it [06:57,  3.84s/it]\u001b[A\n","102it [06:57,  2.76s/it]\u001b[A\n","103it [06:58,  2.00s/it]\u001b[A\n","104it [06:58,  1.47s/it]\u001b[A\n","105it [06:58,  1.10s/it]\u001b[A\n","106it [06:58,  1.19it/s]\u001b[A\n","107it [06:59,  1.52it/s]\u001b[A\n","Epoch: 100% 249/250 [08:21<00:01,  1.66s/it]\n","109it [07:41, 12.89s/it]\u001b[A\n","110it [07:41,  9.10s/it]\u001b[A\n","111it [07:41,  6.44s/it]\u001b[A\n","112it [07:41,  4.57s/it]\u001b[A\n","113it [07:41,  3.27s/it]\u001b[A\n","114it [07:42,  2.36s/it]\u001b[A\n","115it [07:42,  1.72s/it]\u001b[A\n","116it [07:42,  1.27s/it]\u001b[A\n","117it [07:42,  1.04it/s]\u001b[A\n","118it [07:43,  1.35it/s]\u001b[A\n","119it [07:43,  1.70it/s]\u001b[A\n","120it [07:43,  2.22it/s]\u001b[A03/07/2022 05:12:29 - INFO - src.trainer -   Best dev result: 0.9151250123977661\n","Epoch: 100% 250/250 [08:32<00:00,  2.05s/it]\n","03/07/2022 05:12:36 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 05:12:52 - INFO - __main__ -   *** Validate ***\n","\n","121it [08:06,  7.32s/it]\u001b[A\n","122it [08:07,  5.20s/it]\u001b[A\n","123it [08:07,  3.71s/it]\u001b[A\n","124it [08:07,  2.66s/it]\u001b[A\n","125it [08:07,  1.93s/it]\u001b[A\n","126it [08:07,  1.42s/it]\u001b[A\n","127it [08:08,  1.07s/it]\u001b[A\n","128it [08:08,  1.23it/s]\u001b[A\n","129it [08:08,  1.56it/s]\u001b[A\n","130it [08:08,  1.93it/s]\u001b[A\n","131it [08:09,  2.32it/s]\u001b[A\n","132it [08:09,  2.94it/s]\u001b[A03/07/2022 05:12:55 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 05:12:55 - INFO - __main__ -     eval_loss = 3.8013505935668945\n","03/07/2022 05:12:55 - INFO - __main__ -     eval_auroc = 0.9151250123977661\n","03/07/2022 05:12:55 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/07/2022 05:12:55 - INFO - __main__ -     eval_f1 = 0.47524747252464294\n","03/07/2022 05:12:55 - INFO - filelock -   Lock 140677740229584 acquired on log.lock\n","03/07/2022 05:12:55 - INFO - filelock -   Lock 140677740229584 released on log.lock\n","132it [08:09,  3.71s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 05:13:03 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 05:13:03 - INFO - __main__ -   Specify load the 81-th template: *cls*▁No,*mask*.*+sent_0**sep+*\n","03/07/2022 05:13:03 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 05:13:03 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-18783', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_05-13-03_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-18783', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 05:13:03 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 05:13:06 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:13:06 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:13:06 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 05:13:06 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:13:06 - INFO - filelock -   Lock 140215412829776 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:13:06 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 05:13:06 - INFO - filelock -   Lock 140215412829776 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:13:06 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:13:06 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:13:06 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 05:13:06 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:13:06 - INFO - filelock -   Lock 140215412829776 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:13:06 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/07/2022 05:13:06 - INFO - filelock -   Lock 140215412829776 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:13:06 - INFO - src.dataset -   *** Example ***\n","03/07/2022 05:13:06 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 05:13:06 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 3084, 6, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/07/2022 05:13:06 - INFO - src.dataset -   text: <s>▁No,<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 05:13:24 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 05:13:24 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 05:13:24 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 05:13:24 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 05:13:24 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 05:13:24 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 05:13:24 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/07/2022 05:14:08 - INFO - src.trainer -   Best dev result: 0.86287522315979\n","Epoch:  20% 49/250 [01:32<05:39,  1.69s/it]\n","13it [00:52, 15.10s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/07/2022 05:15:01 - INFO - src.trainer -   Best dev result: 0.8842499852180481\n","Epoch:  30% 74/250 [02:24<04:59,  1.70s/it]\n","25it [01:43, 15.11s/it]\u001b[A\n","26it [01:44, 10.65s/it]\u001b[A\n","27it [01:44,  7.52s/it]\u001b[A\n","28it [01:44,  5.34s/it]\u001b[A\n","29it [01:44,  3.81s/it]\u001b[A\n","30it [01:45,  2.73s/it]\u001b[A\n","31it [01:45,  1.98s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:46,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","36it [01:46,  2.02it/s]\u001b[A03/07/2022 05:15:52 - INFO - src.trainer -   Best dev result: 0.9293749928474426\n","Epoch:  40% 99/250 [03:16<04:17,  1.70s/it]\n","37it [02:36, 15.40s/it]\u001b[A\n","38it [02:36, 10.85s/it]\u001b[A\n","39it [02:37,  7.66s/it]\u001b[A\n","40it [02:37,  5.43s/it]\u001b[A\n","41it [02:37,  3.87s/it]\u001b[A\n","42it [02:37,  2.78s/it]\u001b[A\n","43it [02:37,  2.02s/it]\u001b[A\n","44it [02:38,  1.48s/it]\u001b[A\n","45it [02:38,  1.11s/it]\u001b[A\n","46it [02:38,  1.19it/s]\u001b[A\n","47it [02:38,  1.52it/s]\u001b[A\n","Epoch:  50% 124/250 [04:01<03:29,  1.66s/it]\n","49it [03:20, 12.90s/it]\u001b[A\n","50it [03:21,  9.10s/it]\u001b[A\n","51it [03:21,  6.44s/it]\u001b[A\n","52it [03:21,  4.58s/it]\u001b[A\n","53it [03:21,  3.27s/it]\u001b[A\n","54it [03:22,  2.36s/it]\u001b[A\n","55it [03:22,  1.72s/it]\u001b[A\n","56it [03:22,  1.27s/it]\u001b[A\n","57it [03:22,  1.04it/s]\u001b[A\n","58it [03:22,  1.35it/s]\u001b[A\n","59it [03:23,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:45<02:47,  1.66s/it]\n","61it [04:05, 12.86s/it]\u001b[A\n","62it [04:05,  9.07s/it]\u001b[A\n","63it [04:05,  6.42s/it]\u001b[A\n","64it [04:05,  4.56s/it]\u001b[A\n","65it [04:06,  3.26s/it]\u001b[A\n","66it [04:06,  2.35s/it]\u001b[A\n","67it [04:06,  1.72s/it]\u001b[A\n","68it [04:06,  1.27s/it]\u001b[A\n","69it [04:07,  1.04it/s]\u001b[A\n","70it [04:07,  1.35it/s]\u001b[A\n","71it [04:07,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:29<02:06,  1.66s/it]\n","73it [04:49, 12.86s/it]\u001b[A\n","74it [04:49,  9.07s/it]\u001b[A\n","75it [04:49,  6.42s/it]\u001b[A\n","76it [04:50,  4.56s/it]\u001b[A\n","77it [04:50,  3.26s/it]\u001b[A\n","78it [04:50,  2.35s/it]\u001b[A\n","79it [04:50,  1.72s/it]\u001b[A\n","80it [04:51,  1.27s/it]\u001b[A\n","81it [04:51,  1.04it/s]\u001b[A\n","82it [04:51,  1.35it/s]\u001b[A\n","83it [04:51,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:13<01:24,  1.66s/it]\n","85it [05:33, 12.86s/it]\u001b[A\n","86it [05:33,  9.07s/it]\u001b[A\n","87it [05:34,  6.42s/it]\u001b[A\n","88it [05:34,  4.56s/it]\u001b[A\n","89it [05:34,  3.26s/it]\u001b[A\n","90it [05:34,  2.35s/it]\u001b[A\n","91it [05:35,  1.72s/it]\u001b[A\n","92it [05:35,  1.27s/it]\u001b[A\n","93it [05:35,  1.04it/s]\u001b[A\n","94it [05:35,  1.35it/s]\u001b[A\n","95it [05:35,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:58<00:43,  1.66s/it]\n","97it [06:17, 12.85s/it]\u001b[A\n","98it [06:18,  9.07s/it]\u001b[A\n","99it [06:18,  6.42s/it]\u001b[A\n","100it [06:18,  4.56s/it]\u001b[A\n","101it [06:18,  3.26s/it]\u001b[A\n","102it [06:19,  2.35s/it]\u001b[A\n","103it [06:19,  1.72s/it]\u001b[A\n","104it [06:19,  1.27s/it]\u001b[A\n","105it [06:19,  1.04it/s]\u001b[A\n","106it [06:19,  1.35it/s]\u001b[A\n","107it [06:20,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:42<00:01,  1.66s/it]\n","109it [07:02, 12.86s/it]\u001b[A\n","110it [07:02,  9.07s/it]\u001b[A\n","111it [07:02,  6.42s/it]\u001b[A\n","112it [07:02,  4.56s/it]\u001b[A\n","113it [07:03,  3.26s/it]\u001b[A\n","114it [07:03,  2.35s/it]\u001b[A\n","115it [07:03,  1.72s/it]\u001b[A\n","116it [07:03,  1.27s/it]\u001b[A\n","117it [07:03,  1.04it/s]\u001b[A\n","118it [07:04,  1.35it/s]\u001b[A\n","119it [07:04,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:46<00:00,  1.87s/it]\n","03/07/2022 05:21:10 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 05:21:25 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:19,  4.68s/it]\u001b[A\n","122it [07:19,  3.34s/it]\u001b[A\n","123it [07:19,  2.41s/it]\u001b[A\n","124it [07:19,  1.76s/it]\u001b[A\n","125it [07:20,  1.30s/it]\u001b[A\n","126it [07:20,  1.02it/s]\u001b[A\n","127it [07:20,  1.33it/s]\u001b[A\n","128it [07:20,  1.67it/s]\u001b[A\n","129it [07:20,  2.05it/s]\u001b[A\n","130it [07:21,  2.44it/s]\u001b[A\n","131it [07:21,  2.80it/s]\u001b[A\n","132it [07:21,  3.48it/s]\u001b[A03/07/2022 05:21:27 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 05:21:27 - INFO - __main__ -     eval_loss = 1.65813148021698\n","03/07/2022 05:21:27 - INFO - __main__ -     eval_auroc = 0.9293749928474426\n","03/07/2022 05:21:27 - INFO - __main__ -     eval_recall = 0.8399999737739563\n","03/07/2022 05:21:27 - INFO - __main__ -     eval_f1 = 0.6461537480354309\n","03/07/2022 05:21:27 - INFO - filelock -   Lock 140215664662672 acquired on log.lock\n","03/07/2022 05:21:27 - INFO - filelock -   Lock 140215664662672 released on log.lock\n","132it [07:21,  3.35s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 05:21:34 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 05:21:34 - INFO - __main__ -   Specify load the 82-th template: *cls*▁**mask*.*+sent_0**sep+*\n","03/07/2022 05:21:34 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 05:21:34 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-24282', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_05-21-34_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-24282', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 05:21:34 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 05:21:37 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:21:37 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:21:37 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 05:21:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:21:37 - INFO - filelock -   Lock 140446822070544 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:21:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 05:21:37 - INFO - filelock -   Lock 140446822070544 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:21:37 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:21:37 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:21:37 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 05:21:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:21:37 - INFO - filelock -   Lock 140446835153872 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:21:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 05:21:37 - INFO - filelock -   Lock 140446835153872 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:21:37 - INFO - src.dataset -   *** Example ***\n","03/07/2022 05:21:37 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 05:21:37 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 3, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[2], label_word_list=None)\n","03/07/2022 05:21:37 - INFO - src.dataset -   text: <s><unk><mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 05:21:53 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 05:21:53 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 05:21:53 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 05:21:53 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 05:21:53 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 05:21:53 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 05:21:53 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 05:22:38 - INFO - src.trainer -   Best dev result: 0.7788749933242798\n","Epoch:  20% 49/250 [01:32<05:41,  1.70s/it]\n","13it [00:52, 15.10s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:53,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/07/2022 05:23:30 - INFO - src.trainer -   Best dev result: 0.7987499237060547\n","Epoch:  30% 74/250 [02:24<04:58,  1.70s/it]\n","25it [01:43, 15.10s/it]\u001b[A\n","26it [01:44, 10.64s/it]\u001b[A\n","27it [01:44,  7.52s/it]\u001b[A\n","28it [01:44,  5.33s/it]\u001b[A\n","29it [01:44,  3.80s/it]\u001b[A\n","30it [01:45,  2.73s/it]\u001b[A\n","31it [01:45,  1.98s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:46,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","36it [01:46,  2.02it/s]\u001b[A03/07/2022 05:24:22 - INFO - src.trainer -   Best dev result: 0.8393749594688416\n","Epoch:  40% 99/250 [03:15<04:15,  1.69s/it]\n","37it [02:35, 15.15s/it]\u001b[A\n","38it [02:35, 10.67s/it]\u001b[A\n","39it [02:36,  7.54s/it]\u001b[A\n","40it [02:36,  5.35s/it]\u001b[A\n","41it [02:36,  3.81s/it]\u001b[A\n","42it [02:36,  2.74s/it]\u001b[A\n","43it [02:37,  1.99s/it]\u001b[A\n","44it [02:37,  1.46s/it]\u001b[A\n","45it [02:37,  1.09s/it]\u001b[A\n","46it [02:37,  1.20it/s]\u001b[A\n","47it [02:38,  1.53it/s]\u001b[A\n","48it [02:38,  2.02it/s]\u001b[A03/07/2022 05:25:14 - INFO - src.trainer -   Best dev result: 0.8427500128746033\n","Epoch:  50% 124/250 [04:08<03:33,  1.69s/it]\n","49it [03:28, 15.37s/it]\u001b[A\n","50it [03:28, 10.83s/it]\u001b[A\n","51it [03:28,  7.65s/it]\u001b[A\n","52it [03:28,  5.42s/it]\u001b[A\n","53it [03:29,  3.87s/it]\u001b[A\n","54it [03:29,  2.78s/it]\u001b[A\n","55it [03:29,  2.01s/it]\u001b[A\n","56it [03:29,  1.48s/it]\u001b[A\n","57it [03:30,  1.11s/it]\u001b[A\n","58it [03:30,  1.19it/s]\u001b[A\n","59it [03:30,  1.52it/s]\u001b[A\n","60it [03:30,  2.00it/s]\u001b[A03/07/2022 05:26:06 - INFO - src.trainer -   Best dev result: 0.8657500147819519\n","Epoch:  60% 149/250 [05:00<02:51,  1.70s/it]\n","61it [04:20, 15.37s/it]\u001b[A\n","62it [04:20, 10.83s/it]\u001b[A\n","63it [04:21,  7.65s/it]\u001b[A\n","64it [04:21,  5.42s/it]\u001b[A\n","65it [04:21,  3.87s/it]\u001b[A\n","66it [04:21,  2.78s/it]\u001b[A\n","67it [04:22,  2.01s/it]\u001b[A\n","68it [04:22,  1.48s/it]\u001b[A\n","69it [04:22,  1.10s/it]\u001b[A\n","70it [04:22,  1.19it/s]\u001b[A\n","71it [04:23,  1.52it/s]\u001b[A\n","Epoch:  70% 174/250 [05:45<02:06,  1.66s/it]\n","73it [05:05, 12.90s/it]\u001b[A\n","74it [05:05,  9.10s/it]\u001b[A\n","75it [05:05,  6.44s/it]\u001b[A\n","76it [05:05,  4.58s/it]\u001b[A\n","77it [05:05,  3.27s/it]\u001b[A\n","78it [05:06,  2.36s/it]\u001b[A\n","79it [05:06,  1.72s/it]\u001b[A\n","80it [05:06,  1.27s/it]\u001b[A\n","81it [05:06,  1.04it/s]\u001b[A\n","82it [05:07,  1.35it/s]\u001b[A\n","83it [05:07,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:29<01:24,  1.66s/it]\n","85it [05:49, 12.83s/it]\u001b[A\n","86it [05:49,  9.05s/it]\u001b[A\n","87it [05:49,  6.41s/it]\u001b[A\n","88it [05:49,  4.55s/it]\u001b[A\n","89it [05:50,  3.26s/it]\u001b[A\n","90it [05:50,  2.35s/it]\u001b[A\n","91it [05:50,  1.71s/it]\u001b[A\n","92it [05:50,  1.27s/it]\u001b[A\n","93it [05:51,  1.04it/s]\u001b[A\n","94it [05:51,  1.35it/s]\u001b[A\n","95it [05:51,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:13<00:43,  1.66s/it]\n","97it [06:33, 12.84s/it]\u001b[A\n","98it [06:33,  9.06s/it]\u001b[A\n","99it [06:33,  6.41s/it]\u001b[A\n","100it [06:34,  4.56s/it]\u001b[A\n","101it [06:34,  3.26s/it]\u001b[A\n","102it [06:34,  2.35s/it]\u001b[A\n","103it [06:34,  1.71s/it]\u001b[A\n","104it [06:35,  1.27s/it]\u001b[A\n","105it [06:35,  1.04it/s]\u001b[A\n","106it [06:35,  1.35it/s]\u001b[A\n","107it [06:35,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:57<00:01,  1.66s/it]\n","109it [07:17, 12.83s/it]\u001b[A\n","110it [07:17,  9.05s/it]\u001b[A\n","111it [07:17,  6.40s/it]\u001b[A\n","112it [07:18,  4.55s/it]\u001b[A\n","113it [07:18,  3.26s/it]\u001b[A\n","114it [07:18,  2.35s/it]\u001b[A\n","115it [07:18,  1.71s/it]\u001b[A\n","116it [07:19,  1.27s/it]\u001b[A\n","117it [07:19,  1.04it/s]\u001b[A\n","118it [07:19,  1.35it/s]\u001b[A\n","119it [07:19,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [08:02<00:00,  1.93s/it]\n","03/07/2022 05:29:55 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 05:30:10 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:34,  4.67s/it]\u001b[A\n","122it [07:34,  3.34s/it]\u001b[A\n","123it [07:34,  2.41s/it]\u001b[A\n","124it [07:35,  1.76s/it]\u001b[A\n","125it [07:35,  1.30s/it]\u001b[A\n","126it [07:35,  1.02it/s]\u001b[A\n","127it [07:35,  1.33it/s]\u001b[A\n","128it [07:36,  1.67it/s]\u001b[A\n","129it [07:36,  2.05it/s]\u001b[A\n","130it [07:36,  2.44it/s]\u001b[A\n","131it [07:36,  2.80it/s]\u001b[A\n","132it [07:36,  3.48it/s]\u001b[A03/07/2022 05:30:12 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 05:30:12 - INFO - __main__ -     eval_loss = 2.385420083999634\n","03/07/2022 05:30:12 - INFO - __main__ -     eval_auroc = 0.8657500147819519\n","03/07/2022 05:30:12 - INFO - __main__ -     eval_recall = 0.7200000286102295\n","03/07/2022 05:30:12 - INFO - __main__ -     eval_f1 = 0.4675324857234955\n","03/07/2022 05:30:12 - INFO - filelock -   Lock 140446800844048 acquired on log.lock\n","03/07/2022 05:30:12 - INFO - filelock -   Lock 140446800844048 released on log.lock\n","132it [07:36,  3.46s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 05:30:19 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 05:30:19 - INFO - __main__ -   Specify load the 83-th template: *cls*▁5.*mask*.*+sent_0**sep+*\n","03/07/2022 05:30:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 05:30:19 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-2018', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_05-30-19_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-2018', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 05:30:19 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 05:30:22 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:30:22 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:30:22 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 05:30:22 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:30:22 - INFO - filelock -   Lock 139992020357264 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:30:22 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 05:30:22 - INFO - filelock -   Lock 139992020357264 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:30:22 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:30:22 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:30:22 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 05:30:22 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:30:22 - INFO - filelock -   Lock 139992031086480 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:30:22 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 05:30:22 - INFO - filelock -   Lock 139992031086480 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:30:22 - INFO - src.dataset -   *** Example ***\n","03/07/2022 05:30:22 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 05:30:22 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 245, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/07/2022 05:30:22 - INFO - src.dataset -   text: <s>▁5.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 05:30:38 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 05:30:38 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 05:30:38 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 05:30:38 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 05:30:38 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 05:30:38 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 05:30:38 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/07/2022 05:31:23 - INFO - src.trainer -   Best dev result: 0.8889999389648438\n","Epoch:  20% 49/250 [01:32<05:40,  1.69s/it]\n","13it [00:52, 15.11s/it]            \u001b[A\n","14it [00:52, 10.65s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.89s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:37,  6.43s/it]\u001b[A\n","28it [01:37,  4.57s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:01<04:11,  1.66s/it]\n","37it [02:20, 12.86s/it]\u001b[A\n","38it [02:21,  9.07s/it]\u001b[A\n","39it [02:21,  6.42s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:22,  2.35s/it]\u001b[A\n","43it [02:22,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:23,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:29,  1.66s/it]\n","49it [03:05, 12.85s/it]\u001b[A\n","50it [03:05,  9.06s/it]\u001b[A\n","51it [03:05,  6.41s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:06,  3.26s/it]\u001b[A\n","54it [03:06,  2.35s/it]\u001b[A\n","55it [03:06,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:07,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:29<02:48,  1.66s/it]\n","61it [03:49, 12.85s/it]\u001b[A\n","62it [03:49,  9.06s/it]\u001b[A\n","63it [03:49,  6.41s/it]\u001b[A\n","64it [03:49,  4.56s/it]\u001b[A\n","65it [03:50,  3.26s/it]\u001b[A\n","66it [03:50,  2.35s/it]\u001b[A\n","67it [03:50,  1.72s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:51,  1.04it/s]\u001b[A\n","70it [03:51,  1.35it/s]\u001b[A\n","71it [03:51,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:13<02:06,  1.66s/it]\n","73it [04:33, 12.86s/it]\u001b[A\n","74it [04:33,  9.07s/it]\u001b[A\n","75it [04:34,  6.42s/it]\u001b[A\n","76it [04:34,  4.56s/it]\u001b[A\n","77it [04:34,  3.26s/it]\u001b[A\n","78it [04:34,  2.35s/it]\u001b[A\n","79it [04:34,  1.72s/it]\u001b[A\n","80it [04:35,  1.27s/it]\u001b[A\n","81it [04:35,  1.04it/s]\u001b[A\n","82it [04:35,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:58<01:24,  1.66s/it]\n","85it [05:17, 12.86s/it]\u001b[A\n","86it [05:18,  9.07s/it]\u001b[A\n","87it [05:18,  6.42s/it]\u001b[A\n","88it [05:18,  4.56s/it]\u001b[A\n","89it [05:18,  3.26s/it]\u001b[A\n","90it [05:18,  2.35s/it]\u001b[A\n","91it [05:19,  1.72s/it]\u001b[A\n","92it [05:19,  1.27s/it]\u001b[A\n","93it [05:19,  1.04it/s]\u001b[A\n","94it [05:19,  1.35it/s]\u001b[A\n","95it [05:20,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:42<00:43,  1.66s/it]\n","97it [06:02, 12.86s/it]\u001b[A\n","98it [06:02,  9.07s/it]\u001b[A\n","99it [06:02,  6.42s/it]\u001b[A\n","100it [06:02,  4.56s/it]\u001b[A\n","101it [06:02,  3.26s/it]\u001b[A\n","102it [06:03,  2.35s/it]\u001b[A\n","103it [06:03,  1.72s/it]\u001b[A\n","104it [06:03,  1.27s/it]\u001b[A\n","105it [06:03,  1.04it/s]\u001b[A\n","106it [06:04,  1.35it/s]\u001b[A\n","107it [06:04,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:26<00:01,  1.66s/it]\n","109it [06:46, 12.85s/it]\u001b[A\n","110it [06:46,  9.06s/it]\u001b[A\n","111it [06:46,  6.41s/it]\u001b[A\n","112it [06:46,  4.56s/it]\u001b[A\n","113it [06:47,  3.26s/it]\u001b[A\n","114it [06:47,  2.35s/it]\u001b[A\n","115it [06:47,  1.72s/it]\u001b[A\n","116it [06:47,  1.27s/it]\u001b[A\n","117it [06:48,  1.04it/s]\u001b[A\n","118it [06:48,  1.35it/s]\u001b[A\n","119it [06:48,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:30<00:00,  1.80s/it]\n","03/07/2022 05:38:09 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 05:38:22 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:02,  4.31s/it]\u001b[A\n","122it [07:02,  3.09s/it]\u001b[A\n","123it [07:02,  2.23s/it]\u001b[A\n","124it [07:02,  1.63s/it]\u001b[A\n","125it [07:02,  1.21s/it]\u001b[A\n","126it [07:03,  1.09it/s]\u001b[A\n","127it [07:03,  1.41it/s]\u001b[A\n","128it [07:03,  1.76it/s]\u001b[A\n","129it [07:03,  2.14it/s]\u001b[A\n","130it [07:04,  2.53it/s]\u001b[A\n","131it [07:04,  2.89it/s]\u001b[A\n","132it [07:04,  3.57it/s]\u001b[A03/07/2022 05:38:25 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 05:38:25 - INFO - __main__ -     eval_loss = 2.9716975688934326\n","03/07/2022 05:38:25 - INFO - __main__ -     eval_auroc = 0.8889999389648438\n","03/07/2022 05:38:25 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/07/2022 05:38:25 - INFO - __main__ -     eval_f1 = 0.40336135029792786\n","03/07/2022 05:38:25 - INFO - filelock -   Lock 139992340851408 acquired on log.lock\n","03/07/2022 05:38:25 - INFO - filelock -   Lock 139992340851408 released on log.lock\n","132it [07:04,  3.22s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 05:38:31 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 05:38:31 - INFO - __main__ -   Specify load the 84-th template: *cls*▁Just*mask*.*+sent_0**sep+*\n","03/07/2022 05:38:31 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 05:38:31 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-23894', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_05-38-31_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-23894', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 05:38:31 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 05:38:34 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:38:34 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:38:34 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 05:38:34 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:38:34 - INFO - filelock -   Lock 139992795787472 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:38:34 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 05:38:34 - INFO - filelock -   Lock 139992795787472 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:38:34 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:38:34 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:38:34 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 05:38:34 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:38:34 - INFO - filelock -   Lock 139992795872912 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:38:34 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 05:38:34 - INFO - filelock -   Lock 139992795872912 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:38:34 - INFO - src.dataset -   *** Example ***\n","03/07/2022 05:38:34 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 05:38:34 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 6785, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/07/2022 05:38:34 - INFO - src.dataset -   text: <s>▁Just<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 05:38:50 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 05:38:50 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 05:38:50 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 05:38:50 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 05:38:50 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 05:38:50 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 05:38:50 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/07/2022 05:39:34 - INFO - src.trainer -   Best dev result: 0.9513749480247498\n","Epoch:  20% 49/250 [01:32<05:40,  1.70s/it]\n","13it [00:52, 15.11s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:53,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/07/2022 05:40:27 - INFO - src.trainer -   Best dev result: 0.9645000100135803\n","Epoch:  30% 74/250 [02:24<04:57,  1.69s/it]\n","25it [01:43, 15.08s/it]\u001b[A\n","26it [01:44, 10.62s/it]\u001b[A\n","27it [01:44,  7.51s/it]\u001b[A\n","28it [01:44,  5.32s/it]\u001b[A\n","29it [01:44,  3.80s/it]\u001b[A\n","30it [01:45,  2.73s/it]\u001b[A\n","31it [01:45,  1.98s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:45,  1.20it/s]\u001b[A\n","35it [01:46,  1.54it/s]\u001b[A\n","36it [01:46,  2.03it/s]\u001b[A03/07/2022 05:41:18 - INFO - src.trainer -   Best dev result: 0.968625009059906\n","Epoch:  40% 99/250 [03:16<04:16,  1.70s/it]\n","37it [02:36, 15.37s/it]\u001b[A\n","38it [02:36, 10.83s/it]\u001b[A\n","39it [02:36,  7.65s/it]\u001b[A\n","40it [02:37,  5.42s/it]\u001b[A\n","41it [02:37,  3.86s/it]\u001b[A\n","42it [02:37,  2.77s/it]\u001b[A\n","43it [02:37,  2.01s/it]\u001b[A\n","44it [02:38,  1.48s/it]\u001b[A\n","45it [02:38,  1.10s/it]\u001b[A\n","46it [02:38,  1.19it/s]\u001b[A\n","47it [02:38,  1.52it/s]\u001b[A\n","48it [02:38,  2.00it/s]\u001b[A03/07/2022 05:42:11 - INFO - src.trainer -   Best dev result: 0.9722500443458557\n","Epoch:  50% 124/250 [04:09<03:34,  1.71s/it]\n","49it [03:29, 15.42s/it]\u001b[A\n","50it [03:29, 10.87s/it]\u001b[A\n","51it [03:29,  7.68s/it]\u001b[A\n","52it [03:29,  5.44s/it]\u001b[A\n","53it [03:30,  3.88s/it]\u001b[A\n","54it [03:30,  2.79s/it]\u001b[A\n","55it [03:30,  2.02s/it]\u001b[A\n","56it [03:30,  1.48s/it]\u001b[A\n","57it [03:30,  1.11s/it]\u001b[A\n","58it [03:31,  1.18it/s]\u001b[A\n","59it [03:31,  1.51it/s]\u001b[A\n","Epoch:  60% 149/250 [04:53<02:47,  1.66s/it]\n","61it [04:13, 12.91s/it]\u001b[A\n","62it [04:13,  9.10s/it]\u001b[A\n","63it [04:13,  6.44s/it]\u001b[A\n","64it [04:14,  4.58s/it]\u001b[A\n","65it [04:14,  3.27s/it]\u001b[A\n","66it [04:14,  2.36s/it]\u001b[A\n","67it [04:14,  1.72s/it]\u001b[A\n","68it [04:14,  1.27s/it]\u001b[A\n","69it [04:15,  1.04it/s]\u001b[A\n","70it [04:15,  1.35it/s]\u001b[A\n","71it [04:15,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:37<02:06,  1.66s/it]\n","73it [04:57, 12.86s/it]\u001b[A\n","74it [04:57,  9.07s/it]\u001b[A\n","75it [04:58,  6.42s/it]\u001b[A\n","76it [04:58,  4.56s/it]\u001b[A\n","77it [04:58,  3.26s/it]\u001b[A\n","78it [04:58,  2.35s/it]\u001b[A\n","79it [04:59,  1.72s/it]\u001b[A\n","80it [04:59,  1.27s/it]\u001b[A\n","81it [04:59,  1.04it/s]\u001b[A\n","82it [04:59,  1.35it/s]\u001b[A\n","83it [04:59,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:22<01:24,  1.66s/it]\n","85it [05:41, 12.85s/it]\u001b[A\n","86it [05:42,  9.07s/it]\u001b[A\n","87it [05:42,  6.42s/it]\u001b[A\n","88it [05:42,  4.56s/it]\u001b[A\n","89it [05:42,  3.26s/it]\u001b[A\n","90it [05:43,  2.35s/it]\u001b[A\n","91it [05:43,  1.72s/it]\u001b[A\n","92it [05:43,  1.27s/it]\u001b[A\n","93it [05:43,  1.04it/s]\u001b[A\n","94it [05:43,  1.35it/s]\u001b[A\n","95it [05:44,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:06<00:43,  1.66s/it]\n","97it [06:26, 12.86s/it]\u001b[A\n","98it [06:26,  9.07s/it]\u001b[A\n","99it [06:26,  6.42s/it]\u001b[A\n","100it [06:26,  4.56s/it]\u001b[A\n","101it [06:27,  3.26s/it]\u001b[A\n","102it [06:27,  2.35s/it]\u001b[A\n","103it [06:27,  1.72s/it]\u001b[A\n","104it [06:27,  1.27s/it]\u001b[A\n","105it [06:27,  1.04it/s]\u001b[A\n","106it [06:28,  1.35it/s]\u001b[A\n","107it [06:28,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:50<00:01,  1.66s/it]\n","109it [07:10, 12.85s/it]\u001b[A\n","110it [07:10,  9.07s/it]\u001b[A\n","111it [07:10,  6.42s/it]\u001b[A\n","112it [07:11,  4.56s/it]\u001b[A\n","113it [07:11,  3.26s/it]\u001b[A\n","114it [07:11,  2.35s/it]\u001b[A\n","115it [07:11,  1.72s/it]\u001b[A\n","116it [07:11,  1.27s/it]\u001b[A\n","117it [07:12,  1.04it/s]\u001b[A\n","118it [07:12,  1.35it/s]\u001b[A\n","119it [07:12,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:54<00:00,  1.90s/it]\n","03/07/2022 05:46:45 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 05:46:59 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:27,  4.60s/it]\u001b[A\n","122it [07:27,  3.29s/it]\u001b[A\n","123it [07:27,  2.37s/it]\u001b[A\n","124it [07:27,  1.73s/it]\u001b[A\n","125it [07:27,  1.28s/it]\u001b[A\n","126it [07:28,  1.04it/s]\u001b[A\n","127it [07:28,  1.34it/s]\u001b[A\n","128it [07:28,  1.69it/s]\u001b[A\n","129it [07:28,  2.07it/s]\u001b[A\n","130it [07:29,  2.45it/s]\u001b[A\n","131it [07:29,  2.82it/s]\u001b[A\n","132it [07:29,  3.50it/s]\u001b[A03/07/2022 05:47:02 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 05:47:02 - INFO - __main__ -     eval_loss = 1.7998671531677246\n","03/07/2022 05:47:02 - INFO - __main__ -     eval_auroc = 0.9722500443458557\n","03/07/2022 05:47:02 - INFO - __main__ -     eval_recall = 1.0\n","03/07/2022 05:47:02 - INFO - __main__ -     eval_f1 = 0.5882353186607361\n","03/07/2022 05:47:02 - INFO - filelock -   Lock 139992796199952 acquired on log.lock\n","03/07/2022 05:47:02 - INFO - filelock -   Lock 139992796199952 released on log.lock\n","132it [07:29,  3.41s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 05:47:08 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 05:47:08 - INFO - __main__ -   Specify load the 85-th template: *cls*)*mask*.*+sent_0**sep+*\n","03/07/2022 05:47:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 05:47:08 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-16841', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_05-47-08_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-16841', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 05:47:08 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 05:47:11 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:47:11 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:47:11 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 05:47:11 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:47:11 - INFO - filelock -   Lock 139998235901520 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:47:11 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 05:47:11 - INFO - filelock -   Lock 139998235901520 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:47:11 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:47:11 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:47:11 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 05:47:11 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:47:11 - INFO - filelock -   Lock 139998281473168 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:47:11 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 05:47:11 - INFO - filelock -   Lock 139998281473168 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:47:11 - INFO - src.dataset -   *** Example ***\n","03/07/2022 05:47:11 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 05:47:11 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 43, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[2], label_word_list=None)\n","03/07/2022 05:47:11 - INFO - src.dataset -   text: <s>)<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 05:47:26 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 05:47:26 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 05:47:26 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 05:47:26 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 05:47:26 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 05:47:26 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 05:47:26 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/07/2022 05:48:11 - INFO - src.trainer -   Best dev result: 0.9309999942779541\n","Epoch:  20% 49/250 [01:31<05:39,  1.69s/it]\n","13it [00:51, 14.76s/it]            \u001b[A\n","14it [00:51, 10.40s/it]\u001b[A\n","15it [00:51,  7.35s/it]\u001b[A\n","16it [00:51,  5.22s/it]\u001b[A\n","17it [00:52,  3.72s/it]\u001b[A\n","18it [00:52,  2.67s/it]\u001b[A\n","19it [00:52,  1.94s/it]\u001b[A\n","20it [00:52,  1.43s/it]\u001b[A\n","21it [00:53,  1.07s/it]\u001b[A\n","22it [00:53,  1.22it/s]\u001b[A\n","23it [00:53,  1.56it/s]\u001b[A\n","Epoch:  30% 74/250 [02:15<04:51,  1.66s/it]\n","25it [01:35, 12.84s/it]\u001b[A\n","26it [01:35,  9.06s/it]\u001b[A\n","27it [01:35,  6.41s/it]\u001b[A\n","28it [01:35,  4.56s/it]\u001b[A\n","29it [01:36,  3.26s/it]\u001b[A\n","30it [01:36,  2.35s/it]\u001b[A\n","31it [01:36,  1.72s/it]\u001b[A\n","32it [01:36,  1.27s/it]\u001b[A\n","33it [01:37,  1.04it/s]\u001b[A\n","34it [01:37,  1.35it/s]\u001b[A\n","35it [01:37,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [02:59<04:10,  1.66s/it]\n","37it [02:19, 12.83s/it]\u001b[A\n","38it [02:19,  9.05s/it]\u001b[A\n","39it [02:19,  6.40s/it]\u001b[A\n","40it [02:20,  4.55s/it]\u001b[A\n","41it [02:20,  3.26s/it]\u001b[A\n","42it [02:20,  2.35s/it]\u001b[A\n","43it [02:20,  1.71s/it]\u001b[A\n","44it [02:21,  1.27s/it]\u001b[A\n","45it [02:21,  1.04it/s]\u001b[A\n","46it [02:21,  1.35it/s]\u001b[A\n","47it [02:21,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:43<03:28,  1.66s/it]\n","49it [03:03, 12.83s/it]\u001b[A\n","50it [03:03,  9.05s/it]\u001b[A\n","51it [03:04,  6.40s/it]\u001b[A\n","52it [03:04,  4.55s/it]\u001b[A\n","53it [03:04,  3.26s/it]\u001b[A\n","54it [03:04,  2.35s/it]\u001b[A\n","55it [03:04,  1.71s/it]\u001b[A\n","56it [03:05,  1.27s/it]\u001b[A\n","57it [03:05,  1.04it/s]\u001b[A\n","58it [03:05,  1.35it/s]\u001b[A\n","59it [03:05,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:27<02:47,  1.66s/it]\n","61it [03:47, 12.82s/it]\u001b[A\n","62it [03:47,  9.04s/it]\u001b[A\n","63it [03:48,  6.40s/it]\u001b[A\n","64it [03:48,  4.55s/it]\u001b[A\n","65it [03:48,  3.25s/it]\u001b[A\n","66it [03:48,  2.35s/it]\u001b[A\n","67it [03:49,  1.71s/it]\u001b[A\n","68it [03:49,  1.27s/it]\u001b[A\n","69it [03:49,  1.04it/s]\u001b[A\n","70it [03:49,  1.35it/s]\u001b[A\n","71it [03:50,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:12<02:06,  1.66s/it]\n","73it [04:31, 12.82s/it]\u001b[A\n","74it [04:32,  9.05s/it]\u001b[A\n","75it [04:32,  6.40s/it]\u001b[A\n","76it [04:32,  4.55s/it]\u001b[A\n","77it [04:32,  3.25s/it]\u001b[A\n","78it [04:33,  2.35s/it]\u001b[A\n","79it [04:33,  1.71s/it]\u001b[A\n","80it [04:33,  1.27s/it]\u001b[A\n","81it [04:33,  1.04it/s]\u001b[A\n","82it [04:33,  1.35it/s]\u001b[A\n","83it [04:34,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:56<01:24,  1.66s/it]\n","85it [05:16, 12.85s/it]\u001b[A\n","86it [05:16,  9.07s/it]\u001b[A\n","87it [05:16,  6.42s/it]\u001b[A\n","88it [05:16,  4.56s/it]\u001b[A\n","89it [05:17,  3.26s/it]\u001b[A\n","90it [05:17,  2.35s/it]\u001b[A\n","91it [05:17,  1.72s/it]\u001b[A\n","92it [05:17,  1.27s/it]\u001b[A\n","93it [05:17,  1.04it/s]\u001b[A\n","94it [05:18,  1.35it/s]\u001b[A\n","95it [05:18,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:40<00:43,  1.66s/it]\n","97it [06:00, 12.82s/it]\u001b[A\n","98it [06:00,  9.05s/it]\u001b[A\n","99it [06:00,  6.40s/it]\u001b[A\n","100it [06:00,  4.55s/it]\u001b[A\n","101it [06:01,  3.26s/it]\u001b[A\n","102it [06:01,  2.35s/it]\u001b[A\n","103it [06:01,  1.71s/it]\u001b[A\n","104it [06:01,  1.27s/it]\u001b[A\n","105it [06:02,  1.04it/s]\u001b[A\n","106it [06:02,  1.35it/s]\u001b[A\n","107it [06:02,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:24<00:01,  1.66s/it]\n","109it [06:44, 12.82s/it]\u001b[A\n","110it [06:44,  9.04s/it]\u001b[A\n","111it [06:44,  6.40s/it]\u001b[A\n","112it [06:45,  4.55s/it]\u001b[A\n","113it [06:45,  3.25s/it]\u001b[A\n","114it [06:45,  2.35s/it]\u001b[A\n","115it [06:45,  1.71s/it]\u001b[A\n","116it [06:45,  1.27s/it]\u001b[A\n","117it [06:46,  1.04it/s]\u001b[A\n","118it [06:46,  1.35it/s]\u001b[A\n","119it [06:46,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:28<00:00,  1.80s/it]\n","03/07/2022 05:54:55 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 05:55:08 - INFO - __main__ -   *** Validate ***\n","\n","121it [06:59,  4.26s/it]\u001b[A\n","122it [07:00,  3.05s/it]\u001b[A\n","123it [07:00,  2.21s/it]\u001b[A\n","124it [07:00,  1.61s/it]\u001b[A\n","125it [07:00,  1.20s/it]\u001b[A\n","126it [07:01,  1.10it/s]\u001b[A\n","127it [07:01,  1.42it/s]\u001b[A\n","128it [07:01,  1.77it/s]\u001b[A\n","129it [07:01,  2.15it/s]\u001b[A\n","130it [07:02,  2.54it/s]\u001b[A\n","131it [07:02,  2.89it/s]\u001b[A\n","132it [07:02,  3.58it/s]\u001b[A03/07/2022 05:55:11 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 05:55:11 - INFO - __main__ -     eval_loss = 1.9986597299575806\n","03/07/2022 05:55:11 - INFO - __main__ -     eval_auroc = 0.9309999942779541\n","03/07/2022 05:55:11 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/07/2022 05:55:11 - INFO - __main__ -     eval_f1 = 0.5274725556373596\n","03/07/2022 05:55:11 - INFO - filelock -   Lock 139998239816784 acquired on log.lock\n","03/07/2022 05:55:11 - INFO - filelock -   Lock 139998239816784 released on log.lock\n","132it [07:02,  3.20s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 05:55:17 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 05:55:17 - INFO - __main__ -   Specify load the 86-th template: *cls*▁A.*mask*.*+sent_0**sep+*\n","03/07/2022 05:55:17 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 05:55:17 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-29210', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_05-55-17_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-29210', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 05:55:17 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 05:55:20 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:55:20 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:55:20 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 05:55:20 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:55:20 - INFO - filelock -   Lock 140404184071504 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:55:20 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 05:55:20 - INFO - filelock -   Lock 140404184071504 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:55:20 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 05:55:20 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 05:55:20 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 05:55:20 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 05:55:20 - INFO - filelock -   Lock 140404155919568 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:55:20 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 05:55:20 - INFO - filelock -   Lock 140404155919568 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 05:55:20 - INFO - src.dataset -   *** Example ***\n","03/07/2022 05:55:20 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 05:55:20 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 250, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/07/2022 05:55:20 - INFO - src.dataset -   text: <s>▁A.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 05:55:35 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 05:55:35 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 05:55:35 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 05:55:35 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 05:55:35 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 05:55:35 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 05:55:35 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/07/2022 05:56:20 - INFO - src.trainer -   Best dev result: 0.8602500557899475\n","Epoch:  20% 49/250 [01:32<05:41,  1.70s/it]\n","13it [00:52, 15.11s/it]            \u001b[A\n","14it [00:52, 10.65s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.89s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:37,  6.43s/it]\u001b[A\n","28it [01:37,  4.57s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:38,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:01<04:11,  1.66s/it]\n","37it [02:20, 12.86s/it]\u001b[A\n","38it [02:21,  9.07s/it]\u001b[A\n","39it [02:21,  6.42s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:22,  2.35s/it]\u001b[A\n","43it [02:22,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:23,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:29,  1.66s/it]\n","49it [03:05, 12.86s/it]\u001b[A\n","50it [03:05,  9.07s/it]\u001b[A\n","51it [03:05,  6.42s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:06,  3.26s/it]\u001b[A\n","54it [03:06,  2.35s/it]\u001b[A\n","55it [03:06,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:07,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:29<02:48,  1.67s/it]\n","61it [03:49, 12.86s/it]\u001b[A\n","62it [03:49,  9.07s/it]\u001b[A\n","63it [03:49,  6.42s/it]\u001b[A\n","64it [03:50,  4.56s/it]\u001b[A\n","65it [03:50,  3.26s/it]\u001b[A\n","66it [03:50,  2.35s/it]\u001b[A\n","67it [03:50,  1.72s/it]\u001b[A\n","68it [03:51,  1.27s/it]\u001b[A\n","69it [03:51,  1.04it/s]\u001b[A\n","70it [03:51,  1.35it/s]\u001b[A\n","71it [03:51,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:13<02:06,  1.66s/it]\n","73it [04:33, 12.86s/it]\u001b[A\n","74it [04:33,  9.07s/it]\u001b[A\n","75it [04:34,  6.42s/it]\u001b[A\n","76it [04:34,  4.56s/it]\u001b[A\n","77it [04:34,  3.26s/it]\u001b[A\n","78it [04:34,  2.35s/it]\u001b[A\n","79it [04:35,  1.72s/it]\u001b[A\n","80it [04:35,  1.27s/it]\u001b[A\n","81it [04:35,  1.04it/s]\u001b[A\n","82it [04:35,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:58<01:24,  1.66s/it]\n","85it [05:17, 12.85s/it]\u001b[A\n","86it [05:18,  9.06s/it]\u001b[A\n","87it [05:18,  6.41s/it]\u001b[A\n","88it [05:18,  4.56s/it]\u001b[A\n","89it [05:18,  3.26s/it]\u001b[A\n","90it [05:19,  2.35s/it]\u001b[A\n","91it [05:19,  1.72s/it]\u001b[A\n","92it [05:19,  1.27s/it]\u001b[A\n","93it [05:19,  1.04it/s]\u001b[A\n","94it [05:19,  1.35it/s]\u001b[A\n","95it [05:20,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:42<00:43,  1.66s/it]\n","97it [06:02, 12.85s/it]\u001b[A\n","98it [06:02,  9.06s/it]\u001b[A\n","99it [06:02,  6.41s/it]\u001b[A\n","100it [06:02,  4.56s/it]\u001b[A\n","101it [06:02,  3.26s/it]\u001b[A\n","102it [06:03,  2.35s/it]\u001b[A\n","103it [06:03,  1.72s/it]\u001b[A\n","104it [06:03,  1.27s/it]\u001b[A\n","105it [06:03,  1.04it/s]\u001b[A\n","106it [06:04,  1.35it/s]\u001b[A\n","107it [06:04,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:26<00:01,  1.66s/it]\n","109it [06:46, 12.85s/it]\u001b[A\n","110it [06:46,  9.07s/it]\u001b[A\n","111it [06:46,  6.42s/it]\u001b[A\n","112it [06:46,  4.56s/it]\u001b[A\n","113it [06:47,  3.26s/it]\u001b[A\n","114it [06:47,  2.35s/it]\u001b[A\n","115it [06:47,  1.72s/it]\u001b[A\n","116it [06:47,  1.27s/it]\u001b[A\n","117it [06:48,  1.04it/s]\u001b[A\n","118it [06:48,  1.35it/s]\u001b[A\n","119it [06:48,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:30<00:00,  1.80s/it]\n","03/07/2022 06:03:06 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 06:03:19 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:02,  4.31s/it]\u001b[A\n","122it [07:02,  3.09s/it]\u001b[A\n","123it [07:02,  2.23s/it]\u001b[A\n","124it [07:02,  1.63s/it]\u001b[A\n","125it [07:02,  1.21s/it]\u001b[A\n","126it [07:03,  1.09it/s]\u001b[A\n","127it [07:03,  1.41it/s]\u001b[A\n","128it [07:03,  1.76it/s]\u001b[A\n","129it [07:03,  2.14it/s]\u001b[A\n","130it [07:04,  2.53it/s]\u001b[A\n","131it [07:04,  2.89it/s]\u001b[A\n","132it [07:04,  3.57it/s]\u001b[A03/07/2022 06:03:22 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 06:03:22 - INFO - __main__ -     eval_loss = 3.1228644847869873\n","03/07/2022 06:03:22 - INFO - __main__ -     eval_auroc = 0.8602500557899475\n","03/07/2022 06:03:22 - INFO - __main__ -     eval_recall = 0.8399999737739563\n","03/07/2022 06:03:22 - INFO - __main__ -     eval_f1 = 0.4329896867275238\n","03/07/2022 06:03:22 - INFO - filelock -   Lock 140404183676240 acquired on log.lock\n","03/07/2022 06:03:22 - INFO - filelock -   Lock 140404183676240 released on log.lock\n","132it [07:04,  3.22s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 06:03:28 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 06:03:28 - INFO - __main__ -   Specify load the 87-th template: *cls*▁Thanks.*mask*.*+sent_0**sep+*\n","03/07/2022 06:03:28 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 06:03:28 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-14136', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_06-03-28_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-14136', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 06:03:28 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 06:03:31 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:03:31 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:03:31 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 06:03:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:03:31 - INFO - filelock -   Lock 140291727484112 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:03:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 06:03:31 - INFO - filelock -   Lock 140291727484112 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:03:31 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:03:31 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:03:31 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 06:03:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:03:31 - INFO - filelock -   Lock 140291699735504 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:03:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/07/2022 06:03:31 - INFO - filelock -   Lock 140291699735504 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:03:31 - INFO - src.dataset -   *** Example ***\n","03/07/2022 06:03:31 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 06:03:31 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 22086, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/07/2022 06:03:31 - INFO - src.dataset -   text: <s>▁Thanks.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 06:03:47 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 06:03:47 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 06:03:47 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 06:03:47 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 06:03:47 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 06:03:47 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 06:03:47 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 06:04:31 - INFO - src.trainer -   Best dev result: 0.8538749814033508\n","Epoch:  20% 49/250 [01:31<05:39,  1.69s/it]\n","13it [00:51, 14.92s/it]            \u001b[A\n","14it [00:51, 10.51s/it]\u001b[A\n","15it [00:52,  7.43s/it]\u001b[A\n","16it [00:52,  5.27s/it]\u001b[A\n","17it [00:52,  3.76s/it]\u001b[A\n","18it [00:52,  2.70s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.55it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:35, 12.89s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:36,  6.43s/it]\u001b[A\n","28it [01:36,  4.57s/it]\u001b[A\n","29it [01:36,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:20, 12.81s/it]\u001b[A\n","38it [02:20,  9.03s/it]\u001b[A\n","39it [02:20,  6.39s/it]\u001b[A\n","40it [02:20,  4.54s/it]\u001b[A\n","41it [02:20,  3.25s/it]\u001b[A\n","42it [02:21,  2.34s/it]\u001b[A\n","43it [02:21,  1.71s/it]\u001b[A\n","44it [02:21,  1.27s/it]\u001b[A\n","45it [02:21,  1.05it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.71it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:29,  1.66s/it]\n","49it [03:04, 12.80s/it]\u001b[A\n","50it [03:04,  9.03s/it]\u001b[A\n","51it [03:04,  6.39s/it]\u001b[A\n","52it [03:04,  4.54s/it]\u001b[A\n","53it [03:05,  3.25s/it]\u001b[A\n","54it [03:05,  2.34s/it]\u001b[A\n","55it [03:05,  1.71s/it]\u001b[A\n","56it [03:05,  1.27s/it]\u001b[A\n","57it [03:05,  1.05it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.71it/s]\u001b[A\n","60it [03:06,  2.23it/s]\u001b[A03/07/2022 06:07:35 - INFO - src.trainer -   Best dev result: 0.8542500138282776\n","Epoch:  60% 149/250 [04:35<02:50,  1.69s/it]\n","61it [03:55, 15.03s/it]\u001b[A\n","62it [03:55, 10.59s/it]\u001b[A\n","63it [03:56,  7.48s/it]\u001b[A\n","64it [03:56,  5.31s/it]\u001b[A\n","65it [03:56,  3.78s/it]\u001b[A\n","66it [03:56,  2.72s/it]\u001b[A\n","67it [03:57,  1.97s/it]\u001b[A\n","68it [03:57,  1.45s/it]\u001b[A\n","69it [03:57,  1.08s/it]\u001b[A\n","70it [03:57,  1.21it/s]\u001b[A\n","71it [03:57,  1.54it/s]\u001b[A\n","72it [03:58,  2.03it/s]\u001b[A03/07/2022 06:08:27 - INFO - src.trainer -   Best dev result: 0.8563750386238098\n","Epoch:  70% 174/250 [05:27<02:08,  1.68s/it]\n","73it [04:47, 15.21s/it]\u001b[A\n","74it [04:47, 10.72s/it]\u001b[A\n","75it [04:48,  7.57s/it]\u001b[A\n","76it [04:48,  5.37s/it]\u001b[A\n","77it [04:48,  3.83s/it]\u001b[A\n","78it [04:48,  2.75s/it]\u001b[A\n","79it [04:48,  1.99s/it]\u001b[A\n","80it [04:49,  1.46s/it]\u001b[A\n","81it [04:49,  1.09s/it]\u001b[A\n","82it [04:49,  1.20it/s]\u001b[A\n","83it [04:49,  1.53it/s]\u001b[A\n","Epoch:  80% 199/250 [06:11<01:24,  1.65s/it]\n","85it [05:31, 12.84s/it]\u001b[A\n","86it [05:31,  9.05s/it]\u001b[A\n","87it [05:32,  6.41s/it]\u001b[A\n","88it [05:32,  4.55s/it]\u001b[A\n","89it [05:32,  3.26s/it]\u001b[A\n","90it [05:32,  2.35s/it]\u001b[A\n","91it [05:33,  1.71s/it]\u001b[A\n","92it [05:33,  1.27s/it]\u001b[A\n","93it [05:33,  1.04it/s]\u001b[A\n","94it [05:33,  1.35it/s]\u001b[A\n","95it [05:33,  1.70it/s]\u001b[A\n","96it [05:34,  2.23it/s]\u001b[A03/07/2022 06:10:03 - INFO - src.trainer -   Best dev result: 0.8566250205039978\n","Epoch:  90% 224/250 [07:03<00:44,  1.69s/it]\n","97it [06:23, 15.11s/it]\u001b[A\n","98it [06:23, 10.65s/it]\u001b[A\n","99it [06:23,  7.52s/it]\u001b[A\n","100it [06:24,  5.33s/it]\u001b[A\n","101it [06:24,  3.80s/it]\u001b[A\n","102it [06:24,  2.73s/it]\u001b[A\n","103it [06:24,  1.98s/it]\u001b[A\n","104it [06:25,  1.46s/it]\u001b[A\n","105it [06:25,  1.09s/it]\u001b[A\n","106it [06:25,  1.20it/s]\u001b[A\n","107it [06:25,  1.54it/s]\u001b[A\n","Epoch: 100% 249/250 [07:47<00:01,  1.66s/it]\n","109it [07:07, 12.84s/it]\u001b[A\n","110it [07:07,  9.06s/it]\u001b[A\n","111it [07:07,  6.41s/it]\u001b[A\n","112it [07:08,  4.56s/it]\u001b[A\n","113it [07:08,  3.26s/it]\u001b[A\n","114it [07:08,  2.35s/it]\u001b[A\n","115it [07:08,  1.71s/it]\u001b[A\n","116it [07:09,  1.27s/it]\u001b[A\n","117it [07:09,  1.04it/s]\u001b[A\n","118it [07:09,  1.35it/s]\u001b[A\n","119it [07:09,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:52<00:00,  1.89s/it]\n","03/07/2022 06:11:39 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 06:11:53 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:24,  4.65s/it]\u001b[A\n","122it [07:24,  3.33s/it]\u001b[A\n","123it [07:24,  2.40s/it]\u001b[A\n","124it [07:25,  1.75s/it]\u001b[A\n","125it [07:25,  1.29s/it]\u001b[A\n","126it [07:25,  1.03it/s]\u001b[A\n","127it [07:25,  1.33it/s]\u001b[A\n","128it [07:26,  1.68it/s]\u001b[A\n","129it [07:26,  2.06it/s]\u001b[A\n","130it [07:26,  2.44it/s]\u001b[A\n","131it [07:26,  2.81it/s]\u001b[A\n","132it [07:26,  3.49it/s]\u001b[A03/07/2022 06:11:55 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 06:11:55 - INFO - __main__ -     eval_loss = 4.24360990524292\n","03/07/2022 06:11:55 - INFO - __main__ -     eval_auroc = 0.8566250205039978\n","03/07/2022 06:11:55 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/07/2022 06:11:55 - INFO - __main__ -     eval_f1 = 0.40677961707115173\n","03/07/2022 06:11:55 - INFO - filelock -   Lock 140291727789776 acquired on log.lock\n","03/07/2022 06:11:55 - INFO - filelock -   Lock 140291727789776 released on log.lock\n","132it [07:26,  3.39s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 06:12:01 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 06:12:01 - INFO - __main__ -   Specify load the 88-th template: *cls*▁Ah*mask*!*+sent_0**sep+*\n","03/07/2022 06:12:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 06:12:01 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-23666', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_06-12-01_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-23666', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 06:12:01 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 06:12:05 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:12:05 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:12:05 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 06:12:05 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:12:05 - INFO - filelock -   Lock 140292300904784 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:12:05 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 06:12:05 - INFO - filelock -   Lock 140292300904784 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:12:05 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:12:05 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:12:05 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 06:12:05 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:12:05 - INFO - filelock -   Lock 140292286669392 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:12:05 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 06:12:05 - INFO - filelock -   Lock 140292286669392 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:12:05 - INFO - src.dataset -   *** Example ***\n","03/07/2022 06:12:05 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 06:12:05 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 17986, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/07/2022 06:12:05 - INFO - src.dataset -   text: <s>▁Ah<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 06:12:20 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 06:12:20 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 06:12:20 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 06:12:20 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 06:12:20 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 06:12:20 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 06:12:20 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:39<06:13,  1.65s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.68it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.93it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.52it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 06:13:04 - INFO - src.trainer -   Best dev result: 0.8985000252723694\n","Epoch:  20% 49/250 [01:32<05:40,  1.69s/it]\n","13it [00:52, 15.11s/it]            \u001b[A\n","14it [00:52, 10.65s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:51,  1.65s/it]\n","25it [01:36, 12.83s/it]\u001b[A\n","26it [01:36,  9.05s/it]\u001b[A\n","27it [01:36,  6.40s/it]\u001b[A\n","28it [01:37,  4.55s/it]\u001b[A\n","29it [01:37,  3.26s/it]\u001b[A\n","30it [01:37,  2.35s/it]\u001b[A\n","31it [01:37,  1.71s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:09,  1.65s/it]\n","37it [02:20, 12.79s/it]\u001b[A\n","38it [02:20,  9.02s/it]\u001b[A\n","39it [02:20,  6.38s/it]\u001b[A\n","40it [02:21,  4.54s/it]\u001b[A\n","41it [02:21,  3.25s/it]\u001b[A\n","42it [02:21,  2.34s/it]\u001b[A\n","43it [02:21,  1.71s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.05it/s]\u001b[A\n","46it [02:22,  1.36it/s]\u001b[A\n","47it [02:22,  1.71it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:28,  1.66s/it]\n","49it [03:04, 12.80s/it]\u001b[A\n","50it [03:04,  9.03s/it]\u001b[A\n","51it [03:04,  6.39s/it]\u001b[A\n","52it [03:05,  4.54s/it]\u001b[A\n","53it [03:05,  3.25s/it]\u001b[A\n","54it [03:05,  2.34s/it]\u001b[A\n","55it [03:05,  1.71s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.05it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.71it/s]\u001b[A\n","60it [03:06,  2.23it/s]\u001b[A03/07/2022 06:16:09 - INFO - src.trainer -   Best dev result: 0.9044999480247498\n","Epoch:  60% 149/250 [04:35<02:51,  1.69s/it]\n","61it [03:55, 14.97s/it]\u001b[A\n","62it [03:56, 10.55s/it]\u001b[A\n","63it [03:56,  7.45s/it]\u001b[A\n","64it [03:56,  5.29s/it]\u001b[A\n","65it [03:56,  3.77s/it]\u001b[A\n","66it [03:56,  2.71s/it]\u001b[A\n","67it [03:57,  1.97s/it]\u001b[A\n","68it [03:57,  1.45s/it]\u001b[A\n","69it [03:57,  1.08s/it]\u001b[A\n","70it [03:57,  1.21it/s]\u001b[A\n","71it [03:58,  1.54it/s]\u001b[A\n","Epoch:  70% 174/250 [05:19<02:05,  1.66s/it]\n","73it [04:39, 12.83s/it]\u001b[A\n","74it [04:40,  9.05s/it]\u001b[A\n","75it [04:40,  6.40s/it]\u001b[A\n","76it [04:40,  4.55s/it]\u001b[A\n","77it [04:40,  3.26s/it]\u001b[A\n","78it [04:40,  2.35s/it]\u001b[A\n","79it [04:41,  1.71s/it]\u001b[A\n","80it [04:41,  1.27s/it]\u001b[A\n","81it [04:41,  1.04it/s]\u001b[A\n","82it [04:41,  1.35it/s]\u001b[A\n","83it [04:42,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:03<01:24,  1.66s/it]\n","85it [05:23, 12.80s/it]\u001b[A\n","86it [05:24,  9.03s/it]\u001b[A\n","87it [05:24,  6.39s/it]\u001b[A\n","88it [05:24,  4.54s/it]\u001b[A\n","89it [05:24,  3.25s/it]\u001b[A\n","90it [05:25,  2.34s/it]\u001b[A\n","91it [05:25,  1.71s/it]\u001b[A\n","92it [05:25,  1.27s/it]\u001b[A\n","93it [05:25,  1.05it/s]\u001b[A\n","94it [05:25,  1.35it/s]\u001b[A\n","95it [05:26,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [06:47<00:42,  1.65s/it]\n","97it [06:07, 12.81s/it]\u001b[A\n","98it [06:08,  9.04s/it]\u001b[A\n","99it [06:08,  6.39s/it]\u001b[A\n","100it [06:08,  4.55s/it]\u001b[A\n","101it [06:08,  3.25s/it]\u001b[A\n","102it [06:09,  2.35s/it]\u001b[A\n","103it [06:09,  1.71s/it]\u001b[A\n","104it [06:09,  1.27s/it]\u001b[A\n","105it [06:09,  1.05it/s]\u001b[A\n","106it [06:10,  1.35it/s]\u001b[A\n","107it [06:10,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:31<00:01,  1.66s/it]\n","109it [06:51, 12.79s/it]\u001b[A\n","110it [06:52,  9.02s/it]\u001b[A\n","111it [06:52,  6.38s/it]\u001b[A\n","112it [06:52,  4.54s/it]\u001b[A\n","113it [06:52,  3.25s/it]\u001b[A\n","114it [06:53,  2.34s/it]\u001b[A\n","115it [06:53,  1.71s/it]\u001b[A\n","116it [06:53,  1.26s/it]\u001b[A\n","117it [06:53,  1.05it/s]\u001b[A\n","118it [06:54,  1.36it/s]\u001b[A\n","119it [06:54,  1.71it/s]\u001b[A\n","Epoch: 100% 250/250 [07:36<00:00,  1.83s/it]\n","03/07/2022 06:19:56 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 06:20:10 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:08,  4.57s/it]\u001b[A\n","122it [07:08,  3.27s/it]\u001b[A\n","123it [07:09,  2.36s/it]\u001b[A\n","124it [07:09,  1.72s/it]\u001b[A\n","125it [07:09,  1.27s/it]\u001b[A\n","126it [07:09,  1.04it/s]\u001b[A\n","127it [07:10,  1.35it/s]\u001b[A\n","128it [07:10,  1.70it/s]\u001b[A\n","129it [07:10,  2.08it/s]\u001b[A\n","130it [07:10,  2.46it/s]\u001b[A\n","131it [07:10,  2.83it/s]\u001b[A\n","132it [07:11,  3.50it/s]\u001b[A03/07/2022 06:20:13 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 06:20:13 - INFO - __main__ -     eval_loss = 4.2270731925964355\n","03/07/2022 06:20:13 - INFO - __main__ -     eval_auroc = 0.9044999480247498\n","03/07/2022 06:20:13 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/07/2022 06:20:13 - INFO - __main__ -     eval_f1 = 0.4444444179534912\n","03/07/2022 06:20:13 - INFO - filelock -   Lock 140292299771472 acquired on log.lock\n","03/07/2022 06:20:13 - INFO - filelock -   Lock 140292299771472 released on log.lock\n","132it [07:11,  3.27s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 06:20:19 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 06:20:19 - INFO - __main__ -   Specify load the 89-th template: *cls*▁Hell*mask*!*+sent_0**sep+*\n","03/07/2022 06:20:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 06:20:19 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-25381', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_06-20-19_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-25381', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 06:20:19 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 06:20:22 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:20:22 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:20:22 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 06:20:22 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:20:22 - INFO - filelock -   Lock 140686790678736 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:20:22 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 06:20:22 - INFO - filelock -   Lock 140686790678736 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:20:22 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:20:22 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:20:22 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 06:20:22 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:20:22 - INFO - filelock -   Lock 140686789331728 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:20:22 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/07/2022 06:20:22 - INFO - filelock -   Lock 140686789331728 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:20:22 - INFO - src.dataset -   *** Example ***\n","03/07/2022 06:20:22 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 06:20:22 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 38980, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/07/2022 06:20:22 - INFO - src.dataset -   text: <s>▁Hell<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 06:20:37 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 06:20:37 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 06:20:37 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 06:20:37 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 06:20:37 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 06:20:37 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 06:20:37 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 06:21:22 - INFO - src.trainer -   Best dev result: 0.8581249713897705\n","Epoch:  20% 49/250 [01:32<05:38,  1.68s/it]\n","13it [00:52, 15.10s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:53,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/07/2022 06:22:14 - INFO - src.trainer -   Best dev result: 0.8987500071525574\n","Epoch:  30% 74/250 [02:23<04:57,  1.69s/it]\n","25it [01:43, 15.03s/it]\u001b[A\n","26it [01:43, 10.59s/it]\u001b[A\n","27it [01:44,  7.49s/it]\u001b[A\n","28it [01:44,  5.31s/it]\u001b[A\n","29it [01:44,  3.79s/it]\u001b[A\n","30it [01:44,  2.72s/it]\u001b[A\n","31it [01:45,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:46,  1.54it/s]\u001b[A\n","36it [01:46,  2.03it/s]\u001b[A03/07/2022 06:23:05 - INFO - src.trainer -   Best dev result: 0.9022499918937683\n","Epoch:  40% 99/250 [03:16<04:17,  1.71s/it]\n","37it [02:36, 15.34s/it]\u001b[A\n","38it [02:36, 10.81s/it]\u001b[A\n","39it [02:36,  7.64s/it]\u001b[A\n","40it [02:36,  5.41s/it]\u001b[A\n","41it [02:37,  3.86s/it]\u001b[A\n","42it [02:37,  2.77s/it]\u001b[A\n","43it [02:37,  2.01s/it]\u001b[A\n","44it [02:37,  1.48s/it]\u001b[A\n","45it [02:38,  1.10s/it]\u001b[A\n","46it [02:38,  1.19it/s]\u001b[A\n","47it [02:38,  1.52it/s]\u001b[A\n","48it [02:38,  2.01it/s]\u001b[A03/07/2022 06:23:58 - INFO - src.trainer -   Best dev result: 0.9027500152587891\n","Epoch:  50% 124/250 [04:08<03:34,  1.70s/it]\n","49it [03:28, 15.23s/it]\u001b[A\n","50it [03:28, 10.73s/it]\u001b[A\n","51it [03:28,  7.58s/it]\u001b[A\n","52it [03:28,  5.38s/it]\u001b[A\n","53it [03:29,  3.83s/it]\u001b[A\n","54it [03:29,  2.75s/it]\u001b[A\n","55it [03:29,  2.00s/it]\u001b[A\n","56it [03:29,  1.47s/it]\u001b[A\n","57it [03:30,  1.10s/it]\u001b[A\n","58it [03:30,  1.19it/s]\u001b[A\n","59it [03:30,  1.53it/s]\u001b[A\n","60it [03:30,  2.01it/s]\u001b[A03/07/2022 06:24:50 - INFO - src.trainer -   Best dev result: 0.9054999351501465\n","Epoch:  60% 149/250 [05:00<02:52,  1.71s/it]\n","61it [04:20, 15.27s/it]\u001b[A\n","62it [04:20, 10.76s/it]\u001b[A\n","63it [04:20,  7.60s/it]\u001b[A\n","64it [04:21,  5.39s/it]\u001b[A\n","65it [04:21,  3.84s/it]\u001b[A\n","66it [04:21,  2.76s/it]\u001b[A\n","67it [04:21,  2.00s/it]\u001b[A\n","68it [04:22,  1.47s/it]\u001b[A\n","69it [04:22,  1.10s/it]\u001b[A\n","70it [04:22,  1.19it/s]\u001b[A\n","71it [04:22,  1.52it/s]\u001b[A\n","72it [04:22,  2.01it/s]\u001b[A03/07/2022 06:25:42 - INFO - src.trainer -   Best dev result: 0.9240000247955322\n","Epoch:  70% 174/250 [05:52<02:09,  1.70s/it]\n","73it [05:12, 15.24s/it]\u001b[A\n","74it [05:12, 10.74s/it]\u001b[A\n","75it [05:12,  7.58s/it]\u001b[A\n","76it [05:13,  5.38s/it]\u001b[A\n","77it [05:13,  3.83s/it]\u001b[A\n","78it [05:13,  2.75s/it]\u001b[A\n","79it [05:13,  2.00s/it]\u001b[A\n","80it [05:14,  1.47s/it]\u001b[A\n","81it [05:14,  1.10s/it]\u001b[A\n","82it [05:14,  1.19it/s]\u001b[A\n","83it [05:14,  1.53it/s]\u001b[A\n","Epoch:  80% 199/250 [06:36<01:24,  1.66s/it]\n","85it [05:56, 12.88s/it]\u001b[A\n","86it [05:56,  9.09s/it]\u001b[A\n","87it [05:57,  6.43s/it]\u001b[A\n","88it [05:57,  4.57s/it]\u001b[A\n","89it [05:57,  3.27s/it]\u001b[A\n","90it [05:57,  2.36s/it]\u001b[A\n","91it [05:58,  1.72s/it]\u001b[A\n","92it [05:58,  1.27s/it]\u001b[A\n","93it [05:58,  1.04it/s]\u001b[A\n","94it [05:58,  1.35it/s]\u001b[A\n","95it [05:59,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:21<00:43,  1.66s/it]\n","97it [06:40, 12.85s/it]\u001b[A\n","98it [06:41,  9.07s/it]\u001b[A\n","99it [06:41,  6.42s/it]\u001b[A\n","100it [06:41,  4.56s/it]\u001b[A\n","101it [06:41,  3.26s/it]\u001b[A\n","102it [06:42,  2.35s/it]\u001b[A\n","103it [06:42,  1.72s/it]\u001b[A\n","104it [06:42,  1.27s/it]\u001b[A\n","105it [06:42,  1.04it/s]\u001b[A\n","106it [06:43,  1.35it/s]\u001b[A\n","107it [06:43,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [08:05<00:01,  1.66s/it]\n","109it [07:25, 12.86s/it]\u001b[A\n","110it [07:25,  9.07s/it]\u001b[A\n","111it [07:25,  6.42s/it]\u001b[A\n","112it [07:25,  4.56s/it]\u001b[A\n","113it [07:26,  3.26s/it]\u001b[A\n","114it [07:26,  2.35s/it]\u001b[A\n","115it [07:26,  1.72s/it]\u001b[A\n","116it [07:26,  1.27s/it]\u001b[A\n","117it [07:27,  1.04it/s]\u001b[A\n","118it [07:27,  1.35it/s]\u001b[A\n","119it [07:27,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [08:09<00:00,  1.96s/it]\n","03/07/2022 06:28:47 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 06:29:01 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:42,  4.66s/it]\u001b[A\n","122it [07:42,  3.33s/it]\u001b[A\n","123it [07:42,  2.40s/it]\u001b[A\n","124it [07:42,  1.75s/it]\u001b[A\n","125it [07:43,  1.29s/it]\u001b[A\n","126it [07:43,  1.03it/s]\u001b[A\n","127it [07:43,  1.33it/s]\u001b[A\n","128it [07:43,  1.68it/s]\u001b[A\n","129it [07:43,  2.06it/s]\u001b[A\n","130it [07:44,  2.44it/s]\u001b[A\n","131it [07:44,  2.81it/s]\u001b[A\n","132it [07:44,  3.48it/s]\u001b[A03/07/2022 06:29:04 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 06:29:04 - INFO - __main__ -     eval_loss = 1.8403465747833252\n","03/07/2022 06:29:04 - INFO - __main__ -     eval_auroc = 0.9240000247955322\n","03/07/2022 06:29:04 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/07/2022 06:29:04 - INFO - __main__ -     eval_f1 = 0.6571429371833801\n","03/07/2022 06:29:04 - INFO - filelock -   Lock 140686817621520 acquired on log.lock\n","03/07/2022 06:29:04 - INFO - filelock -   Lock 140686817621520 released on log.lock\n","132it [07:44,  3.52s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 06:29:10 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 06:29:10 - INFO - __main__ -   Specify load the 90-th template: *cls*▁\"*mask*,*+sent_0**sep+*\n","03/07/2022 06:29:10 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 06:29:10 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-13878', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_06-29-10_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-13878', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 06:29:10 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 06:29:13 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:29:13 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:29:13 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 06:29:13 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:29:13 - INFO - filelock -   Lock 139926334637968 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:29:13 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 06:29:13 - INFO - filelock -   Lock 139926334637968 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:29:13 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:29:13 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:29:13 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 06:29:13 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:29:13 - INFO - filelock -   Lock 139926305455312 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:29:13 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.005 s]\n","03/07/2022 06:29:13 - INFO - filelock -   Lock 139926305455312 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:29:13 - INFO - src.dataset -   *** Example ***\n","03/07/2022 06:29:13 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 06:29:13 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 113, 50264, 6, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/07/2022 06:29:13 - INFO - src.dataset -   text: <s>▁\"<mask>, Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 06:29:29 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 06:29:29 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 06:29:29 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 06:29:29 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 06:29:29 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 06:29:29 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 06:29:29 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/07/2022 06:30:14 - INFO - src.trainer -   Best dev result: 0.8648749589920044\n","Epoch:  20% 49/250 [01:32<05:42,  1.70s/it]\n","13it [00:51, 15.00s/it]            \u001b[A\n","14it [00:52, 10.57s/it]\u001b[A\n","15it [00:52,  7.47s/it]\u001b[A\n","16it [00:52,  5.30s/it]\u001b[A\n","17it [00:52,  3.78s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.88s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:36,  6.43s/it]\u001b[A\n","28it [01:36,  4.57s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:11,  1.66s/it]\n","37it [02:20, 12.83s/it]\u001b[A\n","38it [02:20,  9.05s/it]\u001b[A\n","39it [02:20,  6.41s/it]\u001b[A\n","40it [02:21,  4.55s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:21,  1.71s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","48it [02:22,  2.23it/s]\u001b[A03/07/2022 06:32:34 - INFO - src.trainer -   Best dev result: 0.8906249403953552\n","Epoch:  50% 124/250 [03:52<03:35,  1.71s/it]\n","49it [03:12, 15.09s/it]\u001b[A\n","50it [03:12, 10.63s/it]\u001b[A\n","51it [03:12,  7.51s/it]\u001b[A\n","52it [03:12,  5.33s/it]\u001b[A\n","53it [03:13,  3.80s/it]\u001b[A\n","54it [03:13,  2.73s/it]\u001b[A\n","55it [03:13,  1.98s/it]\u001b[A\n","56it [03:13,  1.46s/it]\u001b[A\n","57it [03:13,  1.09s/it]\u001b[A\n","58it [03:14,  1.20it/s]\u001b[A\n","59it [03:14,  1.54it/s]\u001b[A\n","Epoch:  60% 149/250 [04:36<02:47,  1.66s/it]\n","61it [03:56, 12.88s/it]\u001b[A\n","62it [03:56,  9.09s/it]\u001b[A\n","63it [03:56,  6.43s/it]\u001b[A\n","64it [03:57,  4.57s/it]\u001b[A\n","65it [03:57,  3.27s/it]\u001b[A\n","66it [03:57,  2.36s/it]\u001b[A\n","67it [03:57,  1.72s/it]\u001b[A\n","68it [03:57,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:58,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:20<02:06,  1.66s/it]\n","73it [04:40, 12.85s/it]\u001b[A\n","74it [04:40,  9.06s/it]\u001b[A\n","75it [04:41,  6.41s/it]\u001b[A\n","76it [04:41,  4.56s/it]\u001b[A\n","77it [04:41,  3.26s/it]\u001b[A\n","78it [04:41,  2.35s/it]\u001b[A\n","79it [04:41,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:42,  1.35it/s]\u001b[A\n","83it [04:42,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:24,  1.66s/it]\n","85it [05:24, 12.84s/it]\u001b[A\n","86it [05:24,  9.05s/it]\u001b[A\n","87it [05:25,  6.41s/it]\u001b[A\n","88it [05:25,  4.55s/it]\u001b[A\n","89it [05:25,  3.26s/it]\u001b[A\n","90it [05:25,  2.35s/it]\u001b[A\n","91it [05:26,  1.71s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:26,  1.04it/s]\u001b[A\n","94it [05:26,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:08, 12.84s/it]\u001b[A\n","98it [06:09,  9.05s/it]\u001b[A\n","99it [06:09,  6.41s/it]\u001b[A\n","100it [06:09,  4.55s/it]\u001b[A\n","101it [06:09,  3.26s/it]\u001b[A\n","102it [06:10,  2.35s/it]\u001b[A\n","103it [06:10,  1.71s/it]\u001b[A\n","104it [06:10,  1.27s/it]\u001b[A\n","105it [06:10,  1.04it/s]\u001b[A\n","106it [06:10,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:33<00:01,  1.66s/it]\n","109it [06:53, 12.84s/it]\u001b[A\n","110it [06:53,  9.05s/it]\u001b[A\n","111it [06:53,  6.41s/it]\u001b[A\n","112it [06:53,  4.55s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:54,  1.71s/it]\u001b[A\n","116it [06:54,  1.27s/it]\u001b[A\n","117it [06:54,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:55,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:37<00:00,  1.83s/it]\n","03/07/2022 06:37:07 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 06:37:21 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:09,  4.54s/it]\u001b[A\n","122it [07:09,  3.25s/it]\u001b[A\n","123it [07:10,  2.34s/it]\u001b[A\n","124it [07:10,  1.71s/it]\u001b[A\n","125it [07:10,  1.27s/it]\u001b[A\n","126it [07:10,  1.05it/s]\u001b[A\n","127it [07:10,  1.35it/s]\u001b[A\n","128it [07:11,  1.71it/s]\u001b[A\n","129it [07:11,  2.08it/s]\u001b[A\n","130it [07:11,  2.47it/s]\u001b[A\n","131it [07:11,  2.83it/s]\u001b[A\n","132it [07:12,  3.51it/s]\u001b[A03/07/2022 06:37:23 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 06:37:23 - INFO - __main__ -     eval_loss = 1.866880178451538\n","03/07/2022 06:37:23 - INFO - __main__ -     eval_auroc = 0.8906249403953552\n","03/07/2022 06:37:23 - INFO - __main__ -     eval_recall = 0.7599999904632568\n","03/07/2022 06:37:23 - INFO - __main__ -     eval_f1 = 0.5277777314186096\n","03/07/2022 06:37:23 - INFO - filelock -   Lock 139926344100624 acquired on log.lock\n","03/07/2022 06:37:23 - INFO - filelock -   Lock 139926344100624 released on log.lock\n","132it [07:12,  3.27s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 06:37:29 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 06:37:29 - INFO - __main__ -   Specify load the 91-th template: *cls*▁Absolutely*mask*.*+sent_0**sep+*\n","03/07/2022 06:37:29 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 06:37:29 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-2745', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_06-37-29_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-2745', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 06:37:29 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 06:37:33 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:37:33 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:37:33 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 06:37:33 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:37:33 - INFO - filelock -   Lock 140058663087952 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:37:33 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 06:37:33 - INFO - filelock -   Lock 140058663087952 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:37:33 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:37:33 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:37:33 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 06:37:33 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:37:33 - INFO - filelock -   Lock 140058663175248 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:37:33 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/07/2022 06:37:33 - INFO - filelock -   Lock 140058663175248 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:37:33 - INFO - src.dataset -   *** Example ***\n","03/07/2022 06:37:33 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 06:37:33 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 32523, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/07/2022 06:37:33 - INFO - src.dataset -   text: <s>▁Absolutely<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 06:37:48 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 06:37:48 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 06:37:48 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 06:37:48 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 06:37:48 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 06:37:48 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 06:37:48 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.71it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.57it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.49it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.44it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.40it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/07/2022 06:38:33 - INFO - src.trainer -   Best dev result: 0.9030000567436218\n","Epoch:  20% 49/250 [01:32<05:40,  1.70s/it]\n","13it [00:52, 15.16s/it]            \u001b[A\n","14it [00:52, 10.68s/it]\u001b[A\n","15it [00:52,  7.55s/it]\u001b[A\n","16it [00:53,  5.35s/it]\u001b[A\n","17it [00:53,  3.82s/it]\u001b[A\n","18it [00:53,  2.74s/it]\u001b[A\n","19it [00:53,  1.99s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/07/2022 06:39:26 - INFO - src.trainer -   Best dev result: 0.9076249599456787\n","Epoch:  30% 74/250 [02:24<04:58,  1.70s/it]\n","25it [01:44, 15.13s/it]\u001b[A\n","26it [01:44, 10.66s/it]\u001b[A\n","27it [01:44,  7.53s/it]\u001b[A\n","28it [01:44,  5.34s/it]\u001b[A\n","29it [01:45,  3.81s/it]\u001b[A\n","30it [01:45,  2.74s/it]\u001b[A\n","31it [01:45,  1.99s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:46,  1.09s/it]\u001b[A\n","34it [01:46,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:10,  1.66s/it]\n","37it [02:28, 12.90s/it]\u001b[A\n","38it [02:28,  9.10s/it]\u001b[A\n","39it [02:29,  6.44s/it]\u001b[A\n","40it [02:29,  4.58s/it]\u001b[A\n","41it [02:29,  3.27s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:30,  1.28s/it]\u001b[A\n","45it [02:30,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:53<03:29,  1.66s/it]\n","49it [03:12, 12.86s/it]\u001b[A\n","50it [03:13,  9.07s/it]\u001b[A\n","51it [03:13,  6.42s/it]\u001b[A\n","52it [03:13,  4.56s/it]\u001b[A\n","53it [03:13,  3.26s/it]\u001b[A\n","54it [03:13,  2.35s/it]\u001b[A\n","55it [03:14,  1.72s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:15,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:37<02:47,  1.66s/it]\n","61it [03:57, 12.86s/it]\u001b[A\n","62it [03:57,  9.07s/it]\u001b[A\n","63it [03:57,  6.42s/it]\u001b[A\n","64it [03:57,  4.56s/it]\u001b[A\n","65it [03:58,  3.26s/it]\u001b[A\n","66it [03:58,  2.35s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:59,  1.35it/s]\u001b[A\n","71it [03:59,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.66s/it]\n","73it [04:41, 12.86s/it]\u001b[A\n","74it [04:41,  9.07s/it]\u001b[A\n","75it [04:41,  6.42s/it]\u001b[A\n","76it [04:42,  4.56s/it]\u001b[A\n","77it [04:42,  3.26s/it]\u001b[A\n","78it [04:42,  2.35s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:43,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:06<01:24,  1.66s/it]\n","85it [05:25, 12.86s/it]\u001b[A\n","86it [05:25,  9.07s/it]\u001b[A\n","87it [05:26,  6.42s/it]\u001b[A\n","88it [05:26,  4.56s/it]\u001b[A\n","89it [05:26,  3.26s/it]\u001b[A\n","90it [05:26,  2.35s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:27,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:50<00:43,  1.67s/it]\n","97it [06:09, 12.86s/it]\u001b[A\n","98it [06:10,  9.07s/it]\u001b[A\n","99it [06:10,  6.42s/it]\u001b[A\n","100it [06:10,  4.56s/it]\u001b[A\n","101it [06:10,  3.26s/it]\u001b[A\n","102it [06:11,  2.35s/it]\u001b[A\n","103it [06:11,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:12,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.66s/it]\n","109it [06:54, 12.87s/it]\u001b[A\n","110it [06:54,  9.08s/it]\u001b[A\n","111it [06:54,  6.42s/it]\u001b[A\n","112it [06:54,  4.57s/it]\u001b[A\n","113it [06:55,  3.27s/it]\u001b[A\n","114it [06:55,  2.36s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:56,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.84s/it]\n","03/07/2022 06:45:27 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 06:45:41 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.60s/it]\u001b[A\n","122it [07:11,  3.29s/it]\u001b[A\n","123it [07:11,  2.37s/it]\u001b[A\n","124it [07:11,  1.73s/it]\u001b[A\n","125it [07:11,  1.28s/it]\u001b[A\n","126it [07:12,  1.04it/s]\u001b[A\n","127it [07:12,  1.34it/s]\u001b[A\n","128it [07:12,  1.69it/s]\u001b[A\n","129it [07:12,  2.07it/s]\u001b[A\n","130it [07:12,  2.45it/s]\u001b[A\n","131it [07:13,  2.82it/s]\u001b[A\n","132it [07:13,  3.49it/s]\u001b[A03/07/2022 06:45:44 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 06:45:44 - INFO - __main__ -     eval_loss = 2.8963706493377686\n","03/07/2022 06:45:44 - INFO - __main__ -     eval_auroc = 0.9076249599456787\n","03/07/2022 06:45:44 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/07/2022 06:45:44 - INFO - __main__ -     eval_f1 = 0.4554455876350403\n","03/07/2022 06:45:44 - INFO - filelock -   Lock 140058635427792 acquired on log.lock\n","03/07/2022 06:45:44 - INFO - filelock -   Lock 140058635427792 released on log.lock\n","132it [07:13,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 06:45:50 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 06:45:50 - INFO - __main__ -   Specify load the 92-th template: *cls*▁Yes!*mask*!*+sent_0**sep+*\n","03/07/2022 06:45:50 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 06:45:50 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-22187', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_06-45-50_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-22187', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 06:45:50 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 06:45:53 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:45:53 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:45:53 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 06:45:53 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:45:53 - INFO - filelock -   Lock 139991065245136 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:45:53 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 06:45:53 - INFO - filelock -   Lock 139991065245136 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:45:53 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:45:53 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:45:53 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 06:45:53 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:45:53 - INFO - filelock -   Lock 139991063808528 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:45:53 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 06:45:53 - INFO - filelock -   Lock 139991063808528 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:45:53 - INFO - src.dataset -   *** Example ***\n","03/07/2022 06:45:53 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 06:45:53 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/07/2022 06:45:53 - INFO - src.dataset -   text: <s>▁Yes!<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 06:46:08 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 06:46:08 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 06:46:08 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 06:46:08 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 06:46:08 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 06:46:08 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 06:46:08 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.68it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.52it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/07/2022 06:46:53 - INFO - src.trainer -   Best dev result: 0.9147499799728394\n","Epoch:  20% 49/250 [01:32<05:42,  1.70s/it]\n","13it [00:52, 15.08s/it]            \u001b[A\n","14it [00:52, 10.62s/it]\u001b[A\n","15it [00:52,  7.51s/it]\u001b[A\n","16it [00:52,  5.32s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.90s/it]\u001b[A\n","26it [01:36,  9.10s/it]\u001b[A\n","27it [01:36,  6.44s/it]\u001b[A\n","28it [01:37,  4.58s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","36it [01:38,  2.22it/s]\u001b[A03/07/2022 06:48:30 - INFO - src.trainer -   Best dev result: 0.9198750257492065\n","Epoch:  40% 99/250 [03:08<04:15,  1.69s/it]\n","37it [02:28, 15.04s/it]\u001b[A\n","38it [02:28, 10.60s/it]\u001b[A\n","39it [02:28,  7.49s/it]\u001b[A\n","40it [02:28,  5.31s/it]\u001b[A\n","41it [02:28,  3.79s/it]\u001b[A\n","42it [02:29,  2.72s/it]\u001b[A\n","43it [02:29,  1.97s/it]\u001b[A\n","44it [02:29,  1.45s/it]\u001b[A\n","45it [02:29,  1.08s/it]\u001b[A\n","46it [02:30,  1.21it/s]\u001b[A\n","47it [02:30,  1.54it/s]\u001b[A\n","48it [02:30,  2.03it/s]\u001b[A03/07/2022 06:49:21 - INFO - src.trainer -   Best dev result: 0.9227499961853027\n","Epoch:  50% 124/250 [04:00<03:34,  1.71s/it]\n","49it [03:20, 15.43s/it]\u001b[A\n","50it [03:20, 10.87s/it]\u001b[A\n","51it [03:21,  7.68s/it]\u001b[A\n","52it [03:21,  5.44s/it]\u001b[A\n","53it [03:21,  3.88s/it]\u001b[A\n","54it [03:21,  2.79s/it]\u001b[A\n","55it [03:22,  2.02s/it]\u001b[A\n","56it [03:22,  1.48s/it]\u001b[A\n","57it [03:22,  1.11s/it]\u001b[A\n","58it [03:22,  1.18it/s]\u001b[A\n","59it [03:23,  1.51it/s]\u001b[A\n","60it [03:23,  2.00it/s]\u001b[A03/07/2022 06:50:14 - INFO - src.trainer -   Best dev result: 0.9255000352859497\n","Epoch:  60% 149/250 [04:52<02:50,  1.69s/it]\n","61it [04:12, 15.14s/it]\u001b[A\n","62it [04:12, 10.67s/it]\u001b[A\n","63it [04:12,  7.54s/it]\u001b[A\n","64it [04:13,  5.35s/it]\u001b[A\n","65it [04:13,  3.81s/it]\u001b[A\n","66it [04:13,  2.74s/it]\u001b[A\n","67it [04:13,  1.99s/it]\u001b[A\n","68it [04:14,  1.46s/it]\u001b[A\n","69it [04:14,  1.09s/it]\u001b[A\n","70it [04:14,  1.20it/s]\u001b[A\n","71it [04:14,  1.53it/s]\u001b[A\n","Epoch:  70% 174/250 [05:37<02:06,  1.66s/it]\n","73it [04:56, 12.90s/it]\u001b[A\n","74it [04:57,  9.10s/it]\u001b[A\n","75it [04:57,  6.44s/it]\u001b[A\n","76it [04:57,  4.58s/it]\u001b[A\n","77it [04:57,  3.27s/it]\u001b[A\n","78it [04:57,  2.36s/it]\u001b[A\n","79it [04:58,  1.72s/it]\u001b[A\n","80it [04:58,  1.27s/it]\u001b[A\n","81it [04:58,  1.04it/s]\u001b[A\n","82it [04:58,  1.35it/s]\u001b[A\n","83it [04:59,  1.70it/s]\u001b[A\n","84it [04:59,  2.22it/s]\u001b[A03/07/2022 06:51:50 - INFO - src.trainer -   Best dev result: 0.9362500905990601\n","Epoch:  80% 199/250 [06:28<01:26,  1.69s/it]\n","85it [05:48, 15.19s/it]\u001b[A\n","86it [05:49, 10.70s/it]\u001b[A\n","87it [05:49,  7.56s/it]\u001b[A\n","88it [05:49,  5.36s/it]\u001b[A\n","89it [05:49,  3.82s/it]\u001b[A\n","90it [05:49,  2.75s/it]\u001b[A\n","91it [05:50,  1.99s/it]\u001b[A\n","92it [05:50,  1.46s/it]\u001b[A\n","93it [05:50,  1.09s/it]\u001b[A\n","94it [05:50,  1.20it/s]\u001b[A\n","95it [05:51,  1.53it/s]\u001b[A\n","Epoch:  90% 224/250 [07:13<00:43,  1.66s/it]\n","97it [06:33, 12.87s/it]\u001b[A\n","98it [06:33,  9.08s/it]\u001b[A\n","99it [06:33,  6.43s/it]\u001b[A\n","100it [06:33,  4.57s/it]\u001b[A\n","101it [06:33,  3.27s/it]\u001b[A\n","102it [06:34,  2.36s/it]\u001b[A\n","103it [06:34,  1.72s/it]\u001b[A\n","104it [06:34,  1.27s/it]\u001b[A\n","105it [06:34,  1.04it/s]\u001b[A\n","106it [06:35,  1.35it/s]\u001b[A\n","107it [06:35,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:57<00:01,  1.66s/it]\n","109it [07:17, 12.83s/it]\u001b[A\n","110it [07:17,  9.05s/it]\u001b[A\n","111it [07:17,  6.41s/it]\u001b[A\n","112it [07:17,  4.55s/it]\u001b[A\n","113it [07:18,  3.26s/it]\u001b[A\n","114it [07:18,  2.35s/it]\u001b[A\n","115it [07:18,  1.71s/it]\u001b[A\n","116it [07:18,  1.27s/it]\u001b[A\n","117it [07:19,  1.04it/s]\u001b[A\n","118it [07:19,  1.35it/s]\u001b[A\n","119it [07:19,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [08:01<00:00,  1.93s/it]\n","03/07/2022 06:54:10 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 06:54:24 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:34,  4.64s/it]\u001b[A\n","122it [07:34,  3.32s/it]\u001b[A\n","123it [07:34,  2.39s/it]\u001b[A\n","124it [07:34,  1.74s/it]\u001b[A\n","125it [07:34,  1.29s/it]\u001b[A\n","126it [07:35,  1.03it/s]\u001b[A\n","127it [07:35,  1.33it/s]\u001b[A\n","128it [07:35,  1.68it/s]\u001b[A\n","129it [07:35,  2.06it/s]\u001b[A\n","130it [07:36,  2.44it/s]\u001b[A\n","131it [07:36,  2.81it/s]\u001b[A\n","132it [07:36,  3.49it/s]\u001b[A03/07/2022 06:54:27 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 06:54:27 - INFO - __main__ -     eval_loss = 2.8370375633239746\n","03/07/2022 06:54:27 - INFO - __main__ -     eval_auroc = 0.9362500905990601\n","03/07/2022 06:54:27 - INFO - __main__ -     eval_recall = 1.0\n","03/07/2022 06:54:27 - INFO - __main__ -     eval_f1 = 0.5882353186607361\n","03/07/2022 06:54:27 - INFO - filelock -   Lock 139991109397200 acquired on log.lock\n","03/07/2022 06:54:27 - INFO - filelock -   Lock 139991109397200 released on log.lock\n","132it [07:36,  3.46s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 06:54:33 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 06:54:33 - INFO - __main__ -   Specify load the 93-th template: *cls*▁•*mask*.*+sent_0**sep+*\n","03/07/2022 06:54:33 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 06:54:33 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-11102', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_06-54-33_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-11102', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 06:54:33 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 06:54:36 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:54:36 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:54:36 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 06:54:36 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:54:36 - INFO - filelock -   Lock 139903209803472 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:54:36 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 06:54:36 - INFO - filelock -   Lock 139903209803472 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:54:36 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 06:54:36 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 06:54:36 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 06:54:36 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 06:54:36 - INFO - filelock -   Lock 139903209957520 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:54:36 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/07/2022 06:54:36 - INFO - filelock -   Lock 139903209957520 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 06:54:36 - INFO - src.dataset -   *** Example ***\n","03/07/2022 06:54:36 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 06:54:36 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 21438, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/07/2022 06:54:36 - INFO - src.dataset -   text: <s>▁•<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 06:54:52 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 06:54:52 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 06:54:52 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 06:54:52 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 06:54:52 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 06:54:52 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 06:54:52 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/07/2022 06:55:36 - INFO - src.trainer -   Best dev result: 0.8180000185966492\n","Epoch:  20% 49/250 [01:32<05:41,  1.70s/it]\n","13it [00:52, 15.09s/it]            \u001b[A\n","14it [00:52, 10.63s/it]\u001b[A\n","15it [00:52,  7.51s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.90s/it]\u001b[A\n","26it [01:36,  9.10s/it]\u001b[A\n","27it [01:37,  6.44s/it]\u001b[A\n","28it [01:37,  4.58s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","36it [01:38,  2.22it/s]\u001b[A03/07/2022 06:57:13 - INFO - src.trainer -   Best dev result: 0.874500036239624\n","Epoch:  40% 99/250 [03:08<04:15,  1.69s/it]\n","37it [02:28, 15.09s/it]\u001b[A\n","38it [02:28, 10.63s/it]\u001b[A\n","39it [02:28,  7.51s/it]\u001b[A\n","40it [02:28,  5.33s/it]\u001b[A\n","41it [02:29,  3.80s/it]\u001b[A\n","42it [02:29,  2.73s/it]\u001b[A\n","43it [02:29,  1.98s/it]\u001b[A\n","44it [02:29,  1.46s/it]\u001b[A\n","45it [02:30,  1.09s/it]\u001b[A\n","46it [02:30,  1.20it/s]\u001b[A\n","47it [02:30,  1.53it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:29,  1.66s/it]\n","49it [03:12, 12.88s/it]\u001b[A\n","50it [03:12,  9.08s/it]\u001b[A\n","51it [03:12,  6.43s/it]\u001b[A\n","52it [03:13,  4.57s/it]\u001b[A\n","53it [03:13,  3.27s/it]\u001b[A\n","54it [03:13,  2.36s/it]\u001b[A\n","55it [03:13,  1.72s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:36<02:47,  1.66s/it]\n","61it [03:56, 12.84s/it]\u001b[A\n","62it [03:56,  9.06s/it]\u001b[A\n","63it [03:57,  6.41s/it]\u001b[A\n","64it [03:57,  4.56s/it]\u001b[A\n","65it [03:57,  3.26s/it]\u001b[A\n","66it [03:57,  2.35s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:58,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.66s/it]\n","73it [04:40, 12.83s/it]\u001b[A\n","74it [04:41,  9.05s/it]\u001b[A\n","75it [04:41,  6.41s/it]\u001b[A\n","76it [04:41,  4.55s/it]\u001b[A\n","77it [04:41,  3.26s/it]\u001b[A\n","78it [04:41,  2.35s/it]\u001b[A\n","79it [04:42,  1.71s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:42,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:24,  1.67s/it]\n","85it [05:25, 12.85s/it]\u001b[A\n","86it [05:25,  9.06s/it]\u001b[A\n","87it [05:25,  6.41s/it]\u001b[A\n","88it [05:25,  4.56s/it]\u001b[A\n","89it [05:25,  3.26s/it]\u001b[A\n","90it [05:26,  2.35s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:26,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:09, 12.85s/it]\u001b[A\n","98it [06:09,  9.06s/it]\u001b[A\n","99it [06:09,  6.41s/it]\u001b[A\n","100it [06:09,  4.56s/it]\u001b[A\n","101it [06:10,  3.26s/it]\u001b[A\n","102it [06:10,  2.35s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:10,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:33<00:01,  1.66s/it]\n","109it [06:53, 12.84s/it]\u001b[A\n","110it [06:53,  9.06s/it]\u001b[A\n","111it [06:53,  6.41s/it]\u001b[A\n","112it [06:54,  4.56s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:54,  1.71s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:55,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/07/2022 07:02:30 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 07:02:44 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.57s/it]\u001b[A\n","122it [07:10,  3.27s/it]\u001b[A\n","123it [07:10,  2.36s/it]\u001b[A\n","124it [07:10,  1.72s/it]\u001b[A\n","125it [07:11,  1.27s/it]\u001b[A\n","126it [07:11,  1.04it/s]\u001b[A\n","127it [07:11,  1.35it/s]\u001b[A\n","128it [07:11,  1.70it/s]\u001b[A\n","129it [07:11,  2.08it/s]\u001b[A\n","130it [07:12,  2.46it/s]\u001b[A\n","131it [07:12,  2.83it/s]\u001b[A\n","132it [07:12,  3.50it/s]\u001b[A03/07/2022 07:02:46 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 07:02:46 - INFO - __main__ -     eval_loss = 1.8127118349075317\n","03/07/2022 07:02:46 - INFO - __main__ -     eval_auroc = 0.874500036239624\n","03/07/2022 07:02:46 - INFO - __main__ -     eval_recall = 0.7599999904632568\n","03/07/2022 07:02:46 - INFO - __main__ -     eval_f1 = 0.5352112650871277\n","03/07/2022 07:02:46 - INFO - filelock -   Lock 139903210184592 acquired on log.lock\n","03/07/2022 07:02:46 - INFO - filelock -   Lock 139903210184592 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 07:02:52 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 07:02:52 - INFO - __main__ -   Specify load the 94-th template: *cls*▁I*mask*.*+sent_0**sep+*\n","03/07/2022 07:02:52 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 07:02:52 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-21265', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_07-02-52_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-21265', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 07:02:52 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 07:02:56 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:02:56 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:02:56 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 07:02:56 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:02:56 - INFO - filelock -   Lock 140266772551184 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:02:56 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 07:02:56 - INFO - filelock -   Lock 140266772551184 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:02:56 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:02:56 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:02:56 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 07:02:56 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:02:56 - INFO - filelock -   Lock 140266775042960 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:02:56 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/07/2022 07:02:56 - INFO - filelock -   Lock 140266775042960 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:02:56 - INFO - src.dataset -   *** Example ***\n","03/07/2022 07:02:56 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 07:02:56 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 100, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/07/2022 07:02:56 - INFO - src.dataset -   text: <s>▁I<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 07:03:11 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 07:03:11 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 07:03:11 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 07:03:11 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 07:03:11 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 07:03:11 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 07:03:11 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:17,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.64it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.89it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.71it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.58it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.44it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.40it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.08it/s]\u001b[A03/07/2022 07:03:56 - INFO - src.trainer -   Best dev result: 0.8502499461174011\n","Epoch:  20% 49/250 [01:32<05:40,  1.69s/it]\n","13it [00:51, 14.95s/it]            \u001b[A\n","14it [00:52, 10.53s/it]\u001b[A\n","15it [00:52,  7.44s/it]\u001b[A\n","16it [00:52,  5.28s/it]\u001b[A\n","17it [00:52,  3.77s/it]\u001b[A\n","18it [00:52,  2.71s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.89s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:36,  6.43s/it]\u001b[A\n","28it [01:36,  4.57s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:11,  1.66s/it]\n","37it [02:20, 12.86s/it]\u001b[A\n","38it [02:20,  9.07s/it]\u001b[A\n","39it [02:20,  6.42s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:21,  1.72s/it]\u001b[A\n","44it [02:21,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:29,  1.66s/it]\n","49it [03:04, 12.85s/it]\u001b[A\n","50it [03:04,  9.07s/it]\u001b[A\n","51it [03:05,  6.42s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:05,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.70it/s]\u001b[A\n","60it [03:07,  2.22it/s]\u001b[A03/07/2022 07:07:00 - INFO - src.trainer -   Best dev result: 0.8503749370574951\n","Epoch:  60% 149/250 [04:36<02:51,  1.70s/it]\n","61it [03:56, 15.13s/it]\u001b[A\n","62it [03:56, 10.66s/it]\u001b[A\n","63it [03:56,  7.53s/it]\u001b[A\n","64it [03:57,  5.34s/it]\u001b[A\n","65it [03:57,  3.81s/it]\u001b[A\n","66it [03:57,  2.74s/it]\u001b[A\n","67it [03:57,  1.98s/it]\u001b[A\n","68it [03:58,  1.46s/it]\u001b[A\n","69it [03:58,  1.09s/it]\u001b[A\n","70it [03:58,  1.20it/s]\u001b[A\n","71it [03:58,  1.53it/s]\u001b[A\n","Epoch:  70% 174/250 [05:20<02:06,  1.66s/it]\n","73it [04:40, 12.89s/it]\u001b[A\n","74it [04:40,  9.10s/it]\u001b[A\n","75it [04:41,  6.44s/it]\u001b[A\n","76it [04:41,  4.58s/it]\u001b[A\n","77it [04:41,  3.27s/it]\u001b[A\n","78it [04:41,  2.36s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:42,  1.35it/s]\u001b[A\n","83it [04:42,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:24,  1.66s/it]\n","85it [05:24, 12.86s/it]\u001b[A\n","86it [05:25,  9.07s/it]\u001b[A\n","87it [05:25,  6.42s/it]\u001b[A\n","88it [05:25,  4.56s/it]\u001b[A\n","89it [05:25,  3.26s/it]\u001b[A\n","90it [05:26,  2.35s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:26,  1.04it/s]\u001b[A\n","94it [05:26,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:09, 12.85s/it]\u001b[A\n","98it [06:09,  9.06s/it]\u001b[A\n","99it [06:09,  6.41s/it]\u001b[A\n","100it [06:09,  4.56s/it]\u001b[A\n","101it [06:10,  3.26s/it]\u001b[A\n","102it [06:10,  2.35s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:10,  1.27s/it]\u001b[A\n","105it [06:10,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:33<00:01,  1.66s/it]\n","109it [06:53, 12.85s/it]\u001b[A\n","110it [06:53,  9.07s/it]\u001b[A\n","111it [06:53,  6.42s/it]\u001b[A\n","112it [06:54,  4.56s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:54,  1.72s/it]\u001b[A\n","116it [06:54,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:55,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/07/2022 07:10:49 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 07:11:03 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:09,  4.41s/it]\u001b[A\n","122it [07:09,  3.16s/it]\u001b[A\n","123it [07:09,  2.28s/it]\u001b[A\n","124it [07:10,  1.67s/it]\u001b[A\n","125it [07:10,  1.24s/it]\u001b[A\n","126it [07:10,  1.07it/s]\u001b[A\n","127it [07:10,  1.38it/s]\u001b[A\n","128it [07:11,  1.74it/s]\u001b[A\n","129it [07:11,  2.12it/s]\u001b[A\n","130it [07:11,  2.50it/s]\u001b[A\n","131it [07:11,  2.86it/s]\u001b[A\n","132it [07:11,  3.54it/s]\u001b[A03/07/2022 07:11:05 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 07:11:05 - INFO - __main__ -     eval_loss = 4.026055812835693\n","03/07/2022 07:11:05 - INFO - __main__ -     eval_auroc = 0.8503749370574951\n","03/07/2022 07:11:05 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/07/2022 07:11:05 - INFO - __main__ -     eval_f1 = 0.40740740299224854\n","03/07/2022 07:11:05 - INFO - filelock -   Lock 140266771430544 acquired on log.lock\n","03/07/2022 07:11:05 - INFO - filelock -   Lock 140266771430544 released on log.lock\n","132it [07:11,  3.27s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 07:11:11 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 07:11:11 - INFO - __main__ -   Specify load the 95-th template: *cls*▁Yes▁and*mask*.*+sent_0**sep+*\n","03/07/2022 07:11:11 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 07:11:11 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-14766', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_07-11-11_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-14766', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 07:11:11 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 07:11:14 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:11:14 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:11:14 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 07:11:14 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:11:14 - INFO - filelock -   Lock 140189777202704 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:11:14 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 07:11:14 - INFO - filelock -   Lock 140189777202704 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:11:14 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:11:14 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:11:14 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 07:11:14 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:11:14 - INFO - filelock -   Lock 140189776870992 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:11:14 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 07:11:14 - INFO - filelock -   Lock 140189776870992 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:11:14 - INFO - src.dataset -   *** Example ***\n","03/07/2022 07:11:14 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 07:11:14 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 48584, 10172, 463, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[7], label_word_list=None)\n","03/07/2022 07:11:14 - INFO - src.dataset -   text: <s>▁Yes▁and<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 07:11:30 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 07:11:30 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 07:11:30 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 07:11:30 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 07:11:30 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 07:11:30 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 07:11:30 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/07/2022 07:12:14 - INFO - src.trainer -   Best dev result: 0.8655000925064087\n","Epoch:  20% 49/250 [01:32<05:41,  1.70s/it]\n","13it [00:52, 15.17s/it]            \u001b[A\n","14it [00:52, 10.69s/it]\u001b[A\n","15it [00:53,  7.55s/it]\u001b[A\n","16it [00:53,  5.36s/it]\u001b[A\n","17it [00:53,  3.82s/it]\u001b[A\n","18it [00:53,  2.74s/it]\u001b[A\n","19it [00:53,  1.99s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/07/2022 07:13:07 - INFO - src.trainer -   Best dev result: 0.9312500357627869\n","Epoch:  30% 74/250 [02:24<04:58,  1.69s/it]\n","25it [01:44, 15.13s/it]\u001b[A\n","26it [01:44, 10.66s/it]\u001b[A\n","27it [01:44,  7.53s/it]\u001b[A\n","28it [01:44,  5.34s/it]\u001b[A\n","29it [01:45,  3.81s/it]\u001b[A\n","30it [01:45,  2.74s/it]\u001b[A\n","31it [01:45,  1.98s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:46,  1.09s/it]\u001b[A\n","34it [01:46,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:10,  1.66s/it]\n","37it [02:28, 12.90s/it]\u001b[A\n","38it [02:28,  9.10s/it]\u001b[A\n","39it [02:29,  6.44s/it]\u001b[A\n","40it [02:29,  4.58s/it]\u001b[A\n","41it [02:29,  3.27s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:30,  1.27s/it]\u001b[A\n","45it [02:30,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:29,  1.66s/it]\n","49it [03:12, 12.84s/it]\u001b[A\n","50it [03:12,  9.06s/it]\u001b[A\n","51it [03:13,  6.41s/it]\u001b[A\n","52it [03:13,  4.56s/it]\u001b[A\n","53it [03:13,  3.26s/it]\u001b[A\n","54it [03:13,  2.35s/it]\u001b[A\n","55it [03:14,  1.71s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:15,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:37<02:47,  1.66s/it]\n","61it [03:56, 12.83s/it]\u001b[A\n","62it [03:57,  9.05s/it]\u001b[A\n","63it [03:57,  6.41s/it]\u001b[A\n","64it [03:57,  4.55s/it]\u001b[A\n","65it [03:57,  3.26s/it]\u001b[A\n","66it [03:58,  2.35s/it]\u001b[A\n","67it [03:58,  1.71s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:59,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.66s/it]\n","73it [04:41, 12.84s/it]\u001b[A\n","74it [04:41,  9.05s/it]\u001b[A\n","75it [04:41,  6.41s/it]\u001b[A\n","76it [04:41,  4.55s/it]\u001b[A\n","77it [04:42,  3.26s/it]\u001b[A\n","78it [04:42,  2.35s/it]\u001b[A\n","79it [04:42,  1.71s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:24,  1.66s/it]\n","85it [05:25, 12.85s/it]\u001b[A\n","86it [05:25,  9.06s/it]\u001b[A\n","87it [05:25,  6.41s/it]\u001b[A\n","88it [05:25,  4.56s/it]\u001b[A\n","89it [05:26,  3.26s/it]\u001b[A\n","90it [05:26,  2.35s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:09, 12.85s/it]\u001b[A\n","98it [06:09,  9.06s/it]\u001b[A\n","99it [06:09,  6.41s/it]\u001b[A\n","100it [06:10,  4.56s/it]\u001b[A\n","101it [06:10,  3.26s/it]\u001b[A\n","102it [06:10,  2.35s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:33<00:01,  1.66s/it]\n","109it [06:53, 12.84s/it]\u001b[A\n","110it [06:53,  9.06s/it]\u001b[A\n","111it [06:54,  6.41s/it]\u001b[A\n","112it [06:54,  4.56s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:55,  1.71s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/07/2022 07:19:08 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 07:19:22 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.59s/it]\u001b[A\n","122it [07:10,  3.28s/it]\u001b[A\n","123it [07:10,  2.37s/it]\u001b[A\n","124it [07:11,  1.73s/it]\u001b[A\n","125it [07:11,  1.28s/it]\u001b[A\n","126it [07:11,  1.04it/s]\u001b[A\n","127it [07:11,  1.34it/s]\u001b[A\n","128it [07:12,  1.69it/s]\u001b[A\n","129it [07:12,  2.07it/s]\u001b[A\n","130it [07:12,  2.46it/s]\u001b[A\n","131it [07:12,  2.82it/s]\u001b[A\n","132it [07:12,  3.50it/s]\u001b[A03/07/2022 07:19:25 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 07:19:25 - INFO - __main__ -     eval_loss = 1.157394289970398\n","03/07/2022 07:19:25 - INFO - __main__ -     eval_auroc = 0.9312500357627869\n","03/07/2022 07:19:25 - INFO - __main__ -     eval_recall = 0.800000011920929\n","03/07/2022 07:19:25 - INFO - __main__ -     eval_f1 = 0.6153846383094788\n","03/07/2022 07:19:25 - INFO - filelock -   Lock 140189760707984 acquired on log.lock\n","03/07/2022 07:19:25 - INFO - filelock -   Lock 140189760707984 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 07:19:31 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 07:19:31 - INFO - __main__ -   Specify load the 96-th template: *cls*com.*mask*.*+sent_0**sep+*\n","03/07/2022 07:19:31 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 07:19:31 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-11002', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_07-19-31_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-11002', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 07:19:31 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 07:19:34 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:19:34 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:19:34 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 07:19:34 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:19:34 - INFO - filelock -   Lock 139986912830608 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:19:34 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 07:19:34 - INFO - filelock -   Lock 139986912830608 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:19:34 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:19:34 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:19:34 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 07:19:34 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:19:34 - INFO - filelock -   Lock 139986912764176 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:19:34 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 07:19:34 - INFO - filelock -   Lock 139986912764176 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:19:34 - INFO - src.dataset -   *** Example ***\n","03/07/2022 07:19:34 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 07:19:34 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 175, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[3], label_word_list=None)\n","03/07/2022 07:19:34 - INFO - src.dataset -   text: <s>com.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 07:19:49 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 07:19:49 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 07:19:49 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 07:19:49 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 07:19:49 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 07:19:49 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 07:19:49 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.68it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 07:20:34 - INFO - src.trainer -   Best dev result: 0.8554999828338623\n","Epoch:  20% 49/250 [01:32<05:41,  1.70s/it]\n","13it [00:52, 15.07s/it]            \u001b[A\n","14it [00:52, 10.62s/it]\u001b[A\n","15it [00:52,  7.50s/it]\u001b[A\n","16it [00:52,  5.32s/it]\u001b[A\n","17it [00:53,  3.79s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/07/2022 07:21:26 - INFO - src.trainer -   Best dev result: 0.8606249094009399\n","Epoch:  30% 74/250 [02:24<04:58,  1.70s/it]\n","25it [01:43, 15.13s/it]\u001b[A\n","26it [01:44, 10.66s/it]\u001b[A\n","27it [01:44,  7.53s/it]\u001b[A\n","28it [01:44,  5.34s/it]\u001b[A\n","29it [01:44,  3.81s/it]\u001b[A\n","30it [01:45,  2.74s/it]\u001b[A\n","31it [01:45,  1.98s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:46,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:11,  1.66s/it]\n","37it [02:28, 12.88s/it]\u001b[A\n","38it [02:28,  9.09s/it]\u001b[A\n","39it [02:28,  6.43s/it]\u001b[A\n","40it [02:28,  4.57s/it]\u001b[A\n","41it [02:29,  3.27s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.27s/it]\u001b[A\n","45it [02:29,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:29,  1.66s/it]\n","49it [03:12, 12.86s/it]\u001b[A\n","50it [03:12,  9.07s/it]\u001b[A\n","51it [03:12,  6.42s/it]\u001b[A\n","52it [03:13,  4.56s/it]\u001b[A\n","53it [03:13,  3.26s/it]\u001b[A\n","54it [03:13,  2.35s/it]\u001b[A\n","55it [03:13,  1.72s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","60it [03:14,  2.23it/s]\u001b[A03/07/2022 07:23:46 - INFO - src.trainer -   Best dev result: 0.8641250133514404\n","Epoch:  60% 149/250 [04:44<02:51,  1.70s/it]\n","61it [04:04, 15.17s/it]\u001b[A\n","62it [04:04, 10.69s/it]\u001b[A\n","63it [04:04,  7.55s/it]\u001b[A\n","64it [04:05,  5.36s/it]\u001b[A\n","65it [04:05,  3.82s/it]\u001b[A\n","66it [04:05,  2.74s/it]\u001b[A\n","67it [04:05,  1.99s/it]\u001b[A\n","68it [04:05,  1.46s/it]\u001b[A\n","69it [04:06,  1.09s/it]\u001b[A\n","70it [04:06,  1.20it/s]\u001b[A\n","71it [04:06,  1.53it/s]\u001b[A\n","Epoch:  70% 174/250 [05:28<02:06,  1.66s/it]\n","73it [04:48, 12.88s/it]\u001b[A\n","74it [04:48,  9.09s/it]\u001b[A\n","75it [04:49,  6.43s/it]\u001b[A\n","76it [04:49,  4.57s/it]\u001b[A\n","77it [04:49,  3.27s/it]\u001b[A\n","78it [04:49,  2.36s/it]\u001b[A\n","79it [04:49,  1.72s/it]\u001b[A\n","80it [04:50,  1.27s/it]\u001b[A\n","81it [04:50,  1.04it/s]\u001b[A\n","82it [04:50,  1.35it/s]\u001b[A\n","83it [04:50,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:13<01:24,  1.66s/it]\n","85it [05:32, 12.85s/it]\u001b[A\n","86it [05:33,  9.06s/it]\u001b[A\n","87it [05:33,  6.41s/it]\u001b[A\n","88it [05:33,  4.56s/it]\u001b[A\n","89it [05:33,  3.26s/it]\u001b[A\n","90it [05:33,  2.35s/it]\u001b[A\n","91it [05:34,  1.72s/it]\u001b[A\n","92it [05:34,  1.27s/it]\u001b[A\n","93it [05:34,  1.04it/s]\u001b[A\n","94it [05:34,  1.35it/s]\u001b[A\n","95it [05:35,  1.70it/s]\u001b[A\n","96it [05:35,  2.23it/s]\u001b[A03/07/2022 07:26:06 - INFO - src.trainer -   Best dev result: 0.8650000095367432\n","Epoch:  90% 224/250 [07:05<00:44,  1.69s/it]\n","97it [06:25, 15.31s/it]\u001b[A\n","98it [06:25, 10.79s/it]\u001b[A\n","99it [06:25,  7.62s/it]\u001b[A\n","100it [06:25,  5.40s/it]\u001b[A\n","101it [06:26,  3.85s/it]\u001b[A\n","102it [06:26,  2.77s/it]\u001b[A\n","103it [06:26,  2.00s/it]\u001b[A\n","104it [06:26,  1.47s/it]\u001b[A\n","105it [06:27,  1.10s/it]\u001b[A\n","106it [06:27,  1.19it/s]\u001b[A\n","107it [06:27,  1.52it/s]\u001b[A\n","Epoch: 100% 249/250 [07:49<00:01,  1.66s/it]\n","109it [07:09, 12.88s/it]\u001b[A\n","110it [07:09,  9.09s/it]\u001b[A\n","111it [07:09,  6.43s/it]\u001b[A\n","112it [07:10,  4.57s/it]\u001b[A\n","113it [07:10,  3.27s/it]\u001b[A\n","114it [07:10,  2.36s/it]\u001b[A\n","115it [07:10,  1.72s/it]\u001b[A\n","116it [07:11,  1.27s/it]\u001b[A\n","117it [07:11,  1.04it/s]\u001b[A\n","118it [07:11,  1.35it/s]\u001b[A\n","119it [07:11,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:53<00:00,  1.90s/it]\n","03/07/2022 07:27:43 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 07:27:57 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:26,  4.68s/it]\u001b[A\n","122it [07:26,  3.35s/it]\u001b[A\n","123it [07:26,  2.41s/it]\u001b[A\n","124it [07:27,  1.76s/it]\u001b[A\n","125it [07:27,  1.30s/it]\u001b[A\n","126it [07:27,  1.02it/s]\u001b[A\n","127it [07:27,  1.32it/s]\u001b[A\n","128it [07:28,  1.67it/s]\u001b[A\n","129it [07:28,  2.05it/s]\u001b[A\n","130it [07:28,  2.43it/s]\u001b[A\n","131it [07:28,  2.80it/s]\u001b[A\n","132it [07:28,  3.48it/s]\u001b[A03/07/2022 07:28:00 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 07:28:00 - INFO - __main__ -     eval_loss = 2.146496534347534\n","03/07/2022 07:28:00 - INFO - __main__ -     eval_auroc = 0.8650000095367432\n","03/07/2022 07:28:00 - INFO - __main__ -     eval_recall = 0.800000011920929\n","03/07/2022 07:28:00 - INFO - __main__ -     eval_f1 = 0.48192769289016724\n","03/07/2022 07:28:00 - INFO - filelock -   Lock 139986923484304 acquired on log.lock\n","03/07/2022 07:28:00 - INFO - filelock -   Lock 139986923484304 released on log.lock\n","132it [07:28,  3.40s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 07:28:06 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 07:28:06 - INFO - __main__ -   Specify load the 97-th template: *cls*▁Thank▁you.*mask*.*+sent_0**sep+*\n","03/07/2022 07:28:06 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 07:28:06 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-12574', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_07-28-06_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-12574', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 07:28:06 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 07:28:09 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:28:09 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:28:09 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 07:28:09 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:28:09 - INFO - filelock -   Lock 140002270262544 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:28:09 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 07:28:09 - INFO - filelock -   Lock 140002270262544 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:28:09 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:28:09 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:28:09 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 07:28:09 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:28:09 - INFO - filelock -   Lock 140002268820752 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:28:09 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 07:28:09 - INFO - filelock -   Lock 140002268820752 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:28:09 - INFO - src.dataset -   *** Example ***\n","03/07/2022 07:28:09 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 07:28:09 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 13987, 48584, 10172, 6968, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[8], label_word_list=None)\n","03/07/2022 07:28:09 - INFO - src.dataset -   text: <s>▁Thank▁you.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 07:28:25 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 07:28:25 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 07:28:25 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 07:28:25 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 07:28:25 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 07:28:25 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 07:28:25 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.52it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 07:29:09 - INFO - src.trainer -   Best dev result: 0.8833749890327454\n","Epoch:  20% 49/250 [01:32<05:41,  1.70s/it]\n","13it [00:52, 15.10s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.89s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:37,  6.43s/it]\u001b[A\n","28it [01:37,  4.57s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:01<04:11,  1.66s/it]\n","37it [02:20, 12.86s/it]\u001b[A\n","38it [02:21,  9.07s/it]\u001b[A\n","39it [02:21,  6.42s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:22,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:23,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:29,  1.66s/it]\n","49it [03:05, 12.85s/it]\u001b[A\n","50it [03:05,  9.06s/it]\u001b[A\n","51it [03:05,  6.41s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:06,  2.35s/it]\u001b[A\n","55it [03:06,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:07,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:29<02:46,  1.65s/it]\n","61it [03:49, 12.81s/it]\u001b[A\n","62it [03:49,  9.04s/it]\u001b[A\n","63it [03:49,  6.39s/it]\u001b[A\n","64it [03:49,  4.54s/it]\u001b[A\n","65it [03:50,  3.25s/it]\u001b[A\n","66it [03:50,  2.34s/it]\u001b[A\n","67it [03:50,  1.71s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:50,  1.05it/s]\u001b[A\n","70it [03:51,  1.35it/s]\u001b[A\n","71it [03:51,  1.71it/s]\u001b[A\n","Epoch:  70% 174/250 [05:13<02:05,  1.65s/it]\n","73it [04:33, 12.78s/it]\u001b[A\n","74it [04:33,  9.02s/it]\u001b[A\n","75it [04:33,  6.38s/it]\u001b[A\n","76it [04:33,  4.53s/it]\u001b[A\n","77it [04:33,  3.24s/it]\u001b[A\n","78it [04:34,  2.34s/it]\u001b[A\n","79it [04:34,  1.71s/it]\u001b[A\n","80it [04:34,  1.26s/it]\u001b[A\n","81it [04:34,  1.05it/s]\u001b[A\n","82it [04:35,  1.36it/s]\u001b[A\n","83it [04:35,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [05:57<01:24,  1.65s/it]\n","85it [05:17, 12.77s/it]\u001b[A\n","86it [05:17,  9.01s/it]\u001b[A\n","87it [05:17,  6.38s/it]\u001b[A\n","88it [05:17,  4.53s/it]\u001b[A\n","89it [05:17,  3.24s/it]\u001b[A\n","90it [05:18,  2.34s/it]\u001b[A\n","91it [05:18,  1.71s/it]\u001b[A\n","92it [05:18,  1.26s/it]\u001b[A\n","93it [05:18,  1.05it/s]\u001b[A\n","94it [05:19,  1.36it/s]\u001b[A\n","95it [05:19,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [06:41<00:42,  1.65s/it]\n","97it [06:01, 12.78s/it]\u001b[A\n","98it [06:01,  9.01s/it]\u001b[A\n","99it [06:01,  6.38s/it]\u001b[A\n","100it [06:01,  4.53s/it]\u001b[A\n","101it [06:01,  3.24s/it]\u001b[A\n","102it [06:02,  2.34s/it]\u001b[A\n","103it [06:02,  1.71s/it]\u001b[A\n","104it [06:02,  1.26s/it]\u001b[A\n","105it [06:02,  1.05it/s]\u001b[A\n","106it [06:03,  1.36it/s]\u001b[A\n","107it [06:03,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:25<00:01,  1.66s/it]\n","109it [06:45, 12.78s/it]\u001b[A\n","110it [06:45,  9.02s/it]\u001b[A\n","111it [06:45,  6.38s/it]\u001b[A\n","112it [06:45,  4.54s/it]\u001b[A\n","113it [06:45,  3.24s/it]\u001b[A\n","114it [06:46,  2.34s/it]\u001b[A\n","115it [06:46,  1.71s/it]\u001b[A\n","116it [06:46,  1.26s/it]\u001b[A\n","117it [06:46,  1.05it/s]\u001b[A\n","118it [06:47,  1.36it/s]\u001b[A\n","119it [06:47,  1.71it/s]\u001b[A\n","Epoch: 100% 250/250 [07:29<00:00,  1.80s/it]\n","03/07/2022 07:35:54 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 07:36:07 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:00,  4.29s/it]\u001b[A\n","122it [07:00,  3.08s/it]\u001b[A\n","123it [07:01,  2.22s/it]\u001b[A\n","124it [07:01,  1.62s/it]\u001b[A\n","125it [07:01,  1.21s/it]\u001b[A\n","126it [07:01,  1.09it/s]\u001b[A\n","127it [07:02,  1.41it/s]\u001b[A\n","128it [07:02,  1.77it/s]\u001b[A\n","129it [07:02,  2.15it/s]\u001b[A\n","130it [07:02,  2.53it/s]\u001b[A\n","131it [07:03,  2.90it/s]\u001b[A\n","132it [07:03,  3.58it/s]\u001b[A03/07/2022 07:36:10 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 07:36:10 - INFO - __main__ -     eval_loss = 3.492830753326416\n","03/07/2022 07:36:10 - INFO - __main__ -     eval_auroc = 0.8833749890327454\n","03/07/2022 07:36:10 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/07/2022 07:36:10 - INFO - __main__ -     eval_f1 = 0.4528301954269409\n","03/07/2022 07:36:10 - INFO - filelock -   Lock 140002269154256 acquired on log.lock\n","03/07/2022 07:36:10 - INFO - filelock -   Lock 140002269154256 released on log.lock\n","132it [07:03,  3.21s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 07:36:16 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 07:36:16 - INFO - __main__ -   Specify load the 98-th template: *cls*,*mask*.*+sent_0**sep+*\n","03/07/2022 07:36:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 07:36:16 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-13793', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_07-36-16_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-13793', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 07:36:16 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 07:36:19 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:36:19 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:36:19 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 07:36:19 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:36:19 - INFO - filelock -   Lock 140240038432336 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:36:19 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/07/2022 07:36:19 - INFO - filelock -   Lock 140240038432336 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:36:19 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:36:19 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:36:19 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 07:36:19 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:36:19 - INFO - filelock -   Lock 140240030522320 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:36:19 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/07/2022 07:36:19 - INFO - filelock -   Lock 140240030522320 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:36:19 - INFO - src.dataset -   *** Example ***\n","03/07/2022 07:36:19 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 07:36:19 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 6, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[2], label_word_list=None)\n","03/07/2022 07:36:19 - INFO - src.dataset -   text: <s>,<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 07:36:34 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 07:36:34 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 07:36:34 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 07:36:34 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 07:36:34 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 07:36:34 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 07:36:34 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/07/2022 07:37:19 - INFO - src.trainer -   Best dev result: 0.8827499747276306\n","Epoch:  20% 49/250 [01:32<05:36,  1.67s/it]\n","13it [00:51, 15.00s/it]            \u001b[A\n","14it [00:52, 10.57s/it]\u001b[A\n","15it [00:52,  7.47s/it]\u001b[A\n","16it [00:52,  5.30s/it]\u001b[A\n","17it [00:52,  3.78s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/07/2022 07:38:11 - INFO - src.trainer -   Best dev result: 0.887999951839447\n","Epoch:  30% 74/250 [02:23<04:55,  1.68s/it]\n","25it [01:43, 15.14s/it]\u001b[A\n","26it [01:43, 10.66s/it]\u001b[A\n","27it [01:44,  7.53s/it]\u001b[A\n","28it [01:44,  5.34s/it]\u001b[A\n","29it [01:44,  3.81s/it]\u001b[A\n","30it [01:44,  2.74s/it]\u001b[A\n","31it [01:45,  1.99s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:45,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:07<04:09,  1.65s/it]\n","37it [02:27, 12.82s/it]\u001b[A\n","38it [02:27,  9.04s/it]\u001b[A\n","39it [02:28,  6.40s/it]\u001b[A\n","40it [02:28,  4.55s/it]\u001b[A\n","41it [02:28,  3.25s/it]\u001b[A\n","42it [02:28,  2.35s/it]\u001b[A\n","43it [02:29,  1.71s/it]\u001b[A\n","44it [02:29,  1.27s/it]\u001b[A\n","45it [02:29,  1.04it/s]\u001b[A\n","46it [02:29,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:51<03:27,  1.65s/it]\n","49it [03:11, 12.77s/it]\u001b[A\n","50it [03:11,  9.01s/it]\u001b[A\n","51it [03:12,  6.38s/it]\u001b[A\n","52it [03:12,  4.53s/it]\u001b[A\n","53it [03:12,  3.24s/it]\u001b[A\n","54it [03:12,  2.34s/it]\u001b[A\n","55it [03:13,  1.71s/it]\u001b[A\n","56it [03:13,  1.26s/it]\u001b[A\n","57it [03:13,  1.05it/s]\u001b[A\n","58it [03:13,  1.36it/s]\u001b[A\n","59it [03:14,  1.71it/s]\u001b[A\n","Epoch:  60% 149/250 [04:35<02:46,  1.65s/it]\n","61it [03:55, 12.78s/it]\u001b[A\n","62it [03:55,  9.01s/it]\u001b[A\n","63it [03:56,  6.38s/it]\u001b[A\n","64it [03:56,  4.53s/it]\u001b[A\n","65it [03:56,  3.24s/it]\u001b[A\n","66it [03:56,  2.34s/it]\u001b[A\n","67it [03:57,  1.71s/it]\u001b[A\n","68it [03:57,  1.26s/it]\u001b[A\n","69it [03:57,  1.05it/s]\u001b[A\n","70it [03:57,  1.36it/s]\u001b[A\n","71it [03:58,  1.71it/s]\u001b[A\n","Epoch:  70% 174/250 [05:19<02:05,  1.65s/it]\n","73it [04:39, 12.78s/it]\u001b[A\n","74it [04:39,  9.01s/it]\u001b[A\n","75it [04:40,  6.38s/it]\u001b[A\n","76it [04:40,  4.53s/it]\u001b[A\n","77it [04:40,  3.24s/it]\u001b[A\n","78it [04:40,  2.34s/it]\u001b[A\n","79it [04:41,  1.71s/it]\u001b[A\n","80it [04:41,  1.26s/it]\u001b[A\n","81it [04:41,  1.05it/s]\u001b[A\n","82it [04:41,  1.36it/s]\u001b[A\n","83it [04:41,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:03<01:24,  1.65s/it]\n","85it [05:23, 12.78s/it]\u001b[A\n","86it [05:23,  9.02s/it]\u001b[A\n","87it [05:24,  6.38s/it]\u001b[A\n","88it [05:24,  4.54s/it]\u001b[A\n","89it [05:24,  3.25s/it]\u001b[A\n","90it [05:24,  2.34s/it]\u001b[A\n","91it [05:25,  1.71s/it]\u001b[A\n","92it [05:25,  1.27s/it]\u001b[A\n","93it [05:25,  1.05it/s]\u001b[A\n","94it [05:25,  1.36it/s]\u001b[A\n","95it [05:26,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [06:47<00:42,  1.65s/it]\n","97it [06:07, 12.78s/it]\u001b[A\n","98it [06:07,  9.01s/it]\u001b[A\n","99it [06:08,  6.38s/it]\u001b[A\n","100it [06:08,  4.54s/it]\u001b[A\n","101it [06:08,  3.24s/it]\u001b[A\n","102it [06:08,  2.34s/it]\u001b[A\n","103it [06:09,  1.71s/it]\u001b[A\n","104it [06:09,  1.26s/it]\u001b[A\n","105it [06:09,  1.05it/s]\u001b[A\n","106it [06:09,  1.36it/s]\u001b[A\n","107it [06:09,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:31<00:01,  1.65s/it]\n","109it [06:51, 12.78s/it]\u001b[A\n","110it [06:51,  9.01s/it]\u001b[A\n","111it [06:52,  6.38s/it]\u001b[A\n","112it [06:52,  4.54s/it]\u001b[A\n","113it [06:52,  3.24s/it]\u001b[A\n","114it [06:52,  2.34s/it]\u001b[A\n","115it [06:53,  1.71s/it]\u001b[A\n","116it [06:53,  1.26s/it]\u001b[A\n","117it [06:53,  1.05it/s]\u001b[A\n","118it [06:53,  1.36it/s]\u001b[A\n","119it [06:53,  1.71it/s]\u001b[A\n","Epoch: 100% 250/250 [07:36<00:00,  1.82s/it]\n","03/07/2022 07:44:10 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 07:44:24 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:08,  4.48s/it]\u001b[A\n","122it [07:08,  3.21s/it]\u001b[A\n","123it [07:08,  2.32s/it]\u001b[A\n","124it [07:08,  1.69s/it]\u001b[A\n","125it [07:08,  1.25s/it]\u001b[A\n","126it [07:09,  1.06it/s]\u001b[A\n","127it [07:09,  1.37it/s]\u001b[A\n","128it [07:09,  1.72it/s]\u001b[A\n","129it [07:09,  2.10it/s]\u001b[A\n","130it [07:10,  2.48it/s]\u001b[A\n","131it [07:10,  2.85it/s]\u001b[A\n","132it [07:10,  3.53it/s]\u001b[A03/07/2022 07:44:27 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 07:44:27 - INFO - __main__ -     eval_loss = 3.6558427810668945\n","03/07/2022 07:44:27 - INFO - __main__ -     eval_auroc = 0.887999951839447\n","03/07/2022 07:44:27 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/07/2022 07:44:27 - INFO - __main__ -     eval_f1 = 0.4403669834136963\n","03/07/2022 07:44:27 - INFO - filelock -   Lock 140240016425872 acquired on log.lock\n","03/07/2022 07:44:27 - INFO - filelock -   Lock 140240016425872 released on log.lock\n","132it [07:10,  3.26s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/07/2022 07:44:32 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.txt\n","03/07/2022 07:44:32 - INFO - __main__ -   Specify load the 99-th template: *cls*▁Oh,*mask*.*+sent_0**sep+*\n","03/07/2022 07:44:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 07:44:32 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-25623', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar07_07-44-32_f79a35b9a31c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-25623', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/07/2022 07:44:32 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/07/2022 07:44:35 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:44:35 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:44:35 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 07:44:36 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:44:36 - INFO - filelock -   Lock 140010928485968 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:44:36 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/07/2022 07:44:36 - INFO - filelock -   Lock 140010928485968 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:44:36 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 07:44:36 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 07:44:36 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/07/2022 07:44:36 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 07:44:36 - INFO - filelock -   Lock 140010917834448 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:44:36 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/07/2022 07:44:36 - INFO - filelock -   Lock 140010917834448 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/07/2022 07:44:36 - INFO - src.dataset -   *** Example ***\n","03/07/2022 07:44:36 - INFO - src.dataset -   guid: dev-0\n","03/07/2022 07:44:36 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 7516, 6, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/07/2022 07:44:36 - INFO - src.dataset -   text: <s>▁Oh,<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 07:44:51 - INFO - src.trainer -   ***** Running training *****\n","03/07/2022 07:44:51 - INFO - src.trainer -     Num examples = 32\n","03/07/2022 07:44:51 - INFO - src.trainer -     Num Epochs = 250\n","03/07/2022 07:44:51 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/07/2022 07:44:51 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/07/2022 07:44:51 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/07/2022 07:44:51 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.69it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.68it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.52it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.11it/s]\u001b[A03/07/2022 07:45:35 - INFO - src.trainer -   Best dev result: 0.9172500371932983\n","Epoch:  20% 49/250 [01:32<05:40,  1.70s/it]\n","13it [00:52, 15.08s/it]            \u001b[A\n","14it [00:52, 10.63s/it]\u001b[A\n","15it [00:52,  7.51s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:51,  1.66s/it]\n","25it [01:36, 12.84s/it]\u001b[A\n","26it [01:36,  9.06s/it]\u001b[A\n","27it [01:36,  6.41s/it]\u001b[A\n","28it [01:37,  4.56s/it]\u001b[A\n","29it [01:37,  3.26s/it]\u001b[A\n","30it [01:37,  2.35s/it]\u001b[A\n","31it [01:37,  1.71s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:20, 12.81s/it]\u001b[A\n","38it [02:20,  9.04s/it]\u001b[A\n","39it [02:20,  6.39s/it]\u001b[A\n","40it [02:21,  4.55s/it]\u001b[A\n","41it [02:21,  3.25s/it]\u001b[A\n","42it [02:21,  2.34s/it]\u001b[A\n","43it [02:21,  1.71s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.05it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.71it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:28,  1.66s/it]\n","49it [03:04, 12.81s/it]\u001b[A\n","50it [03:04,  9.04s/it]\u001b[A\n","51it [03:04,  6.40s/it]\u001b[A\n","52it [03:05,  4.55s/it]\u001b[A\n","53it [03:05,  3.25s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:05,  1.71s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.05it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.71it/s]\u001b[A\n","Epoch:  60% 149/250 [04:28<02:47,  1.66s/it]\n","61it [03:48, 12.81s/it]\u001b[A\n","62it [03:48,  9.03s/it]\u001b[A\n","63it [03:49,  6.39s/it]\u001b[A\n","64it [03:49,  4.54s/it]\u001b[A\n","65it [03:49,  3.25s/it]\u001b[A\n","66it [03:49,  2.34s/it]\u001b[A\n","67it [03:49,  1.71s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:50,  1.05it/s]\u001b[A\n","70it [03:50,  1.36it/s]\u001b[A\n","71it [03:50,  1.71it/s]\u001b[A\n","Epoch:  70% 174/250 [05:12<02:05,  1.65s/it]\n","73it [04:32, 12.81s/it]\u001b[A\n","74it [04:32,  9.03s/it]\u001b[A\n","75it [04:33,  6.39s/it]\u001b[A\n","76it [04:33,  4.54s/it]\u001b[A\n","77it [04:33,  3.25s/it]\u001b[A\n","78it [04:33,  2.34s/it]\u001b[A\n","79it [04:34,  1.71s/it]\u001b[A\n","80it [04:34,  1.27s/it]\u001b[A\n","81it [04:34,  1.05it/s]\u001b[A\n","82it [04:34,  1.36it/s]\u001b[A\n","83it [04:34,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [05:56<01:24,  1.66s/it]\n","85it [05:16, 12.80s/it]\u001b[A\n","86it [05:16,  9.03s/it]\u001b[A\n","87it [05:17,  6.39s/it]\u001b[A\n","88it [05:17,  4.54s/it]\u001b[A\n","89it [05:17,  3.25s/it]\u001b[A\n","90it [05:17,  2.34s/it]\u001b[A\n","91it [05:18,  1.71s/it]\u001b[A\n","92it [05:18,  1.27s/it]\u001b[A\n","93it [05:18,  1.05it/s]\u001b[A\n","94it [05:18,  1.36it/s]\u001b[A\n","95it [05:19,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [06:40<00:43,  1.66s/it]\n","97it [06:00, 12.80s/it]\u001b[A\n","98it [06:01,  9.03s/it]\u001b[A\n","99it [06:01,  6.39s/it]\u001b[A\n","100it [06:01,  4.54s/it]\u001b[A\n","101it [06:01,  3.25s/it]\u001b[A\n","102it [06:01,  2.34s/it]\u001b[A\n","103it [06:02,  1.71s/it]\u001b[A\n","104it [06:02,  1.27s/it]\u001b[A\n","105it [06:02,  1.05it/s]\u001b[A\n","106it [06:02,  1.36it/s]\u001b[A\n","107it [06:03,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:24<00:01,  1.66s/it]\n","109it [06:44, 12.81s/it]\u001b[A\n","110it [06:45,  9.03s/it]\u001b[A\n","111it [06:45,  6.39s/it]\u001b[A\n","112it [06:45,  4.55s/it]\u001b[A\n","113it [06:45,  3.25s/it]\u001b[A\n","114it [06:46,  2.34s/it]\u001b[A\n","115it [06:46,  1.71s/it]\u001b[A\n","116it [06:46,  1.27s/it]\u001b[A\n","117it [06:46,  1.05it/s]\u001b[A\n","118it [06:46,  1.35it/s]\u001b[A\n","119it [06:47,  1.71it/s]\u001b[A\n","Epoch: 100% 250/250 [07:29<00:00,  1.80s/it]\n","03/07/2022 07:52:20 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/07/2022 07:52:33 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:00,  4.28s/it]\u001b[A\n","122it [07:00,  3.07s/it]\u001b[A\n","123it [07:01,  2.22s/it]\u001b[A\n","124it [07:01,  1.62s/it]\u001b[A\n","125it [07:01,  1.20s/it]\u001b[A\n","126it [07:01,  1.10it/s]\u001b[A\n","127it [07:01,  1.41it/s]\u001b[A\n","128it [07:02,  1.77it/s]\u001b[A\n","129it [07:02,  2.15it/s]\u001b[A\n","130it [07:02,  2.54it/s]\u001b[A\n","131it [07:02,  2.90it/s]\u001b[A\n","132it [07:02,  3.58it/s]\u001b[A03/07/2022 07:52:36 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/07/2022 07:52:36 - INFO - __main__ -     eval_loss = 2.8028550148010254\n","03/07/2022 07:52:36 - INFO - __main__ -     eval_auroc = 0.9172500371932983\n","03/07/2022 07:52:36 - INFO - __main__ -     eval_recall = 1.0\n","03/07/2022 07:52:36 - INFO - __main__ -     eval_f1 = 0.505050539970398\n","03/07/2022 07:52:36 - INFO - filelock -   Lock 140010988228432 acquired on log.lock\n","03/07/2022 07:52:36 - INFO - filelock -   Lock 140010988228432 released on log.lock\n","132it [07:03,  3.20s/it]\n"]}],"source":["!source env/bin/activate; bash template_search.sh"]},{"cell_type":"code","source":["!source env/bin/activate; python tools/sort_template.py --condition \"{'tag': 'yes-no-template21', 'task_name': 'spoilers'}\" --template_dir spoilers_auto_template --name yes_no"],"metadata":{"id":"SLkCkQf8-NXl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646646635379,"user_tz":480,"elapsed":1862,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"}},"outputId":"1aa0fd8e-d422-4ea5-bbaf-53b0ce69b1c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Seed 21 has 100 results\n"]}]},{"cell_type":"code","source":["# !source env/bin/activate; bash tools/run_generate_labels.sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JG_3x0XXp0W2","executionInfo":{"status":"ok","timestamp":1646650181208,"user_tz":480,"elapsed":453451,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"}},"outputId":"55ad3508-361f-48f8-b77e-dcecd0c8729a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+ K=16\n","+ DATA_DIR=data/k-shot-10x\n","+ OUTPUT_DIR=spoilers_auto_label_mapping\n","+ MODEL_NAME=roberta-large\n","+ LOAD_TEMPLATES=true\n","+ TEMPLATE_DIR=spoilers_auto_template\n","+ TEMPLATE_NAME=yes_no\n","+ NUM_TEMPLATES=20\n","+ K_LIKELY=100\n","+ K_NEIGHBORS=30\n","+ N_PAIRS=100\n","+ TASKS=spoilers\n","+ SEEDS=21\n","+ TASK_EXTRA=\n","+ for TASK in $TASKS\n","+ for SEED in $SEEDS\n","+ case $TASK in\n","+ TEMPLATE='*cls**sent_0*._Spoiler?*mask*.*sep+*'\n","+ MAPPING='{0:'\\''No'\\'',1:'\\''Yes'\\''}'\n","+ [[ true = \\t\\r\\u\\e ]]\n","+ FILENAME=spoilers_auto_template/yes_no/16-21.sort.txt\n","++ head -n 20 spoilers_auto_template/yes_no/16-21.sort.txt\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*.*mask*s.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:42:13 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:42:13 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-42-13_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:42:13 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:42:29 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:42:29 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:42:29 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:42:29 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:42:29 - INFO - filelock -   Lock 140193110928720 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:42:29 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:42:29 - INFO - src.dataset -   Saving features into cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.004 s]\n","03/07/2022 10:42:29 - INFO - filelock -   Lock 140193110928720 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:02<00:00,  1.14s/it]03/07/2022 10:42:32 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:42:32 - INFO - src.label_search -   \t| Label 0: Ġpl, ĠPl, Ġb, ĠYour, ĠOpp, ĠB, Ġ%, ĠTroll, Ġsm, ĠC\n","03/07/2022 10:42:32 - INFO - src.label_search -   \t| Label 1: Ġpl, Ġb, ĠPl, ĠOpp, ĠYour, Ġsm, ĠB, ĠSh, ĠTroll, Ġ%\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 26% 2581/10000 [00:00<00:00, 25484.11it/s]\u001b[A\n"," 58% 5831/10000 [00:00<00:00, 27231.78it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 31185.85it/s]\n","03/07/2022 10:42:33 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:42:33 - INFO - src.label_search -   \t| ĠPhotograph ĠGr (acc = 0.81)\n","03/07/2022 10:42:33 - INFO - src.label_search -   \t| ĠKind Ġ80 (acc = 0.81)\n","03/07/2022 10:42:33 - INFO - src.label_search -   \t| ĠIf ĠGr (acc = 0.81)\n","03/07/2022 10:42:33 - INFO - src.label_search -   \t| ĠScript ĠAw (acc = 0.81)\n","03/07/2022 10:42:33 - INFO - src.label_search -   \t| ĠPhotograph ĠAw (acc = 0.78)\n","100% 2/2 [00:02<00:00,  1.45s/it]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*▁Just*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:42:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:42:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-42-39_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:42:39 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:42:54 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:42:54 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:42:54 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:42:54 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:42:54 - INFO - filelock -   Lock 140207555928336 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:42:54 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:42:54 - INFO - filelock -   Lock 140207555928336 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.42it/s]03/07/2022 10:42:55 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:42:55 - INFO - src.label_search -   \t| Label 0: Ġkidding, Ġwait, Ġsaying, Ġthat, Ġbecause, Ġwow, Ġthink, Ġjoking, Ġno, Ġsay\n","03/07/2022 10:42:55 - INFO - src.label_search -   \t| Label 1: Ġkidding, Ġsaying, Ġwow, Ġwait, Ġjoking, Ġthink, Ġbecause, Ġgreat, Ġthat, Ġwatch\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 31% 3131/10000 [00:00<00:00, 31230.51it/s]\u001b[A\n"," 63% 6271/10000 [00:00<00:00, 31228.23it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 31187.48it/s]\n","03/07/2022 10:42:56 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:42:56 - INFO - src.label_search -   \t| ĠWait Ġfunny (acc = 0.84)\n","03/07/2022 10:42:56 - INFO - src.label_search -   \t| Ġrepeat Ġfunny (acc = 0.81)\n","03/07/2022 10:42:56 - INFO - src.label_search -   \t| Ġreading Ġbeautiful (acc = 0.81)\n","03/07/2022 10:42:56 - INFO - src.label_search -   \t| Ġclick Ġbeautiful (acc = 0.81)\n","03/07/2022 10:42:56 - INFO - src.label_search -   \t| Ġreading Ġfunny (acc = 0.81)\n","100% 2/2 [00:01<00:00,  1.89it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*.*mask*?*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:43:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:43:01 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-43-01_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:43:01 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:43:16 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:43:16 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:43:16 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:43:16 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:43:16 - INFO - filelock -   Lock 140276202999568 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:43:16 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:43:16 - INFO - filelock -   Lock 140276202999568 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:43:17 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:43:17 - INFO - src.label_search -   \t| Label 0: ĠWhat, ĠReally, ĠWhy, ĠHuh, ĠSee, ĠRight, Ġ., ĠNo, ĠHow, ĠSeriously\n","03/07/2022 10:43:17 - INFO - src.label_search -   \t| Label 1: ĠWhat, ĠReally, ĠWhy, ĠHuh, ĠSee, Ġ., Ġwhat, Ġreally, ĠRight, Ġright\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 30% 3041/10000 [00:00<00:00, 30330.12it/s]\u001b[A\n"," 64% 6381/10000 [00:00<00:00, 31163.04it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32846.52it/s]\n","03/07/2022 10:43:18 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:43:18 - INFO - src.label_search -   \t| ĠYes Ġhuh (acc = 0.81)\n","03/07/2022 10:43:18 - INFO - src.label_search -   \t| ĠThen Ġeh (acc = 0.78)\n","03/07/2022 10:43:18 - INFO - src.label_search -   \t| ĠYou Ġanyone (acc = 0.78)\n","03/07/2022 10:43:18 - INFO - src.label_search -   \t| ĠSorry ĠWow (acc = 0.78)\n","03/07/2022 10:43:18 - INFO - src.label_search -   \t| ĠThis Ġnow (acc = 0.78)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*▁for*mask*.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:43:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:43:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-43-23_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:43:23 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:43:39 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:43:39 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:43:39 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:43:39 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:43:39 - INFO - filelock -   Lock 140614948910288 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:43:39 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:43:39 - INFO - filelock -   Lock 140614948910288 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:43:40 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| Label 0: Ġnow, Ġme, Ġsure, Ġreal, Ġyou, Ġgood, Ġexample, Ġeveryone, Ġit, Ġfree\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| Label 1: Ġnow, Ġme, Ġsure, Ġreal, Ġexample, Ġfun, Ġgood, Ġonce, Ġfree, Ġyou\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 32% 3191/10000 [00:00<00:00, 31807.20it/s]\u001b[A\n"," 65% 6461/10000 [00:00<00:00, 32002.52it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32186.23it/s]\n","03/07/2022 10:43:40 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| ĠâĢ¦ Ġgod (acc = 0.84)\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| ĠâĢ¦ Ġjoy (acc = 0.81)\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| ĠNow Ġjoy (acc = 0.81)\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| ĠNow Ġgod (acc = 0.81)\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| Ġdiscussion Ġsimplicity (acc = 0.78)\n","100% 2/2 [00:01<00:00,  1.89it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*▁The*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:43:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:43:46 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-43-46_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:43:46 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:44:01 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:44:01 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:44:01 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:44:01 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:44:01 - INFO - filelock -   Lock 140068756929168 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:44:01 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:44:01 - INFO - filelock -   Lock 140068756929168 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:44:02 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:44:02 - INFO - src.label_search -   \t| Label 0: Ġend, Ġrest, ĠEnd, Ġgame, Ġmovie, Ġworld, Ġfuture, Ġstory, Ġfilm, Ġshow\n","03/07/2022 10:44:02 - INFO - src.label_search -   \t| Label 1: Ġend, Ġending, Ġrest, ĠEnd, Ġgame, Ġstory, Ġworld, Ġfuture, Ġfinale, Ġbest\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 32% 3181/10000 [00:00<00:00, 31737.62it/s]\u001b[A\n"," 61% 6071/10000 [00:00<00:00, 30688.33it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 30567.99it/s]\n","03/07/2022 10:44:03 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:44:03 - INFO - src.label_search -   \t| Ġseries Ġsword (acc = 0.94)\n","03/07/2022 10:44:03 - INFO - src.label_search -   \t| ĠInternet Ġdead (acc = 0.91)\n","03/07/2022 10:44:03 - INFO - src.label_search -   \t| ĠInternet Ġdeath (acc = 0.91)\n","03/07/2022 10:44:03 - INFO - src.label_search -   \t| ĠInternet Ġgood (acc = 0.91)\n","03/07/2022 10:44:03 - INFO - src.label_search -   \t| ĠShow Ġsword (acc = 0.91)\n","100% 2/2 [00:01<00:00,  1.89it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*▁with*mask*.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:44:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:44:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-44-08_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:44:08 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:44:24 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:44:24 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:44:24 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:44:24 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:44:24 - INFO - filelock -   Lock 139990429371792 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:44:24 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:44:24 - INFO - filelock -   Lock 139990429371792 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:44:25 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| Label 0: Ġit, Ġme, Ġthis, Ġthat, Ġyou, Ġhim, Ġthem, Ġus, Ġlove, Ġeverything\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| Label 1: Ġme, Ġit, Ġus, Ġhim, Ġyou, Ġthis, Ġlove, Ġthat, Ġthem, Ġeverything\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2881/10000 [00:00<00:00, 28801.58it/s]\u001b[A\n"," 63% 6281/10000 [00:00<00:00, 30183.25it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32130.36it/s]\n","03/07/2022 10:44:25 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| Ġwork Ġweapons (acc = 0.84)\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| Ġless Ġdeath (acc = 0.84)\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| Ġphotos Ġweapons (acc = 0.84)\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| ĠA Ġflowers (acc = 0.84)\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| Ġquestions Ġhumor (acc = 0.81)\n","100% 2/2 [00:01<00:00,  1.88it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*.*mask*?*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:44:31 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:44:31 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-44-31_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:44:31 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:44:46 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:44:46 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:44:46 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:44:46 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:44:46 - INFO - filelock -   Lock 140438203924560 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:44:46 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:44:46 - INFO - filelock -   Lock 140438203924560 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.44it/s]03/07/2022 10:44:47 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:44:47 - INFO - src.label_search -   \t| Label 0: ĠWhy, ĠWhat, ĠRight, ĠReally, ĠHow, ĠHuh, ĠSeriously, ĠNo, ĠSee, ĠOkay\n","03/07/2022 10:44:47 - INFO - src.label_search -   \t| Label 1: ĠWhy, ĠRight, ĠReally, ĠWhat, ĠHuh, ĠHow, ĠThoughts, ĠNo, Ġright, ĠSeriously\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 30% 3041/10000 [00:00<00:00, 30272.75it/s]\u001b[A\n"," 64% 6401/10000 [00:00<00:00, 31192.00it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32132.06it/s]\n","03/07/2022 10:44:48 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:44:48 - INFO - src.label_search -   \t| ĠOkay ĠRemember (acc = 0.78)\n","03/07/2022 10:44:48 - INFO - src.label_search -   \t| Ġ1 Ġeh (acc = 0.78)\n","03/07/2022 10:44:48 - INFO - src.label_search -   \t| ĠOkay ĠEh (acc = 0.75)\n","03/07/2022 10:44:48 - INFO - src.label_search -   \t| ĠOK ĠEh (acc = 0.75)\n","03/07/2022 10:44:48 - INFO - src.label_search -   \t| Ġ1 Ġ: (acc = 0.75)\n","100% 2/2 [00:01<00:00,  1.89it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*▁is*mask*.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:44:53 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:44:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-44-53_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:44:53 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:45:09 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:45:09 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:45:09 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:45:09 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:45:09 - INFO - filelock -   Lock 140655652146192 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:45:09 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:45:09 - INFO - filelock -   Lock 140655652146192 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.41it/s]03/07/2022 10:45:10 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| Label 0: Ġit, Ġall, Ġhere, Ġgood, Ġnot, Ġright, Ġtrue, Ġme, Ġthere, Ġthat\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| Label 1: Ġit, Ġall, Ġgood, Ġright, Ġhere, Ġnot, Ġme, Ġawesome, Ġgreat, Ġthere\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2931/10000 [00:00<00:00, 29164.29it/s]\u001b[A\n"," 61% 6131/10000 [00:00<00:00, 29954.22it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32295.82it/s]\n","03/07/2022 10:45:10 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| Ġagain Ġface (acc = 0.81)\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| Ġnew Ġname (acc = 0.78)\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| Ġfine Ġcute (acc = 0.78)\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| Ġpossible Ġlol (acc = 0.78)\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| Ġfine Ġnice (acc = 0.78)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*.*mask*▁thanks.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:45:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:45:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-45-16_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:45:16 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:45:31 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:45:31 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:45:31 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:45:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:45:31 - INFO - filelock -   Lock 140077024103824 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:45:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:45:31 - INFO - filelock -   Lock 140077024103824 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.40it/s]03/07/2022 10:45:32 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:45:32 - INFO - src.label_search -   \t| Label 0: Ġ, Ġ:, Ġ:), Ġ\", Ġc, Ġ(, Ġ>, Ġ#, ĠI, Ġand\n","03/07/2022 10:45:32 - INFO - src.label_search -   \t| Label 1: Ġ, Ġ:, Ġ:), Ġ>, Ġc, Ġ(, Ġ#, Ġand, Ġ+, Ġk\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2941/10000 [00:00<00:00, 29343.08it/s]\u001b[A\n"," 62% 6151/10000 [00:00<00:00, 30063.38it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 31544.42it/s]\n","03/07/2022 10:45:33 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:45:33 - INFO - src.label_search -   \t| Ġ* Ġ^ (acc = 0.75)\n","03/07/2022 10:45:33 - INFO - src.label_search -   \t| Ġx Ġthanks (acc = 0.72)\n","03/07/2022 10:45:33 - INFO - src.label_search -   \t| Ġe Ġt (acc = 0.72)\n","03/07/2022 10:45:33 - INFO - src.label_search -   \t| Ġ' Ġ, (acc = 0.72)\n","03/07/2022 10:45:33 - INFO - src.label_search -   \t| Ġ' Ġ?? (acc = 0.72)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*▁And*mask*!*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:45:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:45:38 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-45-38_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:45:38 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:45:54 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:45:54 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:45:54 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:45:54 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:45:54 - INFO - filelock -   Lock 139993514275088 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:45:54 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:45:54 - INFO - filelock -   Lock 139993514275088 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.42it/s]03/07/2022 10:45:55 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| Label 0: Ġnow, Ġfinally, Ġhey, Ġwait, Ġlook, Ġthen, Ġyes, Ġyet, Ġyeah, Ġso\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| Label 1: Ġfinally, Ġhey, Ġnow, Ġlook, Ġwait, Ġyes, Ġthen, Ġyeah, Ġwow, Ġoh\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 31% 3121/10000 [00:00<00:00, 27788.58it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32534.14it/s]\n","03/07/2022 10:45:55 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| ĠNo Ġbonus (acc = 0.81)\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| ĠBravo Ġgreat (acc = 0.81)\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| Ġsoon Ġfinal (acc = 0.81)\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| ĠâĢ¦ Ġfinal (acc = 0.81)\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| ĠDone Ġfinal (acc = 0.81)\n","100% 2/2 [00:00<00:00,  2.07it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*▁Oh*mask*!*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:46:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:46:01 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-46-01_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:46:01 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:46:16 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:46:16 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:46:16 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:46:16 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:46:16 - INFO - filelock -   Lock 140553886015568 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:46:16 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:46:16 - INFO - filelock -   Lock 140553886015568 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.36it/s]03/07/2022 10:46:17 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:46:17 - INFO - src.label_search -   \t| Label 0: Ġno, Ġyeah, Ġboy, Ġmy, Ġhey, Ġyes, Ġwait, ĠNo, Ġman, Ġshit\n","03/07/2022 10:46:17 - INFO - src.label_search -   \t| Label 1: Ġno, Ġyeah, Ġboy, Ġhey, Ġmy, Ġman, Ġwow, Ġyes, Ġwait, Ġdear\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2871/10000 [00:00<00:00, 28662.33it/s]\u001b[A\n"," 62% 6191/10000 [00:00<00:00, 29867.67it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32328.61it/s]\n","03/07/2022 10:46:18 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:46:18 - INFO - src.label_search -   \t| ĠHell Ġbye (acc = 0.81)\n","03/07/2022 10:46:18 - INFO - src.label_search -   \t| Ġfuck Ġcool (acc = 0.78)\n","03/07/2022 10:46:18 - INFO - src.label_search -   \t| ĠBrother Ġsweet (acc = 0.78)\n","03/07/2022 10:46:18 - INFO - src.label_search -   \t| ĠHell Ġdad (acc = 0.75)\n","03/07/2022 10:46:18 - INFO - src.label_search -   \t| ĠBrother Ġfun (acc = 0.75)\n","100% 2/2 [00:01<00:00,  1.86it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*).*mask*.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:46:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:46:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-46-23_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:46:23 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:46:39 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:46:39 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:46:39 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:46:39 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:46:39 - INFO - filelock -   Lock 140329352908368 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:46:39 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:46:39 - INFO - filelock -   Lock 140329352908368 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.31it/s]03/07/2022 10:46:40 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| Label 0: Ġ2, Ġ3, Ġ1, Ġ5, Ġ4, Ġetc, Ġ6, Ġ7, Ġ8, ĠâĢ¦\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| Label 1: Ġ2, Ġ3, Ġ1, Ġetc, Ġ5, Ġ4, Ġhere, Ġlol, Ġ6, ĠâĢ¦\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2911/10000 [00:00<00:00, 28894.33it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 33388.93it/s]\n","03/07/2022 10:46:40 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| ĠEver Ġthanks (acc = 0.81)\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| ĠNone Ġwow (acc = 0.78)\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| ĠNone Ġthanks (acc = 0.78)\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| ĠNow Ġfinally (acc = 0.78)\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| ĠNow Ġthanks (acc = 0.78)\n","100% 2/2 [00:00<00:00,  2.04it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*▁from*mask*.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:46:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:46:46 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-46-46_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:46:46 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:47:01 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:47:01 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:47:01 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:47:01 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:47:01 - INFO - filelock -   Lock 140642451383440 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:47:01 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:47:01 - INFO - filelock -   Lock 140642451383440 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.37it/s]03/07/2022 10:47:02 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:47:02 - INFO - src.label_search -   \t| Label 0: Ġhere, Ġthere, Ġit, Ġme, Ġabove, Ġthis, Ġnow, Ġbelow, Ġtoday, ĠWikipedia\n","03/07/2022 10:47:02 - INFO - src.label_search -   \t| Label 1: Ġhere, Ġthere, Ġabove, Ġit, Ġme, Ġhell, Ġthis, Ġbelow, Ġnowhere, ĠJapan\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2923/10000 [00:00<00:00, 29229.61it/s]\u001b[A\n"," 62% 6231/10000 [00:00<00:00, 30264.84it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32425.56it/s]\n","03/07/2022 10:47:03 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:47:03 - INFO - src.label_search -   \t| Ġ2008 Ġheaven (acc = 0.81)\n","03/07/2022 10:47:03 - INFO - src.label_search -   \t| ĠâĢ¦ Ġgod (acc = 0.81)\n","03/07/2022 10:47:03 - INFO - src.label_search -   \t| ĠA Ġgod (acc = 0.81)\n","03/07/2022 10:47:03 - INFO - src.label_search -   \t| Ġ2009 Ġheaven (acc = 0.81)\n","03/07/2022 10:47:03 - INFO - src.label_search -   \t| Ġ2013 Ġheaven (acc = 0.81)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*:*mask*!*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:47:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:47:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-47-08_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:47:08 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:47:24 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:47:24 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:47:24 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:47:24 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:47:24 - INFO - filelock -   Lock 140220184513936 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:47:24 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:47:24 - INFO - filelock -   Lock 140220184513936 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.39it/s]03/07/2022 10:47:25 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| Label 0: ĠYes, ĠThanks, ĠNo, ĠWow, ĠYeah, ĠSorry, ĠYES, ĠEnjoy, ĠOh, ĠNO\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| Label 1: ĠEnjoy, ĠThanks, ĠWow, ĠLOL, Ġlol, Ġawesome, ĠYeah, ĠYes, Ġwow, Ġthanks\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 33% 3281/10000 [00:00<00:00, 29322.55it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 33013.33it/s]\n","03/07/2022 10:47:25 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| ĠAgain ĠGG (acc = 0.84)\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| ĠHere ĠGG (acc = 0.81)\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| Ġ0 Ġdie (acc = 0.81)\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| ĠOkay ĠGG (acc = 0.81)\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| ĠHELP ĠPOW (acc = 0.81)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*▁And*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:47:31 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:47:31 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-47-31_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:47:31 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:47:46 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:47:46 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:47:46 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:47:46 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:47:46 - INFO - filelock -   Lock 139790228980944 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:47:46 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:47:46 - INFO - filelock -   Lock 139790228980944 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:47:47 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| Label 0: Ġ2, Ġfinally, Ġ3, Ġ4, Ġthen, Ġnow, Ġ5, Ġyes, Ġ1, Ġso\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| Label 1: Ġfinally, Ġ2, Ġ3, Ġthen, Ġ4, Ġyes, Ġnow, Ġ5, Ġ1, Ġno\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 33% 3271/10000 [00:00<00:00, 32684.24it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 33991.10it/s]\n","03/07/2022 10:47:47 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| ĠNow Ġoh (acc = 0.84)\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| Ġanyway Ġoh (acc = 0.84)\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| ĠSo Ġah (acc = 0.84)\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| ĠI Ġoh (acc = 0.81)\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| Ġit Ġsecond (acc = 0.81)\n","100% 2/2 [00:00<00:00,  2.09it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*▁No*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:47:53 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:47:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-47-53_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:47:53 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:48:08 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:48:08 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:48:08 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:48:08 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:48:08 - INFO - filelock -   Lock 140541305159952 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:48:08 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:48:08 - INFO - filelock -   Lock 140541305159952 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.40it/s]03/07/2022 10:48:09 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:48:09 - INFO - src.label_search -   \t| Label 0: Ġway, Ġcomment, Ġcomments, Ġproblem, Ġkidding, Ġmore, Ġno, Ġjoke, Ġmatter, Ġwait\n","03/07/2022 10:48:09 - INFO - src.label_search -   \t| Label 1: Ġkidding, Ġway, Ġjoke, Ġproblem, Ġno, Ġcomment, Ġwait, Ġmatter, Ġworries, Ġcomments\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 32% 3194/10000 [00:00<00:00, 31939.95it/s]\u001b[A\n"," 64% 6351/10000 [00:00<00:00, 31803.03it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 33080.32it/s]\n","03/07/2022 10:48:10 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:48:10 - INFO - src.label_search -   \t| ĠThanks Ġfair (acc = 0.81)\n","03/07/2022 10:48:10 - INFO - src.label_search -   \t| Ġcontact Ġgo (acc = 0.78)\n","03/07/2022 10:48:10 - INFO - src.label_search -   \t| Ġinfo Ġgo (acc = 0.78)\n","03/07/2022 10:48:10 - INFO - src.label_search -   \t| Ġdetails Ġscene (acc = 0.78)\n","03/07/2022 10:48:10 - INFO - src.label_search -   \t| ĠComment Ġjoking (acc = 0.78)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*▁>*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:48:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:48:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-48-16_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:48:16 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:48:31 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:48:31 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:48:31 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:48:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:48:31 - INFO - filelock -   Lock 140444342324560 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:48:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:48:31 - INFO - filelock -   Lock 140444342324560 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.41it/s]03/07/2022 10:48:32 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| Label 0: Ġ1, Ġ2, Ġ3, Ġ5, Ġ4, ĠNo, Ġ6, ĠA, Ġ8, Ġ10\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| Label 1: Ġ1, Ġ2, Ġ3, Ġ5, Ġ4, Ġ6, ĠNo, Ġ8, Ġ7, Ġ10\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 30% 3001/10000 [00:00<00:00, 29844.66it/s]\u001b[A\n"," 62% 6181/10000 [00:00<00:00, 30396.05it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32245.47it/s]\n","03/07/2022 10:48:32 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| Ġyeah Ġwow (acc = 0.75)\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| ĠX ĠOops (acc = 0.75)\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| ĠU ĠIndeed (acc = 0.75)\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| Ġ... ĠOops (acc = 0.75)\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| ĠM Ġvs (acc = 0.72)\n","100% 2/2 [00:01<00:00,  1.88it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*▁4.*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:48:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:48:38 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-48-38_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:48:38 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:48:53 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:48:53 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:48:53 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:48:53 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:48:53 - INFO - filelock -   Lock 140247307999376 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:48:53 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:48:53 - INFO - filelock -   Lock 140247307999376 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.42it/s]03/07/2022 10:48:54 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:48:54 - INFO - src.label_search -   \t| Label 0: Ġ2, Ġ1, Ġ3, Ġ5, Ġ4, ĠA, Ġ6, Ġ8, Ġ7, ĠB\n","03/07/2022 10:48:54 - INFO - src.label_search -   \t| Label 1: Ġ2, Ġ1, Ġ5, Ġ3, Ġ4, Ġ6, ĠA, Ġ8, Ġ7, Ġ9\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 27% 2691/10000 [00:00<00:00, 26872.67it/s]\u001b[A\n"," 60% 6001/10000 [00:00<00:00, 28295.14it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 31764.59it/s]\n","03/07/2022 10:48:55 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:48:55 - INFO - src.label_search -   \t| ĠAll ĠAh (acc = 0.81)\n","03/07/2022 10:48:55 - INFO - src.label_search -   \t| ĠU ĠZ (acc = 0.78)\n","03/07/2022 10:48:55 - INFO - src.label_search -   \t| Ġ2018 ĠAmazing (acc = 0.78)\n","03/07/2022 10:48:55 - INFO - src.label_search -   \t| ĠO ĠZ (acc = 0.75)\n","03/07/2022 10:48:55 - INFO - src.label_search -   \t| ĠAll ĠOkay (acc = 0.75)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*▁Yes!*mask*!*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:49:00 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:49:00 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-49-00_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:49:00 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:49:16 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:49:16 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:49:16 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:49:16 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:49:16 - INFO - filelock -   Lock 139837995482512 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:49:16 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:49:16 - INFO - filelock -   Lock 139837995482512 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:49:17 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| Label 0: ĠYes, ĠNo, ĠYES, Ġyes, ĠPlease, ĠYeah, ĠFinally, ĠNow, ĠAbsolutely, ĠIndeed\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| Label 1: ĠYes, Ġyes, ĠNo, ĠYES, ĠYeah, ĠPlease, ĠFinally, ĠAbsolutely, ĠIndeed, ĠNow\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 35% 3491/10000 [00:00<00:00, 34659.16it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 34711.63it/s]\n","03/07/2022 10:49:17 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| ĠWe ĠBad (acc = 0.81)\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| ĠHere ĠBad (acc = 0.81)\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| ĠYou ĠGo (acc = 0.81)\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| ĠIt ĠBad (acc = 0.78)\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| ĠActually ĠPerfect (acc = 0.78)\n","100% 2/2 [00:00<00:00,  2.10it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*,*mask*?*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:49:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:49:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-49-23_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:49:23 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:49:38 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/07/2022 10:49:38 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/07/2022 10:49:38 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:49:38 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:49:38 - INFO - filelock -   Lock 139694936154384 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:49:38 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:49:38 - INFO - filelock -   Lock 139694936154384 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.44it/s]03/07/2022 10:49:39 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:49:39 - INFO - src.label_search -   \t| Label 0: Ġright, Ġhuh, Ġno, Ġeh, Ġokay, ĠOK, Ġok, Ġyeah, Ġwhat, Ġyes\n","03/07/2022 10:49:39 - INFO - src.label_search -   \t| Label 1: Ġright, Ġno, Ġhuh, Ġeh, Ġyeah, Ġyes, Ġok, Ġokay, Ġwhat, Ġanyone\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 32% 3151/10000 [00:00<00:00, 31421.26it/s]\u001b[A\n"," 63% 6261/10000 [00:00<00:00, 31316.46it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32861.48it/s]\n","03/07/2022 10:49:40 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:49:40 - INFO - src.label_search -   \t| ĠBro Ġtrue (acc = 0.81)\n","03/07/2022 10:49:40 - INFO - src.label_search -   \t| ĠDad Ġstill (acc = 0.81)\n","03/07/2022 10:49:40 - INFO - src.label_search -   \t| Ġit Ġtrue (acc = 0.78)\n","03/07/2022 10:49:40 - INFO - src.label_search -   \t| ĠDad Ġactually (acc = 0.78)\n","03/07/2022 10:49:40 - INFO - src.label_search -   \t| Ġpeople Ġha (acc = 0.78)\n","100% 2/2 [00:01<00:00,  1.88it/s]\n"]}]},{"cell_type":"code","source":["# !source env/bin/activate; bash label_search.sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-fgYh11vGYF","outputId":"28103d17-215a-4014-bd98-a2ca50ebf27c","executionInfo":{"status":"ok","timestamp":1646729802969,"user_tz":480,"elapsed":29807,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 05:11:42 - INFO - src.trainer -   Best dev result: 0.9383749961853027\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.14s/it]            \u001b[A\n","14it [00:52, 10.67s/it]\u001b[A\n","15it [00:52,  7.54s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.81s/it]\u001b[A\n","18it [00:53,  2.74s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/08/2022 05:12:34 - INFO - src.trainer -   Best dev result: 0.9388749599456787\n","Epoch:  30% 74/250 [02:23<04:52,  1.66s/it]\n","25it [01:43, 15.03s/it]\u001b[A\n","26it [01:44, 10.59s/it]\u001b[A\n","27it [01:44,  7.48s/it]\u001b[A\n","28it [01:44,  5.31s/it]\u001b[A\n","29it [01:44,  3.78s/it]\u001b[A\n","30it [01:44,  2.72s/it]\u001b[A\n","31it [01:45,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:46,  1.54it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:10,  1.66s/it]\n","37it [02:27, 12.86s/it]\u001b[A\n","38it [02:28,  9.07s/it]\u001b[A\n","39it [02:28,  6.42s/it]\u001b[A\n","40it [02:28,  4.56s/it]\u001b[A\n","41it [02:28,  3.26s/it]\u001b[A\n","42it [02:29,  2.35s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.27s/it]\u001b[A\n","45it [02:29,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:29,  1.67s/it]\n","49it [03:12, 12.89s/it]\u001b[A\n","50it [03:12,  9.09s/it]\u001b[A\n","51it [03:12,  6.43s/it]\u001b[A\n","52it [03:13,  4.57s/it]\u001b[A\n","53it [03:13,  3.27s/it]\u001b[A\n","54it [03:13,  2.36s/it]\u001b[A\n","55it [03:13,  1.72s/it]\u001b[A\n","56it [03:13,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:36<02:48,  1.67s/it]\n","61it [03:56, 12.88s/it]\u001b[A\n","62it [03:56,  9.09s/it]\u001b[A\n","63it [03:57,  6.43s/it]\u001b[A\n","64it [03:57,  4.57s/it]\u001b[A\n","65it [03:57,  3.27s/it]\u001b[A\n","66it [03:57,  2.36s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:58,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.67s/it]\n","73it [04:40, 12.88s/it]\u001b[A\n","74it [04:41,  9.09s/it]\u001b[A\n","75it [04:41,  6.43s/it]\u001b[A\n","76it [04:41,  4.57s/it]\u001b[A\n","77it [04:41,  3.27s/it]\u001b[A\n","78it [04:42,  2.36s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:25,  1.67s/it]\n","85it [05:25, 12.88s/it]\u001b[A\n","86it [05:25,  9.09s/it]\u001b[A\n","87it [05:25,  6.43s/it]\u001b[A\n","88it [05:26,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:09, 12.87s/it]\u001b[A\n","98it [06:09,  9.08s/it]\u001b[A\n","99it [06:10,  6.43s/it]\u001b[A\n","100it [06:10,  4.57s/it]\u001b[A\n","101it [06:10,  3.27s/it]\u001b[A\n","102it [06:10,  2.36s/it]\u001b[A\n","103it [06:11,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.67s/it]\n","109it [06:53, 12.88s/it]\u001b[A\n","110it [06:54,  9.08s/it]\u001b[A\n","111it [06:54,  6.43s/it]\u001b[A\n","112it [06:54,  4.57s/it]\u001b[A\n","113it [06:54,  3.27s/it]\u001b[A\n","114it [06:55,  2.36s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:56,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 05:18:36 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 05:18:50 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.49s/it]\u001b[A\n","122it [07:10,  3.22s/it]\u001b[A\n","123it [07:10,  2.32s/it]\u001b[A\n","124it [07:10,  1.69s/it]\u001b[A\n","125it [07:11,  1.25s/it]\u001b[A\n","126it [07:11,  1.06it/s]\u001b[A\n","127it [07:11,  1.36it/s]\u001b[A\n","128it [07:11,  1.72it/s]\u001b[A\n","129it [07:12,  2.10it/s]\u001b[A\n","130it [07:12,  2.48it/s]\u001b[A\n","131it [07:12,  2.84it/s]\u001b[A\n","132it [07:12,  3.52it/s]\u001b[A03/08/2022 05:18:52 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 05:18:52 - INFO - __main__ -     eval_loss = 2.5238890647888184\n","03/08/2022 05:18:52 - INFO - __main__ -     eval_auroc = 0.9388749599456787\n","03/08/2022 05:18:52 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 05:18:52 - INFO - __main__ -     eval_f1 = 0.5952380895614624\n","03/08/2022 05:18:52 - INFO - filelock -   Lock 139850988202896 acquired on log.lock\n","03/08/2022 05:18:52 - INFO - filelock -   Lock 139850988202896 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 05:18:58 - INFO - __main__ -   Specify load the 74-th prompt: *cls*▁And*mask*.*+sent_0**sep+* | {0: \"it\", 1: \"second\"}\n","03/08/2022 05:18:58 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 05:18:58 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-22833', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_05-18-58_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-22833', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 05:18:58 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 05:18:59 - INFO - src.dataset -   Label 0 to word Ġit (24)\n","03/08/2022 05:18:59 - INFO - src.dataset -   Label 1 to word Ġsecond (200)\n","03/08/2022 05:18:59 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 05:18:59 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:18:59 - INFO - filelock -   Lock 139997109647120 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:18:59 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 05:18:59 - INFO - filelock -   Lock 139997109647120 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:18:59 - INFO - src.dataset -   Label 0 to word Ġit (24)\n","03/08/2022 05:18:59 - INFO - src.dataset -   Label 1 to word Ġsecond (200)\n","03/08/2022 05:18:59 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 05:18:59 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:18:59 - INFO - filelock -   Lock 139997109646800 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:18:59 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 05:18:59 - INFO - filelock -   Lock 139997109646800 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:18:59 - INFO - src.dataset -   *** Example ***\n","03/08/2022 05:18:59 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 05:18:59 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 2409, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 05:18:59 - INFO - src.dataset -   text: <s>▁And<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 05:19:15 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 05:19:15 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 05:19:15 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 05:19:15 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 05:19:15 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 05:19:15 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 05:19:15 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.90it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.70it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.58it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.44it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 05:19:59 - INFO - src.trainer -   Best dev result: 0.8761249780654907\n","Epoch:  20% 49/250 [01:31<05:41,  1.70s/it]\n","13it [00:51, 14.92s/it]            \u001b[A\n","14it [00:51, 10.51s/it]\u001b[A\n","15it [00:52,  7.43s/it]\u001b[A\n","16it [00:52,  5.27s/it]\u001b[A\n","17it [00:52,  3.76s/it]\u001b[A\n","18it [00:52,  2.70s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.55it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:35, 12.89s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:36,  6.43s/it]\u001b[A\n","28it [01:36,  4.57s/it]\u001b[A\n","29it [01:36,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:20, 12.84s/it]\u001b[A\n","38it [02:20,  9.06s/it]\u001b[A\n","39it [02:20,  6.41s/it]\u001b[A\n","40it [02:20,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:21,  1.71s/it]\u001b[A\n","44it [02:21,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:29,  1.66s/it]\n","49it [03:04, 12.85s/it]\u001b[A\n","50it [03:04,  9.06s/it]\u001b[A\n","51it [03:04,  6.41s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:05,  1.72s/it]\u001b[A\n","56it [03:05,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:28<02:47,  1.66s/it]\n","61it [03:48, 12.84s/it]\u001b[A\n","62it [03:48,  9.06s/it]\u001b[A\n","63it [03:49,  6.41s/it]\u001b[A\n","64it [03:49,  4.56s/it]\u001b[A\n","65it [03:49,  3.26s/it]\u001b[A\n","66it [03:49,  2.35s/it]\u001b[A\n","67it [03:49,  1.72s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:50,  1.04it/s]\u001b[A\n","70it [03:50,  1.35it/s]\u001b[A\n","71it [03:50,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:12<02:06,  1.66s/it]\n","73it [04:32, 12.84s/it]\u001b[A\n","74it [04:32,  9.06s/it]\u001b[A\n","75it [04:33,  6.41s/it]\u001b[A\n","76it [04:33,  4.56s/it]\u001b[A\n","77it [04:33,  3.26s/it]\u001b[A\n","78it [04:33,  2.35s/it]\u001b[A\n","79it [04:34,  1.72s/it]\u001b[A\n","80it [04:34,  1.27s/it]\u001b[A\n","81it [04:34,  1.04it/s]\u001b[A\n","82it [04:34,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","84it [04:35,  2.23it/s]\u001b[A03/08/2022 05:24:32 - INFO - src.trainer -   Best dev result: 0.8776249289512634\n","Epoch:  80% 199/250 [06:05<01:24,  1.66s/it]\n","85it [05:24, 15.24s/it]\u001b[A\n","86it [05:25, 10.74s/it]\u001b[A\n","87it [05:25,  7.59s/it]\u001b[A\n","88it [05:25,  5.38s/it]\u001b[A\n","89it [05:25,  3.84s/it]\u001b[A\n","90it [05:26,  2.75s/it]\u001b[A\n","91it [05:26,  2.00s/it]\u001b[A\n","92it [05:26,  1.47s/it]\u001b[A\n","93it [05:26,  1.10s/it]\u001b[A\n","94it [05:27,  1.19it/s]\u001b[A\n","95it [05:27,  1.53it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:09, 12.87s/it]\u001b[A\n","98it [06:09,  9.08s/it]\u001b[A\n","99it [06:09,  6.42s/it]\u001b[A\n","100it [06:09,  4.57s/it]\u001b[A\n","101it [06:10,  3.27s/it]\u001b[A\n","102it [06:10,  2.36s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:10,  1.27s/it]\u001b[A\n","105it [06:10,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:33<00:01,  1.66s/it]\n","109it [06:53, 12.83s/it]\u001b[A\n","110it [06:53,  9.05s/it]\u001b[A\n","111it [06:53,  6.41s/it]\u001b[A\n","112it [06:53,  4.55s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:54,  1.71s/it]\u001b[A\n","116it [06:54,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:55,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:37<00:00,  1.83s/it]\n","03/08/2022 05:26:53 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 05:27:07 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:09,  4.56s/it]\u001b[A\n","122it [07:10,  3.26s/it]\u001b[A\n","123it [07:10,  2.35s/it]\u001b[A\n","124it [07:10,  1.72s/it]\u001b[A\n","125it [07:10,  1.27s/it]\u001b[A\n","126it [07:11,  1.04it/s]\u001b[A\n","127it [07:11,  1.35it/s]\u001b[A\n","128it [07:11,  1.70it/s]\u001b[A\n","129it [07:11,  2.08it/s]\u001b[A\n","130it [07:11,  2.46it/s]\u001b[A\n","131it [07:12,  2.83it/s]\u001b[A\n","132it [07:12,  3.51it/s]\u001b[A03/08/2022 05:27:09 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 05:27:09 - INFO - __main__ -     eval_loss = 5.557480812072754\n","03/08/2022 05:27:09 - INFO - __main__ -     eval_auroc = 0.8776249289512634\n","03/08/2022 05:27:09 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 05:27:09 - INFO - __main__ -     eval_f1 = 0.44859808683395386\n","03/08/2022 05:27:09 - INFO - filelock -   Lock 139997111084944 acquired on log.lock\n","03/08/2022 05:27:09 - INFO - filelock -   Lock 139997111084944 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 05:27:15 - INFO - __main__ -   Specify load the 75-th prompt: *cls*▁No*mask*.*+sent_0**sep+* | {0: \"Thanks\", 1: \"fair\"}\n","03/08/2022 05:27:15 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 05:27:15 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-32396', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_05-27-15_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-32396', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 05:27:15 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 05:27:17 - INFO - src.dataset -   Label 0 to word ĠThanks (4557)\n","03/08/2022 05:27:17 - INFO - src.dataset -   Label 1 to word Ġfair (2105)\n","03/08/2022 05:27:17 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 05:27:17 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:27:17 - INFO - filelock -   Lock 139997601188112 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:27:17 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 05:27:17 - INFO - filelock -   Lock 139997601188112 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:27:17 - INFO - src.dataset -   Label 0 to word ĠThanks (4557)\n","03/08/2022 05:27:17 - INFO - src.dataset -   Label 1 to word Ġfair (2105)\n","03/08/2022 05:27:17 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 05:27:17 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:27:17 - INFO - filelock -   Lock 139997599745808 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:27:17 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 05:27:17 - INFO - filelock -   Lock 139997599745808 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:27:17 - INFO - src.dataset -   *** Example ***\n","03/08/2022 05:27:17 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 05:27:17 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 3084, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 05:27:17 - INFO - src.dataset -   text: <s>▁No<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 05:27:32 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 05:27:32 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 05:27:32 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 05:27:32 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 05:27:32 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 05:27:32 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 05:27:32 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.11it/s]\u001b[A03/08/2022 05:28:16 - INFO - src.trainer -   Best dev result: 0.9000000357627869\n","Epoch:  20% 49/250 [01:32<05:34,  1.67s/it]\n","13it [00:51, 14.96s/it]            \u001b[A\n","14it [00:52, 10.54s/it]\u001b[A\n","15it [00:52,  7.45s/it]\u001b[A\n","16it [00:52,  5.28s/it]\u001b[A\n","17it [00:52,  3.77s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.55it/s]\u001b[A\n","24it [00:54,  2.04it/s]\u001b[A03/08/2022 05:29:08 - INFO - src.trainer -   Best dev result: 0.9117500185966492\n","Epoch:  30% 74/250 [02:23<04:54,  1.67s/it]\n","25it [01:43, 15.04s/it]\u001b[A\n","26it [01:43, 10.59s/it]\u001b[A\n","27it [01:43,  7.49s/it]\u001b[A\n","28it [01:43,  5.31s/it]\u001b[A\n","29it [01:44,  3.79s/it]\u001b[A\n","30it [01:44,  2.72s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:44,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","36it [01:45,  2.03it/s]\u001b[A03/08/2022 05:30:00 - INFO - src.trainer -   Best dev result: 0.9122499823570251\n","Epoch:  40% 99/250 [03:15<04:10,  1.66s/it]\n","37it [02:35, 15.16s/it]\u001b[A\n","38it [02:35, 10.68s/it]\u001b[A\n","39it [02:35,  7.55s/it]\u001b[A\n","40it [02:35,  5.35s/it]\u001b[A\n","41it [02:36,  3.81s/it]\u001b[A\n","42it [02:36,  2.74s/it]\u001b[A\n","43it [02:36,  1.99s/it]\u001b[A\n","44it [02:36,  1.46s/it]\u001b[A\n","45it [02:36,  1.09s/it]\u001b[A\n","46it [02:37,  1.20it/s]\u001b[A\n","47it [02:37,  1.53it/s]\u001b[A\n","Epoch:  50% 124/250 [03:59<03:28,  1.66s/it]\n","49it [03:19, 12.84s/it]\u001b[A\n","50it [03:19,  9.05s/it]\u001b[A\n","51it [03:19,  6.41s/it]\u001b[A\n","52it [03:19,  4.55s/it]\u001b[A\n","53it [03:20,  3.26s/it]\u001b[A\n","54it [03:20,  2.35s/it]\u001b[A\n","55it [03:20,  1.71s/it]\u001b[A\n","56it [03:20,  1.27s/it]\u001b[A\n","57it [03:20,  1.04it/s]\u001b[A\n","58it [03:21,  1.35it/s]\u001b[A\n","59it [03:21,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:43<02:47,  1.66s/it]\n","61it [04:03, 12.81s/it]\u001b[A\n","62it [04:03,  9.04s/it]\u001b[A\n","63it [04:03,  6.39s/it]\u001b[A\n","64it [04:03,  4.55s/it]\u001b[A\n","65it [04:04,  3.25s/it]\u001b[A\n","66it [04:04,  2.34s/it]\u001b[A\n","67it [04:04,  1.71s/it]\u001b[A\n","68it [04:04,  1.27s/it]\u001b[A\n","69it [04:05,  1.05it/s]\u001b[A\n","70it [04:05,  1.35it/s]\u001b[A\n","71it [04:05,  1.71it/s]\u001b[A\n","Epoch:  70% 174/250 [05:27<02:05,  1.66s/it]\n","73it [04:47, 12.81s/it]\u001b[A\n","74it [04:47,  9.04s/it]\u001b[A\n","75it [04:47,  6.39s/it]\u001b[A\n","76it [04:48,  4.55s/it]\u001b[A\n","77it [04:48,  3.25s/it]\u001b[A\n","78it [04:48,  2.34s/it]\u001b[A\n","79it [04:48,  1.71s/it]\u001b[A\n","80it [04:48,  1.27s/it]\u001b[A\n","81it [04:49,  1.05it/s]\u001b[A\n","82it [04:49,  1.35it/s]\u001b[A\n","83it [04:49,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:11<01:24,  1.66s/it]\n","85it [05:31, 12.82s/it]\u001b[A\n","86it [05:31,  9.04s/it]\u001b[A\n","87it [05:31,  6.40s/it]\u001b[A\n","88it [05:32,  4.55s/it]\u001b[A\n","89it [05:32,  3.25s/it]\u001b[A\n","90it [05:32,  2.35s/it]\u001b[A\n","91it [05:32,  1.71s/it]\u001b[A\n","92it [05:33,  1.27s/it]\u001b[A\n","93it [05:33,  1.05it/s]\u001b[A\n","94it [05:33,  1.35it/s]\u001b[A\n","95it [05:33,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:55<00:43,  1.66s/it]\n","97it [06:15, 12.82s/it]\u001b[A\n","98it [06:15,  9.04s/it]\u001b[A\n","99it [06:16,  6.40s/it]\u001b[A\n","100it [06:16,  4.55s/it]\u001b[A\n","101it [06:16,  3.25s/it]\u001b[A\n","102it [06:16,  2.35s/it]\u001b[A\n","103it [06:16,  1.71s/it]\u001b[A\n","104it [06:17,  1.27s/it]\u001b[A\n","105it [06:17,  1.05it/s]\u001b[A\n","106it [06:17,  1.35it/s]\u001b[A\n","107it [06:17,  1.70it/s]\u001b[A\n","108it [06:17,  2.23it/s]\u001b[A03/08/2022 05:34:32 - INFO - src.trainer -   Best dev result: 0.9124999642372131\n","Epoch: 100% 249/250 [07:47<00:01,  1.67s/it]\n","109it [07:07, 15.11s/it]\u001b[A\n","110it [07:07, 10.64s/it]\u001b[A\n","111it [07:07,  7.52s/it]\u001b[A\n","112it [07:07,  5.33s/it]\u001b[A\n","113it [07:08,  3.80s/it]\u001b[A\n","114it [07:08,  2.73s/it]\u001b[A\n","115it [07:08,  1.98s/it]\u001b[A\n","116it [07:08,  1.46s/it]\u001b[A\n","117it [07:09,  1.09s/it]\u001b[A\n","118it [07:09,  1.20it/s]\u001b[A\n","119it [07:09,  1.54it/s]\u001b[A\n","Epoch: 100% 250/250 [07:51<00:00,  1.89s/it]\n","03/08/2022 05:35:24 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 05:35:38 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:24,  4.63s/it]\u001b[A\n","122it [07:24,  3.31s/it]\u001b[A\n","123it [07:24,  2.39s/it]\u001b[A\n","124it [07:24,  1.74s/it]\u001b[A\n","125it [07:24,  1.29s/it]\u001b[A\n","126it [07:25,  1.03it/s]\u001b[A\n","127it [07:25,  1.34it/s]\u001b[A\n","128it [07:25,  1.69it/s]\u001b[A\n","129it [07:25,  2.06it/s]\u001b[A\n","130it [07:26,  2.45it/s]\u001b[A\n","131it [07:26,  2.81it/s]\u001b[A\n","132it [07:26,  3.49it/s]\u001b[A03/08/2022 05:35:40 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 05:35:40 - INFO - __main__ -     eval_loss = 2.6098716259002686\n","03/08/2022 05:35:40 - INFO - __main__ -     eval_auroc = 0.9124999642372131\n","03/08/2022 05:35:40 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 05:35:40 - INFO - __main__ -     eval_f1 = 0.5274725556373596\n","03/08/2022 05:35:40 - INFO - filelock -   Lock 139997601187856 acquired on log.lock\n","03/08/2022 05:35:40 - INFO - filelock -   Lock 139997601187856 released on log.lock\n","132it [07:26,  3.38s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 05:35:46 - INFO - __main__ -   Specify load the 76-th prompt: *cls*▁No*mask*.*+sent_0**sep+* | {0: \"contact\", 1: \"go\"}\n","03/08/2022 05:35:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 05:35:46 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-4908', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_05-35-46_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-4908', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 05:35:46 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 05:35:48 - INFO - src.dataset -   Label 0 to word Ġcontact (1511)\n","03/08/2022 05:35:48 - INFO - src.dataset -   Label 1 to word Ġgo (213)\n","03/08/2022 05:35:48 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 05:35:48 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:35:48 - INFO - filelock -   Lock 140219659973840 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:35:48 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 05:35:48 - INFO - filelock -   Lock 140219659973840 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:35:48 - INFO - src.dataset -   Label 0 to word Ġcontact (1511)\n","03/08/2022 05:35:48 - INFO - src.dataset -   Label 1 to word Ġgo (213)\n","03/08/2022 05:35:48 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 05:35:48 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:35:48 - INFO - filelock -   Lock 140219659973776 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:35:48 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 05:35:48 - INFO - filelock -   Lock 140219659973776 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:35:48 - INFO - src.dataset -   *** Example ***\n","03/08/2022 05:35:48 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 05:35:48 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 3084, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 05:35:48 - INFO - src.dataset -   text: <s>▁No<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 05:36:03 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 05:36:03 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 05:36:03 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 05:36:03 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 05:36:03 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 05:36:03 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 05:36:03 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 05:36:47 - INFO - src.trainer -   Best dev result: 0.9597499370574951\n","Epoch:  20% 49/250 [01:32<05:38,  1.69s/it]\n","13it [00:51, 14.97s/it]            \u001b[A\n","14it [00:52, 10.55s/it]\u001b[A\n","15it [00:52,  7.45s/it]\u001b[A\n","16it [00:52,  5.29s/it]\u001b[A\n","17it [00:52,  3.77s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.04it/s]\u001b[A03/08/2022 05:37:39 - INFO - src.trainer -   Best dev result: 0.9663749933242798\n","Epoch:  30% 74/250 [02:23<04:53,  1.67s/it]\n","25it [01:43, 15.11s/it]\u001b[A\n","26it [01:43, 10.64s/it]\u001b[A\n","27it [01:43,  7.52s/it]\u001b[A\n","28it [01:44,  5.33s/it]\u001b[A\n","29it [01:44,  3.80s/it]\u001b[A\n","30it [01:44,  2.73s/it]\u001b[A\n","31it [01:44,  1.98s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:45,  1.20it/s]\u001b[A\n","35it [01:45,  1.53it/s]\u001b[A\n","36it [01:45,  2.02it/s]\u001b[A03/08/2022 05:38:31 - INFO - src.trainer -   Best dev result: 0.9753749966621399\n","Epoch:  40% 99/250 [03:15<04:11,  1.67s/it]\n","37it [02:35, 15.27s/it]\u001b[A\n","38it [02:35, 10.76s/it]\u001b[A\n","39it [02:36,  7.60s/it]\u001b[A\n","40it [02:36,  5.39s/it]\u001b[A\n","41it [02:36,  3.84s/it]\u001b[A\n","42it [02:36,  2.76s/it]\u001b[A\n","43it [02:37,  2.00s/it]\u001b[A\n","44it [02:37,  1.47s/it]\u001b[A\n","45it [02:37,  1.10s/it]\u001b[A\n","46it [02:37,  1.19it/s]\u001b[A\n","47it [02:38,  1.52it/s]\u001b[A\n","48it [02:38,  2.01it/s]\u001b[A03/08/2022 05:39:23 - INFO - src.trainer -   Best dev result: 0.9763749837875366\n","Epoch:  50% 124/250 [04:08<03:33,  1.69s/it]\n","49it [03:27, 15.30s/it]\u001b[A\n","50it [03:28, 10.78s/it]\u001b[A\n","51it [03:28,  7.61s/it]\u001b[A\n","52it [03:28,  5.40s/it]\u001b[A\n","53it [03:28,  3.85s/it]\u001b[A\n","54it [03:29,  2.76s/it]\u001b[A\n","55it [03:29,  2.00s/it]\u001b[A\n","56it [03:29,  1.47s/it]\u001b[A\n","57it [03:29,  1.10s/it]\u001b[A\n","58it [03:30,  1.19it/s]\u001b[A\n","59it [03:30,  1.52it/s]\u001b[A\n","60it [03:30,  2.01it/s]\u001b[A03/08/2022 05:40:15 - INFO - src.trainer -   Best dev result: 0.9763750433921814\n","Epoch:  60% 149/250 [05:00<02:49,  1.67s/it]\n","61it [04:19, 15.20s/it]\u001b[A\n","62it [04:20, 10.71s/it]\u001b[A\n","63it [04:20,  7.56s/it]\u001b[A\n","64it [04:20,  5.36s/it]\u001b[A\n","65it [04:20,  3.82s/it]\u001b[A\n","66it [04:21,  2.75s/it]\u001b[A\n","67it [04:21,  1.99s/it]\u001b[A\n","68it [04:21,  1.46s/it]\u001b[A\n","69it [04:21,  1.09s/it]\u001b[A\n","70it [04:21,  1.20it/s]\u001b[A\n","71it [04:22,  1.53it/s]\u001b[A\n","72it [04:22,  2.02it/s]\u001b[A03/08/2022 05:41:07 - INFO - src.trainer -   Best dev result: 0.9766249656677246\n","Epoch:  70% 174/250 [05:52<02:06,  1.67s/it]\n","73it [05:12, 15.35s/it]\u001b[A\n","74it [05:12, 10.82s/it]\u001b[A\n","75it [05:12,  7.64s/it]\u001b[A\n","76it [05:13,  5.42s/it]\u001b[A\n","77it [05:13,  3.86s/it]\u001b[A\n","78it [05:13,  2.77s/it]\u001b[A\n","79it [05:13,  2.01s/it]\u001b[A\n","80it [05:13,  1.48s/it]\u001b[A\n","81it [05:14,  1.10s/it]\u001b[A\n","82it [05:14,  1.19it/s]\u001b[A\n","83it [05:14,  1.52it/s]\u001b[A\n","Epoch:  80% 199/250 [06:36<01:24,  1.66s/it]\n","85it [05:56, 12.88s/it]\u001b[A\n","86it [05:56,  9.09s/it]\u001b[A\n","87it [05:57,  6.43s/it]\u001b[A\n","88it [05:57,  4.57s/it]\u001b[A\n","89it [05:57,  3.27s/it]\u001b[A\n","90it [05:57,  2.36s/it]\u001b[A\n","91it [05:57,  1.72s/it]\u001b[A\n","92it [05:58,  1.27s/it]\u001b[A\n","93it [05:58,  1.04it/s]\u001b[A\n","94it [05:58,  1.35it/s]\u001b[A\n","95it [05:58,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:21<00:43,  1.66s/it]\n","97it [06:40, 12.84s/it]\u001b[A\n","98it [06:40,  9.06s/it]\u001b[A\n","99it [06:41,  6.41s/it]\u001b[A\n","100it [06:41,  4.56s/it]\u001b[A\n","101it [06:41,  3.26s/it]\u001b[A\n","102it [06:41,  2.35s/it]\u001b[A\n","103it [06:42,  1.71s/it]\u001b[A\n","104it [06:42,  1.27s/it]\u001b[A\n","105it [06:42,  1.04it/s]\u001b[A\n","106it [06:42,  1.35it/s]\u001b[A\n","107it [06:43,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [08:05<00:01,  1.66s/it]\n","109it [07:24, 12.84s/it]\u001b[A\n","110it [07:25,  9.06s/it]\u001b[A\n","111it [07:25,  6.41s/it]\u001b[A\n","112it [07:25,  4.56s/it]\u001b[A\n","113it [07:25,  3.26s/it]\u001b[A\n","114it [07:26,  2.35s/it]\u001b[A\n","115it [07:26,  1.71s/it]\u001b[A\n","116it [07:26,  1.27s/it]\u001b[A\n","117it [07:26,  1.04it/s]\u001b[A\n","118it [07:27,  1.35it/s]\u001b[A\n","119it [07:27,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [08:09<00:00,  1.96s/it]\n","03/08/2022 05:44:12 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 05:44:26 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:41,  4.62s/it]\u001b[A\n","122it [07:41,  3.30s/it]\u001b[A\n","123it [07:42,  2.38s/it]\u001b[A\n","124it [07:42,  1.74s/it]\u001b[A\n","125it [07:42,  1.28s/it]\u001b[A\n","126it [07:42,  1.03it/s]\u001b[A\n","127it [07:43,  1.34it/s]\u001b[A\n","128it [07:43,  1.69it/s]\u001b[A\n","129it [07:43,  2.07it/s]\u001b[A\n","130it [07:43,  2.45it/s]\u001b[A\n","131it [07:44,  2.81it/s]\u001b[A\n","132it [07:44,  3.49it/s]\u001b[A03/08/2022 05:44:29 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 05:44:29 - INFO - __main__ -     eval_loss = 0.7703326940536499\n","03/08/2022 05:44:29 - INFO - __main__ -     eval_auroc = 0.9766249656677246\n","03/08/2022 05:44:29 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 05:44:29 - INFO - __main__ -     eval_f1 = 0.7272726893424988\n","03/08/2022 05:44:29 - INFO - filelock -   Lock 140219643571664 acquired on log.lock\n","03/08/2022 05:44:29 - INFO - filelock -   Lock 140219643571664 released on log.lock\n","132it [07:44,  3.52s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 05:44:35 - INFO - __main__ -   Specify load the 77-th prompt: *cls*▁No*mask*.*+sent_0**sep+* | {0: \"info\", 1: \"go\"}\n","03/08/2022 05:44:35 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 05:44:35 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-2523', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_05-44-35_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-2523', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 05:44:35 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 05:44:36 - INFO - src.dataset -   Label 0 to word Ġinfo (8574)\n","03/08/2022 05:44:36 - INFO - src.dataset -   Label 1 to word Ġgo (213)\n","03/08/2022 05:44:36 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 05:44:36 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:44:36 - INFO - filelock -   Lock 140331303923600 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:44:36 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 05:44:36 - INFO - filelock -   Lock 140331303923600 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:44:36 - INFO - src.dataset -   Label 0 to word Ġinfo (8574)\n","03/08/2022 05:44:36 - INFO - src.dataset -   Label 1 to word Ġgo (213)\n","03/08/2022 05:44:36 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 05:44:36 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:44:36 - INFO - filelock -   Lock 140331303922576 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:44:36 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/08/2022 05:44:36 - INFO - filelock -   Lock 140331303922576 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:44:36 - INFO - src.dataset -   *** Example ***\n","03/08/2022 05:44:36 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 05:44:36 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 3084, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 05:44:36 - INFO - src.dataset -   text: <s>▁No<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 05:44:52 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 05:44:52 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 05:44:52 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 05:44:52 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 05:44:52 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 05:44:52 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 05:44:52 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 05:45:36 - INFO - src.trainer -   Best dev result: 0.9300000071525574\n","Epoch:  20% 49/250 [01:32<05:34,  1.66s/it]\n","13it [00:52, 15.09s/it]            \u001b[A\n","14it [00:52, 10.63s/it]\u001b[A\n","15it [00:52,  7.51s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 05:46:29 - INFO - src.trainer -   Best dev result: 0.9307499527931213\n","Epoch:  30% 74/250 [02:23<04:55,  1.68s/it]\n","25it [01:43, 15.04s/it]\u001b[A\n","26it [01:43, 10.60s/it]\u001b[A\n","27it [01:44,  7.49s/it]\u001b[A\n","28it [01:44,  5.31s/it]\u001b[A\n","29it [01:44,  3.79s/it]\u001b[A\n","30it [01:44,  2.72s/it]\u001b[A\n","31it [01:45,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:46,  1.54it/s]\u001b[A\n","36it [01:46,  2.03it/s]\u001b[A03/08/2022 05:47:20 - INFO - src.trainer -   Best dev result: 0.9373749494552612\n","Epoch:  40% 99/250 [03:15<04:12,  1.67s/it]\n","37it [02:35, 15.16s/it]\u001b[A\n","38it [02:35, 10.68s/it]\u001b[A\n","39it [02:35,  7.55s/it]\u001b[A\n","40it [02:36,  5.35s/it]\u001b[A\n","41it [02:36,  3.82s/it]\u001b[A\n","42it [02:36,  2.74s/it]\u001b[A\n","43it [02:36,  1.99s/it]\u001b[A\n","44it [02:37,  1.46s/it]\u001b[A\n","45it [02:37,  1.09s/it]\u001b[A\n","46it [02:37,  1.20it/s]\u001b[A\n","47it [02:37,  1.53it/s]\u001b[A\n","48it [02:37,  2.02it/s]\u001b[A03/08/2022 05:48:12 - INFO - src.trainer -   Best dev result: 0.9467499852180481\n","Epoch:  50% 124/250 [04:08<03:29,  1.67s/it]\n","49it [03:27, 15.33s/it]\u001b[A\n","50it [03:28, 10.80s/it]\u001b[A\n","51it [03:28,  7.63s/it]\u001b[A\n","52it [03:28,  5.41s/it]\u001b[A\n","53it [03:28,  3.86s/it]\u001b[A\n","54it [03:29,  2.77s/it]\u001b[A\n","55it [03:29,  2.01s/it]\u001b[A\n","56it [03:29,  1.47s/it]\u001b[A\n","57it [03:29,  1.10s/it]\u001b[A\n","58it [03:29,  1.19it/s]\u001b[A\n","59it [03:30,  1.52it/s]\u001b[A\n","Epoch:  60% 149/250 [04:52<02:47,  1.66s/it]\n","61it [04:12, 12.86s/it]\u001b[A\n","62it [04:12,  9.07s/it]\u001b[A\n","63it [04:12,  6.42s/it]\u001b[A\n","64it [04:12,  4.56s/it]\u001b[A\n","65it [04:12,  3.26s/it]\u001b[A\n","66it [04:13,  2.35s/it]\u001b[A\n","67it [04:13,  1.72s/it]\u001b[A\n","68it [04:13,  1.27s/it]\u001b[A\n","69it [04:13,  1.04it/s]\u001b[A\n","70it [04:14,  1.35it/s]\u001b[A\n","71it [04:14,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:36<02:05,  1.66s/it]\n","73it [04:56, 12.82s/it]\u001b[A\n","74it [04:56,  9.04s/it]\u001b[A\n","75it [04:56,  6.40s/it]\u001b[A\n","76it [04:56,  4.55s/it]\u001b[A\n","77it [04:57,  3.25s/it]\u001b[A\n","78it [04:57,  2.35s/it]\u001b[A\n","79it [04:57,  1.71s/it]\u001b[A\n","80it [04:57,  1.27s/it]\u001b[A\n","81it [04:58,  1.05it/s]\u001b[A\n","82it [04:58,  1.35it/s]\u001b[A\n","83it [04:58,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:20<01:24,  1.66s/it]\n","85it [05:40, 12.81s/it]\u001b[A\n","86it [05:40,  9.04s/it]\u001b[A\n","87it [05:40,  6.39s/it]\u001b[A\n","88it [05:40,  4.55s/it]\u001b[A\n","89it [05:41,  3.25s/it]\u001b[A\n","90it [05:41,  2.35s/it]\u001b[A\n","91it [05:41,  1.71s/it]\u001b[A\n","92it [05:41,  1.27s/it]\u001b[A\n","93it [05:42,  1.05it/s]\u001b[A\n","94it [05:42,  1.35it/s]\u001b[A\n","95it [05:42,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [07:04<00:43,  1.66s/it]\n","97it [06:24, 12.81s/it]\u001b[A\n","98it [06:24,  9.04s/it]\u001b[A\n","99it [06:24,  6.40s/it]\u001b[A\n","100it [06:25,  4.55s/it]\u001b[A\n","101it [06:25,  3.25s/it]\u001b[A\n","102it [06:25,  2.35s/it]\u001b[A\n","103it [06:25,  1.71s/it]\u001b[A\n","104it [06:25,  1.27s/it]\u001b[A\n","105it [06:26,  1.05it/s]\u001b[A\n","106it [06:26,  1.35it/s]\u001b[A\n","107it [06:26,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:48<00:01,  1.66s/it]\n","109it [07:08, 12.82s/it]\u001b[A\n","110it [07:08,  9.04s/it]\u001b[A\n","111it [07:08,  6.40s/it]\u001b[A\n","112it [07:09,  4.55s/it]\u001b[A\n","113it [07:09,  3.25s/it]\u001b[A\n","114it [07:09,  2.35s/it]\u001b[A\n","115it [07:09,  1.71s/it]\u001b[A\n","116it [07:10,  1.27s/it]\u001b[A\n","117it [07:10,  1.05it/s]\u001b[A\n","118it [07:10,  1.35it/s]\u001b[A\n","119it [07:10,  1.71it/s]\u001b[A\n","Epoch: 100% 250/250 [07:52<00:00,  1.89s/it]\n","03/08/2022 05:52:45 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 05:52:59 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:25,  4.65s/it]\u001b[A\n","122it [07:25,  3.33s/it]\u001b[A\n","123it [07:25,  2.40s/it]\u001b[A\n","124it [07:26,  1.75s/it]\u001b[A\n","125it [07:26,  1.29s/it]\u001b[A\n","126it [07:26,  1.03it/s]\u001b[A\n","127it [07:26,  1.33it/s]\u001b[A\n","128it [07:26,  1.68it/s]\u001b[A\n","129it [07:27,  2.06it/s]\u001b[A\n","130it [07:27,  2.44it/s]\u001b[A\n","131it [07:27,  2.81it/s]\u001b[A\n","132it [07:27,  3.49it/s]\u001b[A03/08/2022 05:53:02 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 05:53:02 - INFO - __main__ -     eval_loss = 1.7783153057098389\n","03/08/2022 05:53:02 - INFO - __main__ -     eval_auroc = 0.9467499852180481\n","03/08/2022 05:53:02 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 05:53:02 - INFO - __main__ -     eval_f1 = 0.6024096608161926\n","03/08/2022 05:53:02 - INFO - filelock -   Lock 140331304148368 acquired on log.lock\n","03/08/2022 05:53:02 - INFO - filelock -   Lock 140331304148368 released on log.lock\n","132it [07:27,  3.39s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 05:53:08 - INFO - __main__ -   Specify load the 78-th prompt: *cls*▁No*mask*.*+sent_0**sep+* | {0: \"details\", 1: \"scene\"}\n","03/08/2022 05:53:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 05:53:08 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-11013', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_05-53-08_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-11013', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 05:53:08 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 05:53:10 - INFO - src.dataset -   Label 0 to word Ġdetails (1254)\n","03/08/2022 05:53:10 - INFO - src.dataset -   Label 1 to word Ġscene (1310)\n","03/08/2022 05:53:10 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 05:53:10 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:53:10 - INFO - filelock -   Lock 140377742976784 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:53:10 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 05:53:10 - INFO - filelock -   Lock 140377742976784 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:53:10 - INFO - src.dataset -   Label 0 to word Ġdetails (1254)\n","03/08/2022 05:53:10 - INFO - src.dataset -   Label 1 to word Ġscene (1310)\n","03/08/2022 05:53:10 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 05:53:10 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:53:10 - INFO - filelock -   Lock 140377714542928 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:53:10 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 05:53:10 - INFO - filelock -   Lock 140377714542928 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:53:10 - INFO - src.dataset -   *** Example ***\n","03/08/2022 05:53:10 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 05:53:10 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 3084, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 05:53:10 - INFO - src.dataset -   text: <s>▁No<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 05:53:25 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 05:53:25 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 05:53:25 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 05:53:25 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 05:53:25 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 05:53:25 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 05:53:25 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.64it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.08it/s]\u001b[A03/08/2022 05:54:10 - INFO - src.trainer -   Best dev result: 0.9197499752044678\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.12s/it]            \u001b[A\n","14it [00:52, 10.66s/it]\u001b[A\n","15it [00:52,  7.53s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.81s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/08/2022 05:55:02 - INFO - src.trainer -   Best dev result: 0.9332499504089355\n","Epoch:  30% 74/250 [02:24<04:56,  1.69s/it]\n","25it [01:44, 15.13s/it]\u001b[A\n","26it [01:44, 10.66s/it]\u001b[A\n","27it [01:44,  7.53s/it]\u001b[A\n","28it [01:44,  5.34s/it]\u001b[A\n","29it [01:45,  3.81s/it]\u001b[A\n","30it [01:45,  2.74s/it]\u001b[A\n","31it [01:45,  1.98s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:46,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:11,  1.66s/it]\n","37it [02:28, 12.89s/it]\u001b[A\n","38it [02:28,  9.09s/it]\u001b[A\n","39it [02:28,  6.43s/it]\u001b[A\n","40it [02:29,  4.57s/it]\u001b[A\n","41it [02:29,  3.27s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.27s/it]\u001b[A\n","45it [02:30,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:29,  1.66s/it]\n","49it [03:12, 12.85s/it]\u001b[A\n","50it [03:12,  9.07s/it]\u001b[A\n","51it [03:13,  6.42s/it]\u001b[A\n","52it [03:13,  4.56s/it]\u001b[A\n","53it [03:13,  3.26s/it]\u001b[A\n","54it [03:13,  2.35s/it]\u001b[A\n","55it [03:13,  1.72s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:36<02:47,  1.66s/it]\n","61it [03:56, 12.85s/it]\u001b[A\n","62it [03:57,  9.06s/it]\u001b[A\n","63it [03:57,  6.41s/it]\u001b[A\n","64it [03:57,  4.56s/it]\u001b[A\n","65it [03:57,  3.26s/it]\u001b[A\n","66it [03:57,  2.35s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:59,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.66s/it]\n","73it [04:41, 12.85s/it]\u001b[A\n","74it [04:41,  9.06s/it]\u001b[A\n","75it [04:41,  6.41s/it]\u001b[A\n","76it [04:41,  4.56s/it]\u001b[A\n","77it [04:41,  3.26s/it]\u001b[A\n","78it [04:42,  2.35s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:24,  1.66s/it]\n","85it [05:25, 12.86s/it]\u001b[A\n","86it [05:25,  9.07s/it]\u001b[A\n","87it [05:25,  6.42s/it]\u001b[A\n","88it [05:25,  4.56s/it]\u001b[A\n","89it [05:26,  3.26s/it]\u001b[A\n","90it [05:26,  2.35s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:09, 12.85s/it]\u001b[A\n","98it [06:09,  9.06s/it]\u001b[A\n","99it [06:09,  6.41s/it]\u001b[A\n","100it [06:10,  4.56s/it]\u001b[A\n","101it [06:10,  3.26s/it]\u001b[A\n","102it [06:10,  2.35s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:33<00:01,  1.66s/it]\n","109it [06:53, 12.85s/it]\u001b[A\n","110it [06:53,  9.06s/it]\u001b[A\n","111it [06:54,  6.41s/it]\u001b[A\n","112it [06:54,  4.56s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 06:01:04 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:01:18 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.61s/it]\u001b[A\n","122it [07:10,  3.30s/it]\u001b[A\n","123it [07:10,  2.38s/it]\u001b[A\n","124it [07:11,  1.73s/it]\u001b[A\n","125it [07:11,  1.28s/it]\u001b[A\n","126it [07:11,  1.03it/s]\u001b[A\n","127it [07:11,  1.34it/s]\u001b[A\n","128it [07:12,  1.69it/s]\u001b[A\n","129it [07:12,  2.07it/s]\u001b[A\n","130it [07:12,  2.45it/s]\u001b[A\n","131it [07:12,  2.82it/s]\u001b[A\n","132it [07:12,  3.50it/s]\u001b[A03/08/2022 06:01:20 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:01:20 - INFO - __main__ -     eval_loss = 1.5492570400238037\n","03/08/2022 06:01:20 - INFO - __main__ -     eval_auroc = 0.9332499504089355\n","03/08/2022 06:01:20 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/08/2022 06:01:20 - INFO - __main__ -     eval_f1 = 0.6388888955116272\n","03/08/2022 06:01:20 - INFO - filelock -   Lock 140378233225808 acquired on log.lock\n","03/08/2022 06:01:20 - INFO - filelock -   Lock 140378233225808 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:01:26 - INFO - __main__ -   Specify load the 79-th prompt: *cls*▁No*mask*.*+sent_0**sep+* | {0: \"Comment\", 1: \"joking\"}\n","03/08/2022 06:01:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:01:26 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-272', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-01-26_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-272', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:01:26 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:01:28 - INFO - src.dataset -   Label 0 to word ĠComment (14642)\n","03/08/2022 06:01:28 - INFO - src.dataset -   Label 1 to word Ġjoking (22024)\n","03/08/2022 06:01:28 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:01:28 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:01:28 - INFO - filelock -   Lock 139628890232656 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:01:28 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 06:01:28 - INFO - filelock -   Lock 139628890232656 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:01:28 - INFO - src.dataset -   Label 0 to word ĠComment (14642)\n","03/08/2022 06:01:28 - INFO - src.dataset -   Label 1 to word Ġjoking (22024)\n","03/08/2022 06:01:28 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:01:28 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:01:28 - INFO - filelock -   Lock 139628565614544 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:01:28 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:01:28 - INFO - filelock -   Lock 139628565614544 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:01:28 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:01:28 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:01:28 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 3084, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:01:28 - INFO - src.dataset -   text: <s>▁No<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:01:43 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:01:43 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:01:43 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:01:43 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:01:43 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:01:43 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:01:43 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.63it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 06:02:28 - INFO - src.trainer -   Best dev result: 0.9497499465942383\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.04s/it]            \u001b[A\n","14it [00:52, 10.59s/it]\u001b[A\n","15it [00:52,  7.48s/it]\u001b[A\n","16it [00:52,  5.31s/it]\u001b[A\n","17it [00:53,  3.79s/it]\u001b[A\n","18it [00:53,  2.72s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.87s/it]\u001b[A\n","26it [01:36,  9.08s/it]\u001b[A\n","27it [01:36,  6.42s/it]\u001b[A\n","28it [01:36,  4.56s/it]\u001b[A\n","29it [01:37,  3.26s/it]\u001b[A\n","30it [01:37,  2.35s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:20, 12.84s/it]\u001b[A\n","38it [02:20,  9.06s/it]\u001b[A\n","39it [02:20,  6.41s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:21,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:29,  1.66s/it]\n","49it [03:04, 12.84s/it]\u001b[A\n","50it [03:04,  9.06s/it]\u001b[A\n","51it [03:05,  6.41s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:06,  1.71s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:28<02:47,  1.66s/it]\n","61it [03:48, 12.83s/it]\u001b[A\n","62it [03:49,  9.05s/it]\u001b[A\n","63it [03:49,  6.40s/it]\u001b[A\n","64it [03:49,  4.55s/it]\u001b[A\n","65it [03:49,  3.26s/it]\u001b[A\n","66it [03:49,  2.35s/it]\u001b[A\n","67it [03:50,  1.71s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:50,  1.04it/s]\u001b[A\n","70it [03:50,  1.35it/s]\u001b[A\n","71it [03:51,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:13<02:06,  1.66s/it]\n","73it [04:33, 12.84s/it]\u001b[A\n","74it [04:33,  9.06s/it]\u001b[A\n","75it [04:33,  6.41s/it]\u001b[A\n","76it [04:33,  4.56s/it]\u001b[A\n","77it [04:33,  3.26s/it]\u001b[A\n","78it [04:34,  2.35s/it]\u001b[A\n","79it [04:34,  1.71s/it]\u001b[A\n","80it [04:34,  1.27s/it]\u001b[A\n","81it [04:34,  1.04it/s]\u001b[A\n","82it [04:35,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","84it [04:35,  2.23it/s]\u001b[A03/08/2022 06:07:01 - INFO - src.trainer -   Best dev result: 0.9537500143051147\n","Epoch:  80% 199/250 [06:04<01:25,  1.67s/it]\n","85it [05:24, 14.98s/it]\u001b[A\n","86it [05:24, 10.56s/it]\u001b[A\n","87it [05:24,  7.46s/it]\u001b[A\n","88it [05:25,  5.29s/it]\u001b[A\n","89it [05:25,  3.77s/it]\u001b[A\n","90it [05:25,  2.71s/it]\u001b[A\n","91it [05:25,  1.97s/it]\u001b[A\n","92it [05:25,  1.45s/it]\u001b[A\n","93it [05:26,  1.08s/it]\u001b[A\n","94it [05:26,  1.21it/s]\u001b[A\n","95it [05:26,  1.54it/s]\u001b[A\n","Epoch:  90% 224/250 [06:48<00:43,  1.66s/it]\n","97it [06:08, 12.86s/it]\u001b[A\n","98it [06:08,  9.07s/it]\u001b[A\n","99it [06:08,  6.42s/it]\u001b[A\n","100it [06:09,  4.56s/it]\u001b[A\n","101it [06:09,  3.26s/it]\u001b[A\n","102it [06:09,  2.35s/it]\u001b[A\n","103it [06:09,  1.72s/it]\u001b[A\n","104it [06:10,  1.27s/it]\u001b[A\n","105it [06:10,  1.04it/s]\u001b[A\n","106it [06:10,  1.35it/s]\u001b[A\n","107it [06:10,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:32<00:01,  1.66s/it]\n","109it [06:52, 12.83s/it]\u001b[A\n","110it [06:52,  9.05s/it]\u001b[A\n","111it [06:53,  6.40s/it]\u001b[A\n","112it [06:53,  4.55s/it]\u001b[A\n","113it [06:53,  3.26s/it]\u001b[A\n","114it [06:53,  2.35s/it]\u001b[A\n","115it [06:54,  1.71s/it]\u001b[A\n","116it [06:54,  1.27s/it]\u001b[A\n","117it [06:54,  1.04it/s]\u001b[A\n","118it [06:54,  1.35it/s]\u001b[A\n","119it [06:54,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:37<00:00,  1.83s/it]\n","03/08/2022 06:09:20 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:09:34 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:09,  4.56s/it]\u001b[A\n","122it [07:09,  3.26s/it]\u001b[A\n","123it [07:09,  2.35s/it]\u001b[A\n","124it [07:09,  1.71s/it]\u001b[A\n","125it [07:10,  1.27s/it]\u001b[A\n","126it [07:10,  1.04it/s]\u001b[A\n","127it [07:10,  1.35it/s]\u001b[A\n","128it [07:10,  1.70it/s]\u001b[A\n","129it [07:11,  2.08it/s]\u001b[A\n","130it [07:11,  2.46it/s]\u001b[A\n","131it [07:11,  2.83it/s]\u001b[A\n","132it [07:11,  3.50it/s]\u001b[A03/08/2022 06:09:37 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:09:37 - INFO - __main__ -     eval_loss = 0.8776593208312988\n","03/08/2022 06:09:37 - INFO - __main__ -     eval_auroc = 0.9537500143051147\n","03/08/2022 06:09:37 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/08/2022 06:09:37 - INFO - __main__ -     eval_f1 = 0.7457627058029175\n","03/08/2022 06:09:37 - INFO - filelock -   Lock 139628565614544 acquired on log.lock\n","03/08/2022 06:09:37 - INFO - filelock -   Lock 139628565614544 released on log.lock\n","132it [07:11,  3.27s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:09:43 - INFO - __main__ -   Specify load the 80-th prompt: *cls*▁>*mask*.*+sent_0**sep+* | {0: \"yeah\", 1: \"wow\"}\n","03/08/2022 06:09:43 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:09:43 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-14167', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-09-43_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-14167', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:09:43 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:09:44 - INFO - src.dataset -   Label 0 to word Ġyeah (11380)\n","03/08/2022 06:09:44 - INFO - src.dataset -   Label 1 to word Ġwow (26388)\n","03/08/2022 06:09:44 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:09:44 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:09:44 - INFO - filelock -   Lock 140274189581200 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:09:44 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 06:09:44 - INFO - filelock -   Lock 140274189581200 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:09:44 - INFO - src.dataset -   Label 0 to word Ġyeah (11380)\n","03/08/2022 06:09:44 - INFO - src.dataset -   Label 1 to word Ġwow (26388)\n","03/08/2022 06:09:44 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:09:44 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:09:44 - INFO - filelock -   Lock 140273901432656 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:09:44 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:09:44 - INFO - filelock -   Lock 140273901432656 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:09:44 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:09:44 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:09:44 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 15698, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:09:44 - INFO - src.dataset -   text: <s>▁><mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:09:59 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:09:59 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:09:59 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:09:59 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:09:59 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:09:59 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:09:59 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 06:10:44 - INFO - src.trainer -   Best dev result: 0.9353750348091125\n","Epoch:  20% 49/250 [01:32<05:34,  1.67s/it]\n","13it [00:52, 15.14s/it]            \u001b[A\n","14it [00:52, 10.66s/it]\u001b[A\n","15it [00:52,  7.53s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.81s/it]\u001b[A\n","18it [00:53,  2.74s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/08/2022 06:11:36 - INFO - src.trainer -   Best dev result: 0.937749981880188\n","Epoch:  30% 74/250 [02:23<04:53,  1.67s/it]\n","25it [01:43, 15.05s/it]\u001b[A\n","26it [01:44, 10.60s/it]\u001b[A\n","27it [01:44,  7.49s/it]\u001b[A\n","28it [01:44,  5.31s/it]\u001b[A\n","29it [01:44,  3.79s/it]\u001b[A\n","30it [01:45,  2.72s/it]\u001b[A\n","31it [01:45,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:46,  1.54it/s]\u001b[A\n","36it [01:46,  2.03it/s]\u001b[A03/08/2022 06:12:28 - INFO - src.trainer -   Best dev result: 0.9387499690055847\n","Epoch:  40% 99/250 [03:16<04:11,  1.66s/it]\n","37it [02:36, 15.28s/it]\u001b[A\n","38it [02:36, 10.76s/it]\u001b[A\n","39it [02:36,  7.60s/it]\u001b[A\n","40it [02:36,  5.39s/it]\u001b[A\n","41it [02:37,  3.84s/it]\u001b[A\n","42it [02:37,  2.76s/it]\u001b[A\n","43it [02:37,  2.00s/it]\u001b[A\n","44it [02:37,  1.47s/it]\u001b[A\n","45it [02:37,  1.10s/it]\u001b[A\n","46it [02:38,  1.19it/s]\u001b[A\n","47it [02:38,  1.52it/s]\u001b[A\n","48it [02:38,  2.01it/s]\u001b[A03/08/2022 06:13:20 - INFO - src.trainer -   Best dev result: 0.9444999694824219\n","Epoch:  50% 124/250 [04:08<03:30,  1.67s/it]\n","49it [03:28, 15.23s/it]\u001b[A\n","50it [03:28, 10.73s/it]\u001b[A\n","51it [03:28,  7.58s/it]\u001b[A\n","52it [03:28,  5.38s/it]\u001b[A\n","53it [03:29,  3.83s/it]\u001b[A\n","54it [03:29,  2.75s/it]\u001b[A\n","55it [03:29,  2.00s/it]\u001b[A\n","56it [03:29,  1.47s/it]\u001b[A\n","57it [03:29,  1.10s/it]\u001b[A\n","58it [03:30,  1.20it/s]\u001b[A\n","59it [03:30,  1.53it/s]\u001b[A\n","Epoch:  60% 149/250 [04:52<02:47,  1.66s/it]\n","61it [04:12, 12.88s/it]\u001b[A\n","62it [04:12,  9.09s/it]\u001b[A\n","63it [04:12,  6.43s/it]\u001b[A\n","64it [04:13,  4.57s/it]\u001b[A\n","65it [04:13,  3.27s/it]\u001b[A\n","66it [04:13,  2.36s/it]\u001b[A\n","67it [04:13,  1.72s/it]\u001b[A\n","68it [04:13,  1.27s/it]\u001b[A\n","69it [04:14,  1.04it/s]\u001b[A\n","70it [04:14,  1.35it/s]\u001b[A\n","71it [04:14,  1.70it/s]\u001b[A\n","72it [04:14,  2.22it/s]\u001b[A03/08/2022 06:14:56 - INFO - src.trainer -   Best dev result: 0.9448750019073486\n","Epoch:  70% 174/250 [05:44<02:06,  1.66s/it]\n","73it [05:04, 15.33s/it]\u001b[A\n","74it [05:05, 10.80s/it]\u001b[A\n","75it [05:05,  7.63s/it]\u001b[A\n","76it [05:05,  5.41s/it]\u001b[A\n","77it [05:05,  3.86s/it]\u001b[A\n","78it [05:05,  2.77s/it]\u001b[A\n","79it [05:06,  2.01s/it]\u001b[A\n","80it [05:06,  1.47s/it]\u001b[A\n","81it [05:06,  1.10s/it]\u001b[A\n","82it [05:06,  1.19it/s]\u001b[A\n","83it [05:07,  1.52it/s]\u001b[A\n","84it [05:07,  2.01it/s]\u001b[A03/08/2022 06:15:49 - INFO - src.trainer -   Best dev result: 0.9449999928474426\n","Epoch:  80% 199/250 [06:36<01:25,  1.67s/it]\n","85it [05:56, 15.25s/it]\u001b[A\n","86it [05:57, 10.74s/it]\u001b[A\n","87it [05:57,  7.59s/it]\u001b[A\n","88it [05:57,  5.38s/it]\u001b[A\n","89it [05:57,  3.84s/it]\u001b[A\n","90it [05:58,  2.76s/it]\u001b[A\n","91it [05:58,  2.00s/it]\u001b[A\n","92it [05:58,  1.47s/it]\u001b[A\n","93it [05:58,  1.10s/it]\u001b[A\n","94it [05:59,  1.19it/s]\u001b[A\n","95it [05:59,  1.53it/s]\u001b[A\n","Epoch:  90% 224/250 [07:21<00:43,  1.66s/it]\n","97it [06:41, 12.87s/it]\u001b[A\n","98it [06:41,  9.08s/it]\u001b[A\n","99it [06:41,  6.42s/it]\u001b[A\n","100it [06:41,  4.57s/it]\u001b[A\n","101it [06:42,  3.27s/it]\u001b[A\n","102it [06:42,  2.36s/it]\u001b[A\n","103it [06:42,  1.72s/it]\u001b[A\n","104it [06:42,  1.27s/it]\u001b[A\n","105it [06:42,  1.04it/s]\u001b[A\n","106it [06:43,  1.35it/s]\u001b[A\n","107it [06:43,  1.70it/s]\u001b[A\n","108it [06:43,  2.22it/s]\u001b[A03/08/2022 06:17:25 - INFO - src.trainer -   Best dev result: 0.9472500085830688\n","Epoch: 100% 249/250 [08:13<00:01,  1.67s/it]\n","109it [07:33, 15.23s/it]\u001b[A\n","110it [07:33, 10.73s/it]\u001b[A\n","111it [07:33,  7.58s/it]\u001b[A\n","112it [07:33,  5.38s/it]\u001b[A\n","113it [07:34,  3.83s/it]\u001b[A\n","114it [07:34,  2.75s/it]\u001b[A\n","115it [07:34,  2.00s/it]\u001b[A\n","116it [07:34,  1.47s/it]\u001b[A\n","117it [07:35,  1.10s/it]\u001b[A\n","118it [07:35,  1.20it/s]\u001b[A\n","119it [07:35,  1.53it/s]\u001b[A\n","Epoch: 100% 250/250 [08:17<00:00,  1.99s/it]\n","03/08/2022 06:18:17 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:18:31 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:49,  4.60s/it]\u001b[A\n","122it [07:50,  3.29s/it]\u001b[A\n","123it [07:50,  2.37s/it]\u001b[A\n","124it [07:50,  1.73s/it]\u001b[A\n","125it [07:50,  1.28s/it]\u001b[A\n","126it [07:51,  1.04it/s]\u001b[A\n","127it [07:51,  1.34it/s]\u001b[A\n","128it [07:51,  1.69it/s]\u001b[A\n","129it [07:51,  2.07it/s]\u001b[A\n","130it [07:51,  2.45it/s]\u001b[A\n","131it [07:52,  2.82it/s]\u001b[A\n","132it [07:52,  3.50it/s]\u001b[A03/08/2022 06:18:34 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:18:34 - INFO - __main__ -     eval_loss = 1.7733235359191895\n","03/08/2022 06:18:34 - INFO - __main__ -     eval_auroc = 0.9472500085830688\n","03/08/2022 06:18:34 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 06:18:34 - INFO - __main__ -     eval_f1 = 0.5999999642372131\n","03/08/2022 06:18:34 - INFO - filelock -   Lock 140273914162640 acquired on log.lock\n","03/08/2022 06:18:34 - INFO - filelock -   Lock 140273914162640 released on log.lock\n","132it [07:52,  3.58s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:18:40 - INFO - __main__ -   Specify load the 81-th prompt: *cls*▁>*mask*.*+sent_0**sep+* | {0: \"X\", 1: \"Oops\"}\n","03/08/2022 06:18:40 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:18:40 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-1179', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-18-40_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-1179', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:18:40 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:18:41 - INFO - src.dataset -   Label 0 to word ĠX (1577)\n","03/08/2022 06:18:41 - INFO - src.dataset -   Label 1 to word ĠOops (44007)\n","03/08/2022 06:18:41 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:18:41 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:18:41 - INFO - filelock -   Lock 139726880442384 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:18:41 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 06:18:41 - INFO - filelock -   Lock 139726880442384 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:18:41 - INFO - src.dataset -   Label 0 to word ĠX (1577)\n","03/08/2022 06:18:41 - INFO - src.dataset -   Label 1 to word ĠOops (44007)\n","03/08/2022 06:18:41 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:18:41 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:18:41 - INFO - filelock -   Lock 139726884424848 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:18:41 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:18:41 - INFO - filelock -   Lock 139726884424848 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:18:41 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:18:41 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:18:41 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 15698, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:18:41 - INFO - src.dataset -   text: <s>▁><mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:18:57 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:18:57 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:18:57 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:18:57 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:18:57 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:18:57 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:18:57 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.71it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.58it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 06:19:41 - INFO - src.trainer -   Best dev result: 0.8892499804496765\n","Epoch:  20% 49/250 [01:33<05:37,  1.68s/it]\n","13it [00:52, 15.27s/it]            \u001b[A\n","14it [00:53, 10.76s/it]\u001b[A\n","15it [00:53,  7.60s/it]\u001b[A\n","16it [00:53,  5.39s/it]\u001b[A\n","17it [00:53,  3.84s/it]\u001b[A\n","18it [00:54,  2.76s/it]\u001b[A\n","19it [00:54,  2.00s/it]\u001b[A\n","20it [00:54,  1.47s/it]\u001b[A\n","21it [00:54,  1.10s/it]\u001b[A\n","22it [00:54,  1.19it/s]\u001b[A\n","23it [00:55,  1.52it/s]\u001b[A\n","24it [00:55,  2.01it/s]\u001b[A03/08/2022 06:20:34 - INFO - src.trainer -   Best dev result: 0.893500030040741\n","Epoch:  30% 74/250 [02:24<04:54,  1.67s/it]\n","25it [01:44, 15.03s/it]\u001b[A\n","26it [01:44, 10.59s/it]\u001b[A\n","27it [01:44,  7.48s/it]\u001b[A\n","28it [01:44,  5.31s/it]\u001b[A\n","29it [01:45,  3.78s/it]\u001b[A\n","30it [01:45,  2.72s/it]\u001b[A\n","31it [01:45,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:46,  1.08s/it]\u001b[A\n","34it [01:46,  1.21it/s]\u001b[A\n","35it [01:46,  1.54it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:10,  1.66s/it]\n","37it [02:28, 12.87s/it]\u001b[A\n","38it [02:28,  9.08s/it]\u001b[A\n","39it [02:28,  6.43s/it]\u001b[A\n","40it [02:29,  4.57s/it]\u001b[A\n","41it [02:29,  3.27s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:30,  1.27s/it]\u001b[A\n","45it [02:30,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","48it [02:30,  2.23it/s]\u001b[A03/08/2022 06:22:10 - INFO - src.trainer -   Best dev result: 0.9008750319480896\n","Epoch:  50% 124/250 [04:00<03:30,  1.67s/it]\n","49it [03:20, 15.18s/it]\u001b[A\n","50it [03:20, 10.70s/it]\u001b[A\n","51it [03:20,  7.56s/it]\u001b[A\n","52it [03:21,  5.36s/it]\u001b[A\n","53it [03:21,  3.82s/it]\u001b[A\n","54it [03:21,  2.74s/it]\u001b[A\n","55it [03:21,  1.99s/it]\u001b[A\n","56it [03:22,  1.46s/it]\u001b[A\n","57it [03:22,  1.09s/it]\u001b[A\n","58it [03:22,  1.20it/s]\u001b[A\n","59it [03:22,  1.53it/s]\u001b[A\n","60it [03:22,  2.02it/s]\u001b[A03/08/2022 06:23:02 - INFO - src.trainer -   Best dev result: 0.9011250138282776\n","Epoch:  60% 149/250 [04:52<02:48,  1.67s/it]\n","61it [04:12, 15.29s/it]\u001b[A\n","62it [04:12, 10.77s/it]\u001b[A\n","63it [04:13,  7.61s/it]\u001b[A\n","64it [04:13,  5.40s/it]\u001b[A\n","65it [04:13,  3.85s/it]\u001b[A\n","66it [04:13,  2.76s/it]\u001b[A\n","67it [04:14,  2.00s/it]\u001b[A\n","68it [04:14,  1.47s/it]\u001b[A\n","69it [04:14,  1.10s/it]\u001b[A\n","70it [04:14,  1.19it/s]\u001b[A\n","71it [04:15,  1.52it/s]\u001b[A\n","Epoch:  70% 174/250 [05:37<02:06,  1.66s/it]\n","73it [04:56, 12.87s/it]\u001b[A\n","74it [04:57,  9.08s/it]\u001b[A\n","75it [04:57,  6.42s/it]\u001b[A\n","76it [04:57,  4.57s/it]\u001b[A\n","77it [04:57,  3.27s/it]\u001b[A\n","78it [04:58,  2.36s/it]\u001b[A\n","79it [04:58,  1.72s/it]\u001b[A\n","80it [04:58,  1.27s/it]\u001b[A\n","81it [04:58,  1.04it/s]\u001b[A\n","82it [04:58,  1.35it/s]\u001b[A\n","83it [04:59,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:21<01:24,  1.66s/it]\n","85it [05:41, 12.84s/it]\u001b[A\n","86it [05:41,  9.05s/it]\u001b[A\n","87it [05:41,  6.41s/it]\u001b[A\n","88it [05:41,  4.55s/it]\u001b[A\n","89it [05:41,  3.26s/it]\u001b[A\n","90it [05:42,  2.35s/it]\u001b[A\n","91it [05:42,  1.71s/it]\u001b[A\n","92it [05:42,  1.27s/it]\u001b[A\n","93it [05:42,  1.04it/s]\u001b[A\n","94it [05:43,  1.35it/s]\u001b[A\n","95it [05:43,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:05<00:43,  1.66s/it]\n","97it [06:25, 12.84s/it]\u001b[A\n","98it [06:25,  9.06s/it]\u001b[A\n","99it [06:25,  6.41s/it]\u001b[A\n","100it [06:25,  4.56s/it]\u001b[A\n","101it [06:26,  3.26s/it]\u001b[A\n","102it [06:26,  2.35s/it]\u001b[A\n","103it [06:26,  1.71s/it]\u001b[A\n","104it [06:26,  1.27s/it]\u001b[A\n","105it [06:27,  1.04it/s]\u001b[A\n","106it [06:27,  1.35it/s]\u001b[A\n","107it [06:27,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:49<00:01,  1.66s/it]\n","109it [07:09, 12.84s/it]\u001b[A\n","110it [07:09,  9.05s/it]\u001b[A\n","111it [07:09,  6.41s/it]\u001b[A\n","112it [07:10,  4.55s/it]\u001b[A\n","113it [07:10,  3.26s/it]\u001b[A\n","114it [07:10,  2.35s/it]\u001b[A\n","115it [07:10,  1.71s/it]\u001b[A\n","116it [07:11,  1.27s/it]\u001b[A\n","117it [07:11,  1.04it/s]\u001b[A\n","118it [07:11,  1.35it/s]\u001b[A\n","119it [07:11,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:53<00:00,  1.90s/it]\n","03/08/2022 06:26:51 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:27:05 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:26,  4.59s/it]\u001b[A\n","122it [07:26,  3.28s/it]\u001b[A\n","123it [07:26,  2.37s/it]\u001b[A\n","124it [07:26,  1.73s/it]\u001b[A\n","125it [07:27,  1.28s/it]\u001b[A\n","126it [07:27,  1.04it/s]\u001b[A\n","127it [07:27,  1.34it/s]\u001b[A\n","128it [07:27,  1.69it/s]\u001b[A\n","129it [07:27,  2.07it/s]\u001b[A\n","130it [07:28,  2.45it/s]\u001b[A\n","131it [07:28,  2.82it/s]\u001b[A\n","132it [07:28,  3.50it/s]\u001b[A03/08/2022 06:27:07 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:27:07 - INFO - __main__ -     eval_loss = 2.772655487060547\n","03/08/2022 06:27:07 - INFO - __main__ -     eval_auroc = 0.9011250138282776\n","03/08/2022 06:27:07 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 06:27:07 - INFO - __main__ -     eval_f1 = 0.4897959232330322\n","03/08/2022 06:27:08 - INFO - filelock -   Lock 139726941507920 acquired on log.lock\n","03/08/2022 06:27:08 - INFO - filelock -   Lock 139726941507920 released on log.lock\n","132it [07:28,  3.40s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:27:13 - INFO - __main__ -   Specify load the 82-th prompt: *cls*▁>*mask*.*+sent_0**sep+* | {0: \"U\", 1: \"Indeed\"}\n","03/08/2022 06:27:13 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:27:13 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-18016', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-27-13_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-18016', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:27:13 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:27:15 - INFO - src.dataset -   Label 0 to word ĠU (121)\n","03/08/2022 06:27:15 - INFO - src.dataset -   Label 1 to word ĠIndeed (7908)\n","03/08/2022 06:27:15 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:27:15 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:27:15 - INFO - filelock -   Lock 140343389837456 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:27:15 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 06:27:15 - INFO - filelock -   Lock 140343389837456 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:27:15 - INFO - src.dataset -   Label 0 to word ĠU (121)\n","03/08/2022 06:27:15 - INFO - src.dataset -   Label 1 to word ĠIndeed (7908)\n","03/08/2022 06:27:15 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:27:15 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:27:15 - INFO - filelock -   Lock 140343389750224 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:27:15 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:27:15 - INFO - filelock -   Lock 140343389750224 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:27:15 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:27:15 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:27:15 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 15698, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:27:15 - INFO - src.dataset -   text: <s>▁><mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:27:30 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:27:30 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:27:30 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:27:30 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:27:30 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:27:30 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:27:30 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.71it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.71it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.58it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 06:28:15 - INFO - src.trainer -   Best dev result: 0.8996250033378601\n","Epoch:  20% 49/250 [01:32<05:36,  1.67s/it]\n","13it [00:51, 14.98s/it]            \u001b[A\n","14it [00:52, 10.55s/it]\u001b[A\n","15it [00:52,  7.46s/it]\u001b[A\n","16it [00:52,  5.29s/it]\u001b[A\n","17it [00:52,  3.77s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.04it/s]\u001b[A03/08/2022 06:29:06 - INFO - src.trainer -   Best dev result: 0.9086250066757202\n","Epoch:  30% 74/250 [02:24<04:53,  1.67s/it]\n","25it [01:44, 15.25s/it]\u001b[A\n","26it [01:44, 10.75s/it]\u001b[A\n","27it [01:44,  7.59s/it]\u001b[A\n","28it [01:44,  5.38s/it]\u001b[A\n","29it [01:44,  3.84s/it]\u001b[A\n","30it [01:45,  2.76s/it]\u001b[A\n","31it [01:45,  2.00s/it]\u001b[A\n","32it [01:45,  1.47s/it]\u001b[A\n","33it [01:45,  1.10s/it]\u001b[A\n","34it [01:46,  1.19it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","36it [01:46,  2.02it/s]\u001b[A03/08/2022 06:29:59 - INFO - src.trainer -   Best dev result: 0.9106249213218689\n","Epoch:  40% 99/250 [03:15<04:12,  1.67s/it]\n","37it [02:35, 15.14s/it]\u001b[A\n","38it [02:36, 10.67s/it]\u001b[A\n","39it [02:36,  7.54s/it]\u001b[A\n","40it [02:36,  5.35s/it]\u001b[A\n","41it [02:36,  3.81s/it]\u001b[A\n","42it [02:36,  2.74s/it]\u001b[A\n","43it [02:37,  1.99s/it]\u001b[A\n","44it [02:37,  1.46s/it]\u001b[A\n","45it [02:37,  1.09s/it]\u001b[A\n","46it [02:37,  1.20it/s]\u001b[A\n","47it [02:38,  1.53it/s]\u001b[A\n","Epoch:  50% 124/250 [04:00<03:28,  1.66s/it]\n","49it [03:19, 12.84s/it]\u001b[A\n","50it [03:20,  9.06s/it]\u001b[A\n","51it [03:20,  6.41s/it]\u001b[A\n","52it [03:20,  4.56s/it]\u001b[A\n","53it [03:20,  3.26s/it]\u001b[A\n","54it [03:21,  2.35s/it]\u001b[A\n","55it [03:21,  1.71s/it]\u001b[A\n","56it [03:21,  1.27s/it]\u001b[A\n","57it [03:21,  1.04it/s]\u001b[A\n","58it [03:21,  1.35it/s]\u001b[A\n","59it [03:22,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:44<02:47,  1.66s/it]\n","61it [04:03, 12.80s/it]\u001b[A\n","62it [04:04,  9.03s/it]\u001b[A\n","63it [04:04,  6.39s/it]\u001b[A\n","64it [04:04,  4.54s/it]\u001b[A\n","65it [04:04,  3.25s/it]\u001b[A\n","66it [04:05,  2.34s/it]\u001b[A\n","67it [04:05,  1.71s/it]\u001b[A\n","68it [04:05,  1.27s/it]\u001b[A\n","69it [04:05,  1.05it/s]\u001b[A\n","70it [04:06,  1.35it/s]\u001b[A\n","71it [04:06,  1.71it/s]\u001b[A\n","Epoch:  70% 174/250 [05:28<02:06,  1.66s/it]\n","73it [04:48, 12.81s/it]\u001b[A\n","74it [04:48,  9.04s/it]\u001b[A\n","75it [04:48,  6.39s/it]\u001b[A\n","76it [04:48,  4.55s/it]\u001b[A\n","77it [04:48,  3.25s/it]\u001b[A\n","78it [04:49,  2.35s/it]\u001b[A\n","79it [04:49,  1.71s/it]\u001b[A\n","80it [04:49,  1.27s/it]\u001b[A\n","81it [04:49,  1.05it/s]\u001b[A\n","82it [04:50,  1.35it/s]\u001b[A\n","83it [04:50,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:12<01:24,  1.66s/it]\n","85it [05:32, 12.81s/it]\u001b[A\n","86it [05:32,  9.03s/it]\u001b[A\n","87it [05:32,  6.39s/it]\u001b[A\n","88it [05:32,  4.54s/it]\u001b[A\n","89it [05:33,  3.25s/it]\u001b[A\n","90it [05:33,  2.34s/it]\u001b[A\n","91it [05:33,  1.71s/it]\u001b[A\n","92it [05:33,  1.27s/it]\u001b[A\n","93it [05:33,  1.05it/s]\u001b[A\n","94it [05:34,  1.35it/s]\u001b[A\n","95it [05:34,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [06:56<00:43,  1.66s/it]\n","97it [06:16, 12.83s/it]\u001b[A\n","98it [06:16,  9.05s/it]\u001b[A\n","99it [06:16,  6.41s/it]\u001b[A\n","100it [06:16,  4.55s/it]\u001b[A\n","101it [06:17,  3.26s/it]\u001b[A\n","102it [06:17,  2.35s/it]\u001b[A\n","103it [06:17,  1.71s/it]\u001b[A\n","104it [06:17,  1.27s/it]\u001b[A\n","105it [06:18,  1.04it/s]\u001b[A\n","106it [06:18,  1.35it/s]\u001b[A\n","107it [06:18,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:40<00:01,  1.66s/it]\n","109it [07:00, 12.83s/it]\u001b[A\n","110it [07:00,  9.05s/it]\u001b[A\n","111it [07:00,  6.41s/it]\u001b[A\n","112it [07:01,  4.55s/it]\u001b[A\n","113it [07:01,  3.26s/it]\u001b[A\n","114it [07:01,  2.35s/it]\u001b[A\n","115it [07:01,  1.71s/it]\u001b[A\n","116it [07:02,  1.27s/it]\u001b[A\n","117it [07:02,  1.04it/s]\u001b[A\n","118it [07:02,  1.35it/s]\u001b[A\n","119it [07:02,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:44<00:00,  1.86s/it]\n","03/08/2022 06:35:15 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:35:29 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:17,  4.62s/it]\u001b[A\n","122it [07:17,  3.30s/it]\u001b[A\n","123it [07:17,  2.38s/it]\u001b[A\n","124it [07:17,  1.74s/it]\u001b[A\n","125it [07:18,  1.29s/it]\u001b[A\n","126it [07:18,  1.03it/s]\u001b[A\n","127it [07:18,  1.34it/s]\u001b[A\n","128it [07:18,  1.69it/s]\u001b[A\n","129it [07:19,  2.06it/s]\u001b[A\n","130it [07:19,  2.45it/s]\u001b[A\n","131it [07:19,  2.81it/s]\u001b[A\n","132it [07:19,  3.49it/s]\u001b[A03/08/2022 06:35:32 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:35:32 - INFO - __main__ -     eval_loss = 1.949631929397583\n","03/08/2022 06:35:32 - INFO - __main__ -     eval_auroc = 0.9106249213218689\n","03/08/2022 06:35:32 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 06:35:32 - INFO - __main__ -     eval_f1 = 0.5274725556373596\n","03/08/2022 06:35:32 - INFO - filelock -   Lock 140343767938192 acquired on log.lock\n","03/08/2022 06:35:32 - INFO - filelock -   Lock 140343767938192 released on log.lock\n","132it [07:19,  3.33s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:35:38 - INFO - __main__ -   Specify load the 83-th prompt: *cls*▁>*mask*.*+sent_0**sep+* | {0: \"...\", 1: \"Oops\"}\n","03/08/2022 06:35:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:35:38 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-19511', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-35-38_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-19511', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:35:38 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:35:39 - INFO - src.dataset -   Label 0 to word ... (734)\n","03/08/2022 06:35:39 - INFO - src.dataset -   Label 1 to word ĠOops (44007)\n","03/08/2022 06:35:39 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:35:39 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:35:39 - INFO - filelock -   Lock 140315071707280 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:35:39 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 06:35:39 - INFO - filelock -   Lock 140315071707280 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:35:39 - INFO - src.dataset -   Label 0 to word ... (734)\n","03/08/2022 06:35:39 - INFO - src.dataset -   Label 1 to word ĠOops (44007)\n","03/08/2022 06:35:39 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:35:39 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:35:39 - INFO - filelock -   Lock 140315057500624 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:35:39 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:35:39 - INFO - filelock -   Lock 140315057500624 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:35:39 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:35:39 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:35:39 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 15698, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:35:39 - INFO - src.dataset -   text: <s>▁><mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:35:54 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:35:54 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:35:54 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:35:54 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:35:54 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:35:54 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:35:54 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 06:36:39 - INFO - src.trainer -   Best dev result: 0.9127500057220459\n","Epoch:  20% 49/250 [01:31<05:41,  1.70s/it]\n","13it [00:51, 14.90s/it]            \u001b[A\n","14it [00:51, 10.50s/it]\u001b[A\n","15it [00:52,  7.42s/it]\u001b[A\n","16it [00:52,  5.26s/it]\u001b[A\n","17it [00:52,  3.75s/it]\u001b[A\n","18it [00:52,  2.70s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.22it/s]\u001b[A\n","23it [00:53,  1.55it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:51,  1.66s/it]\n","25it [01:35, 12.86s/it]\u001b[A\n","26it [01:36,  9.07s/it]\u001b[A\n","27it [01:36,  6.42s/it]\u001b[A\n","28it [01:36,  4.56s/it]\u001b[A\n","29it [01:36,  3.26s/it]\u001b[A\n","30it [01:36,  2.35s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.04it/s]\u001b[A\n","34it [01:37,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:19, 12.84s/it]\u001b[A\n","38it [02:20,  9.05s/it]\u001b[A\n","39it [02:20,  6.41s/it]\u001b[A\n","40it [02:20,  4.55s/it]\u001b[A\n","41it [02:20,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:21,  1.71s/it]\u001b[A\n","44it [02:21,  1.27s/it]\u001b[A\n","45it [02:21,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:29,  1.66s/it]\n","49it [03:04, 12.84s/it]\u001b[A\n","50it [03:04,  9.06s/it]\u001b[A\n","51it [03:04,  6.41s/it]\u001b[A\n","52it [03:04,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:05,  1.71s/it]\u001b[A\n","56it [03:05,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:28<02:47,  1.66s/it]\n","61it [03:48, 12.84s/it]\u001b[A\n","62it [03:48,  9.06s/it]\u001b[A\n","63it [03:48,  6.41s/it]\u001b[A\n","64it [03:49,  4.56s/it]\u001b[A\n","65it [03:49,  3.26s/it]\u001b[A\n","66it [03:49,  2.35s/it]\u001b[A\n","67it [03:49,  1.71s/it]\u001b[A\n","68it [03:49,  1.27s/it]\u001b[A\n","69it [03:50,  1.04it/s]\u001b[A\n","70it [03:50,  1.35it/s]\u001b[A\n","71it [03:50,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:12<02:06,  1.66s/it]\n","73it [04:32, 12.85s/it]\u001b[A\n","74it [04:32,  9.07s/it]\u001b[A\n","75it [04:33,  6.41s/it]\u001b[A\n","76it [04:33,  4.56s/it]\u001b[A\n","77it [04:33,  3.26s/it]\u001b[A\n","78it [04:33,  2.35s/it]\u001b[A\n","79it [04:33,  1.72s/it]\u001b[A\n","80it [04:34,  1.27s/it]\u001b[A\n","81it [04:34,  1.04it/s]\u001b[A\n","82it [04:34,  1.35it/s]\u001b[A\n","83it [04:34,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:57<01:24,  1.66s/it]\n","85it [05:16, 12.84s/it]\u001b[A\n","86it [05:17,  9.06s/it]\u001b[A\n","87it [05:17,  6.41s/it]\u001b[A\n","88it [05:17,  4.56s/it]\u001b[A\n","89it [05:17,  3.26s/it]\u001b[A\n","90it [05:17,  2.35s/it]\u001b[A\n","91it [05:18,  1.72s/it]\u001b[A\n","92it [05:18,  1.27s/it]\u001b[A\n","93it [05:18,  1.04it/s]\u001b[A\n","94it [05:18,  1.35it/s]\u001b[A\n","95it [05:19,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:41<00:43,  1.66s/it]\n","97it [06:01, 12.85s/it]\u001b[A\n","98it [06:01,  9.06s/it]\u001b[A\n","99it [06:01,  6.41s/it]\u001b[A\n","100it [06:01,  4.56s/it]\u001b[A\n","101it [06:01,  3.26s/it]\u001b[A\n","102it [06:02,  2.35s/it]\u001b[A\n","103it [06:02,  1.72s/it]\u001b[A\n","104it [06:02,  1.27s/it]\u001b[A\n","105it [06:02,  1.04it/s]\u001b[A\n","106it [06:03,  1.35it/s]\u001b[A\n","107it [06:03,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:25<00:01,  1.66s/it]\n","109it [06:45, 12.84s/it]\u001b[A\n","110it [06:45,  9.06s/it]\u001b[A\n","111it [06:45,  6.41s/it]\u001b[A\n","112it [06:45,  4.56s/it]\u001b[A\n","113it [06:46,  3.26s/it]\u001b[A\n","114it [06:46,  2.35s/it]\u001b[A\n","115it [06:46,  1.71s/it]\u001b[A\n","116it [06:46,  1.27s/it]\u001b[A\n","117it [06:47,  1.04it/s]\u001b[A\n","118it [06:47,  1.35it/s]\u001b[A\n","119it [06:47,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:29<00:00,  1.80s/it]\n","03/08/2022 06:43:24 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:43:37 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:00,  4.29s/it]\u001b[A\n","122it [07:01,  3.07s/it]\u001b[A\n","123it [07:01,  2.22s/it]\u001b[A\n","124it [07:01,  1.62s/it]\u001b[A\n","125it [07:01,  1.21s/it]\u001b[A\n","126it [07:02,  1.09it/s]\u001b[A\n","127it [07:02,  1.41it/s]\u001b[A\n","128it [07:02,  1.77it/s]\u001b[A\n","129it [07:02,  2.15it/s]\u001b[A\n","130it [07:02,  2.53it/s]\u001b[A\n","131it [07:03,  2.89it/s]\u001b[A\n","132it [07:03,  3.57it/s]\u001b[A03/08/2022 06:43:40 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:43:40 - INFO - __main__ -     eval_loss = 1.601298451423645\n","03/08/2022 06:43:40 - INFO - __main__ -     eval_auroc = 0.9127500057220459\n","03/08/2022 06:43:40 - INFO - __main__ -     eval_recall = 0.8399999737739563\n","03/08/2022 06:43:40 - INFO - __main__ -     eval_f1 = 0.5600000023841858\n","03/08/2022 06:43:40 - INFO - filelock -   Lock 140315070574288 acquired on log.lock\n","03/08/2022 06:43:40 - INFO - filelock -   Lock 140315070574288 released on log.lock\n","132it [07:03,  3.21s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:43:46 - INFO - __main__ -   Specify load the 84-th prompt: *cls*▁>*mask*.*+sent_0**sep+* | {0: \"M\", 1: \"vs\"}\n","03/08/2022 06:43:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:43:46 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-11278', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-43-46_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-11278', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:43:46 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:43:47 - INFO - src.dataset -   Label 0 to word ĠM (256)\n","03/08/2022 06:43:47 - INFO - src.dataset -   Label 1 to word Ġvs (1954)\n","03/08/2022 06:43:47 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:43:47 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:43:47 - INFO - filelock -   Lock 140344354618064 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:43:47 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 06:43:47 - INFO - filelock -   Lock 140344354618064 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:43:47 - INFO - src.dataset -   Label 0 to word ĠM (256)\n","03/08/2022 06:43:47 - INFO - src.dataset -   Label 1 to word Ġvs (1954)\n","03/08/2022 06:43:47 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:43:47 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:43:47 - INFO - filelock -   Lock 140344326900368 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:43:47 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:43:47 - INFO - filelock -   Lock 140344326900368 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:43:47 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:43:47 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:43:47 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 15698, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:43:47 - INFO - src.dataset -   text: <s>▁><mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:44:02 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:44:02 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:44:02 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:44:02 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:44:02 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:44:02 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:44:02 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 06:44:47 - INFO - src.trainer -   Best dev result: 0.9200000166893005\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.14s/it]            \u001b[A\n","14it [00:52, 10.66s/it]\u001b[A\n","15it [00:52,  7.53s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.81s/it]\u001b[A\n","18it [00:53,  2.74s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.88s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:37,  6.43s/it]\u001b[A\n","28it [01:37,  4.57s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:38,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:01<04:10,  1.66s/it]\n","37it [02:20, 12.85s/it]\u001b[A\n","38it [02:21,  9.06s/it]\u001b[A\n","39it [02:21,  6.41s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:22,  2.35s/it]\u001b[A\n","43it [02:22,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:23,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:29,  1.66s/it]\n","49it [03:05, 12.85s/it]\u001b[A\n","50it [03:05,  9.06s/it]\u001b[A\n","51it [03:05,  6.41s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:06,  3.26s/it]\u001b[A\n","54it [03:06,  2.35s/it]\u001b[A\n","55it [03:06,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:07,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:29<02:47,  1.66s/it]\n","61it [03:49, 12.84s/it]\u001b[A\n","62it [03:49,  9.06s/it]\u001b[A\n","63it [03:49,  6.41s/it]\u001b[A\n","64it [03:49,  4.56s/it]\u001b[A\n","65it [03:50,  3.26s/it]\u001b[A\n","66it [03:50,  2.35s/it]\u001b[A\n","67it [03:50,  1.71s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:51,  1.04it/s]\u001b[A\n","70it [03:51,  1.35it/s]\u001b[A\n","71it [03:51,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:13<02:06,  1.66s/it]\n","73it [04:33, 12.85s/it]\u001b[A\n","74it [04:33,  9.06s/it]\u001b[A\n","75it [04:33,  6.41s/it]\u001b[A\n","76it [04:34,  4.56s/it]\u001b[A\n","77it [04:34,  3.26s/it]\u001b[A\n","78it [04:34,  2.35s/it]\u001b[A\n","79it [04:34,  1.72s/it]\u001b[A\n","80it [04:35,  1.27s/it]\u001b[A\n","81it [04:35,  1.04it/s]\u001b[A\n","82it [04:35,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:57<01:24,  1.66s/it]\n","85it [05:17, 12.83s/it]\u001b[A\n","86it [05:17,  9.05s/it]\u001b[A\n","87it [05:18,  6.40s/it]\u001b[A\n","88it [05:18,  4.55s/it]\u001b[A\n","89it [05:18,  3.26s/it]\u001b[A\n","90it [05:18,  2.35s/it]\u001b[A\n","91it [05:19,  1.71s/it]\u001b[A\n","92it [05:19,  1.27s/it]\u001b[A\n","93it [05:19,  1.04it/s]\u001b[A\n","94it [05:19,  1.35it/s]\u001b[A\n","95it [05:19,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:41<00:43,  1.66s/it]\n","97it [06:01, 12.81s/it]\u001b[A\n","98it [06:01,  9.04s/it]\u001b[A\n","99it [06:02,  6.39s/it]\u001b[A\n","100it [06:02,  4.55s/it]\u001b[A\n","101it [06:02,  3.25s/it]\u001b[A\n","102it [06:02,  2.34s/it]\u001b[A\n","103it [06:03,  1.71s/it]\u001b[A\n","104it [06:03,  1.27s/it]\u001b[A\n","105it [06:03,  1.05it/s]\u001b[A\n","106it [06:03,  1.35it/s]\u001b[A\n","107it [06:04,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:26<00:01,  1.66s/it]\n","109it [06:45, 12.82s/it]\u001b[A\n","110it [06:46,  9.04s/it]\u001b[A\n","111it [06:46,  6.40s/it]\u001b[A\n","112it [06:46,  4.55s/it]\u001b[A\n","113it [06:46,  3.25s/it]\u001b[A\n","114it [06:47,  2.35s/it]\u001b[A\n","115it [06:47,  1.71s/it]\u001b[A\n","116it [06:47,  1.27s/it]\u001b[A\n","117it [06:47,  1.05it/s]\u001b[A\n","118it [06:47,  1.35it/s]\u001b[A\n","119it [06:48,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:30<00:00,  1.80s/it]\n","03/08/2022 06:51:32 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:51:45 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:01,  4.31s/it]\u001b[A\n","122it [07:01,  3.08s/it]\u001b[A\n","123it [07:02,  2.23s/it]\u001b[A\n","124it [07:02,  1.63s/it]\u001b[A\n","125it [07:02,  1.21s/it]\u001b[A\n","126it [07:02,  1.09it/s]\u001b[A\n","127it [07:02,  1.41it/s]\u001b[A\n","128it [07:03,  1.76it/s]\u001b[A\n","129it [07:03,  2.15it/s]\u001b[A\n","130it [07:03,  2.53it/s]\u001b[A\n","131it [07:03,  2.89it/s]\u001b[A\n","132it [07:04,  3.56it/s]\u001b[A03/08/2022 06:51:48 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:51:48 - INFO - __main__ -     eval_loss = 2.6093661785125732\n","03/08/2022 06:51:48 - INFO - __main__ -     eval_auroc = 0.9200000166893005\n","03/08/2022 06:51:48 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 06:51:48 - INFO - __main__ -     eval_f1 = 0.555555522441864\n","03/08/2022 06:51:48 - INFO - filelock -   Lock 140344338222736 acquired on log.lock\n","03/08/2022 06:51:48 - INFO - filelock -   Lock 140344338222736 released on log.lock\n","132it [07:04,  3.21s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:51:54 - INFO - __main__ -   Specify load the 85-th prompt: *cls*▁4.*mask*.*+sent_0**sep+* | {0: \"All\", 1: \"Ah\"}\n","03/08/2022 06:51:54 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:51:54 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-28401', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-51-54_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-28401', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:51:54 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:51:55 - INFO - src.dataset -   Label 0 to word ĠAll (404)\n","03/08/2022 06:51:55 - INFO - src.dataset -   Label 1 to word ĠAh (7746)\n","03/08/2022 06:51:55 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:51:55 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:51:55 - INFO - filelock -   Lock 140409204248656 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:51:55 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 06:51:55 - INFO - filelock -   Lock 140409204248656 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:51:55 - INFO - src.dataset -   Label 0 to word ĠAll (404)\n","03/08/2022 06:51:55 - INFO - src.dataset -   Label 1 to word ĠAh (7746)\n","03/08/2022 06:51:55 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:51:55 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:51:55 - INFO - filelock -   Lock 140408877392144 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:51:55 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:51:55 - INFO - filelock -   Lock 140408877392144 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:51:55 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:51:55 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:51:55 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 306, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 06:51:55 - INFO - src.dataset -   text: <s>▁4.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:52:11 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:52:11 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:52:11 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:52:11 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:52:11 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:52:11 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:52:11 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.64it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.90it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.71it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.58it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.44it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.40it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.08it/s]\u001b[A03/08/2022 06:52:55 - INFO - src.trainer -   Best dev result: 0.8520000576972961\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.06s/it]            \u001b[A\n","14it [00:52, 10.61s/it]\u001b[A\n","15it [00:52,  7.50s/it]\u001b[A\n","16it [00:52,  5.32s/it]\u001b[A\n","17it [00:53,  3.79s/it]\u001b[A\n","18it [00:53,  2.72s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 06:53:47 - INFO - src.trainer -   Best dev result: 0.9036250114440918\n","Epoch:  30% 74/250 [02:24<04:53,  1.67s/it]\n","25it [01:44, 15.25s/it]\u001b[A\n","26it [01:44, 10.75s/it]\u001b[A\n","27it [01:44,  7.59s/it]\u001b[A\n","28it [01:45,  5.39s/it]\u001b[A\n","29it [01:45,  3.84s/it]\u001b[A\n","30it [01:45,  2.76s/it]\u001b[A\n","31it [01:45,  2.00s/it]\u001b[A\n","32it [01:45,  1.47s/it]\u001b[A\n","33it [01:46,  1.10s/it]\u001b[A\n","34it [01:46,  1.19it/s]\u001b[A\n","35it [01:46,  1.52it/s]\u001b[A\n","36it [01:46,  2.01it/s]\u001b[A03/08/2022 06:54:39 - INFO - src.trainer -   Best dev result: 0.9076249599456787\n","Epoch:  40% 99/250 [03:16<04:12,  1.67s/it]\n","37it [02:36, 15.25s/it]\u001b[A\n","38it [02:36, 10.75s/it]\u001b[A\n","39it [02:36,  7.59s/it]\u001b[A\n","40it [02:37,  5.38s/it]\u001b[A\n","41it [02:37,  3.84s/it]\u001b[A\n","42it [02:37,  2.76s/it]\u001b[A\n","43it [02:37,  2.00s/it]\u001b[A\n","44it [02:38,  1.47s/it]\u001b[A\n","45it [02:38,  1.10s/it]\u001b[A\n","46it [02:38,  1.19it/s]\u001b[A\n","47it [02:38,  1.52it/s]\u001b[A\n","48it [02:38,  2.01it/s]\u001b[A03/08/2022 06:55:31 - INFO - src.trainer -   Best dev result: 0.909375011920929\n","Epoch:  50% 124/250 [04:08<03:33,  1.70s/it]\n","49it [03:28, 15.23s/it]\u001b[A\n","50it [03:28, 10.73s/it]\u001b[A\n","51it [03:28,  7.58s/it]\u001b[A\n","52it [03:29,  5.38s/it]\u001b[A\n","53it [03:29,  3.83s/it]\u001b[A\n","54it [03:29,  2.75s/it]\u001b[A\n","55it [03:29,  2.00s/it]\u001b[A\n","56it [03:30,  1.47s/it]\u001b[A\n","57it [03:30,  1.10s/it]\u001b[A\n","58it [03:30,  1.19it/s]\u001b[A\n","59it [03:30,  1.53it/s]\u001b[A\n","Epoch:  60% 149/250 [04:52<02:47,  1.66s/it]\n","61it [04:12, 12.89s/it]\u001b[A\n","62it [04:12,  9.09s/it]\u001b[A\n","63it [04:13,  6.43s/it]\u001b[A\n","64it [04:13,  4.57s/it]\u001b[A\n","65it [04:13,  3.27s/it]\u001b[A\n","66it [04:13,  2.36s/it]\u001b[A\n","67it [04:14,  1.72s/it]\u001b[A\n","68it [04:14,  1.27s/it]\u001b[A\n","69it [04:14,  1.04it/s]\u001b[A\n","70it [04:14,  1.35it/s]\u001b[A\n","71it [04:15,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:37<02:06,  1.66s/it]\n","73it [04:56, 12.84s/it]\u001b[A\n","74it [04:57,  9.06s/it]\u001b[A\n","75it [04:57,  6.41s/it]\u001b[A\n","76it [04:57,  4.56s/it]\u001b[A\n","77it [04:57,  3.26s/it]\u001b[A\n","78it [04:58,  2.35s/it]\u001b[A\n","79it [04:58,  1.72s/it]\u001b[A\n","80it [04:58,  1.27s/it]\u001b[A\n","81it [04:58,  1.04it/s]\u001b[A\n","82it [04:59,  1.35it/s]\u001b[A\n","83it [04:59,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:21<01:24,  1.66s/it]\n","85it [05:41, 12.85s/it]\u001b[A\n","86it [05:41,  9.06s/it]\u001b[A\n","87it [05:41,  6.41s/it]\u001b[A\n","88it [05:41,  4.56s/it]\u001b[A\n","89it [05:42,  3.26s/it]\u001b[A\n","90it [05:42,  2.35s/it]\u001b[A\n","91it [05:42,  1.72s/it]\u001b[A\n","92it [05:42,  1.27s/it]\u001b[A\n","93it [05:43,  1.04it/s]\u001b[A\n","94it [05:43,  1.35it/s]\u001b[A\n","95it [05:43,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:05<00:43,  1.66s/it]\n","97it [06:25, 12.85s/it]\u001b[A\n","98it [06:25,  9.06s/it]\u001b[A\n","99it [06:25,  6.41s/it]\u001b[A\n","100it [06:26,  4.56s/it]\u001b[A\n","101it [06:26,  3.26s/it]\u001b[A\n","102it [06:26,  2.35s/it]\u001b[A\n","103it [06:26,  1.72s/it]\u001b[A\n","104it [06:27,  1.27s/it]\u001b[A\n","105it [06:27,  1.04it/s]\u001b[A\n","106it [06:27,  1.35it/s]\u001b[A\n","107it [06:27,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:49<00:01,  1.66s/it]\n","109it [07:09, 12.85s/it]\u001b[A\n","110it [07:09,  9.06s/it]\u001b[A\n","111it [07:10,  6.41s/it]\u001b[A\n","112it [07:10,  4.56s/it]\u001b[A\n","113it [07:10,  3.26s/it]\u001b[A\n","114it [07:10,  2.35s/it]\u001b[A\n","115it [07:11,  1.72s/it]\u001b[A\n","116it [07:11,  1.27s/it]\u001b[A\n","117it [07:11,  1.04it/s]\u001b[A\n","118it [07:11,  1.35it/s]\u001b[A\n","119it [07:11,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:54<00:00,  1.90s/it]\n","03/08/2022 07:00:05 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:00:19 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:26,  4.64s/it]\u001b[A\n","122it [07:26,  3.32s/it]\u001b[A\n","123it [07:26,  2.39s/it]\u001b[A\n","124it [07:27,  1.74s/it]\u001b[A\n","125it [07:27,  1.29s/it]\u001b[A\n","126it [07:27,  1.03it/s]\u001b[A\n","127it [07:27,  1.33it/s]\u001b[A\n","128it [07:28,  1.68it/s]\u001b[A\n","129it [07:28,  2.06it/s]\u001b[A\n","130it [07:28,  2.44it/s]\u001b[A\n","131it [07:28,  2.81it/s]\u001b[A\n","132it [07:28,  3.48it/s]\u001b[A03/08/2022 07:00:21 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:00:21 - INFO - __main__ -     eval_loss = 3.832409381866455\n","03/08/2022 07:00:21 - INFO - __main__ -     eval_auroc = 0.909375011920929\n","03/08/2022 07:00:21 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 07:00:21 - INFO - __main__ -     eval_f1 = 0.4761904776096344\n","03/08/2022 07:00:22 - INFO - filelock -   Lock 140408901061392 acquired on log.lock\n","03/08/2022 07:00:22 - INFO - filelock -   Lock 140408901061392 released on log.lock\n","132it [07:28,  3.40s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:00:27 - INFO - __main__ -   Specify load the 86-th prompt: *cls*▁4.*mask*.*+sent_0**sep+* | {0: \"U\", 1: \"Z\"}\n","03/08/2022 07:00:27 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:00:27 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-29226', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-00-27_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-29226', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:00:27 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:00:29 - INFO - src.dataset -   Label 0 to word ĠU (121)\n","03/08/2022 07:00:29 - INFO - src.dataset -   Label 1 to word ĠZ (525)\n","03/08/2022 07:00:29 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:00:29 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:00:29 - INFO - filelock -   Lock 139870407921232 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:00:29 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 07:00:29 - INFO - filelock -   Lock 139870407921232 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:00:29 - INFO - src.dataset -   Label 0 to word ĠU (121)\n","03/08/2022 07:00:29 - INFO - src.dataset -   Label 1 to word ĠZ (525)\n","03/08/2022 07:00:29 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:00:29 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:00:29 - INFO - filelock -   Lock 139870395081168 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:00:29 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:00:29 - INFO - filelock -   Lock 139870395081168 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:00:29 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:00:29 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:00:29 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 306, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:00:29 - INFO - src.dataset -   text: <s>▁4.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:00:44 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:00:44 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:00:44 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:00:44 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:00:44 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:00:44 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:00:44 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 07:01:28 - INFO - src.trainer -   Best dev result: 0.934249997138977\n","Epoch:  20% 49/250 [01:31<05:34,  1.66s/it]\n","13it [00:51, 14.91s/it]            \u001b[A\n","14it [00:51, 10.51s/it]\u001b[A\n","15it [00:52,  7.42s/it]\u001b[A\n","16it [00:52,  5.27s/it]\u001b[A\n","17it [00:52,  3.76s/it]\u001b[A\n","18it [00:52,  2.70s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:53,  1.55it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:35, 12.87s/it]\u001b[A\n","26it [01:36,  9.08s/it]\u001b[A\n","27it [01:36,  6.42s/it]\u001b[A\n","28it [01:36,  4.57s/it]\u001b[A\n","29it [01:36,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.04it/s]\u001b[A\n","34it [01:37,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:20, 12.83s/it]\u001b[A\n","38it [02:20,  9.05s/it]\u001b[A\n","39it [02:20,  6.41s/it]\u001b[A\n","40it [02:20,  4.55s/it]\u001b[A\n","41it [02:20,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:21,  1.71s/it]\u001b[A\n","44it [02:21,  1.27s/it]\u001b[A\n","45it [02:21,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:29,  1.66s/it]\n","49it [03:04, 12.84s/it]\u001b[A\n","50it [03:04,  9.05s/it]\u001b[A\n","51it [03:04,  6.41s/it]\u001b[A\n","52it [03:04,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:05,  1.71s/it]\u001b[A\n","56it [03:05,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:28<02:47,  1.66s/it]\n","61it [03:48, 12.85s/it]\u001b[A\n","62it [03:48,  9.07s/it]\u001b[A\n","63it [03:48,  6.42s/it]\u001b[A\n","64it [03:49,  4.56s/it]\u001b[A\n","65it [03:49,  3.26s/it]\u001b[A\n","66it [03:49,  2.35s/it]\u001b[A\n","67it [03:49,  1.72s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:50,  1.04it/s]\u001b[A\n","70it [03:50,  1.35it/s]\u001b[A\n","71it [03:50,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:12<02:06,  1.66s/it]\n","73it [04:32, 12.84s/it]\u001b[A\n","74it [04:32,  9.06s/it]\u001b[A\n","75it [04:33,  6.41s/it]\u001b[A\n","76it [04:33,  4.55s/it]\u001b[A\n","77it [04:33,  3.26s/it]\u001b[A\n","78it [04:33,  2.35s/it]\u001b[A\n","79it [04:34,  1.71s/it]\u001b[A\n","80it [04:34,  1.27s/it]\u001b[A\n","81it [04:34,  1.04it/s]\u001b[A\n","82it [04:34,  1.35it/s]\u001b[A\n","83it [04:34,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:57<01:24,  1.66s/it]\n","85it [05:16, 12.84s/it]\u001b[A\n","86it [05:17,  9.06s/it]\u001b[A\n","87it [05:17,  6.41s/it]\u001b[A\n","88it [05:17,  4.56s/it]\u001b[A\n","89it [05:17,  3.26s/it]\u001b[A\n","90it [05:17,  2.35s/it]\u001b[A\n","91it [05:18,  1.71s/it]\u001b[A\n","92it [05:18,  1.27s/it]\u001b[A\n","93it [05:18,  1.04it/s]\u001b[A\n","94it [05:18,  1.35it/s]\u001b[A\n","95it [05:19,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:41<00:43,  1.66s/it]\n","97it [06:01, 12.84s/it]\u001b[A\n","98it [06:01,  9.06s/it]\u001b[A\n","99it [06:01,  6.41s/it]\u001b[A\n","100it [06:01,  4.56s/it]\u001b[A\n","101it [06:01,  3.26s/it]\u001b[A\n","102it [06:02,  2.35s/it]\u001b[A\n","103it [06:02,  1.71s/it]\u001b[A\n","104it [06:02,  1.27s/it]\u001b[A\n","105it [06:02,  1.04it/s]\u001b[A\n","106it [06:03,  1.35it/s]\u001b[A\n","107it [06:03,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:25<00:01,  1.66s/it]\n","109it [06:45, 12.83s/it]\u001b[A\n","110it [06:45,  9.05s/it]\u001b[A\n","111it [06:45,  6.41s/it]\u001b[A\n","112it [06:45,  4.55s/it]\u001b[A\n","113it [06:46,  3.26s/it]\u001b[A\n","114it [06:46,  2.35s/it]\u001b[A\n","115it [06:46,  1.71s/it]\u001b[A\n","116it [06:46,  1.27s/it]\u001b[A\n","117it [06:47,  1.04it/s]\u001b[A\n","118it [06:47,  1.35it/s]\u001b[A\n","119it [06:47,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:29<00:00,  1.80s/it]\n","03/08/2022 07:08:14 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:08:27 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:00,  4.32s/it]\u001b[A\n","122it [07:01,  3.09s/it]\u001b[A\n","123it [07:01,  2.23s/it]\u001b[A\n","124it [07:01,  1.63s/it]\u001b[A\n","125it [07:01,  1.21s/it]\u001b[A\n","126it [07:02,  1.09it/s]\u001b[A\n","127it [07:02,  1.40it/s]\u001b[A\n","128it [07:02,  1.76it/s]\u001b[A\n","129it [07:02,  2.14it/s]\u001b[A\n","130it [07:03,  2.52it/s]\u001b[A\n","131it [07:03,  2.88it/s]\u001b[A\n","132it [07:03,  3.57it/s]\u001b[A03/08/2022 07:08:29 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:08:29 - INFO - __main__ -     eval_loss = 3.132761001586914\n","03/08/2022 07:08:29 - INFO - __main__ -     eval_auroc = 0.934249997138977\n","03/08/2022 07:08:29 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 07:08:29 - INFO - __main__ -     eval_f1 = 0.5333333611488342\n","03/08/2022 07:08:29 - INFO - filelock -   Lock 139870407857296 acquired on log.lock\n","03/08/2022 07:08:29 - INFO - filelock -   Lock 139870407857296 released on log.lock\n","132it [07:03,  3.21s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:08:35 - INFO - __main__ -   Specify load the 87-th prompt: *cls*▁4.*mask*.*+sent_0**sep+* | {0: \"2018\", 1: \"Amazing\"}\n","03/08/2022 07:08:35 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:08:35 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-27407', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-08-35_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-27407', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:08:35 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:08:37 - INFO - src.dataset -   Label 0 to word Ġ2018 (199)\n","03/08/2022 07:08:37 - INFO - src.dataset -   Label 1 to word ĠAmazing (24361)\n","03/08/2022 07:08:37 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:08:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:08:37 - INFO - filelock -   Lock 139697252100816 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:08:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 07:08:37 - INFO - filelock -   Lock 139697252100816 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:08:37 - INFO - src.dataset -   Label 0 to word Ġ2018 (199)\n","03/08/2022 07:08:37 - INFO - src.dataset -   Label 1 to word ĠAmazing (24361)\n","03/08/2022 07:08:37 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:08:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:08:37 - INFO - filelock -   Lock 139697220452304 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:08:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:08:37 - INFO - filelock -   Lock 139697220452304 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:08:37 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:08:37 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:08:37 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 306, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:08:37 - INFO - src.dataset -   text: <s>▁4.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:08:52 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:08:52 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:08:52 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:08:52 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:08:52 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:08:52 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:08:52 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.11it/s]\u001b[A03/08/2022 07:09:36 - INFO - src.trainer -   Best dev result: 0.9626249670982361\n","Epoch:  20% 49/250 [01:31<05:34,  1.66s/it]\n","13it [00:51, 14.81s/it]            \u001b[A\n","14it [00:51, 10.44s/it]\u001b[A\n","15it [00:51,  7.38s/it]\u001b[A\n","16it [00:52,  5.23s/it]\u001b[A\n","17it [00:52,  3.73s/it]\u001b[A\n","18it [00:52,  2.68s/it]\u001b[A\n","19it [00:52,  1.95s/it]\u001b[A\n","20it [00:52,  1.43s/it]\u001b[A\n","21it [00:53,  1.07s/it]\u001b[A\n","22it [00:53,  1.22it/s]\u001b[A\n","23it [00:53,  1.56it/s]\u001b[A\n","Epoch:  30% 74/250 [02:15<04:51,  1.66s/it]\n","25it [01:35, 12.82s/it]\u001b[A\n","26it [01:35,  9.05s/it]\u001b[A\n","27it [01:35,  6.40s/it]\u001b[A\n","28it [01:36,  4.55s/it]\u001b[A\n","29it [01:36,  3.25s/it]\u001b[A\n","30it [01:36,  2.35s/it]\u001b[A\n","31it [01:36,  1.71s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.05it/s]\u001b[A\n","34it [01:37,  1.35it/s]\u001b[A\n","35it [01:37,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [02:59<04:10,  1.66s/it]\n","37it [02:19, 12.80s/it]\u001b[A\n","38it [02:19,  9.03s/it]\u001b[A\n","39it [02:19,  6.39s/it]\u001b[A\n","40it [02:20,  4.54s/it]\u001b[A\n","41it [02:20,  3.25s/it]\u001b[A\n","42it [02:20,  2.34s/it]\u001b[A\n","43it [02:20,  1.71s/it]\u001b[A\n","44it [02:21,  1.27s/it]\u001b[A\n","45it [02:21,  1.05it/s]\u001b[A\n","46it [02:21,  1.36it/s]\u001b[A\n","47it [02:21,  1.71it/s]\u001b[A\n","Epoch:  50% 124/250 [03:43<03:28,  1.65s/it]\n","49it [03:03, 12.80s/it]\u001b[A\n","50it [03:03,  9.03s/it]\u001b[A\n","51it [03:03,  6.39s/it]\u001b[A\n","52it [03:04,  4.54s/it]\u001b[A\n","53it [03:04,  3.25s/it]\u001b[A\n","54it [03:04,  2.34s/it]\u001b[A\n","55it [03:04,  1.71s/it]\u001b[A\n","56it [03:05,  1.27s/it]\u001b[A\n","57it [03:05,  1.05it/s]\u001b[A\n","58it [03:05,  1.35it/s]\u001b[A\n","59it [03:05,  1.71it/s]\u001b[A\n","Epoch:  60% 149/250 [04:27<02:47,  1.66s/it]\n","61it [03:47, 12.81s/it]\u001b[A\n","62it [03:47,  9.04s/it]\u001b[A\n","63it [03:48,  6.39s/it]\u001b[A\n","64it [03:48,  4.55s/it]\u001b[A\n","65it [03:48,  3.25s/it]\u001b[A\n","66it [03:48,  2.34s/it]\u001b[A\n","67it [03:48,  1.71s/it]\u001b[A\n","68it [03:49,  1.27s/it]\u001b[A\n","69it [03:49,  1.05it/s]\u001b[A\n","70it [03:49,  1.35it/s]\u001b[A\n","71it [03:49,  1.71it/s]\u001b[A\n","Epoch:  70% 174/250 [05:11<02:06,  1.66s/it]\n","73it [04:31, 12.83s/it]\u001b[A\n","74it [04:31,  9.05s/it]\u001b[A\n","75it [04:32,  6.40s/it]\u001b[A\n","76it [04:32,  4.55s/it]\u001b[A\n","77it [04:32,  3.25s/it]\u001b[A\n","78it [04:32,  2.35s/it]\u001b[A\n","79it [04:33,  1.71s/it]\u001b[A\n","80it [04:33,  1.27s/it]\u001b[A\n","81it [04:33,  1.04it/s]\u001b[A\n","82it [04:33,  1.35it/s]\u001b[A\n","83it [04:34,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:56<01:24,  1.66s/it]\n","85it [05:15, 12.83s/it]\u001b[A\n","86it [05:16,  9.05s/it]\u001b[A\n","87it [05:16,  6.41s/it]\u001b[A\n","88it [05:16,  4.55s/it]\u001b[A\n","89it [05:16,  3.26s/it]\u001b[A\n","90it [05:17,  2.35s/it]\u001b[A\n","91it [05:17,  1.71s/it]\u001b[A\n","92it [05:17,  1.27s/it]\u001b[A\n","93it [05:17,  1.04it/s]\u001b[A\n","94it [05:17,  1.35it/s]\u001b[A\n","95it [05:18,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:40<00:43,  1.66s/it]\n","97it [06:00, 12.83s/it]\u001b[A\n","98it [06:00,  9.05s/it]\u001b[A\n","99it [06:00,  6.40s/it]\u001b[A\n","100it [06:00,  4.55s/it]\u001b[A\n","101it [06:00,  3.26s/it]\u001b[A\n","102it [06:01,  2.35s/it]\u001b[A\n","103it [06:01,  1.71s/it]\u001b[A\n","104it [06:01,  1.27s/it]\u001b[A\n","105it [06:01,  1.04it/s]\u001b[A\n","106it [06:02,  1.35it/s]\u001b[A\n","107it [06:02,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:24<00:01,  1.66s/it]\n","109it [06:44, 12.84s/it]\u001b[A\n","110it [06:44,  9.05s/it]\u001b[A\n","111it [06:44,  6.41s/it]\u001b[A\n","112it [06:44,  4.55s/it]\u001b[A\n","113it [06:45,  3.26s/it]\u001b[A\n","114it [06:45,  2.35s/it]\u001b[A\n","115it [06:45,  1.71s/it]\u001b[A\n","116it [06:45,  1.27s/it]\u001b[A\n","117it [06:46,  1.04it/s]\u001b[A\n","118it [06:46,  1.35it/s]\u001b[A\n","119it [06:46,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:28<00:00,  1.79s/it]\n","03/08/2022 07:16:21 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:16:34 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:00,  4.34s/it]\u001b[A\n","122it [07:00,  3.10s/it]\u001b[A\n","123it [07:00,  2.24s/it]\u001b[A\n","124it [07:00,  1.64s/it]\u001b[A\n","125it [07:00,  1.22s/it]\u001b[A\n","126it [07:01,  1.09it/s]\u001b[A\n","127it [07:01,  1.40it/s]\u001b[A\n","128it [07:01,  1.76it/s]\u001b[A\n","129it [07:01,  2.14it/s]\u001b[A\n","130it [07:02,  2.52it/s]\u001b[A\n","131it [07:02,  2.88it/s]\u001b[A\n","132it [07:02,  3.56it/s]\u001b[A03/08/2022 07:16:36 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:16:36 - INFO - __main__ -     eval_loss = 1.1777856349945068\n","03/08/2022 07:16:36 - INFO - __main__ -     eval_auroc = 0.9626249670982361\n","03/08/2022 07:16:36 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 07:16:36 - INFO - __main__ -     eval_f1 = 0.64000004529953\n","03/08/2022 07:16:36 - INFO - filelock -   Lock 139697201192592 acquired on log.lock\n","03/08/2022 07:16:36 - INFO - filelock -   Lock 139697201192592 released on log.lock\n","132it [07:02,  3.20s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:16:42 - INFO - __main__ -   Specify load the 88-th prompt: *cls*▁4.*mask*.*+sent_0**sep+* | {0: \"O\", 1: \"Z\"}\n","03/08/2022 07:16:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:16:42 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-3624', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-16-42_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-3624', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:16:42 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:16:44 - INFO - src.dataset -   Label 0 to word ĠO (384)\n","03/08/2022 07:16:44 - INFO - src.dataset -   Label 1 to word ĠZ (525)\n","03/08/2022 07:16:44 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:16:44 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:16:44 - INFO - filelock -   Lock 140701726471952 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:16:44 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 07:16:44 - INFO - filelock -   Lock 140701726471952 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:16:44 - INFO - src.dataset -   Label 0 to word ĠO (384)\n","03/08/2022 07:16:44 - INFO - src.dataset -   Label 1 to word ĠZ (525)\n","03/08/2022 07:16:44 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:16:44 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:16:44 - INFO - filelock -   Lock 140701697314768 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:16:44 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/08/2022 07:16:44 - INFO - filelock -   Lock 140701697314768 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:16:44 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:16:44 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:16:44 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 306, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:16:44 - INFO - src.dataset -   text: <s>▁4.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:16:59 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:16:59 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:16:59 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:16:59 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:16:59 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:16:59 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:16:59 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 07:17:44 - INFO - src.trainer -   Best dev result: 0.9627500176429749\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.14s/it]            \u001b[A\n","14it [00:52, 10.66s/it]\u001b[A\n","15it [00:52,  7.53s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.81s/it]\u001b[A\n","18it [00:53,  2.74s/it]\u001b[A\n","19it [00:53,  1.99s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.90s/it]\u001b[A\n","26it [01:36,  9.10s/it]\u001b[A\n","27it [01:37,  6.44s/it]\u001b[A\n","28it [01:37,  4.58s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:38,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:39,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:01<04:11,  1.66s/it]\n","37it [02:20, 12.85s/it]\u001b[A\n","38it [02:21,  9.07s/it]\u001b[A\n","39it [02:21,  6.42s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:22,  2.35s/it]\u001b[A\n","43it [02:22,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:23,  1.35it/s]\u001b[A\n","47it [02:23,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:29,  1.66s/it]\n","49it [03:05, 12.85s/it]\u001b[A\n","50it [03:05,  9.06s/it]\u001b[A\n","51it [03:05,  6.41s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:06,  3.26s/it]\u001b[A\n","54it [03:06,  2.35s/it]\u001b[A\n","55it [03:06,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:07,  1.04it/s]\u001b[A\n","58it [03:07,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:29<02:47,  1.66s/it]\n","61it [03:49, 12.84s/it]\u001b[A\n","62it [03:49,  9.05s/it]\u001b[A\n","63it [03:49,  6.41s/it]\u001b[A\n","64it [03:50,  4.55s/it]\u001b[A\n","65it [03:50,  3.26s/it]\u001b[A\n","66it [03:50,  2.35s/it]\u001b[A\n","67it [03:50,  1.71s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:51,  1.04it/s]\u001b[A\n","70it [03:51,  1.35it/s]\u001b[A\n","71it [03:51,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:13<02:06,  1.66s/it]\n","73it [04:33, 12.84s/it]\u001b[A\n","74it [04:33,  9.06s/it]\u001b[A\n","75it [04:34,  6.41s/it]\u001b[A\n","76it [04:34,  4.56s/it]\u001b[A\n","77it [04:34,  3.26s/it]\u001b[A\n","78it [04:34,  2.35s/it]\u001b[A\n","79it [04:34,  1.71s/it]\u001b[A\n","80it [04:35,  1.27s/it]\u001b[A\n","81it [04:35,  1.04it/s]\u001b[A\n","82it [04:35,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:57<01:24,  1.66s/it]\n","85it [05:17, 12.84s/it]\u001b[A\n","86it [05:17,  9.06s/it]\u001b[A\n","87it [05:18,  6.41s/it]\u001b[A\n","88it [05:18,  4.56s/it]\u001b[A\n","89it [05:18,  3.26s/it]\u001b[A\n","90it [05:18,  2.35s/it]\u001b[A\n","91it [05:19,  1.72s/it]\u001b[A\n","92it [05:19,  1.27s/it]\u001b[A\n","93it [05:19,  1.04it/s]\u001b[A\n","94it [05:19,  1.35it/s]\u001b[A\n","95it [05:20,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:42<00:43,  1.67s/it]\n","97it [06:02, 12.86s/it]\u001b[A\n","98it [06:02,  9.07s/it]\u001b[A\n","99it [06:02,  6.42s/it]\u001b[A\n","100it [06:02,  4.56s/it]\u001b[A\n","101it [06:02,  3.26s/it]\u001b[A\n","102it [06:03,  2.35s/it]\u001b[A\n","103it [06:03,  1.72s/it]\u001b[A\n","104it [06:03,  1.27s/it]\u001b[A\n","105it [06:03,  1.04it/s]\u001b[A\n","106it [06:04,  1.35it/s]\u001b[A\n","107it [06:04,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:26<00:01,  1.67s/it]\n","109it [06:46, 12.88s/it]\u001b[A\n","110it [06:46,  9.08s/it]\u001b[A\n","111it [06:46,  6.43s/it]\u001b[A\n","112it [06:47,  4.57s/it]\u001b[A\n","113it [06:47,  3.27s/it]\u001b[A\n","114it [06:47,  2.36s/it]\u001b[A\n","115it [06:47,  1.72s/it]\u001b[A\n","116it [06:47,  1.27s/it]\u001b[A\n","117it [06:48,  1.04it/s]\u001b[A\n","118it [06:48,  1.35it/s]\u001b[A\n","119it [06:48,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:30<00:00,  1.80s/it]\n","03/08/2022 07:24:30 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:24:43 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:01,  4.27s/it]\u001b[A\n","122it [07:02,  3.06s/it]\u001b[A\n","123it [07:02,  2.21s/it]\u001b[A\n","124it [07:02,  1.62s/it]\u001b[A\n","125it [07:02,  1.20s/it]\u001b[A\n","126it [07:03,  1.10it/s]\u001b[A\n","127it [07:03,  1.42it/s]\u001b[A\n","128it [07:03,  1.77it/s]\u001b[A\n","129it [07:03,  2.15it/s]\u001b[A\n","130it [07:04,  2.53it/s]\u001b[A\n","131it [07:04,  2.89it/s]\u001b[A\n","132it [07:04,  3.57it/s]\u001b[A03/08/2022 07:24:46 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:24:46 - INFO - __main__ -     eval_loss = 1.488381266593933\n","03/08/2022 07:24:46 - INFO - __main__ -     eval_auroc = 0.9627500176429749\n","03/08/2022 07:24:46 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 07:24:46 - INFO - __main__ -     eval_f1 = 0.6153846383094788\n","03/08/2022 07:24:46 - INFO - filelock -   Lock 140701707760784 acquired on log.lock\n","03/08/2022 07:24:46 - INFO - filelock -   Lock 140701707760784 released on log.lock\n","132it [07:04,  3.22s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:24:52 - INFO - __main__ -   Specify load the 89-th prompt: *cls*▁4.*mask*.*+sent_0**sep+* | {0: \"All\", 1: \"Okay\"}\n","03/08/2022 07:24:52 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:24:52 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-20976', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-24-52_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-20976', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:24:52 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:24:53 - INFO - src.dataset -   Label 0 to word ĠAll (404)\n","03/08/2022 07:24:53 - INFO - src.dataset -   Label 1 to word ĠOkay (8487)\n","03/08/2022 07:24:53 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:24:53 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:24:53 - INFO - filelock -   Lock 139718637637904 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:24:53 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 07:24:53 - INFO - filelock -   Lock 139718637637904 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:24:53 - INFO - src.dataset -   Label 0 to word ĠAll (404)\n","03/08/2022 07:24:53 - INFO - src.dataset -   Label 1 to word ĠOkay (8487)\n","03/08/2022 07:24:53 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:24:53 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:24:53 - INFO - filelock -   Lock 139718637655440 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:24:53 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:24:53 - INFO - filelock -   Lock 139718637655440 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:24:53 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:24:53 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:24:53 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 306, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:24:53 - INFO - src.dataset -   text: <s>▁4.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:25:08 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:25:08 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:25:08 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:25:08 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:25:08 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:25:08 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:25:08 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:17,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.64it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.44it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 07:25:53 - INFO - src.trainer -   Best dev result: 0.8708750605583191\n","Epoch:  20% 49/250 [01:32<05:40,  1.70s/it]\n","13it [00:51, 14.96s/it]            \u001b[A\n","14it [00:52, 10.54s/it]\u001b[A\n","15it [00:52,  7.45s/it]\u001b[A\n","16it [00:52,  5.28s/it]\u001b[A\n","17it [00:52,  3.77s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:53,  1.67s/it]\n","25it [01:36, 12.99s/it]\u001b[A\n","26it [01:36,  9.16s/it]\u001b[A\n","27it [01:36,  6.48s/it]\u001b[A\n","28it [01:37,  4.61s/it]\u001b[A\n","29it [01:37,  3.29s/it]\u001b[A\n","30it [01:37,  2.38s/it]\u001b[A\n","31it [01:37,  1.73s/it]\u001b[A\n","32it [01:38,  1.28s/it]\u001b[A\n","33it [01:38,  1.03it/s]\u001b[A\n","34it [01:38,  1.34it/s]\u001b[A\n","35it [01:38,  1.69it/s]\u001b[A\n","Epoch:  40% 99/250 [03:01<04:11,  1.66s/it]\n","37it [02:20, 12.87s/it]\u001b[A\n","38it [02:20,  9.08s/it]\u001b[A\n","39it [02:21,  6.43s/it]\u001b[A\n","40it [02:21,  4.57s/it]\u001b[A\n","41it [02:21,  3.27s/it]\u001b[A\n","42it [02:21,  2.36s/it]\u001b[A\n","43it [02:22,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:23,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:30,  1.67s/it]\n","49it [03:05, 12.88s/it]\u001b[A\n","50it [03:05,  9.09s/it]\u001b[A\n","51it [03:05,  6.43s/it]\u001b[A\n","52it [03:05,  4.57s/it]\u001b[A\n","53it [03:05,  3.27s/it]\u001b[A\n","54it [03:06,  2.36s/it]\u001b[A\n","55it [03:06,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:07,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:29<02:48,  1.66s/it]\n","61it [03:49, 12.87s/it]\u001b[A\n","62it [03:49,  9.08s/it]\u001b[A\n","63it [03:49,  6.42s/it]\u001b[A\n","64it [03:50,  4.57s/it]\u001b[A\n","65it [03:50,  3.27s/it]\u001b[A\n","66it [03:50,  2.36s/it]\u001b[A\n","67it [03:50,  1.72s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:51,  1.04it/s]\u001b[A\n","70it [03:51,  1.35it/s]\u001b[A\n","71it [03:51,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:14<02:06,  1.67s/it]\n","73it [04:33, 12.87s/it]\u001b[A\n","74it [04:33,  9.08s/it]\u001b[A\n","75it [04:34,  6.42s/it]\u001b[A\n","76it [04:34,  4.57s/it]\u001b[A\n","77it [04:34,  3.27s/it]\u001b[A\n","78it [04:34,  2.35s/it]\u001b[A\n","79it [04:35,  1.72s/it]\u001b[A\n","80it [04:35,  1.27s/it]\u001b[A\n","81it [04:35,  1.04it/s]\u001b[A\n","82it [04:35,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:58<01:24,  1.67s/it]\n","85it [05:17, 12.87s/it]\u001b[A\n","86it [05:18,  9.08s/it]\u001b[A\n","87it [05:18,  6.43s/it]\u001b[A\n","88it [05:18,  4.57s/it]\u001b[A\n","89it [05:18,  3.27s/it]\u001b[A\n","90it [05:19,  2.36s/it]\u001b[A\n","91it [05:19,  1.72s/it]\u001b[A\n","92it [05:19,  1.27s/it]\u001b[A\n","93it [05:19,  1.04it/s]\u001b[A\n","94it [05:20,  1.35it/s]\u001b[A\n","95it [05:20,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:42<00:43,  1.66s/it]\n","97it [06:02, 12.88s/it]\u001b[A\n","98it [06:02,  9.09s/it]\u001b[A\n","99it [06:02,  6.43s/it]\u001b[A\n","100it [06:02,  4.57s/it]\u001b[A\n","101it [06:03,  3.27s/it]\u001b[A\n","102it [06:03,  2.36s/it]\u001b[A\n","103it [06:03,  1.72s/it]\u001b[A\n","104it [06:03,  1.27s/it]\u001b[A\n","105it [06:04,  1.04it/s]\u001b[A\n","106it [06:04,  1.35it/s]\u001b[A\n","107it [06:04,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:26<00:01,  1.66s/it]\n","109it [06:46, 12.85s/it]\u001b[A\n","110it [06:46,  9.07s/it]\u001b[A\n","111it [06:46,  6.42s/it]\u001b[A\n","112it [06:47,  4.56s/it]\u001b[A\n","113it [06:47,  3.26s/it]\u001b[A\n","114it [06:47,  2.35s/it]\u001b[A\n","115it [06:47,  1.72s/it]\u001b[A\n","116it [06:48,  1.27s/it]\u001b[A\n","117it [06:48,  1.04it/s]\u001b[A\n","118it [06:48,  1.35it/s]\u001b[A\n","119it [06:48,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:31<00:00,  1.80s/it]\n","03/08/2022 07:32:40 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:32:53 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:02,  4.25s/it]\u001b[A\n","122it [07:02,  3.05s/it]\u001b[A\n","123it [07:02,  2.20s/it]\u001b[A\n","124it [07:02,  1.61s/it]\u001b[A\n","125it [07:02,  1.20s/it]\u001b[A\n","126it [07:03,  1.10it/s]\u001b[A\n","127it [07:03,  1.42it/s]\u001b[A\n","128it [07:03,  1.78it/s]\u001b[A\n","129it [07:03,  2.16it/s]\u001b[A\n","130it [07:04,  2.54it/s]\u001b[A\n","131it [07:04,  2.90it/s]\u001b[A\n","132it [07:04,  3.58it/s]\u001b[A03/08/2022 07:32:55 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:32:55 - INFO - __main__ -     eval_loss = 3.1585376262664795\n","03/08/2022 07:32:55 - INFO - __main__ -     eval_auroc = 0.8708750605583191\n","03/08/2022 07:32:55 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/08/2022 07:32:55 - INFO - __main__ -     eval_f1 = 0.5176470875740051\n","03/08/2022 07:32:55 - INFO - filelock -   Lock 139718620827024 acquired on log.lock\n","03/08/2022 07:32:55 - INFO - filelock -   Lock 139718620827024 released on log.lock\n","132it [07:04,  3.22s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:33:01 - INFO - __main__ -   Specify load the 90-th prompt: *cls*▁Yes!*mask*!*+sent_0**sep+* | {0: \"We\", 1: \"Bad\"}\n","03/08/2022 07:33:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:33:01 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-21526', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-33-01_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-21526', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:33:01 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:33:02 - INFO - src.dataset -   Label 0 to word ĠWe (166)\n","03/08/2022 07:33:02 - INFO - src.dataset -   Label 1 to word ĠBad (5654)\n","03/08/2022 07:33:02 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:33:02 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:33:02 - INFO - filelock -   Lock 140041202415312 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:33:02 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 07:33:02 - INFO - filelock -   Lock 140041202415312 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:33:02 - INFO - src.dataset -   Label 0 to word ĠWe (166)\n","03/08/2022 07:33:02 - INFO - src.dataset -   Label 1 to word ĠBad (5654)\n","03/08/2022 07:33:02 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:33:02 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:33:02 - INFO - filelock -   Lock 140041169181456 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:33:02 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:33:02 - INFO - filelock -   Lock 140041169181456 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:33:02 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:33:02 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:33:02 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:33:02 - INFO - src.dataset -   text: <s>▁Yes!<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:33:18 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:33:18 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:33:18 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:33:18 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:33:18 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:33:18 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:33:18 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.64it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.90it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.71it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.58it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.44it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 07:34:02 - INFO - src.trainer -   Best dev result: 0.9154999852180481\n","Epoch:  20% 49/250 [01:32<05:37,  1.68s/it]\n","13it [00:51, 15.00s/it]            \u001b[A\n","14it [00:52, 10.57s/it]\u001b[A\n","15it [00:52,  7.47s/it]\u001b[A\n","16it [00:52,  5.30s/it]\u001b[A\n","17it [00:52,  3.78s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 07:34:54 - INFO - src.trainer -   Best dev result: 0.9516249895095825\n","Epoch:  30% 74/250 [02:24<04:54,  1.67s/it]\n","25it [01:43, 15.16s/it]\u001b[A\n","26it [01:44, 10.68s/it]\u001b[A\n","27it [01:44,  7.55s/it]\u001b[A\n","28it [01:44,  5.35s/it]\u001b[A\n","29it [01:44,  3.82s/it]\u001b[A\n","30it [01:44,  2.74s/it]\u001b[A\n","31it [01:45,  1.99s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:45,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:11,  1.66s/it]\n","37it [02:28, 12.90s/it]\u001b[A\n","38it [02:28,  9.10s/it]\u001b[A\n","39it [02:28,  6.44s/it]\u001b[A\n","40it [02:28,  4.58s/it]\u001b[A\n","41it [02:29,  3.27s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.28s/it]\u001b[A\n","45it [02:29,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:29,  1.67s/it]\n","49it [03:12, 12.87s/it]\u001b[A\n","50it [03:12,  9.08s/it]\u001b[A\n","51it [03:12,  6.43s/it]\u001b[A\n","52it [03:13,  4.57s/it]\u001b[A\n","53it [03:13,  3.27s/it]\u001b[A\n","54it [03:13,  2.36s/it]\u001b[A\n","55it [03:13,  1.72s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:37<02:48,  1.66s/it]\n","61it [03:56, 12.87s/it]\u001b[A\n","62it [03:56,  9.08s/it]\u001b[A\n","63it [03:57,  6.42s/it]\u001b[A\n","64it [03:57,  4.57s/it]\u001b[A\n","65it [03:57,  3.27s/it]\u001b[A\n","66it [03:57,  2.36s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:59,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.67s/it]\n","73it [04:41, 12.88s/it]\u001b[A\n","74it [04:41,  9.08s/it]\u001b[A\n","75it [04:41,  6.43s/it]\u001b[A\n","76it [04:41,  4.57s/it]\u001b[A\n","77it [04:41,  3.27s/it]\u001b[A\n","78it [04:42,  2.36s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:25,  1.67s/it]\n","85it [05:25, 12.88s/it]\u001b[A\n","86it [05:25,  9.08s/it]\u001b[A\n","87it [05:25,  6.43s/it]\u001b[A\n","88it [05:26,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:09, 12.86s/it]\u001b[A\n","98it [06:09,  9.07s/it]\u001b[A\n","99it [06:10,  6.42s/it]\u001b[A\n","100it [06:10,  4.56s/it]\u001b[A\n","101it [06:10,  3.26s/it]\u001b[A\n","102it [06:10,  2.35s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.66s/it]\n","109it [06:53, 12.85s/it]\u001b[A\n","110it [06:54,  9.07s/it]\u001b[A\n","111it [06:54,  6.42s/it]\u001b[A\n","112it [06:54,  4.56s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 07:40:56 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:41:10 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.49s/it]\u001b[A\n","122it [07:10,  3.21s/it]\u001b[A\n","123it [07:10,  2.32s/it]\u001b[A\n","124it [07:10,  1.69s/it]\u001b[A\n","125it [07:11,  1.25s/it]\u001b[A\n","126it [07:11,  1.06it/s]\u001b[A\n","127it [07:11,  1.37it/s]\u001b[A\n","128it [07:11,  1.72it/s]\u001b[A\n","129it [07:12,  2.10it/s]\u001b[A\n","130it [07:12,  2.48it/s]\u001b[A\n","131it [07:12,  2.84it/s]\u001b[A\n","132it [07:12,  3.52it/s]\u001b[A03/08/2022 07:41:13 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:41:13 - INFO - __main__ -     eval_loss = 1.792115569114685\n","03/08/2022 07:41:13 - INFO - __main__ -     eval_auroc = 0.9516249895095825\n","03/08/2022 07:41:13 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/08/2022 07:41:13 - INFO - __main__ -     eval_f1 = 0.6478872895240784\n","03/08/2022 07:41:13 - INFO - filelock -   Lock 140041169411664 acquired on log.lock\n","03/08/2022 07:41:13 - INFO - filelock -   Lock 140041169411664 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:41:18 - INFO - __main__ -   Specify load the 91-th prompt: *cls*▁Yes!*mask*!*+sent_0**sep+* | {0: \"Here\", 1: \"Bad\"}\n","03/08/2022 07:41:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:41:18 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-9298', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-41-18_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-9298', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:41:18 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:41:20 - INFO - src.dataset -   Label 0 to word ĠHere (1398)\n","03/08/2022 07:41:20 - INFO - src.dataset -   Label 1 to word ĠBad (5654)\n","03/08/2022 07:41:20 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:41:20 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:41:20 - INFO - filelock -   Lock 140665396606928 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:41:20 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 07:41:20 - INFO - filelock -   Lock 140665396606928 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:41:20 - INFO - src.dataset -   Label 0 to word ĠHere (1398)\n","03/08/2022 07:41:20 - INFO - src.dataset -   Label 1 to word ĠBad (5654)\n","03/08/2022 07:41:20 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:41:20 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:41:20 - INFO - filelock -   Lock 140665442245776 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:41:20 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:41:20 - INFO - filelock -   Lock 140665442245776 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:41:20 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:41:20 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:41:20 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:41:20 - INFO - src.dataset -   text: <s>▁Yes!<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:41:35 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:41:35 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:41:35 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:41:35 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:41:35 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:41:35 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:41:35 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 07:42:20 - INFO - src.trainer -   Best dev result: 0.8959999680519104\n","Epoch:  20% 49/250 [01:32<05:40,  1.69s/it]\n","13it [00:52, 15.04s/it]            \u001b[A\n","14it [00:52, 10.60s/it]\u001b[A\n","15it [00:52,  7.49s/it]\u001b[A\n","16it [00:52,  5.31s/it]\u001b[A\n","17it [00:53,  3.79s/it]\u001b[A\n","18it [00:53,  2.72s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.90s/it]\u001b[A\n","26it [01:36,  9.10s/it]\u001b[A\n","27it [01:36,  6.44s/it]\u001b[A\n","28it [01:37,  4.58s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:38,  1.28s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:01<04:10,  1.66s/it]\n","37it [02:20, 12.85s/it]\u001b[A\n","38it [02:20,  9.07s/it]\u001b[A\n","39it [02:21,  6.42s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:22,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:29,  1.67s/it]\n","49it [03:04, 12.86s/it]\u001b[A\n","50it [03:05,  9.07s/it]\u001b[A\n","51it [03:05,  6.42s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:06,  2.35s/it]\u001b[A\n","55it [03:06,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","60it [03:07,  2.22it/s]\u001b[A03/08/2022 07:45:24 - INFO - src.trainer -   Best dev result: 0.9273750185966492\n","Epoch:  60% 149/250 [04:36<02:52,  1.71s/it]\n","61it [03:56, 15.10s/it]\u001b[A\n","62it [03:56, 10.64s/it]\u001b[A\n","63it [03:57,  7.52s/it]\u001b[A\n","64it [03:57,  5.33s/it]\u001b[A\n","65it [03:57,  3.80s/it]\u001b[A\n","66it [03:57,  2.73s/it]\u001b[A\n","67it [03:58,  1.98s/it]\u001b[A\n","68it [03:58,  1.46s/it]\u001b[A\n","69it [03:58,  1.09s/it]\u001b[A\n","70it [03:58,  1.20it/s]\u001b[A\n","71it [03:58,  1.54it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.67s/it]\n","73it [04:40, 12.90s/it]\u001b[A\n","74it [04:41,  9.10s/it]\u001b[A\n","75it [04:41,  6.44s/it]\u001b[A\n","76it [04:41,  4.58s/it]\u001b[A\n","77it [04:41,  3.27s/it]\u001b[A\n","78it [04:42,  2.36s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.28s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:24,  1.67s/it]\n","85it [05:25, 12.88s/it]\u001b[A\n","86it [05:25,  9.08s/it]\u001b[A\n","87it [05:25,  6.43s/it]\u001b[A\n","88it [05:25,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.67s/it]\n","97it [06:09, 12.88s/it]\u001b[A\n","98it [06:09,  9.08s/it]\u001b[A\n","99it [06:10,  6.43s/it]\u001b[A\n","100it [06:10,  4.57s/it]\u001b[A\n","101it [06:10,  3.27s/it]\u001b[A\n","102it [06:10,  2.36s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.67s/it]\n","109it [06:53, 12.87s/it]\u001b[A\n","110it [06:54,  9.08s/it]\u001b[A\n","111it [06:54,  6.42s/it]\u001b[A\n","112it [06:54,  4.57s/it]\u001b[A\n","113it [06:54,  3.27s/it]\u001b[A\n","114it [06:55,  2.36s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 07:49:13 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:49:27 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.58s/it]\u001b[A\n","122it [07:10,  3.28s/it]\u001b[A\n","123it [07:10,  2.36s/it]\u001b[A\n","124it [07:11,  1.72s/it]\u001b[A\n","125it [07:11,  1.28s/it]\u001b[A\n","126it [07:11,  1.04it/s]\u001b[A\n","127it [07:11,  1.34it/s]\u001b[A\n","128it [07:12,  1.70it/s]\u001b[A\n","129it [07:12,  2.07it/s]\u001b[A\n","130it [07:12,  2.46it/s]\u001b[A\n","131it [07:12,  2.82it/s]\u001b[A\n","132it [07:12,  3.50it/s]\u001b[A03/08/2022 07:49:30 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:49:30 - INFO - __main__ -     eval_loss = 2.6156797409057617\n","03/08/2022 07:49:30 - INFO - __main__ -     eval_auroc = 0.9273750185966492\n","03/08/2022 07:49:30 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 07:49:30 - INFO - __main__ -     eval_f1 = 0.5263157486915588\n","03/08/2022 07:49:30 - INFO - filelock -   Lock 140665442245776 acquired on log.lock\n","03/08/2022 07:49:30 - INFO - filelock -   Lock 140665442245776 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:49:36 - INFO - __main__ -   Specify load the 92-th prompt: *cls*▁Yes!*mask*!*+sent_0**sep+* | {0: \"You\", 1: \"Go\"}\n","03/08/2022 07:49:36 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:49:36 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-14837', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-49-36_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-14837', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:49:36 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:49:37 - INFO - src.dataset -   Label 0 to word ĠYou (370)\n","03/08/2022 07:49:37 - INFO - src.dataset -   Label 1 to word ĠGo (2381)\n","03/08/2022 07:49:37 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:49:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:49:37 - INFO - filelock -   Lock 140142323509456 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:49:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 07:49:37 - INFO - filelock -   Lock 140142323509456 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:49:37 - INFO - src.dataset -   Label 0 to word ĠYou (370)\n","03/08/2022 07:49:37 - INFO - src.dataset -   Label 1 to word ĠGo (2381)\n","03/08/2022 07:49:37 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:49:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:49:37 - INFO - filelock -   Lock 140142322150992 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:49:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:49:37 - INFO - filelock -   Lock 140142322150992 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:49:37 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:49:37 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:49:37 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:49:37 - INFO - src.dataset -   text: <s>▁Yes!<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:49:52 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:49:52 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:49:52 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:49:52 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:49:52 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:49:52 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:49:52 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 07:50:37 - INFO - src.trainer -   Best dev result: 0.9526249170303345\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.09s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.51s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.88s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:36,  6.43s/it]\u001b[A\n","28it [01:37,  4.57s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:20, 12.82s/it]\u001b[A\n","38it [02:20,  9.05s/it]\u001b[A\n","39it [02:21,  6.40s/it]\u001b[A\n","40it [02:21,  4.55s/it]\u001b[A\n","41it [02:21,  3.25s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:22,  1.71s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:28,  1.66s/it]\n","49it [03:04, 12.82s/it]\u001b[A\n","50it [03:04,  9.04s/it]\u001b[A\n","51it [03:05,  6.40s/it]\u001b[A\n","52it [03:05,  4.55s/it]\u001b[A\n","53it [03:05,  3.25s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:06,  1.71s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.05it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","60it [03:07,  2.23it/s]\u001b[A03/08/2022 07:53:42 - INFO - src.trainer -   Best dev result: 0.9574999809265137\n","Epoch:  60% 149/250 [04:37<02:48,  1.67s/it]\n","61it [03:57, 15.32s/it]\u001b[A\n","62it [03:57, 10.79s/it]\u001b[A\n","63it [03:57,  7.62s/it]\u001b[A\n","64it [03:57,  5.41s/it]\u001b[A\n","65it [03:58,  3.85s/it]\u001b[A\n","66it [03:58,  2.77s/it]\u001b[A\n","67it [03:58,  2.01s/it]\u001b[A\n","68it [03:58,  1.47s/it]\u001b[A\n","69it [03:59,  1.10s/it]\u001b[A\n","70it [03:59,  1.19it/s]\u001b[A\n","71it [03:59,  1.52it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.66s/it]\n","73it [04:41, 12.89s/it]\u001b[A\n","74it [04:41,  9.09s/it]\u001b[A\n","75it [04:41,  6.43s/it]\u001b[A\n","76it [04:42,  4.57s/it]\u001b[A\n","77it [04:42,  3.27s/it]\u001b[A\n","78it [04:42,  2.36s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:43,  1.27s/it]\u001b[A\n","81it [04:43,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:06<01:24,  1.67s/it]\n","85it [05:25, 12.87s/it]\u001b[A\n","86it [05:25,  9.08s/it]\u001b[A\n","87it [05:26,  6.42s/it]\u001b[A\n","88it [05:26,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:27,  1.72s/it]\u001b[A\n","92it [05:27,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:28,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:50<00:43,  1.66s/it]\n","97it [06:09, 12.86s/it]\u001b[A\n","98it [06:10,  9.07s/it]\u001b[A\n","99it [06:10,  6.42s/it]\u001b[A\n","100it [06:10,  4.56s/it]\u001b[A\n","101it [06:10,  3.26s/it]\u001b[A\n","102it [06:11,  2.35s/it]\u001b[A\n","103it [06:11,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:12,  1.35it/s]\u001b[A\n","107it [06:12,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.66s/it]\n","109it [06:54, 12.86s/it]\u001b[A\n","110it [06:54,  9.07s/it]\u001b[A\n","111it [06:54,  6.42s/it]\u001b[A\n","112it [06:54,  4.56s/it]\u001b[A\n","113it [06:55,  3.26s/it]\u001b[A\n","114it [06:55,  2.35s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:56,  1.04it/s]\u001b[A\n","118it [06:56,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.84s/it]\n","03/08/2022 07:57:31 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:57:45 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.59s/it]\u001b[A\n","122it [07:11,  3.28s/it]\u001b[A\n","123it [07:11,  2.37s/it]\u001b[A\n","124it [07:11,  1.72s/it]\u001b[A\n","125it [07:11,  1.28s/it]\u001b[A\n","126it [07:12,  1.04it/s]\u001b[A\n","127it [07:12,  1.34it/s]\u001b[A\n","128it [07:12,  1.69it/s]\u001b[A\n","129it [07:12,  2.07it/s]\u001b[A\n","130it [07:12,  2.46it/s]\u001b[A\n","131it [07:13,  2.82it/s]\u001b[A\n","132it [07:13,  3.50it/s]\u001b[A03/08/2022 07:57:48 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:57:48 - INFO - __main__ -     eval_loss = 1.7647958993911743\n","03/08/2022 07:57:48 - INFO - __main__ -     eval_auroc = 0.9574999809265137\n","03/08/2022 07:57:48 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 07:57:48 - INFO - __main__ -     eval_f1 = 0.6666666865348816\n","03/08/2022 07:57:48 - INFO - filelock -   Lock 140142329991312 acquired on log.lock\n","03/08/2022 07:57:48 - INFO - filelock -   Lock 140142329991312 released on log.lock\n","132it [07:13,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:57:54 - INFO - __main__ -   Specify load the 93-th prompt: *cls*▁Yes!*mask*!*+sent_0**sep+* | {0: \"It\", 1: \"Bad\"}\n","03/08/2022 07:57:54 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:57:54 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-30438', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-57-54_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-30438', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:57:54 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:57:55 - INFO - src.dataset -   Label 0 to word ĠIt (85)\n","03/08/2022 07:57:55 - INFO - src.dataset -   Label 1 to word ĠBad (5654)\n","03/08/2022 07:57:55 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:57:55 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:57:55 - INFO - filelock -   Lock 140064765446224 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:57:55 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 07:57:55 - INFO - filelock -   Lock 140064765446224 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:57:55 - INFO - src.dataset -   Label 0 to word ĠIt (85)\n","03/08/2022 07:57:55 - INFO - src.dataset -   Label 1 to word ĠBad (5654)\n","03/08/2022 07:57:55 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:57:55 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:57:55 - INFO - filelock -   Lock 140064764003728 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:57:55 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:57:55 - INFO - filelock -   Lock 140064764003728 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:57:55 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:57:55 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:57:55 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:57:55 - INFO - src.dataset -   text: <s>▁Yes!<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:58:10 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:58:10 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:58:10 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:58:10 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:58:10 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:58:10 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:58:10 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 07:58:55 - INFO - src.trainer -   Best dev result: 0.9196249842643738\n","Epoch:  20% 49/250 [01:32<05:36,  1.67s/it]\n","13it [00:52, 15.10s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:53,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 07:59:47 - INFO - src.trainer -   Best dev result: 0.921750009059906\n","Epoch:  30% 74/250 [02:24<04:56,  1.68s/it]\n","25it [01:44, 15.18s/it]\u001b[A\n","26it [01:44, 10.69s/it]\u001b[A\n","27it [01:44,  7.55s/it]\u001b[A\n","28it [01:44,  5.36s/it]\u001b[A\n","29it [01:45,  3.82s/it]\u001b[A\n","30it [01:45,  2.74s/it]\u001b[A\n","31it [01:45,  1.99s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:46,  1.09s/it]\u001b[A\n","34it [01:46,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:11,  1.66s/it]\n","37it [02:28, 12.89s/it]\u001b[A\n","38it [02:28,  9.09s/it]\u001b[A\n","39it [02:28,  6.43s/it]\u001b[A\n","40it [02:29,  4.57s/it]\u001b[A\n","41it [02:29,  3.27s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:30,  1.27s/it]\u001b[A\n","45it [02:30,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:53<03:29,  1.66s/it]\n","49it [03:12, 12.86s/it]\u001b[A\n","50it [03:12,  9.07s/it]\u001b[A\n","51it [03:13,  6.42s/it]\u001b[A\n","52it [03:13,  4.56s/it]\u001b[A\n","53it [03:13,  3.26s/it]\u001b[A\n","54it [03:13,  2.35s/it]\u001b[A\n","55it [03:14,  1.72s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:15,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:37<02:47,  1.66s/it]\n","61it [03:56, 12.85s/it]\u001b[A\n","62it [03:57,  9.06s/it]\u001b[A\n","63it [03:57,  6.41s/it]\u001b[A\n","64it [03:57,  4.56s/it]\u001b[A\n","65it [03:57,  3.26s/it]\u001b[A\n","66it [03:58,  2.35s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:59,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.67s/it]\n","73it [04:41, 12.86s/it]\u001b[A\n","74it [04:41,  9.07s/it]\u001b[A\n","75it [04:41,  6.42s/it]\u001b[A\n","76it [04:41,  4.56s/it]\u001b[A\n","77it [04:42,  3.26s/it]\u001b[A\n","78it [04:42,  2.35s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:24,  1.66s/it]\n","85it [05:25, 12.87s/it]\u001b[A\n","86it [05:25,  9.08s/it]\u001b[A\n","87it [05:25,  6.42s/it]\u001b[A\n","88it [05:26,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:27,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:50<00:43,  1.66s/it]\n","97it [06:09, 12.87s/it]\u001b[A\n","98it [06:09,  9.08s/it]\u001b[A\n","99it [06:10,  6.42s/it]\u001b[A\n","100it [06:10,  4.57s/it]\u001b[A\n","101it [06:10,  3.27s/it]\u001b[A\n","102it [06:10,  2.36s/it]\u001b[A\n","103it [06:11,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:12,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.67s/it]\n","109it [06:53, 12.86s/it]\u001b[A\n","110it [06:54,  9.07s/it]\u001b[A\n","111it [06:54,  6.42s/it]\u001b[A\n","112it [06:54,  4.56s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:55,  2.35s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:56,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 08:05:49 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:06:03 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.62s/it]\u001b[A\n","122it [07:10,  3.30s/it]\u001b[A\n","123it [07:11,  2.38s/it]\u001b[A\n","124it [07:11,  1.74s/it]\u001b[A\n","125it [07:11,  1.29s/it]\u001b[A\n","126it [07:11,  1.03it/s]\u001b[A\n","127it [07:12,  1.34it/s]\u001b[A\n","128it [07:12,  1.69it/s]\u001b[A\n","129it [07:12,  2.07it/s]\u001b[A\n","130it [07:12,  2.45it/s]\u001b[A\n","131it [07:13,  2.81it/s]\u001b[A\n","132it [07:13,  3.49it/s]\u001b[A03/08/2022 08:06:06 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:06:06 - INFO - __main__ -     eval_loss = 3.329766035079956\n","03/08/2022 08:06:06 - INFO - __main__ -     eval_auroc = 0.921750009059906\n","03/08/2022 08:06:06 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 08:06:06 - INFO - __main__ -     eval_f1 = 0.5154638886451721\n","03/08/2022 08:06:06 - INFO - filelock -   Lock 140065146110352 acquired on log.lock\n","03/08/2022 08:06:06 - INFO - filelock -   Lock 140065146110352 released on log.lock\n","132it [07:13,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:06:12 - INFO - __main__ -   Specify load the 94-th prompt: *cls*▁Yes!*mask*!*+sent_0**sep+* | {0: \"Actually\", 1: \"Perfect\"}\n","03/08/2022 08:06:12 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:06:12 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-31989', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-06-12_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-31989', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:06:12 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:06:13 - INFO - src.dataset -   Label 0 to word ĠActually (18894)\n","03/08/2022 08:06:13 - INFO - src.dataset -   Label 1 to word ĠPerfect (17586)\n","03/08/2022 08:06:13 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:06:13 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:06:13 - INFO - filelock -   Lock 139854804813904 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:06:13 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 08:06:13 - INFO - filelock -   Lock 139854804813904 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:06:13 - INFO - src.dataset -   Label 0 to word ĠActually (18894)\n","03/08/2022 08:06:13 - INFO - src.dataset -   Label 1 to word ĠPerfect (17586)\n","03/08/2022 08:06:13 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:06:13 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:06:13 - INFO - filelock -   Lock 139854775655376 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:06:13 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:06:13 - INFO - filelock -   Lock 139854775655376 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:06:13 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:06:13 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:06:13 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 08:06:13 - INFO - src.dataset -   text: <s>▁Yes!<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:06:28 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:06:28 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:06:28 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:06:28 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:06:28 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:06:28 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:06:28 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.63it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.63it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.71it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.20it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.90it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.70it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.57it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.49it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.43it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.39it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.07it/s]\u001b[A03/08/2022 08:07:13 - INFO - src.trainer -   Best dev result: 0.9322500228881836\n","Epoch:  20% 49/250 [01:32<05:36,  1.67s/it]\n","13it [00:52, 15.09s/it]            \u001b[A\n","14it [00:52, 10.63s/it]\u001b[A\n","15it [00:52,  7.51s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/08/2022 08:08:05 - INFO - src.trainer -   Best dev result: 0.9361250400543213\n","Epoch:  30% 74/250 [02:24<04:54,  1.67s/it]\n","25it [01:43, 15.08s/it]\u001b[A\n","26it [01:44, 10.63s/it]\u001b[A\n","27it [01:44,  7.51s/it]\u001b[A\n","28it [01:44,  5.33s/it]\u001b[A\n","29it [01:44,  3.80s/it]\u001b[A\n","30it [01:45,  2.73s/it]\u001b[A\n","31it [01:45,  1.98s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:45,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:11,  1.67s/it]\n","37it [02:28, 12.91s/it]\u001b[A\n","38it [02:28,  9.10s/it]\u001b[A\n","39it [02:28,  6.44s/it]\u001b[A\n","40it [02:28,  4.58s/it]\u001b[A\n","41it [02:29,  3.28s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.28s/it]\u001b[A\n","45it [02:30,  1.04it/s]\u001b[A\n","46it [02:30,  1.34it/s]\u001b[A\n","47it [02:30,  1.69it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:29,  1.67s/it]\n","49it [03:12, 12.89s/it]\u001b[A\n","50it [03:12,  9.09s/it]\u001b[A\n","51it [03:12,  6.43s/it]\u001b[A\n","52it [03:13,  4.57s/it]\u001b[A\n","53it [03:13,  3.27s/it]\u001b[A\n","54it [03:13,  2.36s/it]\u001b[A\n","55it [03:13,  1.72s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:37<02:47,  1.66s/it]\n","61it [03:56, 12.86s/it]\u001b[A\n","62it [03:57,  9.07s/it]\u001b[A\n","63it [03:57,  6.42s/it]\u001b[A\n","64it [03:57,  4.56s/it]\u001b[A\n","65it [03:57,  3.26s/it]\u001b[A\n","66it [03:57,  2.35s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:59,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.67s/it]\n","73it [04:41, 12.87s/it]\u001b[A\n","74it [04:41,  9.08s/it]\u001b[A\n","75it [04:41,  6.42s/it]\u001b[A\n","76it [04:41,  4.57s/it]\u001b[A\n","77it [04:42,  3.27s/it]\u001b[A\n","78it [04:42,  2.36s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:25,  1.67s/it]\n","85it [05:25, 12.87s/it]\u001b[A\n","86it [05:25,  9.08s/it]\u001b[A\n","87it [05:25,  6.43s/it]\u001b[A\n","88it [05:26,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:27,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:50<00:43,  1.66s/it]\n","97it [06:09, 12.87s/it]\u001b[A\n","98it [06:09,  9.08s/it]\u001b[A\n","99it [06:10,  6.42s/it]\u001b[A\n","100it [06:10,  4.57s/it]\u001b[A\n","101it [06:10,  3.27s/it]\u001b[A\n","102it [06:10,  2.36s/it]\u001b[A\n","103it [06:11,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:12,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.67s/it]\n","109it [06:54, 12.87s/it]\u001b[A\n","110it [06:54,  9.08s/it]\u001b[A\n","111it [06:54,  6.43s/it]\u001b[A\n","112it [06:54,  4.57s/it]\u001b[A\n","113it [06:54,  3.27s/it]\u001b[A\n","114it [06:55,  2.36s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:56,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 08:14:07 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:14:21 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.51s/it]\u001b[A\n","122it [07:10,  3.23s/it]\u001b[A\n","123it [07:10,  2.33s/it]\u001b[A\n","124it [07:11,  1.70s/it]\u001b[A\n","125it [07:11,  1.26s/it]\u001b[A\n","126it [07:11,  1.05it/s]\u001b[A\n","127it [07:11,  1.36it/s]\u001b[A\n","128it [07:12,  1.71it/s]\u001b[A\n","129it [07:12,  2.09it/s]\u001b[A\n","130it [07:12,  2.47it/s]\u001b[A\n","131it [07:12,  2.83it/s]\u001b[A\n","132it [07:12,  3.51it/s]\u001b[A03/08/2022 08:14:24 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:14:24 - INFO - __main__ -     eval_loss = 2.023900270462036\n","03/08/2022 08:14:24 - INFO - __main__ -     eval_auroc = 0.9361250400543213\n","03/08/2022 08:14:24 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/08/2022 08:14:24 - INFO - __main__ -     eval_f1 = 0.6388888955116272\n","03/08/2022 08:14:24 - INFO - filelock -   Lock 139854804814032 acquired on log.lock\n","03/08/2022 08:14:24 - INFO - filelock -   Lock 139854804814032 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:14:29 - INFO - __main__ -   Specify load the 95-th prompt: *cls**sent_0*,*mask*?*sep+* | {0: \"Bro\", 1: \"true\"}\n","03/08/2022 08:14:29 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:14:29 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-24490', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-14-29_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-24490', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:14:29 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:14:31 - INFO - src.dataset -   Label 0 to word ĠBro (4994)\n","03/08/2022 08:14:31 - INFO - src.dataset -   Label 1 to word Ġtrue (1528)\n","03/08/2022 08:14:31 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:14:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:14:31 - INFO - filelock -   Lock 139881790509136 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:14:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 08:14:31 - INFO - filelock -   Lock 139881790509136 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:14:31 - INFO - src.dataset -   Label 0 to word ĠBro (4994)\n","03/08/2022 08:14:31 - INFO - src.dataset -   Label 1 to word Ġtrue (1528)\n","03/08/2022 08:14:31 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:14:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:14:31 - INFO - filelock -   Lock 139881777824016 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:14:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:14:31 - INFO - filelock -   Lock 139881777824016 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:14:31 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:14:31 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:14:31 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/08/2022 08:14:31 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:14:46 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:14:46 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:14:46 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:14:46 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:14:46 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:14:46 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:14:46 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:17,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 08:15:31 - INFO - src.trainer -   Best dev result: 0.8817499876022339\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.07s/it]            \u001b[A\n","14it [00:52, 10.62s/it]\u001b[A\n","15it [00:52,  7.50s/it]\u001b[A\n","16it [00:52,  5.32s/it]\u001b[A\n","17it [00:53,  3.79s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 08:16:23 - INFO - src.trainer -   Best dev result: 0.9022500514984131\n","Epoch:  30% 74/250 [02:23<04:53,  1.67s/it]\n","25it [01:43, 15.01s/it]\u001b[A\n","26it [01:43, 10.58s/it]\u001b[A\n","27it [01:44,  7.48s/it]\u001b[A\n","28it [01:44,  5.30s/it]\u001b[A\n","29it [01:44,  3.78s/it]\u001b[A\n","30it [01:44,  2.72s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:10,  1.66s/it]\n","37it [02:27, 12.87s/it]\u001b[A\n","38it [02:27,  9.08s/it]\u001b[A\n","39it [02:28,  6.42s/it]\u001b[A\n","40it [02:28,  4.57s/it]\u001b[A\n","41it [02:28,  3.27s/it]\u001b[A\n","42it [02:28,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.27s/it]\u001b[A\n","45it [02:29,  1.04it/s]\u001b[A\n","46it [02:29,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:28,  1.66s/it]\n","49it [03:11, 12.83s/it]\u001b[A\n","50it [03:12,  9.05s/it]\u001b[A\n","51it [03:12,  6.41s/it]\u001b[A\n","52it [03:12,  4.55s/it]\u001b[A\n","53it [03:12,  3.26s/it]\u001b[A\n","54it [03:13,  2.35s/it]\u001b[A\n","55it [03:13,  1.71s/it]\u001b[A\n","56it [03:13,  1.27s/it]\u001b[A\n","57it [03:13,  1.04it/s]\u001b[A\n","58it [03:13,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:36<02:47,  1.66s/it]\n","61it [03:56, 12.82s/it]\u001b[A\n","62it [03:56,  9.05s/it]\u001b[A\n","63it [03:56,  6.40s/it]\u001b[A\n","64it [03:56,  4.55s/it]\u001b[A\n","65it [03:56,  3.25s/it]\u001b[A\n","66it [03:57,  2.35s/it]\u001b[A\n","67it [03:57,  1.71s/it]\u001b[A\n","68it [03:57,  1.27s/it]\u001b[A\n","69it [03:57,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:58,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:20<02:06,  1.66s/it]\n","73it [04:40, 12.83s/it]\u001b[A\n","74it [04:40,  9.05s/it]\u001b[A\n","75it [04:40,  6.41s/it]\u001b[A\n","76it [04:40,  4.55s/it]\u001b[A\n","77it [04:41,  3.26s/it]\u001b[A\n","78it [04:41,  2.35s/it]\u001b[A\n","79it [04:41,  1.71s/it]\u001b[A\n","80it [04:41,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:42,  1.35it/s]\u001b[A\n","83it [04:42,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:04<01:24,  1.66s/it]\n","85it [05:24, 12.84s/it]\u001b[A\n","86it [05:24,  9.06s/it]\u001b[A\n","87it [05:24,  6.41s/it]\u001b[A\n","88it [05:25,  4.56s/it]\u001b[A\n","89it [05:25,  3.26s/it]\u001b[A\n","90it [05:25,  2.35s/it]\u001b[A\n","91it [05:25,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:26,  1.04it/s]\u001b[A\n","94it [05:26,  1.35it/s]\u001b[A\n","95it [05:26,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:48<00:43,  1.66s/it]\n","97it [06:08, 12.85s/it]\u001b[A\n","98it [06:08,  9.06s/it]\u001b[A\n","99it [06:09,  6.41s/it]\u001b[A\n","100it [06:09,  4.56s/it]\u001b[A\n","101it [06:09,  3.26s/it]\u001b[A\n","102it [06:09,  2.35s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:10,  1.27s/it]\u001b[A\n","105it [06:10,  1.04it/s]\u001b[A\n","106it [06:10,  1.35it/s]\u001b[A\n","107it [06:10,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:33<00:01,  1.66s/it]\n","109it [06:52, 12.85s/it]\u001b[A\n","110it [06:53,  9.06s/it]\u001b[A\n","111it [06:53,  6.41s/it]\u001b[A\n","112it [06:53,  4.56s/it]\u001b[A\n","113it [06:53,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:54,  1.72s/it]\u001b[A\n","116it [06:54,  1.27s/it]\u001b[A\n","117it [06:54,  1.04it/s]\u001b[A\n","118it [06:54,  1.35it/s]\u001b[A\n","119it [06:55,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:37<00:00,  1.83s/it]\n","03/08/2022 08:22:23 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:22:37 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:09,  4.51s/it]\u001b[A\n","122it [07:09,  3.23s/it]\u001b[A\n","123it [07:09,  2.33s/it]\u001b[A\n","124it [07:09,  1.70s/it]\u001b[A\n","125it [07:10,  1.26s/it]\u001b[A\n","126it [07:10,  1.05it/s]\u001b[A\n","127it [07:10,  1.36it/s]\u001b[A\n","128it [07:10,  1.71it/s]\u001b[A\n","129it [07:11,  2.09it/s]\u001b[A\n","130it [07:11,  2.47it/s]\u001b[A\n","131it [07:11,  2.84it/s]\u001b[A\n","132it [07:11,  3.52it/s]\u001b[A03/08/2022 08:22:40 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:22:40 - INFO - __main__ -     eval_loss = 2.094346046447754\n","03/08/2022 08:22:40 - INFO - __main__ -     eval_auroc = 0.9022500514984131\n","03/08/2022 08:22:40 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/08/2022 08:22:40 - INFO - __main__ -     eval_f1 = 0.5111111402511597\n","03/08/2022 08:22:40 - INFO - filelock -   Lock 139881790603856 acquired on log.lock\n","03/08/2022 08:22:40 - INFO - filelock -   Lock 139881790603856 released on log.lock\n","132it [07:11,  3.27s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:22:46 - INFO - __main__ -   Specify load the 96-th prompt: *cls**sent_0*,*mask*?*sep+* | {0: \"Dad\", 1: \"still\"}\n","03/08/2022 08:22:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:22:46 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-26474', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-22-46_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-26474', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:22:46 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:22:47 - INFO - src.dataset -   Label 0 to word ĠDad (13404)\n","03/08/2022 08:22:47 - INFO - src.dataset -   Label 1 to word Ġstill (202)\n","03/08/2022 08:22:47 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:22:47 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:22:47 - INFO - filelock -   Lock 140100595468688 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:22:47 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 08:22:47 - INFO - filelock -   Lock 140100595468688 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:22:47 - INFO - src.dataset -   Label 0 to word ĠDad (13404)\n","03/08/2022 08:22:47 - INFO - src.dataset -   Label 1 to word Ġstill (202)\n","03/08/2022 08:22:47 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:22:47 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:22:47 - INFO - filelock -   Lock 140100567658448 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:22:47 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:22:47 - INFO - filelock -   Lock 140100567658448 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:22:47 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:22:47 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:22:47 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/08/2022 08:22:47 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:23:03 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:23:03 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:23:03 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:23:03 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:23:03 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:23:03 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:23:03 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 08:23:47 - INFO - src.trainer -   Best dev result: 0.8742499351501465\n","Epoch:  20% 49/250 [01:32<05:40,  1.70s/it]\n","13it [00:52, 15.10s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.90s/it]\u001b[A\n","26it [01:36,  9.10s/it]\u001b[A\n","27it [01:37,  6.44s/it]\u001b[A\n","28it [01:37,  4.58s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","36it [01:39,  2.22it/s]\u001b[A03/08/2022 08:25:24 - INFO - src.trainer -   Best dev result: 0.9222500324249268\n","Epoch:  40% 99/250 [03:08<04:11,  1.67s/it]\n","37it [02:28, 15.05s/it]\u001b[A\n","38it [02:28, 10.60s/it]\u001b[A\n","39it [02:28,  7.49s/it]\u001b[A\n","40it [02:28,  5.31s/it]\u001b[A\n","41it [02:29,  3.79s/it]\u001b[A\n","42it [02:29,  2.72s/it]\u001b[A\n","43it [02:29,  1.97s/it]\u001b[A\n","44it [02:29,  1.45s/it]\u001b[A\n","45it [02:29,  1.09s/it]\u001b[A\n","46it [02:30,  1.21it/s]\u001b[A\n","47it [02:30,  1.54it/s]\u001b[A\n","48it [02:30,  2.03it/s]\u001b[A03/08/2022 08:26:15 - INFO - src.trainer -   Best dev result: 0.9269999861717224\n","Epoch:  50% 124/250 [04:00<03:34,  1.70s/it]\n","49it [03:20, 15.29s/it]\u001b[A\n","50it [03:20, 10.77s/it]\u001b[A\n","51it [03:20,  7.61s/it]\u001b[A\n","52it [03:21,  5.40s/it]\u001b[A\n","53it [03:21,  3.85s/it]\u001b[A\n","54it [03:21,  2.76s/it]\u001b[A\n","55it [03:21,  2.00s/it]\u001b[A\n","56it [03:22,  1.47s/it]\u001b[A\n","57it [03:22,  1.10s/it]\u001b[A\n","58it [03:22,  1.19it/s]\u001b[A\n","59it [03:22,  1.52it/s]\u001b[A\n","Epoch:  60% 149/250 [04:44<02:47,  1.66s/it]\n","61it [04:04, 12.88s/it]\u001b[A\n","62it [04:04,  9.09s/it]\u001b[A\n","63it [04:05,  6.43s/it]\u001b[A\n","64it [04:05,  4.57s/it]\u001b[A\n","65it [04:05,  3.27s/it]\u001b[A\n","66it [04:05,  2.36s/it]\u001b[A\n","67it [04:06,  1.72s/it]\u001b[A\n","68it [04:06,  1.27s/it]\u001b[A\n","69it [04:06,  1.04it/s]\u001b[A\n","70it [04:06,  1.35it/s]\u001b[A\n","71it [04:06,  1.70it/s]\u001b[A\n","72it [04:07,  2.22it/s]\u001b[A03/08/2022 08:27:52 - INFO - src.trainer -   Best dev result: 0.9284999966621399\n","Epoch:  70% 174/250 [05:36<02:07,  1.67s/it]\n","73it [04:56, 15.16s/it]\u001b[A\n","74it [04:56, 10.68s/it]\u001b[A\n","75it [04:57,  7.55s/it]\u001b[A\n","76it [04:57,  5.35s/it]\u001b[A\n","77it [04:57,  3.82s/it]\u001b[A\n","78it [04:57,  2.74s/it]\u001b[A\n","79it [04:57,  1.99s/it]\u001b[A\n","80it [04:58,  1.46s/it]\u001b[A\n","81it [04:58,  1.09s/it]\u001b[A\n","82it [04:58,  1.20it/s]\u001b[A\n","83it [04:58,  1.53it/s]\u001b[A\n","Epoch:  80% 199/250 [06:21<01:24,  1.66s/it]\n","85it [05:40, 12.88s/it]\u001b[A\n","86it [05:40,  9.08s/it]\u001b[A\n","87it [05:41,  6.43s/it]\u001b[A\n","88it [05:41,  4.57s/it]\u001b[A\n","89it [05:41,  3.27s/it]\u001b[A\n","90it [05:41,  2.36s/it]\u001b[A\n","91it [05:42,  1.72s/it]\u001b[A\n","92it [05:42,  1.27s/it]\u001b[A\n","93it [05:42,  1.04it/s]\u001b[A\n","94it [05:42,  1.35it/s]\u001b[A\n","95it [05:43,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:05<00:43,  1.66s/it]\n","97it [06:24, 12.85s/it]\u001b[A\n","98it [06:25,  9.06s/it]\u001b[A\n","99it [06:25,  6.41s/it]\u001b[A\n","100it [06:25,  4.56s/it]\u001b[A\n","101it [06:25,  3.26s/it]\u001b[A\n","102it [06:26,  2.35s/it]\u001b[A\n","103it [06:26,  1.72s/it]\u001b[A\n","104it [06:26,  1.27s/it]\u001b[A\n","105it [06:26,  1.04it/s]\u001b[A\n","106it [06:27,  1.35it/s]\u001b[A\n","107it [06:27,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:49<00:01,  1.66s/it]\n","109it [07:09, 12.85s/it]\u001b[A\n","110it [07:09,  9.06s/it]\u001b[A\n","111it [07:09,  6.41s/it]\u001b[A\n","112it [07:09,  4.56s/it]\u001b[A\n","113it [07:10,  3.26s/it]\u001b[A\n","114it [07:10,  2.35s/it]\u001b[A\n","115it [07:10,  1.72s/it]\u001b[A\n","116it [07:10,  1.27s/it]\u001b[A\n","117it [07:11,  1.04it/s]\u001b[A\n","118it [07:11,  1.35it/s]\u001b[A\n","119it [07:11,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:53<00:00,  1.90s/it]\n","03/08/2022 08:30:57 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:31:11 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:25,  4.61s/it]\u001b[A\n","122it [07:26,  3.30s/it]\u001b[A\n","123it [07:26,  2.38s/it]\u001b[A\n","124it [07:26,  1.73s/it]\u001b[A\n","125it [07:26,  1.28s/it]\u001b[A\n","126it [07:27,  1.03it/s]\u001b[A\n","127it [07:27,  1.34it/s]\u001b[A\n","128it [07:27,  1.69it/s]\u001b[A\n","129it [07:27,  2.07it/s]\u001b[A\n","130it [07:28,  2.45it/s]\u001b[A\n","131it [07:28,  2.82it/s]\u001b[A\n","132it [07:28,  3.49it/s]\u001b[A03/08/2022 08:31:13 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:31:13 - INFO - __main__ -     eval_loss = 1.7660127878189087\n","03/08/2022 08:31:13 - INFO - __main__ -     eval_auroc = 0.9284999966621399\n","03/08/2022 08:31:13 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/08/2022 08:31:13 - INFO - __main__ -     eval_f1 = 0.5714285373687744\n","03/08/2022 08:31:13 - INFO - filelock -   Lock 140100567658448 acquired on log.lock\n","03/08/2022 08:31:13 - INFO - filelock -   Lock 140100567658448 released on log.lock\n","132it [07:28,  3.40s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:31:19 - INFO - __main__ -   Specify load the 97-th prompt: *cls**sent_0*,*mask*?*sep+* | {0: \"it\", 1: \"true\"}\n","03/08/2022 08:31:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:31:19 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-23171', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-31-19_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-23171', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:31:19 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:31:21 - INFO - src.dataset -   Label 0 to word Ġit (24)\n","03/08/2022 08:31:21 - INFO - src.dataset -   Label 1 to word Ġtrue (1528)\n","03/08/2022 08:31:21 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:31:21 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:31:21 - INFO - filelock -   Lock 139966460970448 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:31:21 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 08:31:21 - INFO - filelock -   Lock 139966460970448 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:31:21 - INFO - src.dataset -   Label 0 to word Ġit (24)\n","03/08/2022 08:31:21 - INFO - src.dataset -   Label 1 to word Ġtrue (1528)\n","03/08/2022 08:31:21 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:31:21 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:31:21 - INFO - filelock -   Lock 139966460971216 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:31:21 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:31:21 - INFO - filelock -   Lock 139966460971216 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:31:21 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:31:21 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:31:21 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/08/2022 08:31:21 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:31:36 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:31:36 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:31:36 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:31:36 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:31:36 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:31:36 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:31:36 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 08:32:20 - INFO - src.trainer -   Best dev result: 0.8577499985694885\n","Epoch:  20% 49/250 [01:31<05:34,  1.67s/it]\n","13it [00:51, 14.90s/it]            \u001b[A\n","14it [00:51, 10.50s/it]\u001b[A\n","15it [00:52,  7.42s/it]\u001b[A\n","16it [00:52,  5.26s/it]\u001b[A\n","17it [00:52,  3.75s/it]\u001b[A\n","18it [00:52,  2.70s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.22it/s]\u001b[A\n","23it [00:53,  1.55it/s]\u001b[A\n","Epoch:  30% 74/250 [02:15<04:52,  1.66s/it]\n","25it [01:35, 12.87s/it]\u001b[A\n","26it [01:36,  9.08s/it]\u001b[A\n","27it [01:36,  6.43s/it]\u001b[A\n","28it [01:36,  4.57s/it]\u001b[A\n","29it [01:36,  3.27s/it]\u001b[A\n","30it [01:36,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.04it/s]\u001b[A\n","34it [01:37,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","36it [01:38,  2.22it/s]\u001b[A03/08/2022 08:33:56 - INFO - src.trainer -   Best dev result: 0.8730000257492065\n","Epoch:  40% 99/250 [03:08<04:11,  1.67s/it]\n","37it [02:27, 15.22s/it]\u001b[A\n","38it [02:28, 10.73s/it]\u001b[A\n","39it [02:28,  7.58s/it]\u001b[A\n","40it [02:28,  5.37s/it]\u001b[A\n","41it [02:28,  3.83s/it]\u001b[A\n","42it [02:29,  2.75s/it]\u001b[A\n","43it [02:29,  2.00s/it]\u001b[A\n","44it [02:29,  1.47s/it]\u001b[A\n","45it [02:29,  1.10s/it]\u001b[A\n","46it [02:30,  1.20it/s]\u001b[A\n","47it [02:30,  1.53it/s]\u001b[A\n","48it [02:30,  2.02it/s]\u001b[A03/08/2022 08:34:48 - INFO - src.trainer -   Best dev result: 0.8731250166893005\n","Epoch:  50% 124/250 [03:59<03:32,  1.69s/it]\n","49it [03:19, 15.13s/it]\u001b[A\n","50it [03:19, 10.66s/it]\u001b[A\n","51it [03:20,  7.53s/it]\u001b[A\n","52it [03:20,  5.34s/it]\u001b[A\n","53it [03:20,  3.81s/it]\u001b[A\n","54it [03:20,  2.73s/it]\u001b[A\n","55it [03:21,  1.98s/it]\u001b[A\n","56it [03:21,  1.46s/it]\u001b[A\n","57it [03:21,  1.09s/it]\u001b[A\n","58it [03:21,  1.20it/s]\u001b[A\n","59it [03:21,  1.53it/s]\u001b[A\n","60it [03:22,  2.02it/s]\u001b[A03/08/2022 08:35:40 - INFO - src.trainer -   Best dev result: 0.8762500882148743\n","Epoch:  60% 149/250 [04:51<02:48,  1.67s/it]\n","61it [04:11, 15.25s/it]\u001b[A\n","62it [04:12, 10.74s/it]\u001b[A\n","63it [04:12,  7.59s/it]\u001b[A\n","64it [04:12,  5.38s/it]\u001b[A\n","65it [04:12,  3.84s/it]\u001b[A\n","66it [04:12,  2.75s/it]\u001b[A\n","67it [04:13,  2.00s/it]\u001b[A\n","68it [04:13,  1.47s/it]\u001b[A\n","69it [04:13,  1.10s/it]\u001b[A\n","70it [04:13,  1.19it/s]\u001b[A\n","71it [04:14,  1.53it/s]\u001b[A\n","Epoch:  70% 174/250 [05:36<02:06,  1.66s/it]\n","73it [04:55, 12.88s/it]\u001b[A\n","74it [04:56,  9.09s/it]\u001b[A\n","75it [04:56,  6.43s/it]\u001b[A\n","76it [04:56,  4.57s/it]\u001b[A\n","77it [04:56,  3.27s/it]\u001b[A\n","78it [04:57,  2.36s/it]\u001b[A\n","79it [04:57,  1.72s/it]\u001b[A\n","80it [04:57,  1.27s/it]\u001b[A\n","81it [04:57,  1.04it/s]\u001b[A\n","82it [04:58,  1.35it/s]\u001b[A\n","83it [04:58,  1.70it/s]\u001b[A\n","84it [04:58,  2.22it/s]\u001b[A03/08/2022 08:37:16 - INFO - src.trainer -   Best dev result: 0.8763750195503235\n","Epoch:  80% 199/250 [06:28<01:25,  1.67s/it]\n","85it [05:48, 15.19s/it]\u001b[A\n","86it [05:48, 10.70s/it]\u001b[A\n","87it [05:48,  7.56s/it]\u001b[A\n","88it [05:48,  5.36s/it]\u001b[A\n","89it [05:48,  3.82s/it]\u001b[A\n","90it [05:49,  2.75s/it]\u001b[A\n","91it [05:49,  1.99s/it]\u001b[A\n","92it [05:49,  1.46s/it]\u001b[A\n","93it [05:49,  1.09s/it]\u001b[A\n","94it [05:50,  1.20it/s]\u001b[A\n","95it [05:50,  1.53it/s]\u001b[A\n","Epoch:  90% 224/250 [07:12<00:43,  1.66s/it]\n","97it [06:32, 12.87s/it]\u001b[A\n","98it [06:32,  9.08s/it]\u001b[A\n","99it [06:32,  6.42s/it]\u001b[A\n","100it [06:32,  4.57s/it]\u001b[A\n","101it [06:33,  3.27s/it]\u001b[A\n","102it [06:33,  2.36s/it]\u001b[A\n","103it [06:33,  1.72s/it]\u001b[A\n","104it [06:33,  1.27s/it]\u001b[A\n","105it [06:34,  1.04it/s]\u001b[A\n","106it [06:34,  1.35it/s]\u001b[A\n","107it [06:34,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:56<00:01,  1.66s/it]\n","109it [07:16, 12.83s/it]\u001b[A\n","110it [07:16,  9.05s/it]\u001b[A\n","111it [07:16,  6.41s/it]\u001b[A\n","112it [07:17,  4.55s/it]\u001b[A\n","113it [07:17,  3.26s/it]\u001b[A\n","114it [07:17,  2.35s/it]\u001b[A\n","115it [07:17,  1.71s/it]\u001b[A\n","116it [07:17,  1.27s/it]\u001b[A\n","117it [07:18,  1.04it/s]\u001b[A\n","118it [07:18,  1.35it/s]\u001b[A\n","119it [07:18,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [08:00<00:00,  1.92s/it]\n","03/08/2022 08:39:37 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:39:51 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:33,  4.64s/it]\u001b[A\n","122it [07:33,  3.32s/it]\u001b[A\n","123it [07:33,  2.39s/it]\u001b[A\n","124it [07:33,  1.74s/it]\u001b[A\n","125it [07:34,  1.29s/it]\u001b[A\n","126it [07:34,  1.03it/s]\u001b[A\n","127it [07:34,  1.33it/s]\u001b[A\n","128it [07:34,  1.68it/s]\u001b[A\n","129it [07:35,  2.06it/s]\u001b[A\n","130it [07:35,  2.44it/s]\u001b[A\n","131it [07:35,  2.81it/s]\u001b[A\n","132it [07:35,  3.49it/s]\u001b[A03/08/2022 08:39:54 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:39:54 - INFO - __main__ -     eval_loss = 4.714199066162109\n","03/08/2022 08:39:54 - INFO - __main__ -     eval_auroc = 0.8763750195503235\n","03/08/2022 08:39:54 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 08:39:54 - INFO - __main__ -     eval_f1 = 0.4363636374473572\n","03/08/2022 08:39:54 - INFO - filelock -   Lock 139966460971216 acquired on log.lock\n","03/08/2022 08:39:54 - INFO - filelock -   Lock 139966460971216 released on log.lock\n","132it [07:35,  3.45s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:40:00 - INFO - __main__ -   Specify load the 98-th prompt: *cls**sent_0*,*mask*?*sep+* | {0: \"Dad\", 1: \"actually\"}\n","03/08/2022 08:40:00 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:40:00 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-29777', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-40-00_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-29777', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:40:00 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:40:01 - INFO - src.dataset -   Label 0 to word ĠDad (13404)\n","03/08/2022 08:40:01 - INFO - src.dataset -   Label 1 to word Ġactually (888)\n","03/08/2022 08:40:01 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:40:01 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:40:01 - INFO - filelock -   Lock 140056565926800 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:40:01 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 08:40:01 - INFO - filelock -   Lock 140056565926800 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:40:01 - INFO - src.dataset -   Label 0 to word ĠDad (13404)\n","03/08/2022 08:40:01 - INFO - src.dataset -   Label 1 to word Ġactually (888)\n","03/08/2022 08:40:01 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:40:01 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:40:01 - INFO - filelock -   Lock 140056565926800 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:40:01 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:40:01 - INFO - filelock -   Lock 140056565926800 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:40:01 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:40:01 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:40:01 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/08/2022 08:40:01 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:40:16 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:40:16 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:40:16 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:40:16 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:40:16 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:40:16 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:40:16 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 08:41:01 - INFO - src.trainer -   Best dev result: 0.8803749680519104\n","Epoch:  20% 49/250 [01:32<05:34,  1.67s/it]\n","13it [00:52, 15.06s/it]            \u001b[A\n","14it [00:52, 10.61s/it]\u001b[A\n","15it [00:52,  7.50s/it]\u001b[A\n","16it [00:52,  5.32s/it]\u001b[A\n","17it [00:53,  3.79s/it]\u001b[A\n","18it [00:53,  2.72s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 08:41:53 - INFO - src.trainer -   Best dev result: 0.8812499642372131\n","Epoch:  30% 74/250 [02:23<04:53,  1.67s/it]\n","25it [01:43, 15.01s/it]\u001b[A\n","26it [01:43, 10.57s/it]\u001b[A\n","27it [01:43,  7.47s/it]\u001b[A\n","28it [01:44,  5.30s/it]\u001b[A\n","29it [01:44,  3.78s/it]\u001b[A\n","30it [01:44,  2.71s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:10,  1.66s/it]\n","37it [02:27, 12.88s/it]\u001b[A\n","38it [02:27,  9.09s/it]\u001b[A\n","39it [02:28,  6.43s/it]\u001b[A\n","40it [02:28,  4.57s/it]\u001b[A\n","41it [02:28,  3.27s/it]\u001b[A\n","42it [02:28,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.27s/it]\u001b[A\n","45it [02:29,  1.04it/s]\u001b[A\n","46it [02:29,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","48it [02:30,  2.22it/s]\u001b[A03/08/2022 08:43:29 - INFO - src.trainer -   Best dev result: 0.8830000162124634\n","Epoch:  50% 124/250 [04:00<03:30,  1.67s/it]\n","49it [03:19, 15.23s/it]\u001b[A\n","50it [03:20, 10.73s/it]\u001b[A\n","51it [03:20,  7.58s/it]\u001b[A\n","52it [03:20,  5.38s/it]\u001b[A\n","53it [03:20,  3.83s/it]\u001b[A\n","54it [03:21,  2.75s/it]\u001b[A\n","55it [03:21,  2.00s/it]\u001b[A\n","56it [03:21,  1.47s/it]\u001b[A\n","57it [03:21,  1.10s/it]\u001b[A\n","58it [03:21,  1.20it/s]\u001b[A\n","59it [03:22,  1.53it/s]\u001b[A\n","Epoch:  60% 149/250 [04:44<02:47,  1.66s/it]\n","61it [04:04, 12.87s/it]\u001b[A\n","62it [04:04,  9.08s/it]\u001b[A\n","63it [04:04,  6.43s/it]\u001b[A\n","64it [04:04,  4.57s/it]\u001b[A\n","65it [04:05,  3.27s/it]\u001b[A\n","66it [04:05,  2.36s/it]\u001b[A\n","67it [04:05,  1.72s/it]\u001b[A\n","68it [04:05,  1.27s/it]\u001b[A\n","69it [04:05,  1.04it/s]\u001b[A\n","70it [04:06,  1.35it/s]\u001b[A\n","71it [04:06,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:28<02:06,  1.66s/it]\n","73it [04:48, 12.84s/it]\u001b[A\n","74it [04:48,  9.06s/it]\u001b[A\n","75it [04:48,  6.41s/it]\u001b[A\n","76it [04:48,  4.56s/it]\u001b[A\n","77it [04:49,  3.26s/it]\u001b[A\n","78it [04:49,  2.35s/it]\u001b[A\n","79it [04:49,  1.71s/it]\u001b[A\n","80it [04:49,  1.27s/it]\u001b[A\n","81it [04:50,  1.04it/s]\u001b[A\n","82it [04:50,  1.35it/s]\u001b[A\n","83it [04:50,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:12<01:24,  1.66s/it]\n","85it [05:32, 12.84s/it]\u001b[A\n","86it [05:32,  9.06s/it]\u001b[A\n","87it [05:32,  6.41s/it]\u001b[A\n","88it [05:33,  4.56s/it]\u001b[A\n","89it [05:33,  3.26s/it]\u001b[A\n","90it [05:33,  2.35s/it]\u001b[A\n","91it [05:33,  1.72s/it]\u001b[A\n","92it [05:34,  1.27s/it]\u001b[A\n","93it [05:34,  1.04it/s]\u001b[A\n","94it [05:34,  1.35it/s]\u001b[A\n","95it [05:34,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:57<00:43,  1.66s/it]\n","97it [06:16, 12.85s/it]\u001b[A\n","98it [06:16,  9.07s/it]\u001b[A\n","99it [06:17,  6.42s/it]\u001b[A\n","100it [06:17,  4.56s/it]\u001b[A\n","101it [06:17,  3.26s/it]\u001b[A\n","102it [06:17,  2.35s/it]\u001b[A\n","103it [06:18,  1.72s/it]\u001b[A\n","104it [06:18,  1.27s/it]\u001b[A\n","105it [06:18,  1.04it/s]\u001b[A\n","106it [06:18,  1.35it/s]\u001b[A\n","107it [06:19,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:41<00:01,  1.66s/it]\n","109it [07:00, 12.84s/it]\u001b[A\n","110it [07:01,  9.06s/it]\u001b[A\n","111it [07:01,  6.41s/it]\u001b[A\n","112it [07:01,  4.56s/it]\u001b[A\n","113it [07:01,  3.26s/it]\u001b[A\n","114it [07:02,  2.35s/it]\u001b[A\n","115it [07:02,  1.71s/it]\u001b[A\n","116it [07:02,  1.27s/it]\u001b[A\n","117it [07:02,  1.04it/s]\u001b[A\n","118it [07:02,  1.35it/s]\u001b[A\n","119it [07:03,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:45<00:00,  1.86s/it]\n","03/08/2022 08:48:02 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:48:16 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:17,  4.67s/it]\u001b[A\n","122it [07:18,  3.34s/it]\u001b[A\n","123it [07:18,  2.41s/it]\u001b[A\n","124it [07:18,  1.75s/it]\u001b[A\n","125it [07:18,  1.30s/it]\u001b[A\n","126it [07:19,  1.02it/s]\u001b[A\n","127it [07:19,  1.33it/s]\u001b[A\n","128it [07:19,  1.68it/s]\u001b[A\n","129it [07:19,  2.05it/s]\u001b[A\n","130it [07:19,  2.44it/s]\u001b[A\n","131it [07:20,  2.80it/s]\u001b[A\n","132it [07:20,  3.48it/s]\u001b[A03/08/2022 08:48:19 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:48:19 - INFO - __main__ -     eval_loss = 3.3439133167266846\n","03/08/2022 08:48:19 - INFO - __main__ -     eval_auroc = 0.8830000162124634\n","03/08/2022 08:48:19 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 08:48:19 - INFO - __main__ -     eval_f1 = 0.4528301954269409\n","03/08/2022 08:48:19 - INFO - filelock -   Lock 140056477513808 acquired on log.lock\n","03/08/2022 08:48:19 - INFO - filelock -   Lock 140056477513808 released on log.lock\n","132it [07:20,  3.34s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:48:25 - INFO - __main__ -   Specify load the 99-th prompt: *cls**sent_0*,*mask*?*sep+* | {0: \"people\", 1: \"ha\"}\n","03/08/2022 08:48:25 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:48:25 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-27119', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-48-25_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-27119', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:48:25 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:48:26 - INFO - src.dataset -   Label 0 to word Ġpeople (82)\n","03/08/2022 08:48:26 - INFO - src.dataset -   Label 1 to word Ġha (2489)\n","03/08/2022 08:48:26 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:48:26 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:48:26 - INFO - filelock -   Lock 139700923133392 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:48:26 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 08:48:26 - INFO - filelock -   Lock 139700923133392 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:48:26 - INFO - src.dataset -   Label 0 to word Ġpeople (82)\n","03/08/2022 08:48:26 - INFO - src.dataset -   Label 1 to word Ġha (2489)\n","03/08/2022 08:48:26 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:48:26 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:48:26 - INFO - filelock -   Lock 139700927142608 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:48:26 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:48:26 - INFO - filelock -   Lock 139700927142608 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:48:26 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:48:26 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:48:26 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/08/2022 08:48:26 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:48:42 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:48:42 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:48:42 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:48:42 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:48:42 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:48:42 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:48:42 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 08:49:26 - INFO - src.trainer -   Best dev result: 0.8546249866485596\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.02s/it]            \u001b[A\n","14it [00:52, 10.58s/it]\u001b[A\n","15it [00:52,  7.48s/it]\u001b[A\n","16it [00:52,  5.30s/it]\u001b[A\n","17it [00:52,  3.78s/it]\u001b[A\n","18it [00:53,  2.72s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 08:50:18 - INFO - src.trainer -   Best dev result: 0.8596249222755432\n","Epoch:  30% 74/250 [02:23<04:53,  1.67s/it]\n","25it [01:43, 15.00s/it]\u001b[A\n","26it [01:43, 10.57s/it]\u001b[A\n","27it [01:43,  7.47s/it]\u001b[A\n","28it [01:44,  5.30s/it]\u001b[A\n","29it [01:44,  3.78s/it]\u001b[A\n","30it [01:44,  2.71s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:44,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","36it [01:45,  2.03it/s]\u001b[A03/08/2022 08:51:09 - INFO - src.trainer -   Best dev result: 0.8643750548362732\n","Epoch:  40% 99/250 [03:14<04:11,  1.67s/it]\n","37it [02:34, 15.01s/it]\u001b[A\n","38it [02:34, 10.58s/it]\u001b[A\n","39it [02:35,  7.47s/it]\u001b[A\n","40it [02:35,  5.30s/it]\u001b[A\n","41it [02:35,  3.78s/it]\u001b[A\n","42it [02:35,  2.72s/it]\u001b[A\n","43it [02:36,  1.97s/it]\u001b[A\n","44it [02:36,  1.45s/it]\u001b[A\n","45it [02:36,  1.08s/it]\u001b[A\n","46it [02:36,  1.21it/s]\u001b[A\n","47it [02:36,  1.54it/s]\u001b[A\n","Epoch:  50% 124/250 [03:58<03:28,  1.66s/it]\n","49it [03:18, 12.85s/it]\u001b[A\n","50it [03:19,  9.06s/it]\u001b[A\n","51it [03:19,  6.41s/it]\u001b[A\n","52it [03:19,  4.56s/it]\u001b[A\n","53it [03:19,  3.26s/it]\u001b[A\n","54it [03:19,  2.35s/it]\u001b[A\n","55it [03:20,  1.72s/it]\u001b[A\n","56it [03:20,  1.27s/it]\u001b[A\n","57it [03:20,  1.04it/s]\u001b[A\n","58it [03:20,  1.35it/s]\u001b[A\n","59it [03:21,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:43<02:48,  1.66s/it]\n","61it [04:02, 12.83s/it]\u001b[A\n","62it [04:03,  9.05s/it]\u001b[A\n","63it [04:03,  6.40s/it]\u001b[A\n","64it [04:03,  4.55s/it]\u001b[A\n","65it [04:03,  3.26s/it]\u001b[A\n","66it [04:04,  2.35s/it]\u001b[A\n","67it [04:04,  1.71s/it]\u001b[A\n","68it [04:04,  1.27s/it]\u001b[A\n","69it [04:04,  1.04it/s]\u001b[A\n","70it [04:05,  1.35it/s]\u001b[A\n","71it [04:05,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:27<02:05,  1.65s/it]\n","73it [04:47, 12.80s/it]\u001b[A\n","74it [04:47,  9.03s/it]\u001b[A\n","75it [04:47,  6.39s/it]\u001b[A\n","76it [04:47,  4.54s/it]\u001b[A\n","77it [04:47,  3.25s/it]\u001b[A\n","78it [04:48,  2.34s/it]\u001b[A\n","79it [04:48,  1.71s/it]\u001b[A\n","80it [04:48,  1.27s/it]\u001b[A\n","81it [04:48,  1.05it/s]\u001b[A\n","82it [04:49,  1.35it/s]\u001b[A\n","83it [04:49,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:11<01:24,  1.66s/it]\n","85it [05:31, 12.79s/it]\u001b[A\n","86it [05:31,  9.02s/it]\u001b[A\n","87it [05:31,  6.39s/it]\u001b[A\n","88it [05:31,  4.54s/it]\u001b[A\n","89it [05:31,  3.25s/it]\u001b[A\n","90it [05:32,  2.34s/it]\u001b[A\n","91it [05:32,  1.71s/it]\u001b[A\n","92it [05:32,  1.27s/it]\u001b[A\n","93it [05:32,  1.05it/s]\u001b[A\n","94it [05:33,  1.36it/s]\u001b[A\n","95it [05:33,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [06:55<00:42,  1.65s/it]\n","97it [06:15, 12.78s/it]\u001b[A\n","98it [06:15,  9.02s/it]\u001b[A\n","99it [06:15,  6.38s/it]\u001b[A\n","100it [06:15,  4.54s/it]\u001b[A\n","101it [06:15,  3.24s/it]\u001b[A\n","102it [06:16,  2.34s/it]\u001b[A\n","103it [06:16,  1.71s/it]\u001b[A\n","104it [06:16,  1.26s/it]\u001b[A\n","105it [06:16,  1.05it/s]\u001b[A\n","106it [06:17,  1.36it/s]\u001b[A\n","107it [06:17,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:39<00:01,  1.66s/it]\n","109it [06:59, 12.83s/it]\u001b[A\n","110it [06:59,  9.05s/it]\u001b[A\n","111it [06:59,  6.40s/it]\u001b[A\n","112it [06:59,  4.55s/it]\u001b[A\n","113it [07:00,  3.26s/it]\u001b[A\n","114it [07:00,  2.35s/it]\u001b[A\n","115it [07:00,  1.71s/it]\u001b[A\n","116it [07:00,  1.27s/it]\u001b[A\n","117it [07:01,  1.04it/s]\u001b[A\n","118it [07:01,  1.35it/s]\u001b[A\n","119it [07:01,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:43<00:00,  1.85s/it]\n","03/08/2022 08:56:25 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:56:39 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:15,  4.61s/it]\u001b[A\n","122it [07:16,  3.30s/it]\u001b[A\n","123it [07:16,  2.38s/it]\u001b[A\n","124it [07:16,  1.73s/it]\u001b[A\n","125it [07:16,  1.28s/it]\u001b[A\n","126it [07:17,  1.03it/s]\u001b[A\n","127it [07:17,  1.34it/s]\u001b[A\n","128it [07:17,  1.69it/s]\u001b[A\n","129it [07:17,  2.07it/s]\u001b[A\n","130it [07:18,  2.45it/s]\u001b[A\n","131it [07:18,  2.82it/s]\u001b[A\n","132it [07:18,  3.49it/s]\u001b[A03/08/2022 08:56:42 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:56:42 - INFO - __main__ -     eval_loss = 2.8205087184906006\n","03/08/2022 08:56:42 - INFO - __main__ -     eval_auroc = 0.8643750548362732\n","03/08/2022 08:56:42 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/08/2022 08:56:42 - INFO - __main__ -     eval_f1 = 0.4888889193534851\n","03/08/2022 08:56:42 - INFO - filelock -   Lock 139700863701840 acquired on log.lock\n","03/08/2022 08:56:42 - INFO - filelock -   Lock 139700863701840 released on log.lock\n","132it [07:18,  3.32s/it]\n"]}]},{"cell_type":"code","source":["!source env/bin/activate; bash ensemble.sh"],"metadata":{"id":"6Or6nfVP3VjV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647251470532,"user_tz":420,"elapsed":4147529,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"}},"outputId":"0964e5fc-7905-487d-a561-4b055a95a2ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","2224it [15:36,  4.33it/s]\u001b[A\n","2225it [15:36,  4.33it/s]\u001b[A\n","2226it [15:36,  4.33it/s]\u001b[A\n","2227it [15:36,  4.33it/s]\u001b[A\n","2228it [15:37,  4.33it/s]\u001b[A\n","2229it [15:37,  4.33it/s]\u001b[A\n","2230it [15:37,  4.33it/s]\u001b[A\n","2231it [15:37,  4.33it/s]\u001b[A\n","2232it [15:38,  4.33it/s]\u001b[A\n","2233it [15:38,  4.33it/s]\u001b[A\n","2234it [15:38,  4.33it/s]\u001b[A\n","2235it [15:38,  4.33it/s]\u001b[A\n","2236it [15:39,  4.33it/s]\u001b[A\n","2237it [15:39,  4.33it/s]\u001b[A\n","2238it [15:39,  4.33it/s]\u001b[A\n","2239it [15:39,  4.33it/s]\u001b[A\n","2240it [15:39,  4.33it/s]\u001b[A\n","2241it [15:40,  4.33it/s]\u001b[A\n","2242it [15:40,  4.33it/s]\u001b[A\n","2243it [15:40,  4.33it/s]\u001b[A\n","2244it [15:40,  4.33it/s]\u001b[A\n","2245it [15:41,  4.33it/s]\u001b[A\n","2246it [15:41,  4.33it/s]\u001b[A\n","2247it [15:41,  4.33it/s]\u001b[A\n","2248it [15:41,  4.33it/s]\u001b[A\n","2249it [15:42,  4.33it/s]\u001b[A\n","2250it [15:42,  4.33it/s]\u001b[A\n","2251it [15:42,  4.33it/s]\u001b[A\n","2252it [15:42,  4.33it/s]\u001b[A\n","2253it [15:42,  4.33it/s]\u001b[A\n","2254it [15:43,  4.33it/s]\u001b[A\n","2255it [15:43,  4.33it/s]\u001b[A\n","2256it [15:43,  4.33it/s]\u001b[A\n","2257it [15:43,  4.33it/s]\u001b[A\n","2258it [15:44,  4.33it/s]\u001b[A\n","2259it [15:44,  4.33it/s]\u001b[A\n","2260it [15:44,  4.33it/s]\u001b[A\n","2261it [15:44,  4.33it/s]\u001b[A\n","2262it [15:45,  4.33it/s]\u001b[A\n","2263it [15:45,  4.33it/s]\u001b[A\n","2264it [15:45,  4.33it/s]\u001b[A\n","2265it [15:45,  4.33it/s]\u001b[A\n","2266it [15:45,  4.33it/s]\u001b[A\n","2267it [15:46,  4.33it/s]\u001b[A\n","2268it [15:46,  4.33it/s]\u001b[A\n","2269it [15:46,  4.33it/s]\u001b[A\n","2270it [15:46,  4.33it/s]\u001b[A\n","2271it [15:47,  4.33it/s]\u001b[A\n","2272it [15:47,  4.33it/s]\u001b[A\n","2273it [15:47,  4.33it/s]\u001b[A\n","2274it [15:47,  4.33it/s]\u001b[A\n","2275it [15:48,  4.33it/s]\u001b[A\n","2276it [15:48,  4.33it/s]\u001b[A\n","2277it [15:48,  4.33it/s]\u001b[A\n","2278it [15:48,  4.33it/s]\u001b[A\n","2279it [15:48,  4.33it/s]\u001b[A\n","2280it [15:49,  4.33it/s]\u001b[A\n","2281it [15:49,  4.33it/s]\u001b[A\n","2282it [15:49,  4.33it/s]\u001b[A\n","2283it [15:49,  4.33it/s]\u001b[A\n","2284it [15:50,  4.33it/s]\u001b[A\n","2285it [15:50,  4.33it/s]\u001b[A\n","2286it [15:50,  4.33it/s]\u001b[A\n","2287it [15:50,  4.33it/s]\u001b[A\n","2288it [15:51,  4.33it/s]\u001b[A\n","2289it [15:51,  4.33it/s]\u001b[A\n","2290it [15:51,  4.33it/s]\u001b[A\n","2291it [15:51,  4.33it/s]\u001b[A\n","2292it [15:51,  4.33it/s]\u001b[A\n","2293it [15:52,  4.33it/s]\u001b[A\n","2294it [15:52,  4.33it/s]\u001b[A\n","2295it [15:52,  4.33it/s]\u001b[A\n","2296it [15:52,  4.33it/s]\u001b[A\n","2297it [15:53,  4.33it/s]\u001b[A\n","2298it [15:53,  4.33it/s]\u001b[A\n","2299it [15:53,  4.33it/s]\u001b[A\n","2300it [15:53,  4.33it/s]\u001b[A\n","2301it [15:54,  4.33it/s]\u001b[A\n","2302it [15:54,  4.33it/s]\u001b[A\n","2303it [15:54,  4.33it/s]\u001b[A\n","2304it [15:54,  4.33it/s]\u001b[A\n","2305it [15:54,  4.33it/s]\u001b[A\n","2306it [15:55,  4.33it/s]\u001b[A\n","2307it [15:55,  4.33it/s]\u001b[A\n","2308it [15:55,  4.33it/s]\u001b[A\n","2309it [15:55,  4.33it/s]\u001b[A\n","2310it [15:56,  4.33it/s]\u001b[A\n","2311it [15:56,  4.33it/s]\u001b[A\n","2312it [15:56,  4.33it/s]\u001b[A\n","2313it [15:56,  4.33it/s]\u001b[A\n","2314it [15:57,  4.33it/s]\u001b[A\n","2315it [15:57,  4.33it/s]\u001b[A\n","2316it [15:57,  4.33it/s]\u001b[A\n","2317it [15:57,  4.33it/s]\u001b[A\n","2318it [15:57,  4.33it/s]\u001b[A\n","2319it [15:58,  4.33it/s]\u001b[A\n","2320it [15:58,  4.33it/s]\u001b[A\n","2321it [15:58,  4.33it/s]\u001b[A\n","2322it [15:58,  4.33it/s]\u001b[A\n","2323it [15:59,  4.33it/s]\u001b[A\n","2324it [15:59,  4.33it/s]\u001b[A\n","2325it [15:59,  4.33it/s]\u001b[A\n","2326it [15:59,  4.33it/s]\u001b[A\n","2327it [16:00,  4.33it/s]\u001b[A\n","2328it [16:00,  4.33it/s]\u001b[A\n","2329it [16:00,  4.33it/s]\u001b[A\n","2330it [16:00,  4.33it/s]\u001b[A\n","2331it [16:00,  4.33it/s]\u001b[A\n","2332it [16:01,  4.33it/s]\u001b[A\n","2333it [16:01,  4.33it/s]\u001b[A\n","2334it [16:01,  4.33it/s]\u001b[A\n","2335it [16:01,  4.33it/s]\u001b[A\n","2336it [16:02,  4.33it/s]\u001b[A\n","2337it [16:02,  4.33it/s]\u001b[A\n","2338it [16:02,  4.33it/s]\u001b[A\n","2339it [16:02,  4.33it/s]\u001b[A\n","2340it [16:03,  4.33it/s]\u001b[A\n","2341it [16:03,  4.33it/s]\u001b[A\n","2342it [16:03,  4.33it/s]\u001b[A\n","2343it [16:03,  4.33it/s]\u001b[A\n","2344it [16:03,  4.33it/s]\u001b[A\n","2345it [16:04,  4.33it/s]\u001b[A\n","2346it [16:04,  4.33it/s]\u001b[A\n","2347it [16:04,  4.33it/s]\u001b[A\n","2348it [16:04,  4.34it/s]\u001b[A\n","2349it [16:05,  4.34it/s]\u001b[A\n","2350it [16:05,  4.33it/s]\u001b[A\n","2351it [16:05,  4.33it/s]\u001b[A\n","2352it [16:05,  4.33it/s]\u001b[A\n","2353it [16:06,  4.33it/s]\u001b[A\n","2354it [16:06,  4.33it/s]\u001b[A03/14/2022 09:17:01 - INFO - __main__ -   ***** Test results spoilers *****\n","03/14/2022 09:17:01 - INFO - __main__ -     eval_loss = 5.440009593963623\n","03/14/2022 09:17:01 - INFO - __main__ -     eval_auroc = 0.824364423751831\n","03/14/2022 09:17:01 - INFO - __main__ -     eval_recall = 0.9166666865348816\n","03/14/2022 09:17:01 - INFO - __main__ -     eval_f1 = 0.24444447457790375\n","03/14/2022 09:17:01 - INFO - filelock -   Lock 140358899154512 acquired on log.lock\n","03/14/2022 09:17:02 - INFO - filelock -   Lock 140358899154512 released on log.lock\n","2354it [16:06,  2.44it/s]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/14/2022 09:17:08 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.sort.txt\n","03/14/2022 09:17:08 - INFO - __main__ -   Specify load the 18-th template: *cls*▁Yes!*mask*!*+sent_0**sep+*\n","03/14/2022 09:17:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/14/2022 09:17:08 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-demo-16-21-roberta-large-13811', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar14_09-17-08_eda9c4d80d5c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-demo-16-21-roberta-large-13811', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=0, model_id=18, save_logit=True, save_logit_dir='ensemble_predict_results', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","03/14/2022 09:17:08 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/14/2022 09:17:08 - INFO - __main__ -   Automatically convert the template to using demonstrations.\n","03/14/2022 09:17:08 - INFO - __main__ -   | *cls*▁Yes!*mask*!*+sent_0**sep+* => *cls*▁Yes!*mask*!*+sent_0**sep+*▁Yes!*label_0*!*+sent_1**sep+*▁Yes!*label_1*!*+sent_2**sep+*\n","03/14/2022 09:17:09 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:17:09 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/14/2022 09:17:09 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/14/2022 09:17:09 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/14/2022 09:17:09 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:17:09 - INFO - filelock -   Lock 140288453070416 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:09 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/14/2022 09:17:09 - INFO - filelock -   Lock 140288453070416 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:09 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:17:09 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/14/2022 09:17:09 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/14/2022 09:17:09 - INFO - src.dataset -   Total num_sample for mode dev: 16\n","03/14/2022 09:17:09 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:17:09 - INFO - filelock -   Lock 140288457060944 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:09 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/14/2022 09:17:09 - INFO - filelock -   Lock 140288457060944 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:09 - INFO - src.dataset -   *** Example ***\n","03/14/2022 09:17:09 - INFO - src.dataset -   guid: dev-0\n","03/14/2022 09:17:09 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 48584, 10172, 9904, 328, 440, 328, 61, 74, 28, 6474, 9724, 2, 48584, 10172, 9904, 328, 3216, 328, 821, 10810, 31985, 364, 329, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/14/2022 09:17:09 - INFO - src.dataset -   text: <s>▁Yes!<mask>! Guys, why goth mikasa was shown?</s>▁Yes! No! which would be lit af</s>▁Yes! Yes! gabi sniper ez</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","03/14/2022 09:17:12 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:17:12 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/14/2022 09:17:12 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/14/2022 09:17:12 - INFO - src.dataset -   Total num_sample for mode test: 16\n","03/14/2022 09:17:12 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:17:12 - INFO - filelock -   Lock 140288453070416 acquired on data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:12 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/14/2022 09:17:12 - INFO - filelock -   Lock 140288453070416 released on data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:12 - INFO - src.dataset -   *** Example ***\n","03/14/2022 09:17:12 - INFO - src.dataset -   guid: test-0\n","03/14/2022 09:17:12 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 364, 2558, 18, 269, 15867, 154, 123, 2, 48584, 10172, 9904, 328, 440, 328, 2230, 2, 48584, 10172, 9904, 328, 3216, 328, 821, 10810, 31985, 364, 329, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[5], label_word_list=None)\n","03/14/2022 09:17:12 - INFO - src.dataset -   text: <s>▁Yes!<mask>! eren's really battering him</s>▁Yes! No! exactly</s>▁Yes! Yes! gabi sniper ez</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/14/2022 09:17:34 - INFO - src.trainer -   ***** Running training *****\n","03/14/2022 09:17:34 - INFO - src.trainer -     Num examples = 32\n","03/14/2022 09:17:34 - INFO - src.trainer -     Num Epochs = 250\n","03/14/2022 09:17:34 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/14/2022 09:17:34 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/14/2022 09:17:34 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/14/2022 09:17:34 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:19,  1.68s/it]\n","  0% 0/185 [00:00<?, ?it/s]\u001b[A\n","  1% 2/185 [00:00<00:21,  8.68it/s]\u001b[A\n","  2% 3/185 [00:00<00:27,  6.67it/s]\u001b[A\n","  2% 4/185 [00:00<00:31,  5.74it/s]\u001b[A\n","  3% 5/185 [00:00<00:34,  5.23it/s]\u001b[A\n","  3% 6/185 [00:01<00:36,  4.93it/s]\u001b[A\n","  4% 7/185 [00:01<00:37,  4.73it/s]\u001b[A\n","  4% 8/185 [00:01<00:38,  4.60it/s]\u001b[A\n","  5% 9/185 [00:01<00:38,  4.52it/s]\u001b[A\n","  5% 10/185 [00:02<00:39,  4.46it/s]\u001b[A\n","  6% 11/185 [00:02<00:39,  4.42it/s]\u001b[A\n","  6% 12/185 [00:02<00:39,  4.39it/s]\u001b[A\n","  7% 13/185 [00:02<00:39,  4.37it/s]\u001b[A\n","  8% 14/185 [00:03<00:39,  4.36it/s]\u001b[A\n","  8% 15/185 [00:03<00:39,  4.35it/s]\u001b[A\n","  9% 16/185 [00:03<00:38,  4.35it/s]\u001b[A\n","  9% 17/185 [00:03<00:38,  4.34it/s]\u001b[A\n"," 10% 18/185 [00:03<00:38,  4.34it/s]\u001b[A\n"," 10% 19/185 [00:04<00:38,  4.34it/s]\u001b[A\n"," 11% 20/185 [00:04<00:38,  4.34it/s]\u001b[A\n"," 11% 21/185 [00:04<00:37,  4.33it/s]\u001b[A\n"," 12% 22/185 [00:04<00:37,  4.33it/s]\u001b[A\n"," 12% 23/185 [00:05<00:37,  4.33it/s]\u001b[A\n"," 13% 24/185 [00:05<00:37,  4.33it/s]\u001b[A\n"," 14% 25/185 [00:05<00:36,  4.33it/s]\u001b[A\n"," 14% 26/185 [00:05<00:36,  4.33it/s]\u001b[A\n"," 15% 27/185 [00:06<00:36,  4.33it/s]\u001b[A\n"," 15% 28/185 [00:06<00:36,  4.33it/s]\u001b[A\n"," 16% 29/185 [00:06<00:36,  4.33it/s]\u001b[A\n"," 16% 30/185 [00:06<00:35,  4.33it/s]\u001b[A\n"," 17% 31/185 [00:06<00:35,  4.33it/s]\u001b[A\n"," 17% 32/185 [00:07<00:35,  4.33it/s]\u001b[A\n"," 18% 33/185 [00:07<00:35,  4.33it/s]\u001b[A\n"," 18% 34/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 35/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 36/185 [00:08<00:34,  4.33it/s]\u001b[A\n"," 20% 37/185 [00:08<00:34,  4.33it/s]\u001b[A\n"," 21% 38/185 [00:08<00:33,  4.33it/s]\u001b[A\n"," 21% 39/185 [00:08<00:33,  4.33it/s]\u001b[A\n"," 22% 40/185 [00:09<00:33,  4.33it/s]\u001b[A\n"," 22% 41/185 [00:09<00:33,  4.33it/s]\u001b[A\n"," 23% 42/185 [00:09<00:33,  4.33it/s]\u001b[A\n"," 23% 43/185 [00:09<00:32,  4.33it/s]\u001b[A\n"," 24% 44/185 [00:09<00:32,  4.33it/s]\u001b[A\n"," 24% 45/185 [00:10<00:32,  4.33it/s]\u001b[A\n"," 25% 46/185 [00:10<00:32,  4.33it/s]\u001b[A\n"," 25% 47/185 [00:10<00:31,  4.33it/s]\u001b[A\n"," 26% 48/185 [00:10<00:31,  4.33it/s]\u001b[A\n"," 26% 49/185 [00:11<00:31,  4.33it/s]\u001b[A\n"," 27% 50/185 [00:11<00:31,  4.33it/s]\u001b[A\n"," 28% 51/185 [00:11<00:30,  4.33it/s]\u001b[A\n"," 28% 52/185 [00:11<00:30,  4.33it/s]\u001b[A\n"," 29% 53/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 29% 54/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 30% 55/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 30% 56/185 [00:12<00:29,  4.33it/s]\u001b[A\n"," 31% 57/185 [00:12<00:29,  4.33it/s]\u001b[A\n"," 31% 58/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 59/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 60/185 [00:13<00:28,  4.33it/s]\u001b[A\n"," 33% 61/185 [00:13<00:28,  4.33it/s]\u001b[A\n"," 34% 62/185 [00:14<00:28,  4.33it/s]\u001b[A\n"," 34% 63/185 [00:14<00:28,  4.33it/s]\u001b[A\n"," 35% 64/185 [00:14<00:27,  4.33it/s]\u001b[A\n"," 35% 65/185 [00:14<00:27,  4.33it/s]\u001b[A\n"," 36% 66/185 [00:15<00:27,  4.32it/s]\u001b[A\n"," 36% 67/185 [00:15<00:27,  4.32it/s]\u001b[A\n"," 37% 68/185 [00:15<00:27,  4.32it/s]\u001b[A\n"," 37% 69/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 38% 70/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 38% 71/185 [00:16<00:26,  4.33it/s]\u001b[A\n"," 39% 72/185 [00:16<00:26,  4.33it/s]\u001b[A\n"," 39% 73/185 [00:16<00:25,  4.33it/s]\u001b[A\n"," 40% 74/185 [00:16<00:25,  4.33it/s]\u001b[A\n"," 41% 75/185 [00:17<00:25,  4.33it/s]\u001b[A\n"," 41% 76/185 [00:17<00:25,  4.33it/s]\u001b[A\n"," 42% 77/185 [00:17<00:24,  4.33it/s]\u001b[A\n"," 42% 78/185 [00:17<00:24,  4.33it/s]\u001b[A\n"," 43% 79/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 43% 80/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 44% 81/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 44% 82/185 [00:18<00:23,  4.33it/s]\u001b[A\n"," 45% 83/185 [00:18<00:23,  4.33it/s]\u001b[A\n"," 45% 84/185 [00:19<00:23,  4.33it/s]\u001b[A\n"," 46% 85/185 [00:19<00:23,  4.33it/s]\u001b[A\n"," 46% 86/185 [00:19<00:22,  4.33it/s]\u001b[A\n"," 47% 87/185 [00:19<00:22,  4.33it/s]\u001b[A\n"," 48% 88/185 [00:20<00:22,  4.33it/s]\u001b[A\n"," 48% 89/185 [00:20<00:22,  4.33it/s]\u001b[A\n"," 49% 90/185 [00:20<00:21,  4.33it/s]\u001b[A\n"," 49% 91/185 [00:20<00:21,  4.33it/s]\u001b[A\n"," 50% 92/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 50% 93/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 51% 94/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 51% 95/185 [00:21<00:20,  4.33it/s]\u001b[A\n"," 52% 96/185 [00:21<00:20,  4.33it/s]\u001b[A\n"," 52% 97/185 [00:22<00:20,  4.33it/s]\u001b[A\n"," 53% 98/185 [00:22<00:20,  4.33it/s]\u001b[A\n"," 54% 99/185 [00:22<00:19,  4.33it/s]\u001b[A\n"," 54% 100/185 [00:22<00:19,  4.33it/s]\u001b[A\n"," 55% 101/185 [00:23<00:19,  4.33it/s]\u001b[A\n"," 55% 102/185 [00:23<00:19,  4.33it/s]\u001b[A\n"," 56% 103/185 [00:23<00:18,  4.33it/s]\u001b[A\n"," 56% 104/185 [00:23<00:18,  4.33it/s]\u001b[A\n"," 57% 105/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 57% 106/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 58% 107/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 58% 108/185 [00:24<00:17,  4.33it/s]\u001b[A\n"," 59% 109/185 [00:24<00:17,  4.33it/s]\u001b[A\n"," 59% 110/185 [00:25<00:17,  4.33it/s]\u001b[A\n"," 60% 111/185 [00:25<00:17,  4.33it/s]\u001b[A\n"," 61% 112/185 [00:25<00:16,  4.33it/s]\u001b[A\n"," 61% 113/185 [00:25<00:16,  4.33it/s]\u001b[A\n"," 62% 114/185 [00:26<00:16,  4.33it/s]\u001b[A\n"," 62% 115/185 [00:26<00:16,  4.33it/s]\u001b[A\n"," 63% 116/185 [00:26<00:15,  4.33it/s]\u001b[A\n"," 63% 117/185 [00:26<00:15,  4.33it/s]\u001b[A\n"," 64% 118/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 64% 119/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 65% 120/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 65% 121/185 [00:27<00:14,  4.33it/s]\u001b[A\n"," 66% 122/185 [00:27<00:14,  4.33it/s]\u001b[A\n"," 66% 123/185 [00:28<00:14,  4.33it/s]\u001b[A\n"," 67% 124/185 [00:28<00:14,  4.33it/s]\u001b[A\n"," 68% 125/185 [00:28<00:13,  4.33it/s]\u001b[A\n"," 68% 126/185 [00:28<00:13,  4.33it/s]\u001b[A\n"," 69% 127/185 [00:29<00:13,  4.33it/s]\u001b[A\n"," 69% 128/185 [00:29<00:13,  4.33it/s]\u001b[A\n"," 70% 129/185 [00:29<00:12,  4.33it/s]\u001b[A\n"," 70% 130/185 [00:29<00:12,  4.33it/s]\u001b[A\n"," 71% 131/185 [00:30<00:12,  4.33it/s]\u001b[A\n"," 71% 132/185 [00:30<00:12,  4.33it/s]\u001b[A\n"," 72% 133/185 [00:30<00:12,  4.33it/s]\u001b[A\n"," 72% 134/185 [00:30<00:11,  4.33it/s]\u001b[A\n"," 73% 135/185 [00:30<00:11,  4.33it/s]\u001b[A\n"," 74% 136/185 [00:31<00:11,  4.33it/s]\u001b[A\n"," 74% 137/185 [00:31<00:11,  4.33it/s]\u001b[A\n"," 75% 138/185 [00:31<00:10,  4.33it/s]\u001b[A\n"," 75% 139/185 [00:31<00:10,  4.33it/s]\u001b[A\n"," 76% 140/185 [00:32<00:10,  4.33it/s]\u001b[A\n"," 76% 141/185 [00:32<00:10,  4.33it/s]\u001b[A\n"," 77% 142/185 [00:32<00:09,  4.33it/s]\u001b[A\n"," 77% 143/185 [00:32<00:09,  4.33it/s]\u001b[A\n"," 78% 144/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 78% 145/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 79% 146/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 79% 147/185 [00:33<00:08,  4.33it/s]\u001b[A\n"," 80% 148/185 [00:33<00:08,  4.33it/s]\u001b[A\n"," 81% 149/185 [00:34<00:08,  4.33it/s]\u001b[A\n"," 81% 150/185 [00:34<00:08,  4.33it/s]\u001b[A\n"," 82% 151/185 [00:34<00:07,  4.33it/s]\u001b[A\n"," 82% 152/185 [00:34<00:07,  4.33it/s]\u001b[A\n"," 83% 153/185 [00:35<00:07,  4.33it/s]\u001b[A\n"," 83% 154/185 [00:35<00:07,  4.33it/s]\u001b[A\n"," 84% 155/185 [00:35<00:06,  4.33it/s]\u001b[A\n"," 84% 156/185 [00:35<00:06,  4.33it/s]\u001b[A\n"," 85% 157/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 85% 158/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 86% 159/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 86% 160/185 [00:36<00:05,  4.33it/s]\u001b[A\n"," 87% 161/185 [00:36<00:05,  4.33it/s]\u001b[A\n"," 88% 162/185 [00:37<00:05,  4.33it/s]\u001b[A\n"," 88% 163/185 [00:37<00:05,  4.33it/s]\u001b[A\n"," 89% 164/185 [00:37<00:04,  4.33it/s]\u001b[A\n"," 89% 165/185 [00:37<00:04,  4.33it/s]\u001b[A\n"," 90% 166/185 [00:38<00:04,  4.33it/s]\u001b[A\n"," 90% 167/185 [00:38<00:04,  4.33it/s]\u001b[A\n"," 91% 168/185 [00:38<00:03,  4.33it/s]\u001b[A\n"," 91% 169/185 [00:38<00:03,  4.33it/s]\u001b[A\n"," 92% 170/185 [00:39<00:03,  4.33it/s]\u001b[A\n"," 92% 171/185 [00:39<00:03,  4.33it/s]\u001b[A\n"," 93% 172/185 [00:39<00:03,  4.33it/s]\u001b[A\n"," 94% 173/185 [00:39<00:02,  4.33it/s]\u001b[A\n"," 94% 174/185 [00:39<00:02,  4.33it/s]\u001b[A\n"," 95% 175/185 [00:40<00:02,  4.33it/s]\u001b[A\n"," 95% 176/185 [00:40<00:02,  4.33it/s]\u001b[A\n"," 96% 177/185 [00:40<00:01,  4.33it/s]\u001b[A\n"," 96% 178/185 [00:40<00:01,  4.33it/s]\u001b[A\n"," 97% 179/185 [00:41<00:01,  4.33it/s]\u001b[A\n"," 97% 180/185 [00:41<00:01,  4.33it/s]\u001b[A\n"," 98% 181/185 [00:41<00:00,  4.33it/s]\u001b[A\n"," 98% 182/185 [00:41<00:00,  4.33it/s]\u001b[A\n"," 99% 183/185 [00:42<00:00,  4.34it/s]\u001b[A\n"," 99% 184/185 [00:42<00:00,  4.33it/s]\u001b[A\n","100% 185/185 [00:42<00:00,  4.33it/s]\u001b[A03/14/2022 09:18:59 - INFO - src.trainer -   Best dev result: 0.872749924659729\n","Epoch:  20% 49/250 [02:13<05:38,  1.69s/it]\n","186it [01:32, 15.21s/it]             \u001b[A\n","187it [01:32, 10.72s/it]\u001b[A\n","188it [01:33,  7.57s/it]\u001b[A\n","189it [01:33,  5.37s/it]\u001b[A\n","190it [01:33,  3.83s/it]\u001b[A\n","191it [01:33,  2.75s/it]\u001b[A\n","192it [01:34,  1.99s/it]\u001b[A\n","193it [01:34,  1.46s/it]\u001b[A\n","194it [01:34,  1.09s/it]\u001b[A\n","195it [01:34,  1.20it/s]\u001b[A\n","196it [01:34,  1.53it/s]\u001b[A\n","197it [01:35,  1.90it/s]\u001b[A\n","198it [01:35,  2.28it/s]\u001b[A\n","199it [01:35,  2.66it/s]\u001b[A\n","200it [01:35,  3.01it/s]\u001b[A\n","201it [01:36,  3.31it/s]\u001b[A\n","202it [01:36,  3.56it/s]\u001b[A\n","203it [01:36,  3.76it/s]\u001b[A\n","204it [01:36,  3.92it/s]\u001b[A\n","205it [01:37,  4.03it/s]\u001b[A\n","206it [01:37,  4.12it/s]\u001b[A\n","207it [01:37,  4.18it/s]\u001b[A\n","208it [01:37,  4.22it/s]\u001b[A\n","209it [01:37,  4.26it/s]\u001b[A\n","210it [01:38,  4.28it/s]\u001b[A\n","211it [01:38,  4.29it/s]\u001b[A\n","212it [01:38,  4.30it/s]\u001b[A\n","213it [01:38,  4.31it/s]\u001b[A\n","214it [01:39,  4.32it/s]\u001b[A\n","215it [01:39,  4.32it/s]\u001b[A\n","216it [01:39,  4.32it/s]\u001b[A\n","217it [01:39,  4.33it/s]\u001b[A\n","218it [01:40,  4.33it/s]\u001b[A\n","219it [01:40,  4.32it/s]\u001b[A\n","220it [01:40,  4.33it/s]\u001b[A\n","221it [01:40,  4.33it/s]\u001b[A\n","222it [01:40,  4.33it/s]\u001b[A\n","223it [01:41,  4.33it/s]\u001b[A\n","224it [01:41,  4.33it/s]\u001b[A\n","225it [01:41,  4.33it/s]\u001b[A\n","226it [01:41,  4.33it/s]\u001b[A\n","227it [01:42,  4.33it/s]\u001b[A\n","228it [01:42,  4.33it/s]\u001b[A\n","229it [01:42,  4.33it/s]\u001b[A\n","230it [01:42,  4.33it/s]\u001b[A\n","231it [01:43,  4.33it/s]\u001b[A\n","232it [01:43,  4.33it/s]\u001b[A\n","233it [01:43,  4.33it/s]\u001b[A\n","234it [01:43,  4.33it/s]\u001b[A\n","235it [01:43,  4.33it/s]\u001b[A\n","236it [01:44,  4.33it/s]\u001b[A\n","237it [01:44,  4.33it/s]\u001b[A\n","238it [01:44,  4.33it/s]\u001b[A\n","239it [01:44,  4.33it/s]\u001b[A\n","240it [01:45,  4.33it/s]\u001b[A\n","241it [01:45,  4.33it/s]\u001b[A\n","242it [01:45,  4.33it/s]\u001b[A\n","243it [01:45,  4.33it/s]\u001b[A\n","244it [01:46,  4.33it/s]\u001b[A\n","245it [01:46,  4.33it/s]\u001b[A\n","246it [01:46,  4.33it/s]\u001b[A\n","247it [01:46,  4.33it/s]\u001b[A\n","248it [01:46,  4.33it/s]\u001b[A\n","249it [01:47,  4.33it/s]\u001b[A\n","250it [01:47,  4.33it/s]\u001b[A\n","251it [01:47,  4.33it/s]\u001b[A\n","252it [01:47,  4.33it/s]\u001b[A\n","253it [01:48,  4.33it/s]\u001b[A\n","254it [01:48,  4.33it/s]\u001b[A\n","255it [01:48,  4.33it/s]\u001b[A\n","256it [01:48,  4.33it/s]\u001b[A\n","257it [01:49,  4.33it/s]\u001b[A\n","258it [01:49,  4.33it/s]\u001b[A\n","259it [01:49,  4.33it/s]\u001b[A\n","260it [01:49,  4.33it/s]\u001b[A\n","261it [01:49,  4.33it/s]\u001b[A\n","262it [01:50,  4.33it/s]\u001b[A\n","263it [01:50,  4.33it/s]\u001b[A\n","264it [01:50,  4.33it/s]\u001b[A\n","265it [01:50,  4.33it/s]\u001b[A\n","266it [01:51,  4.33it/s]\u001b[A\n","267it [01:51,  4.33it/s]\u001b[A\n","268it [01:51,  4.33it/s]\u001b[A\n","269it [01:51,  4.33it/s]\u001b[A\n","270it [01:52,  4.33it/s]\u001b[A\n","271it [01:52,  4.33it/s]\u001b[A\n","272it [01:52,  4.33it/s]\u001b[A\n","273it [01:52,  4.33it/s]\u001b[A\n","274it [01:52,  4.33it/s]\u001b[A\n","275it [01:53,  4.33it/s]\u001b[A\n","276it [01:53,  4.33it/s]\u001b[A\n","277it [01:53,  4.33it/s]\u001b[A\n","278it [01:53,  4.33it/s]\u001b[A\n","279it [01:54,  4.33it/s]\u001b[A\n","280it [01:54,  4.33it/s]\u001b[A\n","281it [01:54,  4.33it/s]\u001b[A\n","282it [01:54,  4.33it/s]\u001b[A\n","283it [01:55,  4.33it/s]\u001b[A\n","284it [01:55,  4.33it/s]\u001b[A\n","285it [01:55,  4.33it/s]\u001b[A\n","286it [01:55,  4.33it/s]\u001b[A\n","287it [01:55,  4.33it/s]\u001b[A\n","288it [01:56,  4.33it/s]\u001b[A\n","289it [01:56,  4.33it/s]\u001b[A\n","290it [01:56,  4.33it/s]\u001b[A\n","291it [01:56,  4.33it/s]\u001b[A\n","292it [01:57,  4.33it/s]\u001b[A\n","293it [01:57,  4.33it/s]\u001b[A\n","294it [01:57,  4.33it/s]\u001b[A\n","295it [01:57,  4.33it/s]\u001b[A\n","296it [01:58,  4.33it/s]\u001b[A\n","297it [01:58,  4.33it/s]\u001b[A\n","298it [01:58,  4.33it/s]\u001b[A\n","299it [01:58,  4.33it/s]\u001b[A\n","300it [01:58,  4.33it/s]\u001b[A\n","301it [01:59,  4.33it/s]\u001b[A\n","302it [01:59,  4.33it/s]\u001b[A\n","303it [01:59,  4.33it/s]\u001b[A\n","304it [01:59,  4.33it/s]\u001b[A\n","305it [02:00,  4.33it/s]\u001b[A\n","306it [02:00,  4.33it/s]\u001b[A\n","307it [02:00,  4.33it/s]\u001b[A\n","308it [02:00,  4.33it/s]\u001b[A\n","309it [02:01,  4.33it/s]\u001b[A\n","310it [02:01,  4.33it/s]\u001b[A\n","311it [02:01,  4.33it/s]\u001b[A\n","312it [02:01,  4.33it/s]\u001b[A\n","313it [02:01,  4.33it/s]\u001b[A\n","314it [02:02,  4.33it/s]\u001b[A\n","315it [02:02,  4.33it/s]\u001b[A\n","316it [02:02,  4.33it/s]\u001b[A\n","317it [02:02,  4.33it/s]\u001b[A\n","318it [02:03,  4.33it/s]\u001b[A\n","319it [02:03,  4.34it/s]\u001b[A\n","320it [02:03,  4.33it/s]\u001b[A\n","321it [02:03,  4.33it/s]\u001b[A\n","322it [02:04,  4.33it/s]\u001b[A\n","323it [02:04,  4.33it/s]\u001b[A\n","324it [02:04,  4.33it/s]\u001b[A\n","325it [02:04,  4.33it/s]\u001b[A\n","326it [02:04,  4.33it/s]\u001b[A\n","327it [02:05,  4.33it/s]\u001b[A\n","328it [02:05,  4.33it/s]\u001b[A\n","329it [02:05,  4.33it/s]\u001b[A\n","330it [02:05,  4.33it/s]\u001b[A\n","331it [02:06,  4.33it/s]\u001b[A\n","332it [02:06,  4.33it/s]\u001b[A\n","333it [02:06,  4.33it/s]\u001b[A\n","334it [02:06,  4.33it/s]\u001b[A\n","335it [02:07,  4.33it/s]\u001b[A\n","336it [02:07,  4.33it/s]\u001b[A\n","337it [02:07,  4.33it/s]\u001b[A\n","338it [02:07,  4.33it/s]\u001b[A\n","339it [02:07,  4.33it/s]\u001b[A\n","340it [02:08,  4.33it/s]\u001b[A\n","341it [02:08,  4.33it/s]\u001b[A\n","342it [02:08,  4.33it/s]\u001b[A\n","343it [02:08,  4.33it/s]\u001b[A\n","344it [02:09,  4.33it/s]\u001b[A\n","345it [02:09,  4.33it/s]\u001b[A\n","346it [02:09,  4.33it/s]\u001b[A\n","347it [02:09,  4.33it/s]\u001b[A\n","348it [02:10,  4.33it/s]\u001b[A\n","349it [02:10,  4.33it/s]\u001b[A\n","350it [02:10,  4.33it/s]\u001b[A\n","351it [02:10,  4.33it/s]\u001b[A\n","352it [02:10,  4.33it/s]\u001b[A\n","353it [02:11,  4.33it/s]\u001b[A\n","354it [02:11,  4.33it/s]\u001b[A\n","355it [02:11,  4.33it/s]\u001b[A\n","356it [02:11,  4.33it/s]\u001b[A\n","357it [02:12,  4.33it/s]\u001b[A\n","358it [02:12,  4.33it/s]\u001b[A\n","359it [02:12,  4.33it/s]\u001b[A\n","360it [02:12,  4.33it/s]\u001b[A\n","361it [02:13,  4.33it/s]\u001b[A\n","362it [02:13,  4.33it/s]\u001b[A\n","363it [02:13,  4.33it/s]\u001b[A\n","364it [02:13,  4.33it/s]\u001b[A\n","365it [02:13,  4.33it/s]\u001b[A\n","366it [02:14,  4.33it/s]\u001b[A\n","367it [02:14,  4.33it/s]\u001b[A\n","368it [02:14,  4.33it/s]\u001b[A\n","369it [02:14,  4.33it/s]\u001b[A\n","370it [02:15,  4.33it/s]\u001b[A03/14/2022 09:20:32 - INFO - src.trainer -   Best dev result: 0.8730000853538513\n","Epoch:  30% 74/250 [03:46<04:56,  1.69s/it]\n","371it [03:05, 15.22s/it]\u001b[A\n","372it [03:05, 10.73s/it]\u001b[A\n","373it [03:05,  7.58s/it]\u001b[A\n","374it [03:06,  5.37s/it]\u001b[A\n","375it [03:06,  3.83s/it]\u001b[A\n","376it [03:06,  2.75s/it]\u001b[A\n","377it [03:06,  1.99s/it]\u001b[A\n","378it [03:06,  1.47s/it]\u001b[A\n","379it [03:07,  1.10s/it]\u001b[A\n","380it [03:07,  1.20it/s]\u001b[A\n","381it [03:07,  1.53it/s]\u001b[A\n","382it [03:07,  1.90it/s]\u001b[A\n","383it [03:08,  2.28it/s]\u001b[A\n","384it [03:08,  2.66it/s]\u001b[A\n","385it [03:08,  3.01it/s]\u001b[A\n","386it [03:08,  3.31it/s]\u001b[A\n","387it [03:09,  3.56it/s]\u001b[A\n","388it [03:09,  3.76it/s]\u001b[A\n","389it [03:09,  3.92it/s]\u001b[A\n","390it [03:09,  4.03it/s]\u001b[A\n","391it [03:09,  4.12it/s]\u001b[A\n","392it [03:10,  4.18it/s]\u001b[A\n","393it [03:10,  4.22it/s]\u001b[A\n","394it [03:10,  4.26it/s]\u001b[A\n","395it [03:10,  4.28it/s]\u001b[A\n","396it [03:11,  4.30it/s]\u001b[A\n","397it [03:11,  4.31it/s]\u001b[A\n","398it [03:11,  4.32it/s]\u001b[A\n","399it [03:11,  4.32it/s]\u001b[A\n","400it [03:12,  4.32it/s]\u001b[A\n","401it [03:12,  4.33it/s]\u001b[A\n","402it [03:12,  4.33it/s]\u001b[A\n","403it [03:12,  4.33it/s]\u001b[A\n","404it [03:12,  4.33it/s]\u001b[A\n","405it [03:13,  4.33it/s]\u001b[A\n","406it [03:13,  4.33it/s]\u001b[A\n","407it [03:13,  4.34it/s]\u001b[A\n","408it [03:13,  4.33it/s]\u001b[A\n","409it [03:14,  4.33it/s]\u001b[A\n","410it [03:14,  4.33it/s]\u001b[A\n","411it [03:14,  4.33it/s]\u001b[A\n","412it [03:14,  4.33it/s]\u001b[A\n","413it [03:15,  4.33it/s]\u001b[A\n","414it [03:15,  4.33it/s]\u001b[A\n","415it [03:15,  4.33it/s]\u001b[A\n","416it [03:15,  4.33it/s]\u001b[A\n","417it [03:15,  4.33it/s]\u001b[A\n","418it [03:16,  4.33it/s]\u001b[A\n","419it [03:16,  4.33it/s]\u001b[A\n","420it [03:16,  4.33it/s]\u001b[A\n","421it [03:16,  4.33it/s]\u001b[A\n","422it [03:17,  4.33it/s]\u001b[A\n","423it [03:17,  4.33it/s]\u001b[A\n","424it [03:17,  4.33it/s]\u001b[A\n","425it [03:17,  4.33it/s]\u001b[A\n","426it [03:18,  4.33it/s]\u001b[A\n","427it [03:18,  4.33it/s]\u001b[A\n","428it [03:18,  4.33it/s]\u001b[A\n","429it [03:18,  4.33it/s]\u001b[A\n","430it [03:18,  4.33it/s]\u001b[A\n","431it [03:19,  4.33it/s]\u001b[A\n","432it [03:19,  4.33it/s]\u001b[A\n","433it [03:19,  4.33it/s]\u001b[A\n","434it [03:19,  4.33it/s]\u001b[A\n","435it [03:20,  4.33it/s]\u001b[A\n","436it [03:20,  4.33it/s]\u001b[A\n","437it [03:20,  4.33it/s]\u001b[A\n","438it [03:20,  4.33it/s]\u001b[A\n","439it [03:21,  4.33it/s]\u001b[A\n","440it [03:21,  4.33it/s]\u001b[A\n","441it [03:21,  4.33it/s]\u001b[A\n","442it [03:21,  4.33it/s]\u001b[A\n","443it [03:21,  4.33it/s]\u001b[A\n","444it [03:22,  4.33it/s]\u001b[A\n","445it [03:22,  4.33it/s]\u001b[A\n","446it [03:22,  4.33it/s]\u001b[A\n","447it [03:22,  4.33it/s]\u001b[A\n","448it [03:23,  4.33it/s]\u001b[A\n","449it [03:23,  4.33it/s]\u001b[A\n","450it [03:23,  4.33it/s]\u001b[A\n","451it [03:23,  4.33it/s]\u001b[A\n","452it [03:24,  4.33it/s]\u001b[A\n","453it [03:24,  4.33it/s]\u001b[A\n","454it [03:24,  4.33it/s]\u001b[A\n","455it [03:24,  4.33it/s]\u001b[A\n","456it [03:24,  4.33it/s]\u001b[A\n","457it [03:25,  4.33it/s]\u001b[A\n","458it [03:25,  4.33it/s]\u001b[A\n","459it [03:25,  4.33it/s]\u001b[A\n","460it [03:25,  4.33it/s]\u001b[A\n","461it [03:26,  4.33it/s]\u001b[A\n","462it [03:26,  4.33it/s]\u001b[A\n","463it [03:26,  4.33it/s]\u001b[A\n","464it [03:26,  4.33it/s]\u001b[A\n","465it [03:27,  4.33it/s]\u001b[A\n","466it [03:27,  4.33it/s]\u001b[A\n","467it [03:27,  4.33it/s]\u001b[A\n","468it [03:27,  4.33it/s]\u001b[A\n","469it [03:27,  4.33it/s]\u001b[A\n","470it [03:28,  4.33it/s]\u001b[A\n","471it [03:28,  4.33it/s]\u001b[A\n","472it [03:28,  4.33it/s]\u001b[A\n","473it [03:28,  4.33it/s]\u001b[A\n","474it [03:29,  4.33it/s]\u001b[A\n","475it [03:29,  4.33it/s]\u001b[A\n","476it [03:29,  4.33it/s]\u001b[A\n","477it [03:29,  4.33it/s]\u001b[A\n","478it [03:30,  4.33it/s]\u001b[A\n","479it [03:30,  4.33it/s]\u001b[A\n","480it [03:30,  4.33it/s]\u001b[A\n","481it [03:30,  4.33it/s]\u001b[A\n","482it [03:30,  4.33it/s]\u001b[A\n","483it [03:31,  4.33it/s]\u001b[A\n","484it [03:31,  4.33it/s]\u001b[A\n","485it [03:31,  4.33it/s]\u001b[A\n","486it [03:31,  4.33it/s]\u001b[A\n","487it [03:32,  4.33it/s]\u001b[A\n","488it [03:32,  4.33it/s]\u001b[A\n","489it [03:32,  4.33it/s]\u001b[A\n","490it [03:32,  4.33it/s]\u001b[A\n","491it [03:33,  4.33it/s]\u001b[A\n","492it [03:33,  4.33it/s]\u001b[A\n","493it [03:33,  4.33it/s]\u001b[A\n","494it [03:33,  4.33it/s]\u001b[A\n","495it [03:33,  4.33it/s]\u001b[A\n","496it [03:34,  4.33it/s]\u001b[A\n","497it [03:34,  4.33it/s]\u001b[A\n","498it [03:34,  4.33it/s]\u001b[A\n","499it [03:34,  4.34it/s]\u001b[A\n","500it [03:35,  4.34it/s]\u001b[A\n","501it [03:35,  4.33it/s]\u001b[A\n","502it [03:35,  4.33it/s]\u001b[A\n","503it [03:35,  4.33it/s]\u001b[A\n","504it [03:36,  4.33it/s]\u001b[A\n","505it [03:36,  4.33it/s]\u001b[A\n","506it [03:36,  4.33it/s]\u001b[A\n","507it [03:36,  4.34it/s]\u001b[A\n","508it [03:36,  4.33it/s]\u001b[A\n","509it [03:37,  4.33it/s]\u001b[A\n","510it [03:37,  4.33it/s]\u001b[A\n","511it [03:37,  4.33it/s]\u001b[A\n","512it [03:37,  4.33it/s]\u001b[A\n","513it [03:38,  4.33it/s]\u001b[A\n","514it [03:38,  4.33it/s]\u001b[A\n","515it [03:38,  4.33it/s]\u001b[A\n","516it [03:38,  4.33it/s]\u001b[A\n","517it [03:39,  4.33it/s]\u001b[A\n","518it [03:39,  4.33it/s]\u001b[A\n","519it [03:39,  4.33it/s]\u001b[A\n","520it [03:39,  4.33it/s]\u001b[A\n","521it [03:39,  4.33it/s]\u001b[A\n","522it [03:40,  4.33it/s]\u001b[A\n","523it [03:40,  4.33it/s]\u001b[A\n","524it [03:40,  4.33it/s]\u001b[A\n","525it [03:40,  4.33it/s]\u001b[A\n","526it [03:41,  4.33it/s]\u001b[A\n","527it [03:41,  4.33it/s]\u001b[A\n","528it [03:41,  4.33it/s]\u001b[A\n","529it [03:41,  4.33it/s]\u001b[A\n","530it [03:42,  4.33it/s]\u001b[A\n","531it [03:42,  4.33it/s]\u001b[A\n","532it [03:42,  4.33it/s]\u001b[A\n","533it [03:42,  4.33it/s]\u001b[A\n","534it [03:42,  4.33it/s]\u001b[A\n","535it [03:43,  4.33it/s]\u001b[A\n","536it [03:43,  4.33it/s]\u001b[A\n","537it [03:43,  4.33it/s]\u001b[A\n","538it [03:43,  4.33it/s]\u001b[A\n","539it [03:44,  4.33it/s]\u001b[A\n","540it [03:44,  4.33it/s]\u001b[A\n","541it [03:44,  4.33it/s]\u001b[A\n","542it [03:44,  4.33it/s]\u001b[A\n","543it [03:45,  4.33it/s]\u001b[A\n","544it [03:45,  4.33it/s]\u001b[A\n","545it [03:45,  4.33it/s]\u001b[A\n","546it [03:45,  4.33it/s]\u001b[A\n","547it [03:45,  4.33it/s]\u001b[A\n","548it [03:46,  4.33it/s]\u001b[A\n","549it [03:46,  4.33it/s]\u001b[A\n","550it [03:46,  4.33it/s]\u001b[A\n","551it [03:46,  4.33it/s]\u001b[A\n","552it [03:47,  4.33it/s]\u001b[A\n","553it [03:47,  4.32it/s]\u001b[A\n","554it [03:47,  4.33it/s]\u001b[A\n","Epoch:  40% 99/250 [05:10<04:14,  1.69s/it]\n","556it [04:29, 12.80s/it]\u001b[A\n","557it [04:30,  9.03s/it]\u001b[A\n","558it [04:30,  6.39s/it]\u001b[A\n","559it [04:30,  4.54s/it]\u001b[A\n","560it [04:30,  3.25s/it]\u001b[A\n","561it [04:31,  2.34s/it]\u001b[A\n","562it [04:31,  1.71s/it]\u001b[A\n","563it [04:31,  1.27s/it]\u001b[A\n","564it [04:31,  1.05it/s]\u001b[A\n","565it [04:32,  1.36it/s]\u001b[A\n","566it [04:32,  1.71it/s]\u001b[A\n","567it [04:32,  2.09it/s]\u001b[A\n","568it [04:32,  2.47it/s]\u001b[A\n","569it [04:32,  2.84it/s]\u001b[A\n","570it [04:33,  3.16it/s]\u001b[A\n","571it [04:33,  3.44it/s]\u001b[A\n","572it [04:33,  3.67it/s]\u001b[A\n","573it [04:33,  3.85it/s]\u001b[A\n","574it [04:34,  3.98it/s]\u001b[A\n","575it [04:34,  4.08it/s]\u001b[A\n","576it [04:34,  4.15it/s]\u001b[A\n","577it [04:34,  4.20it/s]\u001b[A\n","578it [04:35,  4.24it/s]\u001b[A\n","579it [04:35,  4.27it/s]\u001b[A\n","580it [04:35,  4.29it/s]\u001b[A\n","581it [04:35,  4.30it/s]\u001b[A\n","582it [04:35,  4.31it/s]\u001b[A\n","583it [04:36,  4.32it/s]\u001b[A\n","584it [04:36,  4.33it/s]\u001b[A\n","585it [04:36,  4.33it/s]\u001b[A\n","586it [04:36,  4.33it/s]\u001b[A\n","587it [04:37,  4.33it/s]\u001b[A\n","588it [04:37,  4.33it/s]\u001b[A\n","589it [04:37,  4.33it/s]\u001b[A\n","590it [04:37,  4.33it/s]\u001b[A\n","591it [04:38,  4.33it/s]\u001b[A\n","592it [04:38,  4.33it/s]\u001b[A\n","593it [04:38,  4.33it/s]\u001b[A\n","594it [04:38,  4.34it/s]\u001b[A\n","595it [04:38,  4.34it/s]\u001b[A\n","596it [04:39,  4.33it/s]\u001b[A\n","597it [04:39,  4.34it/s]\u001b[A\n","598it [04:39,  4.33it/s]\u001b[A\n","599it [04:39,  4.33it/s]\u001b[A\n","600it [04:40,  4.33it/s]\u001b[A\n","601it [04:40,  4.33it/s]\u001b[A\n","602it [04:40,  4.34it/s]\u001b[A\n","603it [04:40,  4.34it/s]\u001b[A\n","604it [04:41,  4.34it/s]\u001b[A\n","605it [04:41,  4.33it/s]\u001b[A\n","606it [04:41,  4.33it/s]\u001b[A\n","607it [04:41,  4.33it/s]\u001b[A\n","608it [04:41,  4.33it/s]\u001b[A\n","609it [04:42,  4.33it/s]\u001b[A\n","610it [04:42,  4.33it/s]\u001b[A\n","611it [04:42,  4.33it/s]\u001b[A\n","612it [04:42,  4.34it/s]\u001b[A\n","613it [04:43,  4.34it/s]\u001b[A\n","614it [04:43,  4.34it/s]\u001b[A\n","615it [04:43,  4.33it/s]\u001b[A\n","616it [04:43,  4.33it/s]\u001b[A\n","617it [04:44,  4.33it/s]\u001b[A\n","618it [04:44,  4.33it/s]\u001b[A\n","619it [04:44,  4.33it/s]\u001b[A\n","620it [04:44,  4.33it/s]\u001b[A\n","621it [04:44,  4.33it/s]\u001b[A\n","622it [04:45,  4.33it/s]\u001b[A\n","623it [04:45,  4.33it/s]\u001b[A\n","624it [04:45,  4.33it/s]\u001b[A\n","625it [04:45,  4.33it/s]\u001b[A\n","626it [04:46,  4.33it/s]\u001b[A\n","627it [04:46,  4.33it/s]\u001b[A\n","628it [04:46,  4.33it/s]\u001b[A\n","629it [04:46,  4.33it/s]\u001b[A\n","630it [04:47,  4.33it/s]\u001b[A\n","631it [04:47,  4.33it/s]\u001b[A\n","632it [04:47,  4.34it/s]\u001b[A\n","633it [04:47,  4.34it/s]\u001b[A\n","634it [04:47,  4.34it/s]\u001b[A\n","635it [04:48,  4.33it/s]\u001b[A\n","636it [04:48,  4.33it/s]\u001b[A\n","637it [04:48,  4.34it/s]\u001b[A\n","638it [04:48,  4.33it/s]\u001b[A\n","639it [04:49,  4.33it/s]\u001b[A\n","640it [04:49,  4.33it/s]\u001b[A\n","641it [04:49,  4.34it/s]\u001b[A\n","642it [04:49,  4.34it/s]\u001b[A\n","643it [04:50,  4.34it/s]\u001b[A\n","644it [04:50,  4.34it/s]\u001b[A\n","645it [04:50,  4.33it/s]\u001b[A\n","646it [04:50,  4.33it/s]\u001b[A\n","647it [04:50,  4.34it/s]\u001b[A\n","648it [04:51,  4.33it/s]\u001b[A\n","649it [04:51,  4.33it/s]\u001b[A\n","650it [04:51,  4.33it/s]\u001b[A\n","651it [04:51,  4.33it/s]\u001b[A\n","652it [04:52,  4.34it/s]\u001b[A\n","653it [04:52,  4.34it/s]\u001b[A\n","654it [04:52,  4.34it/s]\u001b[A\n","655it [04:52,  4.34it/s]\u001b[A\n","656it [04:53,  4.33it/s]\u001b[A\n","657it [04:53,  4.34it/s]\u001b[A\n","658it [04:53,  4.33it/s]\u001b[A\n","659it [04:53,  4.33it/s]\u001b[A\n","660it [04:53,  4.33it/s]\u001b[A\n","661it [04:54,  4.33it/s]\u001b[A\n","662it [04:54,  4.33it/s]\u001b[A\n","663it [04:54,  4.33it/s]\u001b[A\n","664it [04:54,  4.34it/s]\u001b[A\n","665it [04:55,  4.34it/s]\u001b[A\n","666it [04:55,  4.33it/s]\u001b[A\n","667it [04:55,  4.33it/s]\u001b[A\n","668it [04:55,  4.33it/s]\u001b[A\n","669it [04:56,  4.33it/s]\u001b[A\n","670it [04:56,  4.33it/s]\u001b[A\n","671it [04:56,  4.34it/s]\u001b[A\n","672it [04:56,  4.34it/s]\u001b[A\n","673it [04:56,  4.34it/s]\u001b[A\n","674it [04:57,  4.34it/s]\u001b[A\n","675it [04:57,  4.34it/s]\u001b[A\n","676it [04:57,  4.33it/s]\u001b[A\n","677it [04:57,  4.33it/s]\u001b[A\n","678it [04:58,  4.33it/s]\u001b[A\n","679it [04:58,  4.33it/s]\u001b[A\n","680it [04:58,  4.33it/s]\u001b[A\n","681it [04:58,  4.34it/s]\u001b[A\n","682it [04:59,  4.34it/s]\u001b[A\n","683it [04:59,  4.34it/s]\u001b[A\n","684it [04:59,  4.34it/s]\u001b[A\n","685it [04:59,  4.33it/s]\u001b[A\n","686it [04:59,  4.33it/s]\u001b[A\n","687it [05:00,  4.33it/s]\u001b[A\n","688it [05:00,  4.33it/s]\u001b[A\n","689it [05:00,  4.33it/s]\u001b[A\n","690it [05:00,  4.33it/s]\u001b[A\n","691it [05:01,  4.33it/s]\u001b[A\n","692it [05:01,  4.33it/s]\u001b[A\n","693it [05:01,  4.34it/s]\u001b[A\n","694it [05:01,  4.34it/s]\u001b[A\n","695it [05:02,  4.33it/s]\u001b[A\n","696it [05:02,  4.33it/s]\u001b[A\n","697it [05:02,  4.33it/s]\u001b[A\n","698it [05:02,  4.33it/s]\u001b[A\n","699it [05:02,  4.33it/s]\u001b[A\n","700it [05:03,  4.33it/s]\u001b[A\n","701it [05:03,  4.33it/s]\u001b[A\n","702it [05:03,  4.33it/s]\u001b[A\n","703it [05:03,  4.33it/s]\u001b[A\n","704it [05:04,  4.34it/s]\u001b[A\n","705it [05:04,  4.34it/s]\u001b[A\n","706it [05:04,  4.34it/s]\u001b[A\n","707it [05:04,  4.33it/s]\u001b[A\n","708it [05:05,  4.33it/s]\u001b[A\n","709it [05:05,  4.33it/s]\u001b[A\n","710it [05:05,  4.33it/s]\u001b[A\n","711it [05:05,  4.33it/s]\u001b[A\n","712it [05:05,  4.33it/s]\u001b[A\n","713it [05:06,  4.34it/s]\u001b[A\n","714it [05:06,  4.34it/s]\u001b[A\n","715it [05:06,  4.34it/s]\u001b[A\n","716it [05:06,  4.34it/s]\u001b[A\n","717it [05:07,  4.34it/s]\u001b[A\n","718it [05:07,  4.33it/s]\u001b[A\n","719it [05:07,  4.33it/s]\u001b[A\n","720it [05:07,  4.33it/s]\u001b[A\n","721it [05:08,  4.34it/s]\u001b[A\n","722it [05:08,  4.34it/s]\u001b[A\n","723it [05:08,  4.34it/s]\u001b[A\n","724it [05:08,  4.34it/s]\u001b[A\n","725it [05:08,  4.33it/s]\u001b[A\n","726it [05:09,  4.33it/s]\u001b[A\n","727it [05:09,  4.33it/s]\u001b[A\n","728it [05:09,  4.33it/s]\u001b[A\n","729it [05:09,  4.33it/s]\u001b[A\n","730it [05:10,  4.33it/s]\u001b[A\n","731it [05:10,  4.33it/s]\u001b[A\n","732it [05:10,  4.33it/s]\u001b[A\n","733it [05:10,  4.34it/s]\u001b[A\n","734it [05:11,  4.34it/s]\u001b[A\n","735it [05:11,  4.33it/s]\u001b[A\n","736it [05:11,  4.33it/s]\u001b[A\n","737it [05:11,  4.33it/s]\u001b[A\n","738it [05:11,  4.33it/s]\u001b[A\n","739it [05:12,  4.33it/s]\u001b[A\n","Epoch:  50% 124/250 [06:35<03:31,  1.68s/it]\n","741it [05:54, 12.79s/it]\u001b[A\n","742it [05:54,  9.02s/it]\u001b[A\n","743it [05:54,  6.39s/it]\u001b[A\n","744it [05:55,  4.54s/it]\u001b[A\n","745it [05:55,  3.25s/it]\u001b[A\n","746it [05:55,  2.34s/it]\u001b[A\n","747it [05:55,  1.71s/it]\u001b[A\n","748it [05:56,  1.27s/it]\u001b[A\n","749it [05:56,  1.05it/s]\u001b[A\n","750it [05:56,  1.36it/s]\u001b[A\n","751it [05:56,  1.71it/s]\u001b[A\n","752it [05:57,  2.09it/s]\u001b[A\n","753it [05:57,  2.47it/s]\u001b[A\n","754it [05:57,  2.84it/s]\u001b[A\n","755it [05:57,  3.16it/s]\u001b[A\n","756it [05:57,  3.44it/s]\u001b[A\n","757it [05:58,  3.67it/s]\u001b[A\n","758it [05:58,  3.85it/s]\u001b[A\n","759it [05:58,  3.98it/s]\u001b[A\n","760it [05:58,  4.08it/s]\u001b[A\n","761it [05:59,  4.15it/s]\u001b[A\n","762it [05:59,  4.20it/s]\u001b[A\n","763it [05:59,  4.24it/s]\u001b[A\n","764it [05:59,  4.26it/s]\u001b[A\n","765it [06:00,  4.28it/s]\u001b[A\n","766it [06:00,  4.30it/s]\u001b[A\n","767it [06:00,  4.31it/s]\u001b[A\n","768it [06:00,  4.32it/s]\u001b[A\n","769it [06:00,  4.32it/s]\u001b[A\n","770it [06:01,  4.32it/s]\u001b[A\n","771it [06:01,  4.32it/s]\u001b[A\n","772it [06:01,  4.33it/s]\u001b[A\n","773it [06:01,  4.33it/s]\u001b[A\n","774it [06:02,  4.32it/s]\u001b[A\n","775it [06:02,  4.33it/s]\u001b[A\n","776it [06:02,  4.33it/s]\u001b[A\n","777it [06:02,  4.33it/s]\u001b[A\n","778it [06:03,  4.33it/s]\u001b[A\n","779it [06:03,  4.33it/s]\u001b[A\n","780it [06:03,  4.33it/s]\u001b[A\n","781it [06:03,  4.33it/s]\u001b[A\n","782it [06:03,  4.33it/s]\u001b[A\n","783it [06:04,  4.33it/s]\u001b[A\n","784it [06:04,  4.33it/s]\u001b[A\n","785it [06:04,  4.33it/s]\u001b[A\n","786it [06:04,  4.33it/s]\u001b[A\n","787it [06:05,  4.33it/s]\u001b[A\n","788it [06:05,  4.33it/s]\u001b[A\n","789it [06:05,  4.33it/s]\u001b[A\n","790it [06:05,  4.33it/s]\u001b[A\n","791it [06:06,  4.33it/s]\u001b[A\n","792it [06:06,  4.33it/s]\u001b[A\n","793it [06:06,  4.33it/s]\u001b[A\n","794it [06:06,  4.33it/s]\u001b[A\n","795it [06:06,  4.33it/s]\u001b[A\n","796it [06:07,  4.33it/s]\u001b[A\n","797it [06:07,  4.33it/s]\u001b[A\n","798it [06:07,  4.33it/s]\u001b[A\n","799it [06:07,  4.33it/s]\u001b[A\n","800it [06:08,  4.33it/s]\u001b[A\n","801it [06:08,  4.33it/s]\u001b[A\n","802it [06:08,  4.33it/s]\u001b[A\n","803it [06:08,  4.33it/s]\u001b[A\n","804it [06:09,  4.33it/s]\u001b[A\n","805it [06:09,  4.33it/s]\u001b[A\n","806it [06:09,  4.33it/s]\u001b[A\n","807it [06:09,  4.33it/s]\u001b[A\n","808it [06:09,  4.33it/s]\u001b[A\n","809it [06:10,  4.33it/s]\u001b[A\n","810it [06:10,  4.33it/s]\u001b[A\n","811it [06:10,  4.33it/s]\u001b[A\n","812it [06:10,  4.33it/s]\u001b[A\n","813it [06:11,  4.33it/s]\u001b[A\n","814it [06:11,  4.33it/s]\u001b[A\n","815it [06:11,  4.33it/s]\u001b[A\n","816it [06:11,  4.33it/s]\u001b[A\n","817it [06:12,  4.33it/s]\u001b[A\n","818it [06:12,  4.33it/s]\u001b[A\n","819it [06:12,  4.33it/s]\u001b[A\n","820it [06:12,  4.33it/s]\u001b[A\n","821it [06:12,  4.33it/s]\u001b[A\n","822it [06:13,  4.33it/s]\u001b[A\n","823it [06:13,  4.33it/s]\u001b[A\n","824it [06:13,  4.33it/s]\u001b[A\n","825it [06:13,  4.33it/s]\u001b[A\n","826it [06:14,  4.33it/s]\u001b[A\n","827it [06:14,  4.33it/s]\u001b[A\n","828it [06:14,  4.33it/s]\u001b[A\n","829it [06:14,  4.33it/s]\u001b[A\n","830it [06:15,  4.33it/s]\u001b[A\n","831it [06:15,  4.33it/s]\u001b[A\n","832it [06:15,  4.33it/s]\u001b[A\n","833it [06:15,  4.33it/s]\u001b[A\n","834it [06:15,  4.33it/s]\u001b[A\n","835it [06:16,  4.33it/s]\u001b[A\n","836it [06:16,  4.33it/s]\u001b[A\n","837it [06:16,  4.33it/s]\u001b[A\n","838it [06:16,  4.33it/s]\u001b[A\n","839it [06:17,  4.33it/s]\u001b[A\n","840it [06:17,  4.33it/s]\u001b[A\n","841it [06:17,  4.33it/s]\u001b[A\n","842it [06:17,  4.33it/s]\u001b[A\n","843it [06:18,  4.33it/s]\u001b[A\n","844it [06:18,  4.33it/s]\u001b[A\n","845it [06:18,  4.33it/s]\u001b[A\n","846it [06:18,  4.33it/s]\u001b[A\n","847it [06:18,  4.33it/s]\u001b[A\n","848it [06:19,  4.33it/s]\u001b[A\n","849it [06:19,  4.33it/s]\u001b[A\n","850it [06:19,  4.33it/s]\u001b[A\n","851it [06:19,  4.33it/s]\u001b[A\n","852it [06:20,  4.33it/s]\u001b[A\n","853it [06:20,  4.33it/s]\u001b[A\n","854it [06:20,  4.33it/s]\u001b[A\n","855it [06:20,  4.33it/s]\u001b[A\n","856it [06:21,  4.33it/s]\u001b[A\n","857it [06:21,  4.32it/s]\u001b[A\n","858it [06:21,  4.33it/s]\u001b[A\n","859it [06:21,  4.33it/s]\u001b[A\n","860it [06:21,  4.33it/s]\u001b[A\n","861it [06:22,  4.33it/s]\u001b[A\n","862it [06:22,  4.33it/s]\u001b[A\n","863it [06:22,  4.33it/s]\u001b[A\n","864it [06:22,  4.33it/s]\u001b[A\n","865it [06:23,  4.33it/s]\u001b[A\n","866it [06:23,  4.33it/s]\u001b[A\n","867it [06:23,  4.33it/s]\u001b[A\n","868it [06:23,  4.33it/s]\u001b[A\n","869it [06:24,  4.33it/s]\u001b[A\n","870it [06:24,  4.33it/s]\u001b[A\n","871it [06:24,  4.33it/s]\u001b[A\n","872it [06:24,  4.33it/s]\u001b[A\n","873it [06:24,  4.33it/s]\u001b[A\n","874it [06:25,  4.32it/s]\u001b[A\n","875it [06:25,  4.33it/s]\u001b[A\n","876it [06:25,  4.33it/s]\u001b[A\n","877it [06:25,  4.33it/s]\u001b[A\n","878it [06:26,  4.33it/s]\u001b[A\n","879it [06:26,  4.33it/s]\u001b[A\n","880it [06:26,  4.33it/s]\u001b[A\n","881it [06:26,  4.33it/s]\u001b[A\n","882it [06:27,  4.33it/s]\u001b[A\n","883it [06:27,  4.33it/s]\u001b[A\n","884it [06:27,  4.33it/s]\u001b[A\n","885it [06:27,  4.33it/s]\u001b[A\n","886it [06:27,  4.33it/s]\u001b[A\n","887it [06:28,  4.33it/s]\u001b[A\n","888it [06:28,  4.33it/s]\u001b[A\n","889it [06:28,  4.33it/s]\u001b[A\n","890it [06:28,  4.33it/s]\u001b[A\n","891it [06:29,  4.33it/s]\u001b[A\n","892it [06:29,  4.33it/s]\u001b[A\n","893it [06:29,  4.33it/s]\u001b[A\n","894it [06:29,  4.33it/s]\u001b[A\n","895it [06:30,  4.33it/s]\u001b[A\n","896it [06:30,  4.33it/s]\u001b[A\n","897it [06:30,  4.33it/s]\u001b[A\n","898it [06:30,  4.33it/s]\u001b[A\n","899it [06:30,  4.33it/s]\u001b[A\n","900it [06:31,  4.33it/s]\u001b[A\n","901it [06:31,  4.33it/s]\u001b[A\n","902it [06:31,  4.33it/s]\u001b[A\n","903it [06:31,  4.33it/s]\u001b[A\n","904it [06:32,  4.33it/s]\u001b[A\n","905it [06:32,  4.33it/s]\u001b[A\n","906it [06:32,  4.33it/s]\u001b[A\n","907it [06:32,  4.32it/s]\u001b[A\n","908it [06:33,  4.33it/s]\u001b[A\n","909it [06:33,  4.32it/s]\u001b[A\n","910it [06:33,  4.32it/s]\u001b[A\n","911it [06:33,  4.33it/s]\u001b[A\n","912it [06:33,  4.33it/s]\u001b[A\n","913it [06:34,  4.33it/s]\u001b[A\n","914it [06:34,  4.33it/s]\u001b[A\n","915it [06:34,  4.33it/s]\u001b[A\n","916it [06:34,  4.33it/s]\u001b[A\n","917it [06:35,  4.33it/s]\u001b[A\n","918it [06:35,  4.33it/s]\u001b[A\n","919it [06:35,  4.33it/s]\u001b[A\n","920it [06:35,  4.33it/s]\u001b[A\n","921it [06:36,  4.33it/s]\u001b[A\n","922it [06:36,  4.33it/s]\u001b[A\n","923it [06:36,  4.33it/s]\u001b[A\n","924it [06:36,  4.33it/s]\u001b[A\n","Epoch:  60% 149/250 [07:59<02:49,  1.68s/it]\n","926it [07:19, 12.81s/it]\u001b[A\n","927it [07:19,  9.04s/it]\u001b[A\n","928it [07:19,  6.39s/it]\u001b[A\n","929it [07:19,  4.55s/it]\u001b[A\n","930it [07:20,  3.25s/it]\u001b[A\n","931it [07:20,  2.35s/it]\u001b[A\n","932it [07:20,  1.71s/it]\u001b[A\n","933it [07:20,  1.27s/it]\u001b[A\n","934it [07:21,  1.05it/s]\u001b[A\n","935it [07:21,  1.35it/s]\u001b[A\n","936it [07:21,  1.71it/s]\u001b[A\n","937it [07:21,  2.08it/s]\u001b[A\n","938it [07:21,  2.47it/s]\u001b[A\n","939it [07:22,  2.83it/s]\u001b[A\n","940it [07:22,  3.16it/s]\u001b[A\n","941it [07:22,  3.44it/s]\u001b[A\n","942it [07:22,  3.67it/s]\u001b[A\n","943it [07:23,  3.84it/s]\u001b[A\n","944it [07:23,  3.98it/s]\u001b[A\n","945it [07:23,  4.08it/s]\u001b[A\n","946it [07:23,  4.15it/s]\u001b[A\n","947it [07:24,  4.20it/s]\u001b[A\n","948it [07:24,  4.24it/s]\u001b[A\n","949it [07:24,  4.27it/s]\u001b[A\n","950it [07:24,  4.28it/s]\u001b[A\n","951it [07:24,  4.30it/s]\u001b[A\n","952it [07:25,  4.31it/s]\u001b[A\n","953it [07:25,  4.32it/s]\u001b[A\n","954it [07:25,  4.32it/s]\u001b[A\n","955it [07:25,  4.33it/s]\u001b[A\n","956it [07:26,  4.33it/s]\u001b[A\n","957it [07:26,  4.33it/s]\u001b[A\n","958it [07:26,  4.33it/s]\u001b[A\n","959it [07:26,  4.33it/s]\u001b[A\n","960it [07:27,  4.33it/s]\u001b[A\n","961it [07:27,  4.33it/s]\u001b[A\n","962it [07:27,  4.33it/s]\u001b[A\n","963it [07:27,  4.33it/s]\u001b[A\n","964it [07:27,  4.33it/s]\u001b[A\n","965it [07:28,  4.34it/s]\u001b[A\n","966it [07:28,  4.33it/s]\u001b[A\n","967it [07:28,  4.33it/s]\u001b[A\n","968it [07:28,  4.33it/s]\u001b[A\n","969it [07:29,  4.33it/s]\u001b[A\n","970it [07:29,  4.33it/s]\u001b[A\n","971it [07:29,  4.33it/s]\u001b[A\n","972it [07:29,  4.33it/s]\u001b[A\n","973it [07:30,  4.33it/s]\u001b[A\n","974it [07:30,  4.33it/s]\u001b[A\n","975it [07:30,  4.33it/s]\u001b[A\n","976it [07:30,  4.33it/s]\u001b[A\n","977it [07:30,  4.33it/s]\u001b[A\n","978it [07:31,  4.33it/s]\u001b[A\n","979it [07:31,  4.33it/s]\u001b[A\n","980it [07:31,  4.33it/s]\u001b[A\n","981it [07:31,  4.33it/s]\u001b[A\n","982it [07:32,  4.33it/s]\u001b[A\n","983it [07:32,  4.33it/s]\u001b[A\n","984it [07:32,  4.33it/s]\u001b[A\n","985it [07:32,  4.33it/s]\u001b[A\n","986it [07:33,  4.33it/s]\u001b[A\n","987it [07:33,  4.33it/s]\u001b[A\n","988it [07:33,  4.33it/s]\u001b[A\n","989it [07:33,  4.33it/s]\u001b[A\n","990it [07:33,  4.33it/s]\u001b[A\n","991it [07:34,  4.33it/s]\u001b[A\n","992it [07:34,  4.33it/s]\u001b[A\n","993it [07:34,  4.33it/s]\u001b[A\n","994it [07:34,  4.33it/s]\u001b[A\n","995it [07:35,  4.33it/s]\u001b[A\n","996it [07:35,  4.33it/s]\u001b[A\n","997it [07:35,  4.33it/s]\u001b[A\n","998it [07:35,  4.33it/s]\u001b[A\n","999it [07:36,  4.33it/s]\u001b[A\n","1000it [07:36,  4.32it/s]\u001b[A\n","1001it [07:36,  4.32it/s]\u001b[A\n","1002it [07:36,  4.32it/s]\u001b[A\n","1003it [07:36,  4.32it/s]\u001b[A\n","1004it [07:37,  4.33it/s]\u001b[A\n","1005it [07:37,  4.33it/s]\u001b[A\n","1006it [07:37,  4.33it/s]\u001b[A\n","1007it [07:37,  4.33it/s]\u001b[A\n","1008it [07:38,  4.33it/s]\u001b[A\n","1009it [07:38,  4.33it/s]\u001b[A\n","1010it [07:38,  4.33it/s]\u001b[A\n","1011it [07:38,  4.33it/s]\u001b[A\n","1012it [07:39,  4.33it/s]\u001b[A\n","1013it [07:39,  4.33it/s]\u001b[A\n","1014it [07:39,  4.33it/s]\u001b[A\n","1015it [07:39,  4.33it/s]\u001b[A\n","1016it [07:39,  4.33it/s]\u001b[A\n","1017it [07:40,  4.33it/s]\u001b[A\n","1018it [07:40,  4.33it/s]\u001b[A\n","1019it [07:40,  4.33it/s]\u001b[A\n","1020it [07:40,  4.33it/s]\u001b[A\n","1021it [07:41,  4.33it/s]\u001b[A\n","1022it [07:41,  4.33it/s]\u001b[A\n","1023it [07:41,  4.33it/s]\u001b[A\n","1024it [07:41,  4.33it/s]\u001b[A\n","1025it [07:42,  4.33it/s]\u001b[A\n","1026it [07:42,  4.33it/s]\u001b[A\n","1027it [07:42,  4.33it/s]\u001b[A\n","1028it [07:42,  4.33it/s]\u001b[A\n","1029it [07:42,  4.33it/s]\u001b[A\n","1030it [07:43,  4.33it/s]\u001b[A\n","1031it [07:43,  4.33it/s]\u001b[A\n","1032it [07:43,  4.33it/s]\u001b[A\n","1033it [07:43,  4.33it/s]\u001b[A\n","1034it [07:44,  4.33it/s]\u001b[A\n","1035it [07:44,  4.33it/s]\u001b[A\n","1036it [07:44,  4.33it/s]\u001b[A\n","1037it [07:44,  4.33it/s]\u001b[A\n","1038it [07:45,  4.33it/s]\u001b[A\n","1039it [07:45,  4.33it/s]\u001b[A\n","1040it [07:45,  4.33it/s]\u001b[A\n","1041it [07:45,  4.33it/s]\u001b[A\n","1042it [07:45,  4.33it/s]\u001b[A\n","1043it [07:46,  4.33it/s]\u001b[A\n","1044it [07:46,  4.33it/s]\u001b[A\n","1045it [07:46,  4.33it/s]\u001b[A\n","1046it [07:46,  4.33it/s]\u001b[A\n","1047it [07:47,  4.33it/s]\u001b[A\n","1048it [07:47,  4.33it/s]\u001b[A\n","1049it [07:47,  4.33it/s]\u001b[A\n","1050it [07:47,  4.33it/s]\u001b[A\n","1051it [07:48,  4.33it/s]\u001b[A\n","1052it [07:48,  4.33it/s]\u001b[A\n","1053it [07:48,  4.33it/s]\u001b[A\n","1054it [07:48,  4.33it/s]\u001b[A\n","1055it [07:48,  4.33it/s]\u001b[A\n","1056it [07:49,  4.33it/s]\u001b[A\n","1057it [07:49,  4.34it/s]\u001b[A\n","1058it [07:49,  4.33it/s]\u001b[A\n","1059it [07:49,  4.33it/s]\u001b[A\n","1060it [07:50,  4.33it/s]\u001b[A\n","1061it [07:50,  4.33it/s]\u001b[A\n","1062it [07:50,  4.33it/s]\u001b[A\n","1063it [07:50,  4.33it/s]\u001b[A\n","1064it [07:51,  4.33it/s]\u001b[A\n","1065it [07:51,  4.33it/s]\u001b[A\n","1066it [07:51,  4.33it/s]\u001b[A\n","1067it [07:51,  4.33it/s]\u001b[A\n","1068it [07:51,  4.33it/s]\u001b[A\n","1069it [07:52,  4.33it/s]\u001b[A\n","1070it [07:52,  4.33it/s]\u001b[A\n","1071it [07:52,  4.33it/s]\u001b[A\n","1072it [07:52,  4.33it/s]\u001b[A\n","1073it [07:53,  4.33it/s]\u001b[A\n","1074it [07:53,  4.33it/s]\u001b[A\n","1075it [07:53,  4.33it/s]\u001b[A\n","1076it [07:53,  4.33it/s]\u001b[A\n","1077it [07:54,  4.33it/s]\u001b[A\n","1078it [07:54,  4.33it/s]\u001b[A\n","1079it [07:54,  4.33it/s]\u001b[A\n","1080it [07:54,  4.33it/s]\u001b[A\n","1081it [07:54,  4.33it/s]\u001b[A\n","1082it [07:55,  4.33it/s]\u001b[A\n","1083it [07:55,  4.33it/s]\u001b[A\n","1084it [07:55,  4.33it/s]\u001b[A\n","1085it [07:55,  4.33it/s]\u001b[A\n","1086it [07:56,  4.33it/s]\u001b[A\n","1087it [07:56,  4.33it/s]\u001b[A\n","1088it [07:56,  4.33it/s]\u001b[A\n","1089it [07:56,  4.33it/s]\u001b[A\n","1090it [07:57,  4.33it/s]\u001b[A\n","1091it [07:57,  4.33it/s]\u001b[A\n","1092it [07:57,  4.33it/s]\u001b[A\n","1093it [07:57,  4.33it/s]\u001b[A\n","1094it [07:57,  4.33it/s]\u001b[A\n","1095it [07:58,  4.33it/s]\u001b[A\n","1096it [07:58,  4.33it/s]\u001b[A\n","1097it [07:58,  4.33it/s]\u001b[A\n","1098it [07:58,  4.33it/s]\u001b[A\n","1099it [07:59,  4.33it/s]\u001b[A\n","1100it [07:59,  4.33it/s]\u001b[A\n","1101it [07:59,  4.32it/s]\u001b[A\n","1102it [07:59,  4.32it/s]\u001b[A\n","1103it [08:00,  4.32it/s]\u001b[A\n","1104it [08:00,  4.33it/s]\u001b[A\n","1105it [08:00,  4.32it/s]\u001b[A\n","1106it [08:00,  4.32it/s]\u001b[A\n","1107it [08:00,  4.32it/s]\u001b[A\n","1108it [08:01,  4.32it/s]\u001b[A\n","1109it [08:01,  4.32it/s]\u001b[A\n","Epoch:  70% 174/250 [09:24<02:08,  1.69s/it]\n","1111it [08:43, 12.83s/it]\u001b[A\n","1112it [08:44,  9.05s/it]\u001b[A\n","1113it [08:44,  6.41s/it]\u001b[A\n","1114it [08:44,  4.55s/it]\u001b[A\n","1115it [08:44,  3.26s/it]\u001b[A\n","1116it [08:45,  2.35s/it]\u001b[A\n","1117it [08:45,  1.71s/it]\u001b[A\n","1118it [08:45,  1.27s/it]\u001b[A\n","1119it [08:45,  1.04it/s]\u001b[A\n","1120it [08:45,  1.35it/s]\u001b[A\n","1121it [08:46,  1.70it/s]\u001b[A\n","1122it [08:46,  2.08it/s]\u001b[A\n","1123it [08:46,  2.47it/s]\u001b[A\n","1124it [08:46,  2.83it/s]\u001b[A\n","1125it [08:47,  3.16it/s]\u001b[A\n","1126it [08:47,  3.44it/s]\u001b[A\n","1127it [08:47,  3.66it/s]\u001b[A\n","1128it [08:47,  3.84it/s]\u001b[A\n","1129it [08:48,  3.98it/s]\u001b[A\n","1130it [08:48,  4.08it/s]\u001b[A\n","1131it [08:48,  4.15it/s]\u001b[A\n","1132it [08:48,  4.20it/s]\u001b[A\n","1133it [08:48,  4.24it/s]\u001b[A\n","1134it [08:49,  4.26it/s]\u001b[A\n","1135it [08:49,  4.28it/s]\u001b[A\n","1136it [08:49,  4.30it/s]\u001b[A\n","1137it [08:49,  4.31it/s]\u001b[A\n","1138it [08:50,  4.31it/s]\u001b[A\n","1139it [08:50,  4.32it/s]\u001b[A\n","1140it [08:50,  4.32it/s]\u001b[A\n","1141it [08:50,  4.32it/s]\u001b[A\n","1142it [08:51,  4.32it/s]\u001b[A\n","1143it [08:51,  4.32it/s]\u001b[A\n","1144it [08:51,  4.32it/s]\u001b[A\n","1145it [08:51,  4.32it/s]\u001b[A\n","1146it [08:51,  4.33it/s]\u001b[A\n","1147it [08:52,  4.33it/s]\u001b[A\n","1148it [08:52,  4.33it/s]\u001b[A\n","1149it [08:52,  4.33it/s]\u001b[A\n","1150it [08:52,  4.33it/s]\u001b[A\n","1151it [08:53,  4.33it/s]\u001b[A\n","1152it [08:53,  4.33it/s]\u001b[A\n","1153it [08:53,  4.33it/s]\u001b[A\n","1154it [08:53,  4.32it/s]\u001b[A\n","1155it [08:54,  4.33it/s]\u001b[A\n","1156it [08:54,  4.33it/s]\u001b[A\n","1157it [08:54,  4.33it/s]\u001b[A\n","1158it [08:54,  4.33it/s]\u001b[A\n","1159it [08:54,  4.33it/s]\u001b[A\n","1160it [08:55,  4.33it/s]\u001b[A\n","1161it [08:55,  4.33it/s]\u001b[A\n","1162it [08:55,  4.33it/s]\u001b[A\n","1163it [08:55,  4.33it/s]\u001b[A\n","1164it [08:56,  4.33it/s]\u001b[A\n","1165it [08:56,  4.33it/s]\u001b[A\n","1166it [08:56,  4.33it/s]\u001b[A\n","1167it [08:56,  4.33it/s]\u001b[A\n","1168it [08:57,  4.33it/s]\u001b[A\n","1169it [08:57,  4.33it/s]\u001b[A\n","1170it [08:57,  4.33it/s]\u001b[A\n","1171it [08:57,  4.33it/s]\u001b[A\n","1172it [08:57,  4.33it/s]\u001b[A\n","1173it [08:58,  4.33it/s]\u001b[A\n","1174it [08:58,  4.33it/s]\u001b[A\n","1175it [08:58,  4.33it/s]\u001b[A\n","1176it [08:58,  4.33it/s]\u001b[A\n","1177it [08:59,  4.33it/s]\u001b[A\n","1178it [08:59,  4.33it/s]\u001b[A\n","1179it [08:59,  4.33it/s]\u001b[A\n","1180it [08:59,  4.33it/s]\u001b[A\n","1181it [09:00,  4.33it/s]\u001b[A\n","1182it [09:00,  4.33it/s]\u001b[A\n","1183it [09:00,  4.33it/s]\u001b[A\n","1184it [09:00,  4.33it/s]\u001b[A\n","1185it [09:00,  4.33it/s]\u001b[A\n","1186it [09:01,  4.33it/s]\u001b[A\n","1187it [09:01,  4.33it/s]\u001b[A\n","1188it [09:01,  4.33it/s]\u001b[A\n","1189it [09:01,  4.33it/s]\u001b[A\n","1190it [09:02,  4.33it/s]\u001b[A\n","1191it [09:02,  4.33it/s]\u001b[A\n","1192it [09:02,  4.33it/s]\u001b[A\n","1193it [09:02,  4.33it/s]\u001b[A\n","1194it [09:03,  4.33it/s]\u001b[A\n","1195it [09:03,  4.33it/s]\u001b[A\n","1196it [09:03,  4.33it/s]\u001b[A\n","1197it [09:03,  4.33it/s]\u001b[A\n","1198it [09:03,  4.33it/s]\u001b[A\n","1199it [09:04,  4.33it/s]\u001b[A\n","1200it [09:04,  4.33it/s]\u001b[A\n","1201it [09:04,  4.33it/s]\u001b[A\n","1202it [09:04,  4.33it/s]\u001b[A\n","1203it [09:05,  4.33it/s]\u001b[A\n","1204it [09:05,  4.33it/s]\u001b[A\n","1205it [09:05,  4.33it/s]\u001b[A\n","1206it [09:05,  4.33it/s]\u001b[A\n","1207it [09:06,  4.33it/s]\u001b[A\n","1208it [09:06,  4.33it/s]\u001b[A\n","1209it [09:06,  4.33it/s]\u001b[A\n","1210it [09:06,  4.33it/s]\u001b[A\n","1211it [09:06,  4.33it/s]\u001b[A\n","1212it [09:07,  4.33it/s]\u001b[A\n","1213it [09:07,  4.33it/s]\u001b[A\n","1214it [09:07,  4.33it/s]\u001b[A\n","1215it [09:07,  4.33it/s]\u001b[A\n","1216it [09:08,  4.33it/s]\u001b[A\n","1217it [09:08,  4.33it/s]\u001b[A\n","1218it [09:08,  4.33it/s]\u001b[A\n","1219it [09:08,  4.33it/s]\u001b[A\n","1220it [09:09,  4.33it/s]\u001b[A\n","1221it [09:09,  4.33it/s]\u001b[A\n","1222it [09:09,  4.33it/s]\u001b[A\n","1223it [09:09,  4.33it/s]\u001b[A\n","1224it [09:09,  4.33it/s]\u001b[A\n","1225it [09:10,  4.33it/s]\u001b[A\n","1226it [09:10,  4.33it/s]\u001b[A\n","1227it [09:10,  4.33it/s]\u001b[A\n","1228it [09:10,  4.33it/s]\u001b[A\n","1229it [09:11,  4.33it/s]\u001b[A\n","1230it [09:11,  4.33it/s]\u001b[A\n","1231it [09:11,  4.33it/s]\u001b[A\n","1232it [09:11,  4.33it/s]\u001b[A\n","1233it [09:12,  4.33it/s]\u001b[A\n","1234it [09:12,  4.33it/s]\u001b[A\n","1235it [09:12,  4.33it/s]\u001b[A\n","1236it [09:12,  4.33it/s]\u001b[A\n","1237it [09:12,  4.33it/s]\u001b[A\n","1238it [09:13,  4.33it/s]\u001b[A\n","1239it [09:13,  4.33it/s]\u001b[A\n","1240it [09:13,  4.33it/s]\u001b[A\n","1241it [09:13,  4.33it/s]\u001b[A\n","1242it [09:14,  4.33it/s]\u001b[A\n","1243it [09:14,  4.33it/s]\u001b[A\n","1244it [09:14,  4.33it/s]\u001b[A\n","1245it [09:14,  4.33it/s]\u001b[A\n","1246it [09:15,  4.33it/s]\u001b[A\n","1247it [09:15,  4.33it/s]\u001b[A\n","1248it [09:15,  4.33it/s]\u001b[A\n","1249it [09:15,  4.33it/s]\u001b[A\n","1250it [09:15,  4.33it/s]\u001b[A\n","1251it [09:16,  4.34it/s]\u001b[A\n","1252it [09:16,  4.34it/s]\u001b[A\n","1253it [09:16,  4.33it/s]\u001b[A\n","1254it [09:16,  4.33it/s]\u001b[A\n","1255it [09:17,  4.33it/s]\u001b[A\n","1256it [09:17,  4.33it/s]\u001b[A\n","1257it [09:17,  4.32it/s]\u001b[A\n","1258it [09:17,  4.33it/s]\u001b[A\n","1259it [09:18,  4.33it/s]\u001b[A\n","1260it [09:18,  4.33it/s]\u001b[A\n","1261it [09:18,  4.33it/s]\u001b[A\n","1262it [09:18,  4.33it/s]\u001b[A\n","1263it [09:18,  4.33it/s]\u001b[A\n","1264it [09:19,  4.33it/s]\u001b[A\n","1265it [09:19,  4.33it/s]\u001b[A\n","1266it [09:19,  4.33it/s]\u001b[A\n","1267it [09:19,  4.32it/s]\u001b[A\n","1268it [09:20,  4.33it/s]\u001b[A\n","1269it [09:20,  4.33it/s]\u001b[A\n","1270it [09:20,  4.33it/s]\u001b[A\n","1271it [09:20,  4.33it/s]\u001b[A\n","1272it [09:21,  4.33it/s]\u001b[A\n","1273it [09:21,  4.33it/s]\u001b[A\n","1274it [09:21,  4.33it/s]\u001b[A\n","1275it [09:21,  4.33it/s]\u001b[A\n","1276it [09:21,  4.33it/s]\u001b[A\n","1277it [09:22,  4.33it/s]\u001b[A\n","1278it [09:22,  4.33it/s]\u001b[A\n","1279it [09:22,  4.33it/s]\u001b[A\n","1280it [09:22,  4.33it/s]\u001b[A\n","1281it [09:23,  4.33it/s]\u001b[A\n","1282it [09:23,  4.33it/s]\u001b[A\n","1283it [09:23,  4.33it/s]\u001b[A\n","1284it [09:23,  4.33it/s]\u001b[A\n","1285it [09:24,  4.33it/s]\u001b[A\n","1286it [09:24,  4.33it/s]\u001b[A\n","1287it [09:24,  4.33it/s]\u001b[A\n","1288it [09:24,  4.33it/s]\u001b[A\n","1289it [09:24,  4.33it/s]\u001b[A\n","1290it [09:25,  4.33it/s]\u001b[A\n","1291it [09:25,  4.33it/s]\u001b[A\n","1292it [09:25,  4.33it/s]\u001b[A\n","1293it [09:25,  4.33it/s]\u001b[A\n","1294it [09:26,  4.33it/s]\u001b[A\n","Epoch:  80% 199/250 [10:49<01:25,  1.68s/it]\n","1296it [10:08, 12.82s/it]\u001b[A\n","1297it [10:08,  9.04s/it]\u001b[A\n","1298it [10:09,  6.40s/it]\u001b[A\n","1299it [10:09,  4.55s/it]\u001b[A\n","1300it [10:09,  3.25s/it]\u001b[A\n","1301it [10:09,  2.35s/it]\u001b[A\n","1302it [10:09,  1.71s/it]\u001b[A\n","1303it [10:10,  1.27s/it]\u001b[A\n","1304it [10:10,  1.05it/s]\u001b[A\n","1305it [10:10,  1.35it/s]\u001b[A\n","1306it [10:10,  1.71it/s]\u001b[A\n","1307it [10:11,  2.08it/s]\u001b[A\n","1308it [10:11,  2.47it/s]\u001b[A\n","1309it [10:11,  2.84it/s]\u001b[A\n","1310it [10:11,  3.16it/s]\u001b[A\n","1311it [10:12,  3.44it/s]\u001b[A\n","1312it [10:12,  3.67it/s]\u001b[A\n","1313it [10:12,  3.84it/s]\u001b[A\n","1314it [10:12,  3.98it/s]\u001b[A\n","1315it [10:12,  4.08it/s]\u001b[A\n","1316it [10:13,  4.15it/s]\u001b[A\n","1317it [10:13,  4.20it/s]\u001b[A\n","1318it [10:13,  4.24it/s]\u001b[A\n","1319it [10:13,  4.27it/s]\u001b[A\n","1320it [10:14,  4.29it/s]\u001b[A\n","1321it [10:14,  4.30it/s]\u001b[A\n","1322it [10:14,  4.30it/s]\u001b[A\n","1323it [10:14,  4.31it/s]\u001b[A\n","1324it [10:15,  4.32it/s]\u001b[A\n","1325it [10:15,  4.32it/s]\u001b[A\n","1326it [10:15,  4.33it/s]\u001b[A\n","1327it [10:15,  4.33it/s]\u001b[A\n","1328it [10:15,  4.33it/s]\u001b[A\n","1329it [10:16,  4.33it/s]\u001b[A\n","1330it [10:16,  4.33it/s]\u001b[A\n","1331it [10:16,  4.33it/s]\u001b[A\n","1332it [10:16,  4.33it/s]\u001b[A\n","1333it [10:17,  4.33it/s]\u001b[A\n","1334it [10:17,  4.33it/s]\u001b[A\n","1335it [10:17,  4.33it/s]\u001b[A\n","1336it [10:17,  4.33it/s]\u001b[A\n","1337it [10:18,  4.33it/s]\u001b[A\n","1338it [10:18,  4.33it/s]\u001b[A\n","1339it [10:18,  4.33it/s]\u001b[A\n","1340it [10:18,  4.33it/s]\u001b[A\n","1341it [10:18,  4.33it/s]\u001b[A\n","1342it [10:19,  4.33it/s]\u001b[A\n","1343it [10:19,  4.33it/s]\u001b[A\n","1344it [10:19,  4.33it/s]\u001b[A\n","1345it [10:19,  4.33it/s]\u001b[A\n","1346it [10:20,  4.33it/s]\u001b[A\n","1347it [10:20,  4.33it/s]\u001b[A\n","1348it [10:20,  4.33it/s]\u001b[A\n","1349it [10:20,  4.33it/s]\u001b[A\n","1350it [10:21,  4.33it/s]\u001b[A\n","1351it [10:21,  4.33it/s]\u001b[A\n","1352it [10:21,  4.33it/s]\u001b[A\n","1353it [10:21,  4.33it/s]\u001b[A\n","1354it [10:21,  4.33it/s]\u001b[A\n","1355it [10:22,  4.33it/s]\u001b[A\n","1356it [10:22,  4.33it/s]\u001b[A\n","1357it [10:22,  4.33it/s]\u001b[A\n","1358it [10:22,  4.33it/s]\u001b[A\n","1359it [10:23,  4.33it/s]\u001b[A\n","1360it [10:23,  4.33it/s]\u001b[A\n","1361it [10:23,  4.33it/s]\u001b[A\n","1362it [10:23,  4.33it/s]\u001b[A\n","1363it [10:24,  4.33it/s]\u001b[A\n","1364it [10:24,  4.33it/s]\u001b[A\n","1365it [10:24,  4.33it/s]\u001b[A\n","1366it [10:24,  4.33it/s]\u001b[A\n","1367it [10:24,  4.33it/s]\u001b[A\n","1368it [10:25,  4.33it/s]\u001b[A\n","1369it [10:25,  4.33it/s]\u001b[A\n","1370it [10:25,  4.33it/s]\u001b[A\n","1371it [10:25,  4.33it/s]\u001b[A\n","1372it [10:26,  4.33it/s]\u001b[A\n","1373it [10:26,  4.33it/s]\u001b[A\n","1374it [10:26,  4.33it/s]\u001b[A\n","1375it [10:26,  4.33it/s]\u001b[A\n","1376it [10:27,  4.33it/s]\u001b[A\n","1377it [10:27,  4.33it/s]\u001b[A\n","1378it [10:27,  4.34it/s]\u001b[A\n","1379it [10:27,  4.34it/s]\u001b[A\n","1380it [10:27,  4.33it/s]\u001b[A\n","1381it [10:28,  4.33it/s]\u001b[A\n","1382it [10:28,  4.33it/s]\u001b[A\n","1383it [10:28,  4.33it/s]\u001b[A\n","1384it [10:28,  4.33it/s]\u001b[A\n","1385it [10:29,  4.33it/s]\u001b[A\n","1386it [10:29,  4.33it/s]\u001b[A\n","1387it [10:29,  4.33it/s]\u001b[A\n","1388it [10:29,  4.33it/s]\u001b[A\n","1389it [10:30,  4.33it/s]\u001b[A\n","1390it [10:30,  4.33it/s]\u001b[A\n","1391it [10:30,  4.33it/s]\u001b[A\n","1392it [10:30,  4.33it/s]\u001b[A\n","1393it [10:30,  4.33it/s]\u001b[A\n","1394it [10:31,  4.33it/s]\u001b[A\n","1395it [10:31,  4.33it/s]\u001b[A\n","1396it [10:31,  4.33it/s]\u001b[A\n","1397it [10:31,  4.33it/s]\u001b[A\n","1398it [10:32,  4.33it/s]\u001b[A\n","1399it [10:32,  4.33it/s]\u001b[A\n","1400it [10:32,  4.33it/s]\u001b[A\n","1401it [10:32,  4.33it/s]\u001b[A\n","1402it [10:33,  4.33it/s]\u001b[A\n","1403it [10:33,  4.33it/s]\u001b[A\n","1404it [10:33,  4.33it/s]\u001b[A\n","1405it [10:33,  4.33it/s]\u001b[A\n","1406it [10:33,  4.33it/s]\u001b[A\n","1407it [10:34,  4.33it/s]\u001b[A\n","1408it [10:34,  4.33it/s]\u001b[A\n","1409it [10:34,  4.33it/s]\u001b[A\n","1410it [10:34,  4.33it/s]\u001b[A\n","1411it [10:35,  4.33it/s]\u001b[A\n","1412it [10:35,  4.33it/s]\u001b[A\n","1413it [10:35,  4.33it/s]\u001b[A\n","1414it [10:35,  4.33it/s]\u001b[A\n","1415it [10:36,  4.33it/s]\u001b[A\n","1416it [10:36,  4.33it/s]\u001b[A\n","1417it [10:36,  4.33it/s]\u001b[A\n","1418it [10:36,  4.33it/s]\u001b[A\n","1419it [10:36,  4.33it/s]\u001b[A\n","1420it [10:37,  4.33it/s]\u001b[A\n","1421it [10:37,  4.33it/s]\u001b[A\n","1422it [10:37,  4.33it/s]\u001b[A\n","1423it [10:37,  4.33it/s]\u001b[A\n","1424it [10:38,  4.33it/s]\u001b[A\n","1425it [10:38,  4.33it/s]\u001b[A\n","1426it [10:38,  4.33it/s]\u001b[A\n","1427it [10:38,  4.33it/s]\u001b[A\n","1428it [10:39,  4.33it/s]\u001b[A\n","1429it [10:39,  4.33it/s]\u001b[A\n","1430it [10:39,  4.33it/s]\u001b[A\n","1431it [10:39,  4.33it/s]\u001b[A\n","1432it [10:39,  4.33it/s]\u001b[A\n","1433it [10:40,  4.33it/s]\u001b[A\n","1434it [10:40,  4.33it/s]\u001b[A\n","1435it [10:40,  4.33it/s]\u001b[A\n","1436it [10:40,  4.33it/s]\u001b[A\n","1437it [10:41,  4.33it/s]\u001b[A\n","1438it [10:41,  4.33it/s]\u001b[A\n","1439it [10:41,  4.33it/s]\u001b[A\n","1440it [10:41,  4.33it/s]\u001b[A\n","1441it [10:42,  4.33it/s]\u001b[A\n","1442it [10:42,  4.33it/s]\u001b[A\n","1443it [10:42,  4.33it/s]\u001b[A\n","1444it [10:42,  4.33it/s]\u001b[A\n","1445it [10:42,  4.33it/s]\u001b[A\n","1446it [10:43,  4.33it/s]\u001b[A\n","1447it [10:43,  4.33it/s]\u001b[A\n","1448it [10:43,  4.33it/s]\u001b[A\n","1449it [10:43,  4.33it/s]\u001b[A\n","1450it [10:44,  4.33it/s]\u001b[A\n","1451it [10:44,  4.33it/s]\u001b[A\n","1452it [10:44,  4.33it/s]\u001b[A\n","1453it [10:44,  4.33it/s]\u001b[A\n","1454it [10:45,  4.33it/s]\u001b[A\n","1455it [10:45,  4.33it/s]\u001b[A\n","1456it [10:45,  4.33it/s]\u001b[A\n","1457it [10:45,  4.33it/s]\u001b[A\n","1458it [10:45,  4.33it/s]\u001b[A\n","1459it [10:46,  4.33it/s]\u001b[A\n","1460it [10:46,  4.33it/s]\u001b[A\n","1461it [10:46,  4.33it/s]\u001b[A\n","1462it [10:46,  4.33it/s]\u001b[A\n","1463it [10:47,  4.33it/s]\u001b[A\n","1464it [10:47,  4.33it/s]\u001b[A\n","1465it [10:47,  4.33it/s]\u001b[A\n","1466it [10:47,  4.33it/s]\u001b[A\n","1467it [10:48,  4.33it/s]\u001b[A\n","1468it [10:48,  4.33it/s]\u001b[A\n","1469it [10:48,  4.33it/s]\u001b[A\n","1470it [10:48,  4.33it/s]\u001b[A\n","1471it [10:48,  4.33it/s]\u001b[A\n","1472it [10:49,  4.33it/s]\u001b[A\n","1473it [10:49,  4.32it/s]\u001b[A\n","1474it [10:49,  4.33it/s]\u001b[A\n","1475it [10:49,  4.32it/s]\u001b[A\n","1476it [10:50,  4.32it/s]\u001b[A\n","1477it [10:50,  4.33it/s]\u001b[A\n","1478it [10:50,  4.33it/s]\u001b[A\n","1479it [10:50,  4.33it/s]\u001b[A\n","Epoch:  90% 224/250 [12:14<00:43,  1.68s/it]\n","1481it [11:33, 12.84s/it]\u001b[A\n","1482it [11:33,  9.06s/it]\u001b[A\n","1483it [11:33,  6.41s/it]\u001b[A\n","1484it [11:34,  4.56s/it]\u001b[A\n","1485it [11:34,  3.26s/it]\u001b[A\n","1486it [11:34,  2.35s/it]\u001b[A\n","1487it [11:34,  1.71s/it]\u001b[A\n","1488it [11:34,  1.27s/it]\u001b[A\n","1489it [11:35,  1.04it/s]\u001b[A\n","1490it [11:35,  1.35it/s]\u001b[A\n","1491it [11:35,  1.70it/s]\u001b[A\n","1492it [11:35,  2.08it/s]\u001b[A\n","1493it [11:36,  2.47it/s]\u001b[A\n","1494it [11:36,  2.83it/s]\u001b[A\n","1495it [11:36,  3.16it/s]\u001b[A\n","1496it [11:36,  3.44it/s]\u001b[A\n","1497it [11:37,  3.66it/s]\u001b[A\n","1498it [11:37,  3.84it/s]\u001b[A\n","1499it [11:37,  3.97it/s]\u001b[A\n","1500it [11:37,  4.07it/s]\u001b[A\n","1501it [11:37,  4.15it/s]\u001b[A\n","1502it [11:38,  4.20it/s]\u001b[A\n","1503it [11:38,  4.24it/s]\u001b[A\n","1504it [11:38,  4.27it/s]\u001b[A\n","1505it [11:38,  4.29it/s]\u001b[A\n","1506it [11:39,  4.30it/s]\u001b[A\n","1507it [11:39,  4.31it/s]\u001b[A\n","1508it [11:39,  4.31it/s]\u001b[A\n","1509it [11:39,  4.31it/s]\u001b[A\n","1510it [11:40,  4.32it/s]\u001b[A\n","1511it [11:40,  4.32it/s]\u001b[A\n","1512it [11:40,  4.32it/s]\u001b[A\n","1513it [11:40,  4.33it/s]\u001b[A\n","1514it [11:40,  4.33it/s]\u001b[A\n","1515it [11:41,  4.33it/s]\u001b[A\n","1516it [11:41,  4.33it/s]\u001b[A\n","1517it [11:41,  4.33it/s]\u001b[A\n","1518it [11:41,  4.33it/s]\u001b[A\n","1519it [11:42,  4.33it/s]\u001b[A\n","1520it [11:42,  4.33it/s]\u001b[A\n","1521it [11:42,  4.33it/s]\u001b[A\n","1522it [11:42,  4.33it/s]\u001b[A\n","1523it [11:43,  4.33it/s]\u001b[A\n","1524it [11:43,  4.33it/s]\u001b[A\n","1525it [11:43,  4.33it/s]\u001b[A\n","1526it [11:43,  4.33it/s]\u001b[A\n","1527it [11:43,  4.33it/s]\u001b[A\n","1528it [11:44,  4.33it/s]\u001b[A\n","1529it [11:44,  4.33it/s]\u001b[A\n","1530it [11:44,  4.33it/s]\u001b[A\n","1531it [11:44,  4.33it/s]\u001b[A\n","1532it [11:45,  4.33it/s]\u001b[A\n","1533it [11:45,  4.33it/s]\u001b[A\n","1534it [11:45,  4.33it/s]\u001b[A\n","1535it [11:45,  4.33it/s]\u001b[A\n","1536it [11:46,  4.33it/s]\u001b[A\n","1537it [11:46,  4.33it/s]\u001b[A\n","1538it [11:46,  4.33it/s]\u001b[A\n","1539it [11:46,  4.33it/s]\u001b[A\n","1540it [11:46,  4.33it/s]\u001b[A\n","1541it [11:47,  4.33it/s]\u001b[A\n","1542it [11:47,  4.33it/s]\u001b[A\n","1543it [11:47,  4.33it/s]\u001b[A\n","1544it [11:47,  4.33it/s]\u001b[A\n","1545it [11:48,  4.33it/s]\u001b[A\n","1546it [11:48,  4.33it/s]\u001b[A\n","1547it [11:48,  4.33it/s]\u001b[A\n","1548it [11:48,  4.33it/s]\u001b[A\n","1549it [11:49,  4.33it/s]\u001b[A\n","1550it [11:49,  4.33it/s]\u001b[A\n","1551it [11:49,  4.33it/s]\u001b[A\n","1552it [11:49,  4.33it/s]\u001b[A\n","1553it [11:49,  4.33it/s]\u001b[A\n","1554it [11:50,  4.33it/s]\u001b[A\n","1555it [11:50,  4.33it/s]\u001b[A\n","1556it [11:50,  4.33it/s]\u001b[A\n","1557it [11:50,  4.33it/s]\u001b[A\n","1558it [11:51,  4.33it/s]\u001b[A\n","1559it [11:51,  4.33it/s]\u001b[A\n","1560it [11:51,  4.33it/s]\u001b[A\n","1561it [11:51,  4.33it/s]\u001b[A\n","1562it [11:52,  4.33it/s]\u001b[A\n","1563it [11:52,  4.33it/s]\u001b[A\n","1564it [11:52,  4.33it/s]\u001b[A\n","1565it [11:52,  4.33it/s]\u001b[A\n","1566it [11:52,  4.33it/s]\u001b[A\n","1567it [11:53,  4.33it/s]\u001b[A\n","1568it [11:53,  4.33it/s]\u001b[A\n","1569it [11:53,  4.33it/s]\u001b[A\n","1570it [11:53,  4.33it/s]\u001b[A\n","1571it [11:54,  4.33it/s]\u001b[A\n","1572it [11:54,  4.33it/s]\u001b[A\n","1573it [11:54,  4.33it/s]\u001b[A\n","1574it [11:54,  4.33it/s]\u001b[A\n","1575it [11:55,  4.33it/s]\u001b[A\n","1576it [11:55,  4.33it/s]\u001b[A\n","1577it [11:55,  4.33it/s]\u001b[A\n","1578it [11:55,  4.33it/s]\u001b[A\n","1579it [11:55,  4.33it/s]\u001b[A\n","1580it [11:56,  4.33it/s]\u001b[A\n","1581it [11:56,  4.33it/s]\u001b[A\n","1582it [11:56,  4.33it/s]\u001b[A\n","1583it [11:56,  4.33it/s]\u001b[A\n","1584it [11:57,  4.33it/s]\u001b[A\n","1585it [11:57,  4.33it/s]\u001b[A\n","1586it [11:57,  4.33it/s]\u001b[A\n","1587it [11:57,  4.33it/s]\u001b[A\n","1588it [11:58,  4.33it/s]\u001b[A\n","1589it [11:58,  4.33it/s]\u001b[A\n","1590it [11:58,  4.33it/s]\u001b[A\n","1591it [11:58,  4.33it/s]\u001b[A\n","1592it [11:58,  4.33it/s]\u001b[A\n","1593it [11:59,  4.33it/s]\u001b[A\n","1594it [11:59,  4.33it/s]\u001b[A\n","1595it [11:59,  4.33it/s]\u001b[A\n","1596it [11:59,  4.33it/s]\u001b[A\n","1597it [12:00,  4.33it/s]\u001b[A\n","1598it [12:00,  4.33it/s]\u001b[A\n","1599it [12:00,  4.33it/s]\u001b[A\n","1600it [12:00,  4.33it/s]\u001b[A\n","1601it [12:01,  4.33it/s]\u001b[A\n","1602it [12:01,  4.33it/s]\u001b[A\n","1603it [12:01,  4.33it/s]\u001b[A\n","1604it [12:01,  4.33it/s]\u001b[A\n","1605it [12:01,  4.34it/s]\u001b[A\n","1606it [12:02,  4.34it/s]\u001b[A\n","1607it [12:02,  4.33it/s]\u001b[A\n","1608it [12:02,  4.33it/s]\u001b[A\n","1609it [12:02,  4.34it/s]\u001b[A\n","1610it [12:03,  4.33it/s]\u001b[A\n","1611it [12:03,  4.33it/s]\u001b[A\n","1612it [12:03,  4.33it/s]\u001b[A\n","1613it [12:03,  4.33it/s]\u001b[A\n","1614it [12:04,  4.33it/s]\u001b[A\n","1615it [12:04,  4.33it/s]\u001b[A\n","1616it [12:04,  4.33it/s]\u001b[A\n","1617it [12:04,  4.33it/s]\u001b[A\n","1618it [12:04,  4.33it/s]\u001b[A\n","1619it [12:05,  4.33it/s]\u001b[A\n","1620it [12:05,  4.33it/s]\u001b[A\n","1621it [12:05,  4.33it/s]\u001b[A\n","1622it [12:05,  4.33it/s]\u001b[A\n","1623it [12:06,  4.33it/s]\u001b[A\n","1624it [12:06,  4.33it/s]\u001b[A\n","1625it [12:06,  4.33it/s]\u001b[A\n","1626it [12:06,  4.33it/s]\u001b[A\n","1627it [12:07,  4.33it/s]\u001b[A\n","1628it [12:07,  4.33it/s]\u001b[A\n","1629it [12:07,  4.33it/s]\u001b[A\n","1630it [12:07,  4.33it/s]\u001b[A\n","1631it [12:07,  4.33it/s]\u001b[A\n","1632it [12:08,  4.33it/s]\u001b[A\n","1633it [12:08,  4.33it/s]\u001b[A\n","1634it [12:08,  4.33it/s]\u001b[A\n","1635it [12:08,  4.33it/s]\u001b[A\n","1636it [12:09,  4.33it/s]\u001b[A\n","1637it [12:09,  4.33it/s]\u001b[A\n","1638it [12:09,  4.33it/s]\u001b[A\n","1639it [12:09,  4.33it/s]\u001b[A\n","1640it [12:10,  4.33it/s]\u001b[A\n","1641it [12:10,  4.33it/s]\u001b[A\n","1642it [12:10,  4.33it/s]\u001b[A\n","1643it [12:10,  4.33it/s]\u001b[A\n","1644it [12:10,  4.33it/s]\u001b[A\n","1645it [12:11,  4.33it/s]\u001b[A\n","1646it [12:11,  4.33it/s]\u001b[A\n","1647it [12:11,  4.33it/s]\u001b[A\n","1648it [12:11,  4.33it/s]\u001b[A\n","1649it [12:12,  4.33it/s]\u001b[A\n","1650it [12:12,  4.33it/s]\u001b[A\n","1651it [12:12,  4.33it/s]\u001b[A\n","1652it [12:12,  4.33it/s]\u001b[A\n","1653it [12:13,  4.33it/s]\u001b[A\n","1654it [12:13,  4.33it/s]\u001b[A\n","1655it [12:13,  4.33it/s]\u001b[A\n","1656it [12:13,  4.33it/s]\u001b[A\n","1657it [12:13,  4.34it/s]\u001b[A\n","1658it [12:14,  4.34it/s]\u001b[A\n","1659it [12:14,  4.34it/s]\u001b[A\n","1660it [12:14,  4.34it/s]\u001b[A\n","1661it [12:14,  4.34it/s]\u001b[A\n","1662it [12:15,  4.34it/s]\u001b[A\n","1663it [12:15,  4.34it/s]\u001b[A\n","1664it [12:15,  4.34it/s]\u001b[A\n","Epoch: 100% 249/250 [13:38<00:01,  1.68s/it]\n","1666it [12:57, 12.81s/it]\u001b[A\n","1667it [12:58,  9.04s/it]\u001b[A\n","1668it [12:58,  6.40s/it]\u001b[A\n","1669it [12:58,  4.55s/it]\u001b[A\n","1670it [12:58,  3.25s/it]\u001b[A\n","1671it [12:59,  2.35s/it]\u001b[A\n","1672it [12:59,  1.71s/it]\u001b[A\n","1673it [12:59,  1.27s/it]\u001b[A\n","1674it [12:59,  1.05it/s]\u001b[A\n","1675it [13:00,  1.35it/s]\u001b[A\n","1676it [13:00,  1.71it/s]\u001b[A\n","1677it [13:00,  2.09it/s]\u001b[A\n","1678it [13:00,  2.47it/s]\u001b[A\n","1679it [13:00,  2.84it/s]\u001b[A\n","1680it [13:01,  3.17it/s]\u001b[A\n","1681it [13:01,  3.45it/s]\u001b[A\n","1682it [13:01,  3.67it/s]\u001b[A\n","1683it [13:01,  3.85it/s]\u001b[A\n","1684it [13:02,  3.98it/s]\u001b[A\n","1685it [13:02,  4.08it/s]\u001b[A\n","1686it [13:02,  4.15it/s]\u001b[A\n","1687it [13:02,  4.21it/s]\u001b[A\n","1688it [13:03,  4.25it/s]\u001b[A\n","1689it [13:03,  4.27it/s]\u001b[A\n","1690it [13:03,  4.29it/s]\u001b[A\n","1691it [13:03,  4.30it/s]\u001b[A\n","1692it [13:03,  4.31it/s]\u001b[A\n","1693it [13:04,  4.32it/s]\u001b[A\n","1694it [13:04,  4.32it/s]\u001b[A\n","1695it [13:04,  4.33it/s]\u001b[A\n","1696it [13:04,  4.33it/s]\u001b[A\n","1697it [13:05,  4.33it/s]\u001b[A\n","1698it [13:05,  4.33it/s]\u001b[A\n","1699it [13:05,  4.33it/s]\u001b[A\n","1700it [13:05,  4.33it/s]\u001b[A\n","1701it [13:06,  4.33it/s]\u001b[A\n","1702it [13:06,  4.33it/s]\u001b[A\n","1703it [13:06,  4.33it/s]\u001b[A\n","1704it [13:06,  4.33it/s]\u001b[A\n","1705it [13:06,  4.33it/s]\u001b[A\n","1706it [13:07,  4.33it/s]\u001b[A\n","1707it [13:07,  4.33it/s]\u001b[A\n","1708it [13:07,  4.33it/s]\u001b[A\n","1709it [13:07,  4.33it/s]\u001b[A\n","1710it [13:08,  4.33it/s]\u001b[A\n","1711it [13:08,  4.33it/s]\u001b[A\n","1712it [13:08,  4.33it/s]\u001b[A\n","1713it [13:08,  4.33it/s]\u001b[A\n","1714it [13:09,  4.33it/s]\u001b[A\n","1715it [13:09,  4.33it/s]\u001b[A\n","1716it [13:09,  4.33it/s]\u001b[A\n","1717it [13:09,  4.33it/s]\u001b[A\n","1718it [13:09,  4.33it/s]\u001b[A\n","1719it [13:10,  4.34it/s]\u001b[A\n","1720it [13:10,  4.34it/s]\u001b[A\n","1721it [13:10,  4.34it/s]\u001b[A\n","1722it [13:10,  4.34it/s]\u001b[A\n","1723it [13:11,  4.33it/s]\u001b[A\n","1724it [13:11,  4.33it/s]\u001b[A\n","1725it [13:11,  4.33it/s]\u001b[A\n","1726it [13:11,  4.33it/s]\u001b[A\n","1727it [13:12,  4.33it/s]\u001b[A\n","1728it [13:12,  4.33it/s]\u001b[A\n","1729it [13:12,  4.34it/s]\u001b[A\n","1730it [13:12,  4.34it/s]\u001b[A\n","1731it [13:12,  4.34it/s]\u001b[A\n","1732it [13:13,  4.34it/s]\u001b[A\n","1733it [13:13,  4.33it/s]\u001b[A\n","1734it [13:13,  4.34it/s]\u001b[A\n","1735it [13:13,  4.33it/s]\u001b[A\n","1736it [13:14,  4.33it/s]\u001b[A\n","1737it [13:14,  4.33it/s]\u001b[A\n","1738it [13:14,  4.34it/s]\u001b[A\n","1739it [13:14,  4.34it/s]\u001b[A\n","1740it [13:15,  4.33it/s]\u001b[A\n","1741it [13:15,  4.34it/s]\u001b[A\n","1742it [13:15,  4.34it/s]\u001b[A\n","1743it [13:15,  4.33it/s]\u001b[A\n","1744it [13:15,  4.34it/s]\u001b[A\n","1745it [13:16,  4.33it/s]\u001b[A\n","1746it [13:16,  4.33it/s]\u001b[A\n","1747it [13:16,  4.34it/s]\u001b[A\n","1748it [13:16,  4.34it/s]\u001b[A\n","1749it [13:17,  4.34it/s]\u001b[A\n","1750it [13:17,  4.34it/s]\u001b[A\n","1751it [13:17,  4.34it/s]\u001b[A\n","1752it [13:17,  4.34it/s]\u001b[A\n","1753it [13:18,  4.33it/s]\u001b[A\n","1754it [13:18,  4.33it/s]\u001b[A\n","1755it [13:18,  4.33it/s]\u001b[A\n","1756it [13:18,  4.33it/s]\u001b[A\n","1757it [13:18,  4.33it/s]\u001b[A\n","1758it [13:19,  4.33it/s]\u001b[A\n","1759it [13:19,  4.33it/s]\u001b[A\n","1760it [13:19,  4.33it/s]\u001b[A\n","1761it [13:19,  4.33it/s]\u001b[A\n","1762it [13:20,  4.33it/s]\u001b[A\n","1763it [13:20,  4.33it/s]\u001b[A\n","1764it [13:20,  4.33it/s]\u001b[A\n","1765it [13:20,  4.33it/s]\u001b[A\n","1766it [13:21,  4.33it/s]\u001b[A\n","1767it [13:21,  4.33it/s]\u001b[A\n","1768it [13:21,  4.33it/s]\u001b[A\n","1769it [13:21,  4.34it/s]\u001b[A\n","1770it [13:21,  4.34it/s]\u001b[A\n","1771it [13:22,  4.34it/s]\u001b[A\n","1772it [13:22,  4.34it/s]\u001b[A\n","1773it [13:22,  4.34it/s]\u001b[A\n","1774it [13:22,  4.34it/s]\u001b[A\n","1775it [13:23,  4.33it/s]\u001b[A\n","1776it [13:23,  4.33it/s]\u001b[A\n","1777it [13:23,  4.33it/s]\u001b[A\n","1778it [13:23,  4.33it/s]\u001b[A\n","1779it [13:24,  4.33it/s]\u001b[A\n","1780it [13:24,  4.33it/s]\u001b[A\n","1781it [13:24,  4.33it/s]\u001b[A\n","1782it [13:24,  4.33it/s]\u001b[A\n","1783it [13:24,  4.33it/s]\u001b[A\n","1784it [13:25,  4.33it/s]\u001b[A\n","1785it [13:25,  4.33it/s]\u001b[A\n","1786it [13:25,  4.33it/s]\u001b[A\n","1787it [13:25,  4.33it/s]\u001b[A\n","1788it [13:26,  4.33it/s]\u001b[A\n","1789it [13:26,  4.33it/s]\u001b[A\n","1790it [13:26,  4.34it/s]\u001b[A\n","1791it [13:26,  4.33it/s]\u001b[A\n","1792it [13:27,  4.33it/s]\u001b[A\n","1793it [13:27,  4.33it/s]\u001b[A\n","1794it [13:27,  4.33it/s]\u001b[A\n","1795it [13:27,  4.33it/s]\u001b[A\n","1796it [13:27,  4.33it/s]\u001b[A\n","1797it [13:28,  4.33it/s]\u001b[A\n","1798it [13:28,  4.33it/s]\u001b[A\n","1799it [13:28,  4.33it/s]\u001b[A\n","1800it [13:28,  4.33it/s]\u001b[A\n","1801it [13:29,  4.33it/s]\u001b[A\n","1802it [13:29,  4.34it/s]\u001b[A\n","1803it [13:29,  4.33it/s]\u001b[A\n","1804it [13:29,  4.33it/s]\u001b[A\n","1805it [13:30,  4.33it/s]\u001b[A\n","1806it [13:30,  4.33it/s]\u001b[A\n","1807it [13:30,  4.33it/s]\u001b[A\n","1808it [13:30,  4.34it/s]\u001b[A\n","1809it [13:30,  4.33it/s]\u001b[A\n","1810it [13:31,  4.33it/s]\u001b[A\n","1811it [13:31,  4.33it/s]\u001b[A\n","1812it [13:31,  4.33it/s]\u001b[A\n","1813it [13:31,  4.33it/s]\u001b[A\n","1814it [13:32,  4.33it/s]\u001b[A\n","1815it [13:32,  4.33it/s]\u001b[A\n","1816it [13:32,  4.33it/s]\u001b[A\n","1817it [13:32,  4.34it/s]\u001b[A\n","1818it [13:33,  4.34it/s]\u001b[A\n","1819it [13:33,  4.33it/s]\u001b[A\n","1820it [13:33,  4.33it/s]\u001b[A\n","1821it [13:33,  4.34it/s]\u001b[A\n","1822it [13:33,  4.34it/s]\u001b[A\n","1823it [13:34,  4.34it/s]\u001b[A\n","1824it [13:34,  4.34it/s]\u001b[A\n","1825it [13:34,  4.34it/s]\u001b[A\n","1826it [13:34,  4.33it/s]\u001b[A\n","1827it [13:35,  4.34it/s]\u001b[A\n","1828it [13:35,  4.34it/s]\u001b[A\n","1829it [13:35,  4.34it/s]\u001b[A\n","1830it [13:35,  4.34it/s]\u001b[A\n","1831it [13:36,  4.34it/s]\u001b[A\n","1832it [13:36,  4.33it/s]\u001b[A\n","1833it [13:36,  4.33it/s]\u001b[A\n","1834it [13:36,  4.34it/s]\u001b[A\n","1835it [13:36,  4.34it/s]\u001b[A\n","1836it [13:37,  4.33it/s]\u001b[A\n","1837it [13:37,  4.33it/s]\u001b[A\n","1838it [13:37,  4.34it/s]\u001b[A\n","1839it [13:37,  4.34it/s]\u001b[A\n","1840it [13:38,  4.34it/s]\u001b[A\n","1841it [13:38,  4.34it/s]\u001b[A\n","1842it [13:38,  4.34it/s]\u001b[A\n","1843it [13:38,  4.33it/s]\u001b[A\n","1844it [13:39,  4.34it/s]\u001b[A\n","1845it [13:39,  4.33it/s]\u001b[A\n","1846it [13:39,  4.33it/s]\u001b[A\n","1847it [13:39,  4.33it/s]\u001b[A\n","1848it [13:39,  4.33it/s]\u001b[A\n","1849it [13:40,  4.34it/s]\u001b[A\n","Epoch: 100% 250/250 [14:23<00:00,  3.45s/it]\n","03/14/2022 09:31:57 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/14/2022 09:32:11 - INFO - __main__ -   *** Validate ***\n","\n","1851it [13:54,  4.30s/it]\u001b[A\n","1852it [13:54,  3.08s/it]\u001b[A\n","1853it [13:54,  2.22s/it]\u001b[A\n","1854it [13:54,  1.63s/it]\u001b[A\n","1855it [13:55,  1.21s/it]\u001b[A\n","1856it [13:55,  1.09it/s]\u001b[A\n","1857it [13:55,  1.41it/s]\u001b[A\n","1858it [13:55,  1.77it/s]\u001b[A\n","1859it [13:56,  2.15it/s]\u001b[A\n","1860it [13:56,  2.53it/s]\u001b[A\n","1861it [13:56,  2.89it/s]\u001b[A\n","1862it [13:56,  3.21it/s]\u001b[A\n","1863it [13:56,  3.48it/s]\u001b[A\n","1864it [13:57,  3.70it/s]\u001b[A\n","1865it [13:57,  3.87it/s]\u001b[A\n","1866it [13:57,  3.99it/s]\u001b[A\n","1867it [13:57,  4.09it/s]\u001b[A\n","1868it [13:58,  4.16it/s]\u001b[A\n","1869it [13:58,  4.21it/s]\u001b[A\n","1870it [13:58,  4.24it/s]\u001b[A\n","1871it [13:58,  4.27it/s]\u001b[A\n","1872it [13:59,  4.29it/s]\u001b[A\n","1873it [13:59,  4.31it/s]\u001b[A\n","1874it [13:59,  4.31it/s]\u001b[A\n","1875it [13:59,  4.32it/s]\u001b[A\n","1876it [13:59,  4.32it/s]\u001b[A\n","1877it [14:00,  4.32it/s]\u001b[A\n","1878it [14:00,  4.32it/s]\u001b[A\n","1879it [14:00,  4.32it/s]\u001b[A\n","1880it [14:00,  4.33it/s]\u001b[A\n","1881it [14:01,  4.33it/s]\u001b[A\n","1882it [14:01,  4.33it/s]\u001b[A\n","1883it [14:01,  4.33it/s]\u001b[A\n","1884it [14:01,  4.33it/s]\u001b[A\n","1885it [14:02,  4.33it/s]\u001b[A\n","1886it [14:02,  4.33it/s]\u001b[A\n","1887it [14:02,  4.33it/s]\u001b[A\n","1888it [14:02,  4.33it/s]\u001b[A\n","1889it [14:02,  4.33it/s]\u001b[A\n","1890it [14:03,  4.33it/s]\u001b[A\n","1891it [14:03,  4.33it/s]\u001b[A\n","1892it [14:03,  4.33it/s]\u001b[A\n","1893it [14:03,  4.33it/s]\u001b[A\n","1894it [14:04,  4.33it/s]\u001b[A\n","1895it [14:04,  4.33it/s]\u001b[A\n","1896it [14:04,  4.33it/s]\u001b[A\n","1897it [14:04,  4.33it/s]\u001b[A\n","1898it [14:05,  4.33it/s]\u001b[A\n","1899it [14:05,  4.33it/s]\u001b[A\n","1900it [14:05,  4.33it/s]\u001b[A\n","1901it [14:05,  4.33it/s]\u001b[A\n","1902it [14:05,  4.33it/s]\u001b[A\n","1903it [14:06,  4.33it/s]\u001b[A\n","1904it [14:06,  4.33it/s]\u001b[A\n","1905it [14:06,  4.33it/s]\u001b[A\n","1906it [14:06,  4.33it/s]\u001b[A\n","1907it [14:07,  4.33it/s]\u001b[A\n","1908it [14:07,  4.33it/s]\u001b[A\n","1909it [14:07,  4.33it/s]\u001b[A\n","1910it [14:07,  4.33it/s]\u001b[A\n","1911it [14:08,  4.33it/s]\u001b[A\n","1912it [14:08,  4.33it/s]\u001b[A\n","1913it [14:08,  4.33it/s]\u001b[A\n","1914it [14:08,  4.33it/s]\u001b[A\n","1915it [14:08,  4.33it/s]\u001b[A\n","1916it [14:09,  4.33it/s]\u001b[A\n","1917it [14:09,  4.33it/s]\u001b[A\n","1918it [14:09,  4.33it/s]\u001b[A\n","1919it [14:09,  4.33it/s]\u001b[A\n","1920it [14:10,  4.33it/s]\u001b[A\n","1921it [14:10,  4.33it/s]\u001b[A\n","1922it [14:10,  4.33it/s]\u001b[A\n","1923it [14:10,  4.33it/s]\u001b[A\n","1924it [14:11,  4.33it/s]\u001b[A\n","1925it [14:11,  4.33it/s]\u001b[A\n","1926it [14:11,  4.33it/s]\u001b[A\n","1927it [14:11,  4.33it/s]\u001b[A\n","1928it [14:11,  4.33it/s]\u001b[A\n","1929it [14:12,  4.33it/s]\u001b[A\n","1930it [14:12,  4.33it/s]\u001b[A\n","1931it [14:12,  4.33it/s]\u001b[A\n","1932it [14:12,  4.33it/s]\u001b[A\n","1933it [14:13,  4.33it/s]\u001b[A\n","1934it [14:13,  4.33it/s]\u001b[A\n","1935it [14:13,  4.33it/s]\u001b[A\n","1936it [14:13,  4.33it/s]\u001b[A\n","1937it [14:14,  4.33it/s]\u001b[A\n","1938it [14:14,  4.33it/s]\u001b[A\n","1939it [14:14,  4.33it/s]\u001b[A\n","1940it [14:14,  4.33it/s]\u001b[A\n","1941it [14:14,  4.33it/s]\u001b[A\n","1942it [14:15,  4.33it/s]\u001b[A\n","1943it [14:15,  4.33it/s]\u001b[A\n","1944it [14:15,  4.33it/s]\u001b[A\n","1945it [14:15,  4.33it/s]\u001b[A\n","1946it [14:16,  4.33it/s]\u001b[A\n","1947it [14:16,  4.33it/s]\u001b[A\n","1948it [14:16,  4.33it/s]\u001b[A\n","1949it [14:16,  4.33it/s]\u001b[A\n","1950it [14:17,  4.33it/s]\u001b[A\n","1951it [14:17,  4.33it/s]\u001b[A\n","1952it [14:17,  4.33it/s]\u001b[A\n","1953it [14:17,  4.33it/s]\u001b[A\n","1954it [14:17,  4.33it/s]\u001b[A\n","1955it [14:18,  4.33it/s]\u001b[A\n","1956it [14:18,  4.33it/s]\u001b[A\n","1957it [14:18,  4.33it/s]\u001b[A\n","1958it [14:18,  4.33it/s]\u001b[A\n","1959it [14:19,  4.33it/s]\u001b[A\n","1960it [14:19,  4.33it/s]\u001b[A\n","1961it [14:19,  4.33it/s]\u001b[A\n","1962it [14:19,  4.33it/s]\u001b[A\n","1963it [14:20,  4.33it/s]\u001b[A\n","1964it [14:20,  4.33it/s]\u001b[A\n","1965it [14:20,  4.33it/s]\u001b[A\n","1966it [14:20,  4.33it/s]\u001b[A\n","1967it [14:20,  4.33it/s]\u001b[A\n","1968it [14:21,  4.32it/s]\u001b[A\n","1969it [14:21,  4.32it/s]\u001b[A\n","1970it [14:21,  4.33it/s]\u001b[A\n","1971it [14:21,  4.33it/s]\u001b[A\n","1972it [14:22,  4.32it/s]\u001b[A\n","1973it [14:22,  4.33it/s]\u001b[A\n","1974it [14:22,  4.33it/s]\u001b[A\n","1975it [14:22,  4.33it/s]\u001b[A\n","1976it [14:23,  4.33it/s]\u001b[A\n","1977it [14:23,  4.33it/s]\u001b[A\n","1978it [14:23,  4.33it/s]\u001b[A\n","1979it [14:23,  4.33it/s]\u001b[A\n","1980it [14:23,  4.33it/s]\u001b[A\n","1981it [14:24,  4.33it/s]\u001b[A\n","1982it [14:24,  4.33it/s]\u001b[A\n","1983it [14:24,  4.33it/s]\u001b[A\n","1984it [14:24,  4.33it/s]\u001b[A\n","1985it [14:25,  4.33it/s]\u001b[A\n","1986it [14:25,  4.33it/s]\u001b[A\n","1987it [14:25,  4.33it/s]\u001b[A\n","1988it [14:25,  4.33it/s]\u001b[A\n","1989it [14:26,  4.33it/s]\u001b[A\n","1990it [14:26,  4.33it/s]\u001b[A\n","1991it [14:26,  4.33it/s]\u001b[A\n","1992it [14:26,  4.33it/s]\u001b[A\n","1993it [14:26,  4.33it/s]\u001b[A\n","1994it [14:27,  4.33it/s]\u001b[A\n","1995it [14:27,  4.33it/s]\u001b[A\n","1996it [14:27,  4.33it/s]\u001b[A\n","1997it [14:27,  4.33it/s]\u001b[A\n","1998it [14:28,  4.33it/s]\u001b[A\n","1999it [14:28,  4.33it/s]\u001b[A\n","2000it [14:28,  4.33it/s]\u001b[A\n","2001it [14:28,  4.33it/s]\u001b[A\n","2002it [14:29,  4.33it/s]\u001b[A\n","2003it [14:29,  4.33it/s]\u001b[A\n","2004it [14:29,  4.33it/s]\u001b[A\n","2005it [14:29,  4.33it/s]\u001b[A\n","2006it [14:29,  4.33it/s]\u001b[A\n","2007it [14:30,  4.33it/s]\u001b[A\n","2008it [14:30,  4.33it/s]\u001b[A\n","2009it [14:30,  4.33it/s]\u001b[A\n","2010it [14:30,  4.33it/s]\u001b[A\n","2011it [14:31,  4.33it/s]\u001b[A\n","2012it [14:31,  4.33it/s]\u001b[A\n","2013it [14:31,  4.33it/s]\u001b[A\n","2014it [14:31,  4.33it/s]\u001b[A\n","2015it [14:32,  4.33it/s]\u001b[A\n","2016it [14:32,  4.33it/s]\u001b[A\n","2017it [14:32,  4.33it/s]\u001b[A\n","2018it [14:32,  4.33it/s]\u001b[A\n","2019it [14:33,  4.33it/s]\u001b[A\n","2020it [14:33,  4.33it/s]\u001b[A\n","2021it [14:33,  4.33it/s]\u001b[A\n","2022it [14:33,  4.33it/s]\u001b[A\n","2023it [14:33,  4.33it/s]\u001b[A\n","2024it [14:34,  4.33it/s]\u001b[A\n","2025it [14:34,  4.33it/s]\u001b[A\n","2026it [14:34,  4.33it/s]\u001b[A\n","2027it [14:34,  4.33it/s]\u001b[A\n","2028it [14:35,  4.33it/s]\u001b[A\n","2029it [14:35,  4.33it/s]\u001b[A\n","2030it [14:35,  4.33it/s]\u001b[A\n","2031it [14:35,  4.33it/s]\u001b[A\n","2032it [14:36,  4.33it/s]\u001b[A\n","2033it [14:36,  4.33it/s]\u001b[A\n","2034it [14:36,  4.33it/s]\u001b[A\n","2035it [14:36,  4.33it/s]\u001b[A03/14/2022 09:32:54 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/14/2022 09:32:54 - INFO - __main__ -     eval_loss = 2.6190853118896484\n","03/14/2022 09:32:54 - INFO - __main__ -     eval_auroc = 0.8730000853538513\n","03/14/2022 09:32:54 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/14/2022 09:32:54 - INFO - __main__ -     eval_f1 = 0.47422680258750916\n","03/14/2022 09:32:54 - INFO - root -   *** Test ***\n","\n","2036it [14:36,  4.28it/s]\u001b[A\n","2037it [14:37,  4.29it/s]\u001b[A\n","2038it [14:37,  4.30it/s]\u001b[A\n","2039it [14:37,  4.31it/s]\u001b[A\n","2040it [14:37,  4.32it/s]\u001b[A\n","2041it [14:38,  4.32it/s]\u001b[A\n","2042it [14:38,  4.33it/s]\u001b[A\n","2043it [14:38,  4.33it/s]\u001b[A\n","2044it [14:38,  4.33it/s]\u001b[A\n","2045it [14:39,  4.33it/s]\u001b[A\n","2046it [14:39,  4.33it/s]\u001b[A\n","2047it [14:39,  4.33it/s]\u001b[A\n","2048it [14:39,  4.33it/s]\u001b[A\n","2049it [14:39,  4.33it/s]\u001b[A\n","2050it [14:40,  4.33it/s]\u001b[A\n","2051it [14:40,  4.33it/s]\u001b[A\n","2052it [14:40,  4.33it/s]\u001b[A\n","2053it [14:40,  4.33it/s]\u001b[A\n","2054it [14:41,  4.33it/s]\u001b[A\n","2055it [14:41,  4.33it/s]\u001b[A\n","2056it [14:41,  4.33it/s]\u001b[A\n","2057it [14:41,  4.33it/s]\u001b[A\n","2058it [14:42,  4.33it/s]\u001b[A\n","2059it [14:42,  4.33it/s]\u001b[A\n","2060it [14:42,  4.33it/s]\u001b[A\n","2061it [14:42,  4.33it/s]\u001b[A\n","2062it [14:42,  4.33it/s]\u001b[A\n","2063it [14:43,  4.33it/s]\u001b[A\n","2064it [14:43,  4.33it/s]\u001b[A\n","2065it [14:43,  4.33it/s]\u001b[A\n","2066it [14:43,  4.33it/s]\u001b[A\n","2067it [14:44,  4.33it/s]\u001b[A\n","2068it [14:44,  4.33it/s]\u001b[A\n","2069it [14:44,  4.33it/s]\u001b[A\n","2070it [14:44,  4.33it/s]\u001b[A\n","2071it [14:45,  4.33it/s]\u001b[A\n","2072it [14:45,  4.33it/s]\u001b[A\n","2073it [14:45,  4.33it/s]\u001b[A\n","2074it [14:45,  4.33it/s]\u001b[A\n","2075it [14:45,  4.33it/s]\u001b[A\n","2076it [14:46,  4.33it/s]\u001b[A\n","2077it [14:46,  4.33it/s]\u001b[A\n","2078it [14:46,  4.33it/s]\u001b[A\n","2079it [14:46,  4.33it/s]\u001b[A\n","2080it [14:47,  4.33it/s]\u001b[A\n","2081it [14:47,  4.33it/s]\u001b[A\n","2082it [14:47,  4.33it/s]\u001b[A\n","2083it [14:47,  4.34it/s]\u001b[A\n","2084it [14:48,  4.33it/s]\u001b[A\n","2085it [14:48,  4.33it/s]\u001b[A\n","2086it [14:48,  4.33it/s]\u001b[A\n","2087it [14:48,  4.33it/s]\u001b[A\n","2088it [14:48,  4.33it/s]\u001b[A\n","2089it [14:49,  4.33it/s]\u001b[A\n","2090it [14:49,  4.33it/s]\u001b[A\n","2091it [14:49,  4.33it/s]\u001b[A\n","2092it [14:49,  4.33it/s]\u001b[A\n","2093it [14:50,  4.33it/s]\u001b[A\n","2094it [14:50,  4.33it/s]\u001b[A\n","2095it [14:50,  4.33it/s]\u001b[A\n","2096it [14:50,  4.33it/s]\u001b[A\n","2097it [14:51,  4.33it/s]\u001b[A\n","2098it [14:51,  4.33it/s]\u001b[A\n","2099it [14:51,  4.33it/s]\u001b[A\n","2100it [14:51,  4.33it/s]\u001b[A\n","2101it [14:51,  4.33it/s]\u001b[A\n","2102it [14:52,  4.33it/s]\u001b[A\n","2103it [14:52,  4.33it/s]\u001b[A\n","2104it [14:52,  4.33it/s]\u001b[A\n","2105it [14:52,  4.33it/s]\u001b[A\n","2106it [14:53,  4.33it/s]\u001b[A\n","2107it [14:53,  4.33it/s]\u001b[A\n","2108it [14:53,  4.33it/s]\u001b[A\n","2109it [14:53,  4.33it/s]\u001b[A\n","2110it [14:54,  4.33it/s]\u001b[A\n","2111it [14:54,  4.33it/s]\u001b[A\n","2112it [14:54,  4.33it/s]\u001b[A\n","2113it [14:54,  4.33it/s]\u001b[A\n","2114it [14:54,  4.33it/s]\u001b[A\n","2115it [14:55,  4.33it/s]\u001b[A\n","2116it [14:55,  4.33it/s]\u001b[A\n","2117it [14:55,  4.33it/s]\u001b[A\n","2118it [14:55,  4.33it/s]\u001b[A\n","2119it [14:56,  4.33it/s]\u001b[A\n","2120it [14:56,  4.33it/s]\u001b[A\n","2121it [14:56,  4.33it/s]\u001b[A\n","2122it [14:56,  4.33it/s]\u001b[A\n","2123it [14:57,  4.33it/s]\u001b[A\n","2124it [14:57,  4.33it/s]\u001b[A\n","2125it [14:57,  4.33it/s]\u001b[A\n","2126it [14:57,  4.33it/s]\u001b[A\n","2127it [14:57,  4.33it/s]\u001b[A\n","2128it [14:58,  4.33it/s]\u001b[A\n","2129it [14:58,  4.33it/s]\u001b[A\n","2130it [14:58,  4.33it/s]\u001b[A\n","2131it [14:58,  4.33it/s]\u001b[A\n","2132it [14:59,  4.33it/s]\u001b[A\n","2133it [14:59,  4.33it/s]\u001b[A\n","2134it [14:59,  4.33it/s]\u001b[A\n","2135it [14:59,  4.33it/s]\u001b[A\n","2136it [15:00,  4.33it/s]\u001b[A\n","2137it [15:00,  4.33it/s]\u001b[A\n","2138it [15:00,  4.33it/s]\u001b[A\n","2139it [15:00,  4.33it/s]\u001b[A\n","2140it [15:00,  4.33it/s]\u001b[A\n","2141it [15:01,  4.33it/s]\u001b[A\n","2142it [15:01,  4.33it/s]\u001b[A\n","2143it [15:01,  4.33it/s]\u001b[A\n","2144it [15:01,  4.33it/s]\u001b[A\n","2145it [15:02,  4.33it/s]\u001b[A\n","2146it [15:02,  4.33it/s]\u001b[A\n","2147it [15:02,  4.33it/s]\u001b[A\n","2148it [15:02,  4.33it/s]\u001b[A\n","2149it [15:03,  4.33it/s]\u001b[A\n","2150it [15:03,  4.33it/s]\u001b[A\n","2151it [15:03,  4.33it/s]\u001b[A\n","2152it [15:03,  4.34it/s]\u001b[A\n","2153it [15:03,  4.33it/s]\u001b[A\n","2154it [15:04,  4.33it/s]\u001b[A\n","2155it [15:04,  4.33it/s]\u001b[A\n","2156it [15:04,  4.33it/s]\u001b[A\n","2157it [15:04,  4.33it/s]\u001b[A\n","2158it [15:05,  4.33it/s]\u001b[A\n","2159it [15:05,  4.33it/s]\u001b[A\n","2160it [15:05,  4.33it/s]\u001b[A\n","2161it [15:05,  4.33it/s]\u001b[A\n","2162it [15:06,  4.33it/s]\u001b[A\n","2163it [15:06,  4.33it/s]\u001b[A\n","2164it [15:06,  4.33it/s]\u001b[A\n","2165it [15:06,  4.33it/s]\u001b[A\n","2166it [15:06,  4.33it/s]\u001b[A\n","2167it [15:07,  4.33it/s]\u001b[A\n","2168it [15:07,  4.33it/s]\u001b[A\n","2169it [15:07,  4.33it/s]\u001b[A\n","2170it [15:07,  4.33it/s]\u001b[A\n","2171it [15:08,  4.33it/s]\u001b[A\n","2172it [15:08,  4.33it/s]\u001b[A\n","2173it [15:08,  4.33it/s]\u001b[A\n","2174it [15:08,  4.33it/s]\u001b[A\n","2175it [15:09,  4.33it/s]\u001b[A\n","2176it [15:09,  4.33it/s]\u001b[A\n","2177it [15:09,  4.33it/s]\u001b[A\n","2178it [15:09,  4.33it/s]\u001b[A\n","2179it [15:09,  4.33it/s]\u001b[A\n","2180it [15:10,  4.33it/s]\u001b[A\n","2181it [15:10,  4.33it/s]\u001b[A\n","2182it [15:10,  4.33it/s]\u001b[A\n","2183it [15:10,  4.33it/s]\u001b[A\n","2184it [15:11,  4.33it/s]\u001b[A\n","2185it [15:11,  4.33it/s]\u001b[A\n","2186it [15:11,  4.33it/s]\u001b[A\n","2187it [15:11,  4.33it/s]\u001b[A\n","2188it [15:12,  4.33it/s]\u001b[A\n","2189it [15:12,  4.33it/s]\u001b[A\n","2190it [15:12,  4.33it/s]\u001b[A\n","2191it [15:12,  4.33it/s]\u001b[A\n","2192it [15:12,  4.33it/s]\u001b[A\n","2193it [15:13,  4.33it/s]\u001b[A\n","2194it [15:13,  4.33it/s]\u001b[A\n","2195it [15:13,  4.33it/s]\u001b[A\n","2196it [15:13,  4.33it/s]\u001b[A\n","2197it [15:14,  4.33it/s]\u001b[A\n","2198it [15:14,  4.33it/s]\u001b[A\n","2199it [15:14,  4.33it/s]\u001b[A\n","2200it [15:14,  4.33it/s]\u001b[A\n","2201it [15:15,  4.33it/s]\u001b[A\n","2202it [15:15,  4.33it/s]\u001b[A\n","2203it [15:15,  4.33it/s]\u001b[A\n","2204it [15:15,  4.33it/s]\u001b[A\n","2205it [15:15,  4.33it/s]\u001b[A\n","2206it [15:16,  4.33it/s]\u001b[A\n","2207it [15:16,  4.33it/s]\u001b[A\n","2208it [15:16,  4.33it/s]\u001b[A\n","2209it [15:16,  4.33it/s]\u001b[A\n","2210it [15:17,  4.33it/s]\u001b[A\n","2211it [15:17,  4.33it/s]\u001b[A\n","2212it [15:17,  4.33it/s]\u001b[A\n","2213it [15:17,  4.33it/s]\u001b[A\n","2214it [15:18,  4.33it/s]\u001b[A\n","2215it [15:18,  4.33it/s]\u001b[A\n","2216it [15:18,  4.33it/s]\u001b[A\n","2217it [15:18,  4.33it/s]\u001b[A\n","2218it [15:18,  4.33it/s]\u001b[A\n","2219it [15:19,  4.33it/s]\u001b[A\n","2220it [15:19,  4.33it/s]\u001b[A\n","2221it [15:19,  4.33it/s]\u001b[A\n","2222it [15:19,  4.33it/s]\u001b[A\n","2223it [15:20,  4.33it/s]\u001b[A\n","2224it [15:20,  4.33it/s]\u001b[A\n","2225it [15:20,  4.33it/s]\u001b[A\n","2226it [15:20,  4.33it/s]\u001b[A\n","2227it [15:21,  4.33it/s]\u001b[A\n","2228it [15:21,  4.33it/s]\u001b[A\n","2229it [15:21,  4.33it/s]\u001b[A\n","2230it [15:21,  4.33it/s]\u001b[A\n","2231it [15:21,  4.32it/s]\u001b[A\n","2232it [15:22,  4.33it/s]\u001b[A\n","2233it [15:22,  4.33it/s]\u001b[A\n","2234it [15:22,  4.33it/s]\u001b[A\n","2235it [15:22,  4.33it/s]\u001b[A\n","2236it [15:23,  4.33it/s]\u001b[A\n","2237it [15:23,  4.33it/s]\u001b[A\n","2238it [15:23,  4.33it/s]\u001b[A\n","2239it [15:23,  4.33it/s]\u001b[A\n","2240it [15:24,  4.33it/s]\u001b[A\n","2241it [15:24,  4.33it/s]\u001b[A\n","2242it [15:24,  4.33it/s]\u001b[A\n","2243it [15:24,  4.33it/s]\u001b[A\n","2244it [15:24,  4.33it/s]\u001b[A\n","2245it [15:25,  4.33it/s]\u001b[A\n","2246it [15:25,  4.33it/s]\u001b[A\n","2247it [15:25,  4.33it/s]\u001b[A\n","2248it [15:25,  4.33it/s]\u001b[A\n","2249it [15:26,  4.33it/s]\u001b[A\n","2250it [15:26,  4.33it/s]\u001b[A\n","2251it [15:26,  4.33it/s]\u001b[A\n","2252it [15:26,  4.33it/s]\u001b[A\n","2253it [15:27,  4.33it/s]\u001b[A\n","2254it [15:27,  4.33it/s]\u001b[A\n","2255it [15:27,  4.33it/s]\u001b[A\n","2256it [15:27,  4.33it/s]\u001b[A\n","2257it [15:27,  4.33it/s]\u001b[A\n","2258it [15:28,  4.33it/s]\u001b[A\n","2259it [15:28,  4.33it/s]\u001b[A\n","2260it [15:28,  4.33it/s]\u001b[A\n","2261it [15:28,  4.33it/s]\u001b[A\n","2262it [15:29,  4.33it/s]\u001b[A\n","2263it [15:29,  4.33it/s]\u001b[A\n","2264it [15:29,  4.33it/s]\u001b[A\n","2265it [15:29,  4.33it/s]\u001b[A\n","2266it [15:30,  4.33it/s]\u001b[A\n","2267it [15:30,  4.33it/s]\u001b[A\n","2268it [15:30,  4.33it/s]\u001b[A\n","2269it [15:30,  4.33it/s]\u001b[A\n","2270it [15:30,  4.33it/s]\u001b[A\n","2271it [15:31,  4.33it/s]\u001b[A\n","2272it [15:31,  4.33it/s]\u001b[A\n","2273it [15:31,  4.33it/s]\u001b[A\n","2274it [15:31,  4.33it/s]\u001b[A\n","2275it [15:32,  4.33it/s]\u001b[A\n","2276it [15:32,  4.33it/s]\u001b[A\n","2277it [15:32,  4.33it/s]\u001b[A\n","2278it [15:32,  4.33it/s]\u001b[A\n","2279it [15:33,  4.33it/s]\u001b[A\n","2280it [15:33,  4.33it/s]\u001b[A\n","2281it [15:33,  4.33it/s]\u001b[A\n","2282it [15:33,  4.33it/s]\u001b[A\n","2283it [15:33,  4.33it/s]\u001b[A\n","2284it [15:34,  4.33it/s]\u001b[A\n","2285it [15:34,  4.33it/s]\u001b[A\n","2286it [15:34,  4.33it/s]\u001b[A\n","2287it [15:34,  4.33it/s]\u001b[A\n","2288it [15:35,  4.33it/s]\u001b[A\n","2289it [15:35,  4.33it/s]\u001b[A\n","2290it [15:35,  4.33it/s]\u001b[A\n","2291it [15:35,  4.33it/s]\u001b[A\n","2292it [15:36,  4.33it/s]\u001b[A\n","2293it [15:36,  4.33it/s]\u001b[A\n","2294it [15:36,  4.33it/s]\u001b[A\n","2295it [15:36,  4.33it/s]\u001b[A\n","2296it [15:36,  4.33it/s]\u001b[A\n","2297it [15:37,  4.33it/s]\u001b[A\n","2298it [15:37,  4.33it/s]\u001b[A\n","2299it [15:37,  4.34it/s]\u001b[A\n","2300it [15:37,  4.33it/s]\u001b[A\n","2301it [15:38,  4.33it/s]\u001b[A\n","2302it [15:38,  4.33it/s]\u001b[A\n","2303it [15:38,  4.33it/s]\u001b[A\n","2304it [15:38,  4.33it/s]\u001b[A\n","2305it [15:39,  4.33it/s]\u001b[A\n","2306it [15:39,  4.33it/s]\u001b[A\n","2307it [15:39,  4.33it/s]\u001b[A\n","2308it [15:39,  4.33it/s]\u001b[A\n","2309it [15:39,  4.33it/s]\u001b[A\n","2310it [15:40,  4.33it/s]\u001b[A\n","2311it [15:40,  4.33it/s]\u001b[A\n","2312it [15:40,  4.33it/s]\u001b[A\n","2313it [15:40,  4.33it/s]\u001b[A\n","2314it [15:41,  4.33it/s]\u001b[A\n","2315it [15:41,  4.33it/s]\u001b[A\n","2316it [15:41,  4.33it/s]\u001b[A\n","2317it [15:41,  4.33it/s]\u001b[A\n","2318it [15:42,  4.33it/s]\u001b[A\n","2319it [15:42,  4.33it/s]\u001b[A\n","2320it [15:42,  4.33it/s]\u001b[A\n","2321it [15:42,  4.33it/s]\u001b[A\n","2322it [15:42,  4.33it/s]\u001b[A\n","2323it [15:43,  4.33it/s]\u001b[A\n","2324it [15:43,  4.33it/s]\u001b[A\n","2325it [15:43,  4.33it/s]\u001b[A\n","2326it [15:43,  4.33it/s]\u001b[A\n","2327it [15:44,  4.33it/s]\u001b[A\n","2328it [15:44,  4.33it/s]\u001b[A\n","2329it [15:44,  4.33it/s]\u001b[A\n","2330it [15:44,  4.33it/s]\u001b[A\n","2331it [15:45,  4.33it/s]\u001b[A\n","2332it [15:45,  4.33it/s]\u001b[A\n","2333it [15:45,  4.33it/s]\u001b[A\n","2334it [15:45,  4.33it/s]\u001b[A\n","2335it [15:45,  4.33it/s]\u001b[A\n","2336it [15:46,  4.33it/s]\u001b[A\n","2337it [15:46,  4.33it/s]\u001b[A\n","2338it [15:46,  4.33it/s]\u001b[A\n","2339it [15:46,  4.33it/s]\u001b[A\n","2340it [15:47,  4.33it/s]\u001b[A\n","2341it [15:47,  4.33it/s]\u001b[A\n","2342it [15:47,  4.33it/s]\u001b[A\n","2343it [15:47,  4.33it/s]\u001b[A\n","2344it [15:48,  4.33it/s]\u001b[A\n","2345it [15:48,  4.33it/s]\u001b[A\n","2346it [15:48,  4.33it/s]\u001b[A\n","2347it [15:48,  4.33it/s]\u001b[A\n","2348it [15:48,  4.33it/s]\u001b[A\n","2349it [15:49,  4.33it/s]\u001b[A\n","2350it [15:49,  4.33it/s]\u001b[A\n","2351it [15:49,  4.33it/s]\u001b[A\n","2352it [15:49,  4.33it/s]\u001b[A\n","2353it [15:50,  4.33it/s]\u001b[A\n","2354it [15:50,  4.33it/s]\u001b[A03/14/2022 09:34:07 - INFO - __main__ -   ***** Test results spoilers *****\n","03/14/2022 09:34:07 - INFO - __main__ -     eval_loss = 3.1177148818969727\n","03/14/2022 09:34:07 - INFO - __main__ -     eval_auroc = 0.8237287998199463\n","03/14/2022 09:34:07 - INFO - __main__ -     eval_recall = 0.8333333134651184\n","03/14/2022 09:34:07 - INFO - __main__ -     eval_f1 = 0.26490065455436707\n","03/14/2022 09:34:07 - INFO - filelock -   Lock 140288413982160 acquired on log.lock\n","03/14/2022 09:34:07 - INFO - filelock -   Lock 140288413982160 released on log.lock\n","2354it [15:50,  2.48it/s]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/14/2022 09:34:13 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.sort.txt\n","03/14/2022 09:34:13 - INFO - __main__ -   Specify load the 19-th template: *cls**sent_0*,*mask*?*sep+*\n","03/14/2022 09:34:13 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/14/2022 09:34:13 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-demo-16-21-roberta-large-29866', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar14_09-34-13_eda9c4d80d5c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-demo-16-21-roberta-large-29866', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=0, model_id=19, save_logit=True, save_logit_dir='ensemble_predict_results', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","03/14/2022 09:34:13 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/14/2022 09:34:13 - INFO - __main__ -   Automatically convert the template to using demonstrations.\n","03/14/2022 09:34:13 - INFO - __main__ -   | *cls**sent_0*,*mask*?*sep+* => *cls**sent_0*,*mask*?*sep+**sent_1*,*label_0*?*sep+**sent_2*,*label_1*?*sep+*\n","03/14/2022 09:34:14 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:34:14 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/14/2022 09:34:14 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/14/2022 09:34:14 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/14/2022 09:34:14 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:34:14 - INFO - filelock -   Lock 140578892290384 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:14 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/14/2022 09:34:14 - INFO - filelock -   Lock 140578892290384 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:14 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:34:14 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/14/2022 09:34:14 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/14/2022 09:34:14 - INFO - src.dataset -   Total num_sample for mode dev: 16\n","03/14/2022 09:34:14 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:34:15 - INFO - filelock -   Lock 140578879090960 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:15 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/14/2022 09:34:15 - INFO - filelock -   Lock 140578879090960 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:15 - INFO - src.dataset -   *** Example ***\n","03/14/2022 09:34:15 - INFO - src.dataset -   guid: dev-0\n","03/14/2022 09:34:15 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 5488, 74, 28, 6474, 9724, 6, 440, 116, 2, 571, 10810, 31985, 364, 329, 6, 3216, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/14/2022 09:34:15 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s>which would be lit af, No?</s>gabi sniper ez, Yes?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","03/14/2022 09:34:17 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:34:17 - INFO - src.dataset -   Label 0 to word ĠNo (440)\n","03/14/2022 09:34:17 - INFO - src.dataset -   Label 1 to word ĠYes (3216)\n","03/14/2022 09:34:17 - INFO - src.dataset -   Total num_sample for mode test: 16\n","03/14/2022 09:34:17 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:34:17 - INFO - filelock -   Lock 140578879090960 acquired on data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:17 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/14/2022 09:34:17 - INFO - filelock -   Lock 140578879090960 released on data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:17 - INFO - src.dataset -   *** Example ***\n","03/14/2022 09:34:17 - INFO - src.dataset -   guid: test-0\n","03/14/2022 09:34:17 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 8663, 18, 269, 15867, 154, 123, 6, 50264, 116, 2, 3463, 45548, 6, 440, 116, 2, 571, 10810, 31985, 364, 329, 6, 3216, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[8], label_word_list=None)\n","03/14/2022 09:34:17 - INFO - src.dataset -   text: <s>eren's really battering him,<mask>?</s>exactly, No?</s>gabi sniper ez, Yes?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/14/2022 09:34:37 - INFO - src.trainer -   ***** Running training *****\n","03/14/2022 09:34:37 - INFO - src.trainer -     Num examples = 32\n","03/14/2022 09:34:37 - INFO - src.trainer -     Num Epochs = 250\n","03/14/2022 09:34:37 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/14/2022 09:34:37 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/14/2022 09:34:37 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/14/2022 09:34:37 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:17,  1.67s/it]\n","  0% 0/185 [00:00<?, ?it/s]\u001b[A\n","  1% 2/185 [00:00<00:21,  8.67it/s]\u001b[A\n","  2% 3/185 [00:00<00:27,  6.67it/s]\u001b[A\n","  2% 4/185 [00:00<00:31,  5.74it/s]\u001b[A\n","  3% 5/185 [00:00<00:34,  5.23it/s]\u001b[A\n","  3% 6/185 [00:01<00:36,  4.92it/s]\u001b[A\n","  4% 7/185 [00:01<00:37,  4.72it/s]\u001b[A\n","  4% 8/185 [00:01<00:38,  4.60it/s]\u001b[A\n","  5% 9/185 [00:01<00:38,  4.51it/s]\u001b[A\n","  5% 10/185 [00:02<00:39,  4.46it/s]\u001b[A\n","  6% 11/185 [00:02<00:39,  4.42it/s]\u001b[A\n","  6% 12/185 [00:02<00:39,  4.39it/s]\u001b[A\n","  7% 13/185 [00:02<00:39,  4.38it/s]\u001b[A\n","  8% 14/185 [00:03<00:39,  4.36it/s]\u001b[A\n","  8% 15/185 [00:03<00:39,  4.35it/s]\u001b[A\n","  9% 16/185 [00:03<00:38,  4.34it/s]\u001b[A\n","  9% 17/185 [00:03<00:38,  4.33it/s]\u001b[A\n"," 10% 18/185 [00:03<00:38,  4.33it/s]\u001b[A\n"," 10% 19/185 [00:04<00:38,  4.33it/s]\u001b[A\n"," 11% 20/185 [00:04<00:38,  4.33it/s]\u001b[A\n"," 11% 21/185 [00:04<00:37,  4.33it/s]\u001b[A\n"," 12% 22/185 [00:04<00:37,  4.33it/s]\u001b[A\n"," 12% 23/185 [00:05<00:37,  4.33it/s]\u001b[A\n"," 13% 24/185 [00:05<00:37,  4.33it/s]\u001b[A\n"," 14% 25/185 [00:05<00:36,  4.33it/s]\u001b[A\n"," 14% 26/185 [00:05<00:36,  4.33it/s]\u001b[A\n"," 15% 27/185 [00:06<00:36,  4.32it/s]\u001b[A\n"," 15% 28/185 [00:06<00:36,  4.32it/s]\u001b[A\n"," 16% 29/185 [00:06<00:36,  4.33it/s]\u001b[A\n"," 16% 30/185 [00:06<00:35,  4.33it/s]\u001b[A\n"," 17% 31/185 [00:06<00:35,  4.32it/s]\u001b[A\n"," 17% 32/185 [00:07<00:35,  4.33it/s]\u001b[A\n"," 18% 33/185 [00:07<00:35,  4.32it/s]\u001b[A\n"," 18% 34/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 35/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 36/185 [00:08<00:34,  4.33it/s]\u001b[A\n"," 20% 37/185 [00:08<00:34,  4.32it/s]\u001b[A\n"," 21% 38/185 [00:08<00:33,  4.32it/s]\u001b[A\n"," 21% 39/185 [00:08<00:33,  4.33it/s]\u001b[A\n"," 22% 40/185 [00:09<00:33,  4.32it/s]\u001b[A\n"," 22% 41/185 [00:09<00:33,  4.32it/s]\u001b[A\n"," 23% 42/185 [00:09<00:33,  4.33it/s]\u001b[A\n"," 23% 43/185 [00:09<00:32,  4.33it/s]\u001b[A\n"," 24% 44/185 [00:09<00:32,  4.33it/s]\u001b[A\n"," 24% 45/185 [00:10<00:32,  4.33it/s]\u001b[A\n"," 25% 46/185 [00:10<00:32,  4.33it/s]\u001b[A\n"," 25% 47/185 [00:10<00:31,  4.33it/s]\u001b[A\n"," 26% 48/185 [00:10<00:31,  4.33it/s]\u001b[A\n"," 26% 49/185 [00:11<00:31,  4.33it/s]\u001b[A\n"," 27% 50/185 [00:11<00:31,  4.33it/s]\u001b[A\n"," 28% 51/185 [00:11<00:30,  4.32it/s]\u001b[A\n"," 28% 52/185 [00:11<00:30,  4.32it/s]\u001b[A\n"," 29% 53/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 29% 54/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 30% 55/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 30% 56/185 [00:12<00:29,  4.33it/s]\u001b[A\n"," 31% 57/185 [00:12<00:29,  4.33it/s]\u001b[A\n"," 31% 58/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 59/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 60/185 [00:13<00:28,  4.32it/s]\u001b[A\n"," 33% 61/185 [00:13<00:28,  4.32it/s]\u001b[A\n"," 34% 62/185 [00:14<00:28,  4.32it/s]\u001b[A\n"," 34% 63/185 [00:14<00:28,  4.32it/s]\u001b[A\n"," 35% 64/185 [00:14<00:27,  4.32it/s]\u001b[A\n"," 35% 65/185 [00:14<00:27,  4.33it/s]\u001b[A\n"," 36% 66/185 [00:15<00:27,  4.33it/s]\u001b[A\n"," 36% 67/185 [00:15<00:27,  4.32it/s]\u001b[A\n"," 37% 68/185 [00:15<00:27,  4.32it/s]\u001b[A\n"," 37% 69/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 38% 70/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 38% 71/185 [00:16<00:26,  4.33it/s]\u001b[A\n"," 39% 72/185 [00:16<00:26,  4.33it/s]\u001b[A\n"," 39% 73/185 [00:16<00:25,  4.33it/s]\u001b[A\n"," 40% 74/185 [00:16<00:25,  4.32it/s]\u001b[A\n"," 41% 75/185 [00:17<00:25,  4.33it/s]\u001b[A\n"," 41% 76/185 [00:17<00:25,  4.33it/s]\u001b[A\n"," 42% 77/185 [00:17<00:24,  4.33it/s]\u001b[A\n"," 42% 78/185 [00:17<00:24,  4.33it/s]\u001b[A\n"," 43% 79/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 43% 80/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 44% 81/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 44% 82/185 [00:18<00:23,  4.33it/s]\u001b[A\n"," 45% 83/185 [00:18<00:23,  4.33it/s]\u001b[A\n"," 45% 84/185 [00:19<00:23,  4.32it/s]\u001b[A\n"," 46% 85/185 [00:19<00:23,  4.32it/s]\u001b[A\n"," 46% 86/185 [00:19<00:22,  4.33it/s]\u001b[A\n"," 47% 87/185 [00:19<00:22,  4.32it/s]\u001b[A\n"," 48% 88/185 [00:20<00:22,  4.32it/s]\u001b[A\n"," 48% 89/185 [00:20<00:22,  4.32it/s]\u001b[A\n"," 49% 90/185 [00:20<00:21,  4.32it/s]\u001b[A\n"," 49% 91/185 [00:20<00:21,  4.32it/s]\u001b[A\n"," 50% 92/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 50% 93/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 51% 94/185 [00:21<00:21,  4.32it/s]\u001b[A\n"," 51% 95/185 [00:21<00:20,  4.33it/s]\u001b[A\n"," 52% 96/185 [00:21<00:20,  4.32it/s]\u001b[A\n"," 52% 97/185 [00:22<00:20,  4.32it/s]\u001b[A\n"," 53% 98/185 [00:22<00:20,  4.32it/s]\u001b[A\n"," 54% 99/185 [00:22<00:19,  4.33it/s]\u001b[A\n"," 54% 100/185 [00:22<00:19,  4.33it/s]\u001b[A\n"," 55% 101/185 [00:23<00:19,  4.33it/s]\u001b[A\n"," 55% 102/185 [00:23<00:19,  4.33it/s]\u001b[A\n"," 56% 103/185 [00:23<00:18,  4.33it/s]\u001b[A\n"," 56% 104/185 [00:23<00:18,  4.33it/s]\u001b[A\n"," 57% 105/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 57% 106/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 58% 107/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 58% 108/185 [00:24<00:17,  4.33it/s]\u001b[A\n"," 59% 109/185 [00:24<00:17,  4.33it/s]\u001b[A\n"," 59% 110/185 [00:25<00:17,  4.33it/s]\u001b[A\n"," 60% 111/185 [00:25<00:17,  4.33it/s]\u001b[A\n"," 61% 112/185 [00:25<00:16,  4.33it/s]\u001b[A\n"," 61% 113/185 [00:25<00:16,  4.33it/s]\u001b[A\n"," 62% 114/185 [00:26<00:16,  4.33it/s]\u001b[A\n"," 62% 115/185 [00:26<00:16,  4.33it/s]\u001b[A\n"," 63% 116/185 [00:26<00:15,  4.33it/s]\u001b[A\n"," 63% 117/185 [00:26<00:15,  4.33it/s]\u001b[A\n"," 64% 118/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 64% 119/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 65% 120/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 65% 121/185 [00:27<00:14,  4.33it/s]\u001b[A\n"," 66% 122/185 [00:27<00:14,  4.33it/s]\u001b[A\n"," 66% 123/185 [00:28<00:14,  4.33it/s]\u001b[A\n"," 67% 124/185 [00:28<00:14,  4.32it/s]\u001b[A\n"," 68% 125/185 [00:28<00:13,  4.32it/s]\u001b[A\n"," 68% 126/185 [00:28<00:13,  4.32it/s]\u001b[A\n"," 69% 127/185 [00:29<00:13,  4.32it/s]\u001b[A\n"," 69% 128/185 [00:29<00:13,  4.32it/s]\u001b[A\n"," 70% 129/185 [00:29<00:12,  4.32it/s]\u001b[A\n"," 70% 130/185 [00:29<00:12,  4.32it/s]\u001b[A\n"," 71% 131/185 [00:30<00:12,  4.32it/s]\u001b[A\n"," 71% 132/185 [00:30<00:12,  4.32it/s]\u001b[A\n"," 72% 133/185 [00:30<00:12,  4.33it/s]\u001b[A\n"," 72% 134/185 [00:30<00:11,  4.33it/s]\u001b[A\n"," 73% 135/185 [00:30<00:11,  4.33it/s]\u001b[A\n"," 74% 136/185 [00:31<00:11,  4.33it/s]\u001b[A\n"," 74% 137/185 [00:31<00:11,  4.33it/s]\u001b[A\n"," 75% 138/185 [00:31<00:10,  4.33it/s]\u001b[A\n"," 75% 139/185 [00:31<00:10,  4.33it/s]\u001b[A\n"," 76% 140/185 [00:32<00:10,  4.33it/s]\u001b[A\n"," 76% 141/185 [00:32<00:10,  4.33it/s]\u001b[A\n"," 77% 142/185 [00:32<00:09,  4.33it/s]\u001b[A\n"," 77% 143/185 [00:32<00:09,  4.33it/s]\u001b[A\n"," 78% 144/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 78% 145/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 79% 146/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 79% 147/185 [00:33<00:08,  4.33it/s]\u001b[A\n"," 80% 148/185 [00:33<00:08,  4.33it/s]\u001b[A\n"," 81% 149/185 [00:34<00:08,  4.33it/s]\u001b[A\n"," 81% 150/185 [00:34<00:08,  4.33it/s]\u001b[A\n"," 82% 151/185 [00:34<00:07,  4.33it/s]\u001b[A\n"," 82% 152/185 [00:34<00:07,  4.33it/s]\u001b[A\n"," 83% 153/185 [00:35<00:07,  4.33it/s]\u001b[A\n"," 83% 154/185 [00:35<00:07,  4.32it/s]\u001b[A\n"," 84% 155/185 [00:35<00:06,  4.32it/s]\u001b[A\n"," 84% 156/185 [00:35<00:06,  4.33it/s]\u001b[A\n"," 85% 157/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 85% 158/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 86% 159/185 [00:36<00:06,  4.32it/s]\u001b[A\n"," 86% 160/185 [00:36<00:05,  4.32it/s]\u001b[A\n"," 87% 161/185 [00:36<00:05,  4.32it/s]\u001b[A\n"," 88% 162/185 [00:37<00:05,  4.32it/s]\u001b[A\n"," 88% 163/185 [00:37<00:05,  4.32it/s]\u001b[A\n"," 89% 164/185 [00:37<00:04,  4.32it/s]\u001b[A\n"," 89% 165/185 [00:37<00:04,  4.32it/s]\u001b[A\n"," 90% 166/185 [00:38<00:04,  4.32it/s]\u001b[A\n"," 90% 167/185 [00:38<00:04,  4.32it/s]\u001b[A\n"," 91% 168/185 [00:38<00:03,  4.33it/s]\u001b[A\n"," 91% 169/185 [00:38<00:03,  4.33it/s]\u001b[A\n"," 92% 170/185 [00:39<00:03,  4.32it/s]\u001b[A\n"," 92% 171/185 [00:39<00:03,  4.33it/s]\u001b[A\n"," 93% 172/185 [00:39<00:03,  4.33it/s]\u001b[A\n"," 94% 173/185 [00:39<00:02,  4.32it/s]\u001b[A\n"," 94% 174/185 [00:39<00:02,  4.33it/s]\u001b[A\n"," 95% 175/185 [00:40<00:02,  4.33it/s]\u001b[A\n"," 95% 176/185 [00:40<00:02,  4.33it/s]\u001b[A\n"," 96% 177/185 [00:40<00:01,  4.33it/s]\u001b[A\n"," 96% 178/185 [00:40<00:01,  4.33it/s]\u001b[A\n"," 97% 179/185 [00:41<00:01,  4.33it/s]\u001b[A\n"," 97% 180/185 [00:41<00:01,  4.33it/s]\u001b[A\n"," 98% 181/185 [00:41<00:00,  4.33it/s]\u001b[A\n"," 98% 182/185 [00:41<00:00,  4.33it/s]\u001b[A\n"," 99% 183/185 [00:42<00:00,  4.32it/s]\u001b[A\n"," 99% 184/185 [00:42<00:00,  4.33it/s]\u001b[A\n","100% 185/185 [00:42<00:00,  4.33it/s]\u001b[A03/14/2022 09:36:02 - INFO - src.trainer -   Best dev result: 0.9031249284744263\n","Epoch:  20% 49/250 [02:13<05:37,  1.68s/it]\n","186it [01:33, 15.38s/it]             \u001b[A\n","187it [01:33, 10.84s/it]\u001b[A\n","188it [01:33,  7.66s/it]\u001b[A\n","189it [01:33,  5.43s/it]\u001b[A\n","190it [01:34,  3.87s/it]\u001b[A\n","191it [01:34,  2.78s/it]\u001b[A\n","192it [01:34,  2.01s/it]\u001b[A\n","193it [01:34,  1.48s/it]\u001b[A\n","194it [01:35,  1.10s/it]\u001b[A\n","195it [01:35,  1.19it/s]\u001b[A\n","196it [01:35,  1.52it/s]\u001b[A\n","197it [01:35,  1.88it/s]\u001b[A\n","198it [01:36,  2.27it/s]\u001b[A\n","199it [01:36,  2.65it/s]\u001b[A\n","200it [01:36,  3.00it/s]\u001b[A\n","201it [01:36,  3.30it/s]\u001b[A\n","202it [01:36,  3.55it/s]\u001b[A\n","203it [01:37,  3.75it/s]\u001b[A\n","204it [01:37,  3.91it/s]\u001b[A\n","205it [01:37,  4.03it/s]\u001b[A\n","206it [01:37,  4.11it/s]\u001b[A\n","207it [01:38,  4.17it/s]\u001b[A\n","208it [01:38,  4.21it/s]\u001b[A\n","209it [01:38,  4.25it/s]\u001b[A\n","210it [01:38,  4.27it/s]\u001b[A\n","211it [01:39,  4.29it/s]\u001b[A\n","212it [01:39,  4.30it/s]\u001b[A\n","213it [01:39,  4.31it/s]\u001b[A\n","214it [01:39,  4.31it/s]\u001b[A\n","215it [01:39,  4.32it/s]\u001b[A\n","216it [01:40,  4.32it/s]\u001b[A\n","217it [01:40,  4.32it/s]\u001b[A\n","218it [01:40,  4.32it/s]\u001b[A\n","219it [01:40,  4.33it/s]\u001b[A\n","220it [01:41,  4.33it/s]\u001b[A\n","221it [01:41,  4.33it/s]\u001b[A\n","222it [01:41,  4.33it/s]\u001b[A\n","223it [01:41,  4.33it/s]\u001b[A\n","224it [01:42,  4.33it/s]\u001b[A\n","225it [01:42,  4.33it/s]\u001b[A\n","226it [01:42,  4.33it/s]\u001b[A\n","227it [01:42,  4.32it/s]\u001b[A\n","228it [01:42,  4.32it/s]\u001b[A\n","229it [01:43,  4.32it/s]\u001b[A\n","230it [01:43,  4.32it/s]\u001b[A\n","231it [01:43,  4.32it/s]\u001b[A\n","232it [01:43,  4.33it/s]\u001b[A\n","233it [01:44,  4.33it/s]\u001b[A\n","234it [01:44,  4.33it/s]\u001b[A\n","235it [01:44,  4.33it/s]\u001b[A\n","236it [01:44,  4.33it/s]\u001b[A\n","237it [01:45,  4.33it/s]\u001b[A\n","238it [01:45,  4.33it/s]\u001b[A\n","239it [01:45,  4.33it/s]\u001b[A\n","240it [01:45,  4.33it/s]\u001b[A\n","241it [01:45,  4.33it/s]\u001b[A\n","242it [01:46,  4.33it/s]\u001b[A\n","243it [01:46,  4.33it/s]\u001b[A\n","244it [01:46,  4.33it/s]\u001b[A\n","245it [01:46,  4.33it/s]\u001b[A\n","246it [01:47,  4.33it/s]\u001b[A\n","247it [01:47,  4.33it/s]\u001b[A\n","248it [01:47,  4.33it/s]\u001b[A\n","249it [01:47,  4.33it/s]\u001b[A\n","250it [01:48,  4.33it/s]\u001b[A\n","251it [01:48,  4.33it/s]\u001b[A\n","252it [01:48,  4.33it/s]\u001b[A\n","253it [01:48,  4.33it/s]\u001b[A\n","254it [01:48,  4.33it/s]\u001b[A\n","255it [01:49,  4.33it/s]\u001b[A\n","256it [01:49,  4.33it/s]\u001b[A\n","257it [01:49,  4.33it/s]\u001b[A\n","258it [01:49,  4.33it/s]\u001b[A\n","259it [01:50,  4.33it/s]\u001b[A\n","260it [01:50,  4.33it/s]\u001b[A\n","261it [01:50,  4.33it/s]\u001b[A\n","262it [01:50,  4.33it/s]\u001b[A\n","263it [01:51,  4.32it/s]\u001b[A\n","264it [01:51,  4.33it/s]\u001b[A\n","265it [01:51,  4.33it/s]\u001b[A\n","266it [01:51,  4.33it/s]\u001b[A\n","267it [01:51,  4.33it/s]\u001b[A\n","268it [01:52,  4.33it/s]\u001b[A\n","269it [01:52,  4.33it/s]\u001b[A\n","270it [01:52,  4.32it/s]\u001b[A\n","271it [01:52,  4.33it/s]\u001b[A\n","272it [01:53,  4.32it/s]\u001b[A\n","273it [01:53,  4.32it/s]\u001b[A\n","274it [01:53,  4.33it/s]\u001b[A\n","275it [01:53,  4.33it/s]\u001b[A\n","276it [01:54,  4.33it/s]\u001b[A\n","277it [01:54,  4.33it/s]\u001b[A\n","278it [01:54,  4.33it/s]\u001b[A\n","279it [01:54,  4.33it/s]\u001b[A\n","280it [01:54,  4.33it/s]\u001b[A\n","281it [01:55,  4.33it/s]\u001b[A\n","282it [01:55,  4.33it/s]\u001b[A\n","283it [01:55,  4.32it/s]\u001b[A\n","284it [01:55,  4.33it/s]\u001b[A\n","285it [01:56,  4.33it/s]\u001b[A\n","286it [01:56,  4.33it/s]\u001b[A\n","287it [01:56,  4.33it/s]\u001b[A\n","288it [01:56,  4.33it/s]\u001b[A\n","289it [01:57,  4.33it/s]\u001b[A\n","290it [01:57,  4.33it/s]\u001b[A\n","291it [01:57,  4.33it/s]\u001b[A\n","292it [01:57,  4.33it/s]\u001b[A\n","293it [01:58,  4.32it/s]\u001b[A\n","294it [01:58,  4.32it/s]\u001b[A\n","295it [01:58,  4.32it/s]\u001b[A\n","296it [01:58,  4.32it/s]\u001b[A\n","297it [01:58,  4.32it/s]\u001b[A\n","298it [01:59,  4.32it/s]\u001b[A\n","299it [01:59,  4.32it/s]\u001b[A\n","300it [01:59,  4.33it/s]\u001b[A\n","301it [01:59,  4.33it/s]\u001b[A\n","302it [02:00,  4.33it/s]\u001b[A\n","303it [02:00,  4.33it/s]\u001b[A\n","304it [02:00,  4.33it/s]\u001b[A\n","305it [02:00,  4.32it/s]\u001b[A\n","306it [02:01,  4.32it/s]\u001b[A\n","307it [02:01,  4.32it/s]\u001b[A\n","308it [02:01,  4.33it/s]\u001b[A\n","309it [02:01,  4.33it/s]\u001b[A\n","310it [02:01,  4.33it/s]\u001b[A\n","311it [02:02,  4.33it/s]\u001b[A\n","312it [02:02,  4.33it/s]\u001b[A\n","313it [02:02,  4.33it/s]\u001b[A\n","314it [02:02,  4.33it/s]\u001b[A\n","315it [02:03,  4.32it/s]\u001b[A\n","316it [02:03,  4.32it/s]\u001b[A\n","317it [02:03,  4.33it/s]\u001b[A\n","318it [02:03,  4.33it/s]\u001b[A\n","319it [02:04,  4.33it/s]\u001b[A\n","320it [02:04,  4.33it/s]\u001b[A\n","321it [02:04,  4.33it/s]\u001b[A\n","322it [02:04,  4.33it/s]\u001b[A\n","323it [02:04,  4.33it/s]\u001b[A\n","324it [02:05,  4.33it/s]\u001b[A\n","325it [02:05,  4.33it/s]\u001b[A\n","326it [02:05,  4.33it/s]\u001b[A\n","327it [02:05,  4.33it/s]\u001b[A\n","328it [02:06,  4.33it/s]\u001b[A\n","329it [02:06,  4.33it/s]\u001b[A\n","330it [02:06,  4.33it/s]\u001b[A\n","331it [02:06,  4.33it/s]\u001b[A\n","332it [02:07,  4.33it/s]\u001b[A\n","333it [02:07,  4.33it/s]\u001b[A\n","334it [02:07,  4.33it/s]\u001b[A\n","335it [02:07,  4.33it/s]\u001b[A\n","336it [02:07,  4.33it/s]\u001b[A\n","337it [02:08,  4.33it/s]\u001b[A\n","338it [02:08,  4.33it/s]\u001b[A\n","339it [02:08,  4.32it/s]\u001b[A\n","340it [02:08,  4.32it/s]\u001b[A\n","341it [02:09,  4.33it/s]\u001b[A\n","342it [02:09,  4.33it/s]\u001b[A\n","343it [02:09,  4.33it/s]\u001b[A\n","344it [02:09,  4.33it/s]\u001b[A\n","345it [02:10,  4.33it/s]\u001b[A\n","346it [02:10,  4.33it/s]\u001b[A\n","347it [02:10,  4.33it/s]\u001b[A\n","348it [02:10,  4.33it/s]\u001b[A\n","349it [02:10,  4.32it/s]\u001b[A\n","350it [02:11,  4.33it/s]\u001b[A\n","351it [02:11,  4.33it/s]\u001b[A\n","352it [02:11,  4.33it/s]\u001b[A\n","353it [02:11,  4.33it/s]\u001b[A\n","354it [02:12,  4.33it/s]\u001b[A\n","355it [02:12,  4.33it/s]\u001b[A\n","356it [02:12,  4.33it/s]\u001b[A\n","357it [02:12,  4.33it/s]\u001b[A\n","358it [02:13,  4.32it/s]\u001b[A\n","359it [02:13,  4.33it/s]\u001b[A\n","360it [02:13,  4.33it/s]\u001b[A\n","361it [02:13,  4.33it/s]\u001b[A\n","362it [02:13,  4.33it/s]\u001b[A\n","363it [02:14,  4.33it/s]\u001b[A\n","364it [02:14,  4.33it/s]\u001b[A\n","365it [02:14,  4.33it/s]\u001b[A\n","366it [02:14,  4.33it/s]\u001b[A\n","367it [02:15,  4.32it/s]\u001b[A\n","368it [02:15,  4.32it/s]\u001b[A\n","369it [02:15,  4.32it/s]\u001b[A\n","Epoch:  30% 74/250 [03:38<04:55,  1.68s/it]\n","371it [02:57, 12.76s/it]\u001b[A\n","372it [02:58,  9.00s/it]\u001b[A\n","373it [02:58,  6.37s/it]\u001b[A\n","374it [02:58,  4.53s/it]\u001b[A\n","375it [02:58,  3.24s/it]\u001b[A\n","376it [02:58,  2.34s/it]\u001b[A\n","377it [02:59,  1.71s/it]\u001b[A\n","378it [02:59,  1.26s/it]\u001b[A\n","379it [02:59,  1.05it/s]\u001b[A\n","380it [02:59,  1.36it/s]\u001b[A\n","381it [03:00,  1.71it/s]\u001b[A\n","382it [03:00,  2.09it/s]\u001b[A\n","383it [03:00,  2.47it/s]\u001b[A\n","384it [03:00,  2.84it/s]\u001b[A\n","385it [03:01,  3.16it/s]\u001b[A\n","386it [03:01,  3.44it/s]\u001b[A\n","387it [03:01,  3.67it/s]\u001b[A\n","388it [03:01,  3.84it/s]\u001b[A\n","389it [03:01,  3.98it/s]\u001b[A\n","390it [03:02,  4.08it/s]\u001b[A\n","391it [03:02,  4.15it/s]\u001b[A\n","392it [03:02,  4.20it/s]\u001b[A\n","393it [03:02,  4.24it/s]\u001b[A\n","394it [03:03,  4.26it/s]\u001b[A\n","395it [03:03,  4.28it/s]\u001b[A\n","396it [03:03,  4.30it/s]\u001b[A\n","397it [03:03,  4.31it/s]\u001b[A\n","398it [03:04,  4.31it/s]\u001b[A\n","399it [03:04,  4.31it/s]\u001b[A\n","400it [03:04,  4.32it/s]\u001b[A\n","401it [03:04,  4.32it/s]\u001b[A\n","402it [03:04,  4.32it/s]\u001b[A\n","403it [03:05,  4.32it/s]\u001b[A\n","404it [03:05,  4.32it/s]\u001b[A\n","405it [03:05,  4.32it/s]\u001b[A\n","406it [03:05,  4.33it/s]\u001b[A\n","407it [03:06,  4.32it/s]\u001b[A\n","408it [03:06,  4.32it/s]\u001b[A\n","409it [03:06,  4.32it/s]\u001b[A\n","410it [03:06,  4.33it/s]\u001b[A\n","411it [03:07,  4.32it/s]\u001b[A\n","412it [03:07,  4.32it/s]\u001b[A\n","413it [03:07,  4.33it/s]\u001b[A\n","414it [03:07,  4.32it/s]\u001b[A\n","415it [03:07,  4.32it/s]\u001b[A\n","416it [03:08,  4.32it/s]\u001b[A\n","417it [03:08,  4.32it/s]\u001b[A\n","418it [03:08,  4.33it/s]\u001b[A\n","419it [03:08,  4.33it/s]\u001b[A\n","420it [03:09,  4.33it/s]\u001b[A\n","421it [03:09,  4.33it/s]\u001b[A\n","422it [03:09,  4.33it/s]\u001b[A\n","423it [03:09,  4.32it/s]\u001b[A\n","424it [03:10,  4.32it/s]\u001b[A\n","425it [03:10,  4.32it/s]\u001b[A\n","426it [03:10,  4.32it/s]\u001b[A\n","427it [03:10,  4.33it/s]\u001b[A\n","428it [03:10,  4.33it/s]\u001b[A\n","429it [03:11,  4.33it/s]\u001b[A\n","430it [03:11,  4.33it/s]\u001b[A\n","431it [03:11,  4.33it/s]\u001b[A\n","432it [03:11,  4.32it/s]\u001b[A\n","433it [03:12,  4.33it/s]\u001b[A\n","434it [03:12,  4.32it/s]\u001b[A\n","435it [03:12,  4.32it/s]\u001b[A\n","436it [03:12,  4.33it/s]\u001b[A\n","437it [03:13,  4.32it/s]\u001b[A\n","438it [03:13,  4.32it/s]\u001b[A\n","439it [03:13,  4.32it/s]\u001b[A\n","440it [03:13,  4.33it/s]\u001b[A\n","441it [03:13,  4.33it/s]\u001b[A\n","442it [03:14,  4.33it/s]\u001b[A\n","443it [03:14,  4.33it/s]\u001b[A\n","444it [03:14,  4.33it/s]\u001b[A\n","445it [03:14,  4.32it/s]\u001b[A\n","446it [03:15,  4.32it/s]\u001b[A\n","447it [03:15,  4.32it/s]\u001b[A\n","448it [03:15,  4.32it/s]\u001b[A\n","449it [03:15,  4.32it/s]\u001b[A\n","450it [03:16,  4.33it/s]\u001b[A\n","451it [03:16,  4.33it/s]\u001b[A\n","452it [03:16,  4.33it/s]\u001b[A\n","453it [03:16,  4.33it/s]\u001b[A\n","454it [03:16,  4.33it/s]\u001b[A\n","455it [03:17,  4.33it/s]\u001b[A\n","456it [03:17,  4.33it/s]\u001b[A\n","457it [03:17,  4.33it/s]\u001b[A\n","458it [03:17,  4.32it/s]\u001b[A\n","459it [03:18,  4.32it/s]\u001b[A\n","460it [03:18,  4.32it/s]\u001b[A\n","461it [03:18,  4.32it/s]\u001b[A\n","462it [03:18,  4.32it/s]\u001b[A\n","463it [03:19,  4.33it/s]\u001b[A\n","464it [03:19,  4.33it/s]\u001b[A\n","465it [03:19,  4.33it/s]\u001b[A\n","466it [03:19,  4.33it/s]\u001b[A\n","467it [03:19,  4.33it/s]\u001b[A\n","468it [03:20,  4.33it/s]\u001b[A\n","469it [03:20,  4.33it/s]\u001b[A\n","470it [03:20,  4.32it/s]\u001b[A\n","471it [03:20,  4.32it/s]\u001b[A\n","472it [03:21,  4.33it/s]\u001b[A\n","473it [03:21,  4.33it/s]\u001b[A\n","474it [03:21,  4.33it/s]\u001b[A\n","475it [03:21,  4.33it/s]\u001b[A\n","476it [03:22,  4.33it/s]\u001b[A\n","477it [03:22,  4.32it/s]\u001b[A\n","478it [03:22,  4.32it/s]\u001b[A\n","479it [03:22,  4.33it/s]\u001b[A\n","480it [03:22,  4.32it/s]\u001b[A\n","481it [03:23,  4.32it/s]\u001b[A\n","482it [03:23,  4.32it/s]\u001b[A\n","483it [03:23,  4.33it/s]\u001b[A\n","484it [03:23,  4.33it/s]\u001b[A\n","485it [03:24,  4.33it/s]\u001b[A\n","486it [03:24,  4.33it/s]\u001b[A\n","487it [03:24,  4.33it/s]\u001b[A\n","488it [03:24,  4.32it/s]\u001b[A\n","489it [03:25,  4.33it/s]\u001b[A\n","490it [03:25,  4.33it/s]\u001b[A\n","491it [03:25,  4.32it/s]\u001b[A\n","492it [03:25,  4.33it/s]\u001b[A\n","493it [03:25,  4.33it/s]\u001b[A\n","494it [03:26,  4.32it/s]\u001b[A\n","495it [03:26,  4.33it/s]\u001b[A\n","496it [03:26,  4.33it/s]\u001b[A\n","497it [03:26,  4.33it/s]\u001b[A\n","498it [03:27,  4.32it/s]\u001b[A\n","499it [03:27,  4.32it/s]\u001b[A\n","500it [03:27,  4.32it/s]\u001b[A\n","501it [03:27,  4.32it/s]\u001b[A\n","502it [03:28,  4.32it/s]\u001b[A\n","503it [03:28,  4.32it/s]\u001b[A\n","504it [03:28,  4.32it/s]\u001b[A\n","505it [03:28,  4.32it/s]\u001b[A\n","506it [03:29,  4.32it/s]\u001b[A\n","507it [03:29,  4.32it/s]\u001b[A\n","508it [03:29,  4.33it/s]\u001b[A\n","509it [03:29,  4.32it/s]\u001b[A\n","510it [03:29,  4.32it/s]\u001b[A\n","511it [03:30,  4.32it/s]\u001b[A\n","512it [03:30,  4.32it/s]\u001b[A\n","513it [03:30,  4.32it/s]\u001b[A\n","514it [03:30,  4.32it/s]\u001b[A\n","515it [03:31,  4.32it/s]\u001b[A\n","516it [03:31,  4.32it/s]\u001b[A\n","517it [03:31,  4.32it/s]\u001b[A\n","518it [03:31,  4.33it/s]\u001b[A\n","519it [03:32,  4.33it/s]\u001b[A\n","520it [03:32,  4.33it/s]\u001b[A\n","521it [03:32,  4.33it/s]\u001b[A\n","522it [03:32,  4.33it/s]\u001b[A\n","523it [03:32,  4.33it/s]\u001b[A\n","524it [03:33,  4.33it/s]\u001b[A\n","525it [03:33,  4.32it/s]\u001b[A\n","526it [03:33,  4.32it/s]\u001b[A\n","527it [03:33,  4.32it/s]\u001b[A\n","528it [03:34,  4.32it/s]\u001b[A\n","529it [03:34,  4.33it/s]\u001b[A\n","530it [03:34,  4.33it/s]\u001b[A\n","531it [03:34,  4.33it/s]\u001b[A\n","532it [03:35,  4.33it/s]\u001b[A\n","533it [03:35,  4.33it/s]\u001b[A\n","534it [03:35,  4.33it/s]\u001b[A\n","535it [03:35,  4.33it/s]\u001b[A\n","536it [03:35,  4.32it/s]\u001b[A\n","537it [03:36,  4.32it/s]\u001b[A\n","538it [03:36,  4.32it/s]\u001b[A\n","539it [03:36,  4.33it/s]\u001b[A\n","540it [03:36,  4.33it/s]\u001b[A\n","541it [03:37,  4.33it/s]\u001b[A\n","542it [03:37,  4.33it/s]\u001b[A\n","543it [03:37,  4.33it/s]\u001b[A\n","544it [03:37,  4.33it/s]\u001b[A\n","545it [03:38,  4.33it/s]\u001b[A\n","546it [03:38,  4.33it/s]\u001b[A\n","547it [03:38,  4.32it/s]\u001b[A\n","548it [03:38,  4.33it/s]\u001b[A\n","549it [03:38,  4.33it/s]\u001b[A\n","550it [03:39,  4.33it/s]\u001b[A\n","551it [03:39,  4.32it/s]\u001b[A\n","552it [03:39,  4.32it/s]\u001b[A\n","553it [03:39,  4.32it/s]\u001b[A\n","554it [03:40,  4.32it/s]\u001b[A\n","555it [03:40,  4.32it/s]\u001b[A03/14/2022 09:39:00 - INFO - src.trainer -   Best dev result: 0.937250018119812\n","Epoch:  40% 99/250 [05:10<04:13,  1.68s/it]\n","556it [04:29, 15.05s/it]\u001b[A\n","557it [04:30, 10.60s/it]\u001b[A\n","558it [04:30,  7.49s/it]\u001b[A\n","559it [04:30,  5.31s/it]\u001b[A\n","560it [04:30,  3.79s/it]\u001b[A\n","561it [04:31,  2.72s/it]\u001b[A\n","562it [04:31,  1.97s/it]\u001b[A\n","563it [04:31,  1.45s/it]\u001b[A\n","564it [04:31,  1.09s/it]\u001b[A\n","565it [04:32,  1.21it/s]\u001b[A\n","566it [04:32,  1.54it/s]\u001b[A\n","567it [04:32,  1.91it/s]\u001b[A\n","568it [04:32,  2.29it/s]\u001b[A\n","569it [04:32,  2.67it/s]\u001b[A\n","570it [04:33,  3.02it/s]\u001b[A\n","571it [04:33,  3.32it/s]\u001b[A\n","572it [04:33,  3.57it/s]\u001b[A\n","573it [04:33,  3.76it/s]\u001b[A\n","574it [04:34,  3.92it/s]\u001b[A\n","575it [04:34,  4.03it/s]\u001b[A\n","576it [04:34,  4.11it/s]\u001b[A\n","577it [04:34,  4.18it/s]\u001b[A\n","578it [04:35,  4.22it/s]\u001b[A\n","579it [04:35,  4.25it/s]\u001b[A\n","580it [04:35,  4.27it/s]\u001b[A\n","581it [04:35,  4.29it/s]\u001b[A\n","582it [04:35,  4.30it/s]\u001b[A\n","583it [04:36,  4.31it/s]\u001b[A\n","584it [04:36,  4.31it/s]\u001b[A\n","585it [04:36,  4.32it/s]\u001b[A\n","586it [04:36,  4.32it/s]\u001b[A\n","587it [04:37,  4.32it/s]\u001b[A\n","588it [04:37,  4.32it/s]\u001b[A\n","589it [04:37,  4.32it/s]\u001b[A\n","590it [04:37,  4.32it/s]\u001b[A\n","591it [04:38,  4.32it/s]\u001b[A\n","592it [04:38,  4.32it/s]\u001b[A\n","593it [04:38,  4.33it/s]\u001b[A\n","594it [04:38,  4.33it/s]\u001b[A\n","595it [04:38,  4.33it/s]\u001b[A\n","596it [04:39,  4.33it/s]\u001b[A\n","597it [04:39,  4.33it/s]\u001b[A\n","598it [04:39,  4.33it/s]\u001b[A\n","599it [04:39,  4.32it/s]\u001b[A\n","600it [04:40,  4.33it/s]\u001b[A\n","601it [04:40,  4.33it/s]\u001b[A\n","602it [04:40,  4.33it/s]\u001b[A\n","603it [04:40,  4.33it/s]\u001b[A\n","604it [04:41,  4.33it/s]\u001b[A\n","605it [04:41,  4.33it/s]\u001b[A\n","606it [04:41,  4.33it/s]\u001b[A\n","607it [04:41,  4.33it/s]\u001b[A\n","608it [04:41,  4.33it/s]\u001b[A\n","609it [04:42,  4.32it/s]\u001b[A\n","610it [04:42,  4.33it/s]\u001b[A\n","611it [04:42,  4.33it/s]\u001b[A\n","612it [04:42,  4.32it/s]\u001b[A\n","613it [04:43,  4.33it/s]\u001b[A\n","614it [04:43,  4.33it/s]\u001b[A\n","615it [04:43,  4.33it/s]\u001b[A\n","616it [04:43,  4.33it/s]\u001b[A\n","617it [04:44,  4.33it/s]\u001b[A\n","618it [04:44,  4.33it/s]\u001b[A\n","619it [04:44,  4.33it/s]\u001b[A\n","620it [04:44,  4.33it/s]\u001b[A\n","621it [04:44,  4.32it/s]\u001b[A\n","622it [04:45,  4.32it/s]\u001b[A\n","623it [04:45,  4.33it/s]\u001b[A\n","624it [04:45,  4.33it/s]\u001b[A\n","625it [04:45,  4.33it/s]\u001b[A\n","626it [04:46,  4.33it/s]\u001b[A\n","627it [04:46,  4.33it/s]\u001b[A\n","628it [04:46,  4.33it/s]\u001b[A\n","629it [04:46,  4.33it/s]\u001b[A\n","630it [04:47,  4.33it/s]\u001b[A\n","631it [04:47,  4.32it/s]\u001b[A\n","632it [04:47,  4.32it/s]\u001b[A\n","633it [04:47,  4.32it/s]\u001b[A\n","634it [04:47,  4.33it/s]\u001b[A\n","635it [04:48,  4.33it/s]\u001b[A\n","636it [04:48,  4.33it/s]\u001b[A\n","637it [04:48,  4.33it/s]\u001b[A\n","638it [04:48,  4.33it/s]\u001b[A\n","639it [04:49,  4.33it/s]\u001b[A\n","640it [04:49,  4.33it/s]\u001b[A\n","641it [04:49,  4.33it/s]\u001b[A\n","642it [04:49,  4.32it/s]\u001b[A\n","643it [04:50,  4.32it/s]\u001b[A\n","644it [04:50,  4.33it/s]\u001b[A\n","645it [04:50,  4.33it/s]\u001b[A\n","646it [04:50,  4.33it/s]\u001b[A\n","647it [04:50,  4.33it/s]\u001b[A\n","648it [04:51,  4.33it/s]\u001b[A\n","649it [04:51,  4.33it/s]\u001b[A\n","650it [04:51,  4.33it/s]\u001b[A\n","651it [04:51,  4.33it/s]\u001b[A\n","652it [04:52,  4.32it/s]\u001b[A\n","653it [04:52,  4.32it/s]\u001b[A\n","654it [04:52,  4.32it/s]\u001b[A\n","655it [04:52,  4.32it/s]\u001b[A\n","656it [04:53,  4.32it/s]\u001b[A\n","657it [04:53,  4.33it/s]\u001b[A\n","658it [04:53,  4.33it/s]\u001b[A\n","659it [04:53,  4.33it/s]\u001b[A\n","660it [04:53,  4.33it/s]\u001b[A\n","661it [04:54,  4.33it/s]\u001b[A\n","662it [04:54,  4.32it/s]\u001b[A\n","663it [04:54,  4.33it/s]\u001b[A\n","664it [04:54,  4.32it/s]\u001b[A\n","665it [04:55,  4.32it/s]\u001b[A\n","666it [04:55,  4.33it/s]\u001b[A\n","667it [04:55,  4.33it/s]\u001b[A\n","668it [04:55,  4.33it/s]\u001b[A\n","669it [04:56,  4.33it/s]\u001b[A\n","670it [04:56,  4.33it/s]\u001b[A\n","671it [04:56,  4.33it/s]\u001b[A\n","672it [04:56,  4.33it/s]\u001b[A\n","673it [04:56,  4.33it/s]\u001b[A\n","674it [04:57,  4.33it/s]\u001b[A\n","675it [04:57,  4.33it/s]\u001b[A\n","676it [04:57,  4.33it/s]\u001b[A\n","677it [04:57,  4.33it/s]\u001b[A\n","678it [04:58,  4.33it/s]\u001b[A\n","679it [04:58,  4.33it/s]\u001b[A\n","680it [04:58,  4.33it/s]\u001b[A\n","681it [04:58,  4.33it/s]\u001b[A\n","682it [04:59,  4.32it/s]\u001b[A\n","683it [04:59,  4.33it/s]\u001b[A\n","684it [04:59,  4.33it/s]\u001b[A\n","685it [04:59,  4.32it/s]\u001b[A\n","686it [04:59,  4.33it/s]\u001b[A\n","687it [05:00,  4.32it/s]\u001b[A\n","688it [05:00,  4.32it/s]\u001b[A\n","689it [05:00,  4.32it/s]\u001b[A\n","690it [05:00,  4.32it/s]\u001b[A\n","691it [05:01,  4.33it/s]\u001b[A\n","692it [05:01,  4.33it/s]\u001b[A\n","693it [05:01,  4.33it/s]\u001b[A\n","694it [05:01,  4.33it/s]\u001b[A\n","695it [05:02,  4.33it/s]\u001b[A\n","696it [05:02,  4.33it/s]\u001b[A\n","697it [05:02,  4.32it/s]\u001b[A\n","698it [05:02,  4.32it/s]\u001b[A\n","699it [05:03,  4.33it/s]\u001b[A\n","700it [05:03,  4.33it/s]\u001b[A\n","701it [05:03,  4.33it/s]\u001b[A\n","702it [05:03,  4.33it/s]\u001b[A\n","703it [05:03,  4.33it/s]\u001b[A\n","704it [05:04,  4.33it/s]\u001b[A\n","705it [05:04,  4.33it/s]\u001b[A\n","706it [05:04,  4.33it/s]\u001b[A\n","707it [05:04,  4.32it/s]\u001b[A\n","708it [05:05,  4.32it/s]\u001b[A\n","709it [05:05,  4.32it/s]\u001b[A\n","710it [05:05,  4.33it/s]\u001b[A\n","711it [05:05,  4.33it/s]\u001b[A\n","712it [05:06,  4.33it/s]\u001b[A\n","713it [05:06,  4.33it/s]\u001b[A\n","714it [05:06,  4.33it/s]\u001b[A\n","715it [05:06,  4.33it/s]\u001b[A\n","716it [05:06,  4.33it/s]\u001b[A\n","717it [05:07,  4.33it/s]\u001b[A\n","718it [05:07,  4.32it/s]\u001b[A\n","719it [05:07,  4.32it/s]\u001b[A\n","720it [05:07,  4.32it/s]\u001b[A\n","721it [05:08,  4.32it/s]\u001b[A\n","722it [05:08,  4.32it/s]\u001b[A\n","723it [05:08,  4.32it/s]\u001b[A\n","724it [05:08,  4.33it/s]\u001b[A\n","725it [05:09,  4.33it/s]\u001b[A\n","726it [05:09,  4.33it/s]\u001b[A\n","727it [05:09,  4.33it/s]\u001b[A\n","728it [05:09,  4.32it/s]\u001b[A\n","729it [05:09,  4.32it/s]\u001b[A\n","730it [05:10,  4.32it/s]\u001b[A\n","731it [05:10,  4.32it/s]\u001b[A\n","732it [05:10,  4.32it/s]\u001b[A\n","733it [05:10,  4.32it/s]\u001b[A\n","734it [05:11,  4.32it/s]\u001b[A\n","735it [05:11,  4.33it/s]\u001b[A\n","736it [05:11,  4.33it/s]\u001b[A\n","737it [05:11,  4.33it/s]\u001b[A\n","738it [05:12,  4.32it/s]\u001b[A\n","739it [05:12,  4.33it/s]\u001b[A\n","Epoch:  50% 124/250 [06:34<03:30,  1.67s/it]\n","741it [05:54, 12.76s/it]\u001b[A\n","742it [05:54,  9.01s/it]\u001b[A\n","743it [05:54,  6.37s/it]\u001b[A\n","744it [05:55,  4.53s/it]\u001b[A\n","745it [05:55,  3.24s/it]\u001b[A\n","746it [05:55,  2.34s/it]\u001b[A\n","747it [05:55,  1.71s/it]\u001b[A\n","748it [05:56,  1.26s/it]\u001b[A\n","749it [05:56,  1.05it/s]\u001b[A\n","750it [05:56,  1.36it/s]\u001b[A\n","751it [05:56,  1.71it/s]\u001b[A\n","752it [05:57,  2.09it/s]\u001b[A\n","753it [05:57,  2.47it/s]\u001b[A\n","754it [05:57,  2.84it/s]\u001b[A\n","755it [05:57,  3.16it/s]\u001b[A\n","756it [05:57,  3.44it/s]\u001b[A\n","757it [05:58,  3.67it/s]\u001b[A\n","758it [05:58,  3.84it/s]\u001b[A\n","759it [05:58,  3.98it/s]\u001b[A\n","760it [05:58,  4.07it/s]\u001b[A\n","761it [05:59,  4.15it/s]\u001b[A\n","762it [05:59,  4.20it/s]\u001b[A\n","763it [05:59,  4.23it/s]\u001b[A\n","764it [05:59,  4.26it/s]\u001b[A\n","765it [06:00,  4.28it/s]\u001b[A\n","766it [06:00,  4.29it/s]\u001b[A\n","767it [06:00,  4.30it/s]\u001b[A\n","768it [06:00,  4.31it/s]\u001b[A\n","769it [06:00,  4.32it/s]\u001b[A\n","770it [06:01,  4.32it/s]\u001b[A\n","771it [06:01,  4.32it/s]\u001b[A\n","772it [06:01,  4.32it/s]\u001b[A\n","773it [06:01,  4.32it/s]\u001b[A\n","774it [06:02,  4.32it/s]\u001b[A\n","775it [06:02,  4.32it/s]\u001b[A\n","776it [06:02,  4.32it/s]\u001b[A\n","777it [06:02,  4.32it/s]\u001b[A\n","778it [06:03,  4.33it/s]\u001b[A\n","779it [06:03,  4.33it/s]\u001b[A\n","780it [06:03,  4.33it/s]\u001b[A\n","781it [06:03,  4.33it/s]\u001b[A\n","782it [06:03,  4.32it/s]\u001b[A\n","783it [06:04,  4.32it/s]\u001b[A\n","784it [06:04,  4.33it/s]\u001b[A\n","785it [06:04,  4.32it/s]\u001b[A\n","786it [06:04,  4.32it/s]\u001b[A\n","787it [06:05,  4.32it/s]\u001b[A\n","788it [06:05,  4.33it/s]\u001b[A\n","789it [06:05,  4.33it/s]\u001b[A\n","790it [06:05,  4.33it/s]\u001b[A\n","791it [06:06,  4.33it/s]\u001b[A\n","792it [06:06,  4.33it/s]\u001b[A\n","793it [06:06,  4.33it/s]\u001b[A\n","794it [06:06,  4.33it/s]\u001b[A\n","795it [06:06,  4.32it/s]\u001b[A\n","796it [06:07,  4.32it/s]\u001b[A\n","797it [06:07,  4.32it/s]\u001b[A\n","798it [06:07,  4.32it/s]\u001b[A\n","799it [06:07,  4.32it/s]\u001b[A\n","800it [06:08,  4.32it/s]\u001b[A\n","801it [06:08,  4.32it/s]\u001b[A\n","802it [06:08,  4.32it/s]\u001b[A\n","803it [06:08,  4.33it/s]\u001b[A\n","804it [06:09,  4.33it/s]\u001b[A\n","805it [06:09,  4.33it/s]\u001b[A\n","806it [06:09,  4.33it/s]\u001b[A\n","807it [06:09,  4.33it/s]\u001b[A\n","808it [06:09,  4.33it/s]\u001b[A\n","809it [06:10,  4.32it/s]\u001b[A\n","810it [06:10,  4.32it/s]\u001b[A\n","811it [06:10,  4.33it/s]\u001b[A\n","812it [06:10,  4.33it/s]\u001b[A\n","813it [06:11,  4.33it/s]\u001b[A\n","814it [06:11,  4.33it/s]\u001b[A\n","815it [06:11,  4.32it/s]\u001b[A\n","816it [06:11,  4.32it/s]\u001b[A\n","817it [06:12,  4.32it/s]\u001b[A\n","818it [06:12,  4.32it/s]\u001b[A\n","819it [06:12,  4.32it/s]\u001b[A\n","820it [06:12,  4.32it/s]\u001b[A\n","821it [06:12,  4.32it/s]\u001b[A\n","822it [06:13,  4.32it/s]\u001b[A\n","823it [06:13,  4.32it/s]\u001b[A\n","824it [06:13,  4.32it/s]\u001b[A\n","825it [06:13,  4.33it/s]\u001b[A\n","826it [06:14,  4.33it/s]\u001b[A\n","827it [06:14,  4.33it/s]\u001b[A\n","828it [06:14,  4.33it/s]\u001b[A\n","829it [06:14,  4.32it/s]\u001b[A\n","830it [06:15,  4.33it/s]\u001b[A\n","831it [06:15,  4.32it/s]\u001b[A\n","832it [06:15,  4.32it/s]\u001b[A\n","833it [06:15,  4.33it/s]\u001b[A\n","834it [06:15,  4.33it/s]\u001b[A\n","835it [06:16,  4.32it/s]\u001b[A\n","836it [06:16,  4.33it/s]\u001b[A\n","837it [06:16,  4.33it/s]\u001b[A\n","838it [06:16,  4.33it/s]\u001b[A\n","839it [06:17,  4.32it/s]\u001b[A\n","840it [06:17,  4.33it/s]\u001b[A\n","841it [06:17,  4.32it/s]\u001b[A\n","842it [06:17,  4.32it/s]\u001b[A\n","843it [06:18,  4.33it/s]\u001b[A\n","844it [06:18,  4.33it/s]\u001b[A\n","845it [06:18,  4.33it/s]\u001b[A\n","846it [06:18,  4.33it/s]\u001b[A\n","847it [06:19,  4.33it/s]\u001b[A\n","848it [06:19,  4.33it/s]\u001b[A\n","849it [06:19,  4.32it/s]\u001b[A\n","850it [06:19,  4.33it/s]\u001b[A\n","851it [06:19,  4.33it/s]\u001b[A\n","852it [06:20,  4.33it/s]\u001b[A\n","853it [06:20,  4.33it/s]\u001b[A\n","854it [06:20,  4.33it/s]\u001b[A\n","855it [06:20,  4.33it/s]\u001b[A\n","856it [06:21,  4.33it/s]\u001b[A\n","857it [06:21,  4.33it/s]\u001b[A\n","858it [06:21,  4.33it/s]\u001b[A\n","859it [06:21,  4.33it/s]\u001b[A\n","860it [06:22,  4.33it/s]\u001b[A\n","861it [06:22,  4.33it/s]\u001b[A\n","862it [06:22,  4.32it/s]\u001b[A\n","863it [06:22,  4.32it/s]\u001b[A\n","864it [06:22,  4.32it/s]\u001b[A\n","865it [06:23,  4.32it/s]\u001b[A\n","866it [06:23,  4.32it/s]\u001b[A\n","867it [06:23,  4.33it/s]\u001b[A\n","868it [06:23,  4.33it/s]\u001b[A\n","869it [06:24,  4.33it/s]\u001b[A\n","870it [06:24,  4.33it/s]\u001b[A\n","871it [06:24,  4.33it/s]\u001b[A\n","872it [06:24,  4.32it/s]\u001b[A\n","873it [06:25,  4.33it/s]\u001b[A\n","874it [06:25,  4.32it/s]\u001b[A\n","875it [06:25,  4.32it/s]\u001b[A\n","876it [06:25,  4.33it/s]\u001b[A\n","877it [06:25,  4.33it/s]\u001b[A\n","878it [06:26,  4.33it/s]\u001b[A\n","879it [06:26,  4.33it/s]\u001b[A\n","880it [06:26,  4.33it/s]\u001b[A\n","881it [06:26,  4.33it/s]\u001b[A\n","882it [06:27,  4.33it/s]\u001b[A\n","883it [06:27,  4.33it/s]\u001b[A\n","884it [06:27,  4.32it/s]\u001b[A\n","885it [06:27,  4.32it/s]\u001b[A\n","886it [06:28,  4.32it/s]\u001b[A\n","887it [06:28,  4.32it/s]\u001b[A\n","888it [06:28,  4.33it/s]\u001b[A\n","889it [06:28,  4.32it/s]\u001b[A\n","890it [06:28,  4.32it/s]\u001b[A\n","891it [06:29,  4.32it/s]\u001b[A\n","892it [06:29,  4.32it/s]\u001b[A\n","893it [06:29,  4.32it/s]\u001b[A\n","894it [06:29,  4.32it/s]\u001b[A\n","895it [06:30,  4.32it/s]\u001b[A\n","896it [06:30,  4.32it/s]\u001b[A\n","897it [06:30,  4.32it/s]\u001b[A\n","898it [06:30,  4.32it/s]\u001b[A\n","899it [06:31,  4.32it/s]\u001b[A\n","900it [06:31,  4.33it/s]\u001b[A\n","901it [06:31,  4.33it/s]\u001b[A\n","902it [06:31,  4.33it/s]\u001b[A\n","903it [06:31,  4.33it/s]\u001b[A\n","904it [06:32,  4.33it/s]\u001b[A\n","905it [06:32,  4.33it/s]\u001b[A\n","906it [06:32,  4.33it/s]\u001b[A\n","907it [06:32,  4.32it/s]\u001b[A\n","908it [06:33,  4.32it/s]\u001b[A\n","909it [06:33,  4.32it/s]\u001b[A\n","910it [06:33,  4.32it/s]\u001b[A\n","911it [06:33,  4.33it/s]\u001b[A\n","912it [06:34,  4.33it/s]\u001b[A\n","913it [06:34,  4.33it/s]\u001b[A\n","914it [06:34,  4.33it/s]\u001b[A\n","915it [06:34,  4.33it/s]\u001b[A\n","916it [06:34,  4.33it/s]\u001b[A\n","917it [06:35,  4.33it/s]\u001b[A\n","918it [06:35,  4.32it/s]\u001b[A\n","919it [06:35,  4.33it/s]\u001b[A\n","920it [06:35,  4.32it/s]\u001b[A\n","921it [06:36,  4.32it/s]\u001b[A\n","922it [06:36,  4.33it/s]\u001b[A\n","923it [06:36,  4.33it/s]\u001b[A\n","924it [06:36,  4.33it/s]\u001b[A\n","Epoch:  60% 149/250 [07:59<02:48,  1.67s/it]\n","926it [07:19, 12.76s/it]\u001b[A\n","927it [07:19,  9.00s/it]\u001b[A\n","928it [07:19,  6.37s/it]\u001b[A\n","929it [07:19,  4.53s/it]\u001b[A\n","930it [07:19,  3.24s/it]\u001b[A\n","931it [07:20,  2.34s/it]\u001b[A\n","932it [07:20,  1.71s/it]\u001b[A\n","933it [07:20,  1.26s/it]\u001b[A\n","934it [07:20,  1.05it/s]\u001b[A\n","935it [07:21,  1.36it/s]\u001b[A\n","936it [07:21,  1.71it/s]\u001b[A\n","937it [07:21,  2.09it/s]\u001b[A\n","938it [07:21,  2.47it/s]\u001b[A\n","939it [07:22,  2.84it/s]\u001b[A\n","940it [07:22,  3.16it/s]\u001b[A\n","941it [07:22,  3.44it/s]\u001b[A\n","942it [07:22,  3.66it/s]\u001b[A\n","943it [07:22,  3.84it/s]\u001b[A\n","944it [07:23,  3.97it/s]\u001b[A\n","945it [07:23,  4.07it/s]\u001b[A\n","946it [07:23,  4.15it/s]\u001b[A\n","947it [07:23,  4.20it/s]\u001b[A\n","948it [07:24,  4.23it/s]\u001b[A\n","949it [07:24,  4.26it/s]\u001b[A\n","950it [07:24,  4.28it/s]\u001b[A\n","951it [07:24,  4.29it/s]\u001b[A\n","952it [07:25,  4.30it/s]\u001b[A\n","953it [07:25,  4.31it/s]\u001b[A\n","954it [07:25,  4.32it/s]\u001b[A\n","955it [07:25,  4.32it/s]\u001b[A\n","956it [07:25,  4.32it/s]\u001b[A\n","957it [07:26,  4.32it/s]\u001b[A\n","958it [07:26,  4.33it/s]\u001b[A\n","959it [07:26,  4.33it/s]\u001b[A\n","960it [07:26,  4.32it/s]\u001b[A\n","961it [07:27,  4.32it/s]\u001b[A\n","962it [07:27,  4.32it/s]\u001b[A\n","963it [07:27,  4.32it/s]\u001b[A\n","964it [07:27,  4.32it/s]\u001b[A\n","965it [07:28,  4.32it/s]\u001b[A\n","966it [07:28,  4.33it/s]\u001b[A\n","967it [07:28,  4.33it/s]\u001b[A\n","968it [07:28,  4.33it/s]\u001b[A\n","969it [07:28,  4.33it/s]\u001b[A\n","970it [07:29,  4.33it/s]\u001b[A\n","971it [07:29,  4.33it/s]\u001b[A\n","972it [07:29,  4.33it/s]\u001b[A\n","973it [07:29,  4.32it/s]\u001b[A\n","974it [07:30,  4.32it/s]\u001b[A\n","975it [07:30,  4.32it/s]\u001b[A\n","976it [07:30,  4.33it/s]\u001b[A\n","977it [07:30,  4.33it/s]\u001b[A\n","978it [07:31,  4.33it/s]\u001b[A\n","979it [07:31,  4.33it/s]\u001b[A\n","980it [07:31,  4.33it/s]\u001b[A\n","981it [07:31,  4.33it/s]\u001b[A\n","982it [07:31,  4.33it/s]\u001b[A\n","983it [07:32,  4.33it/s]\u001b[A\n","984it [07:32,  4.32it/s]\u001b[A\n","985it [07:32,  4.33it/s]\u001b[A\n","986it [07:32,  4.33it/s]\u001b[A\n","987it [07:33,  4.33it/s]\u001b[A\n","988it [07:33,  4.33it/s]\u001b[A\n","989it [07:33,  4.33it/s]\u001b[A\n","990it [07:33,  4.33it/s]\u001b[A\n","991it [07:34,  4.33it/s]\u001b[A\n","992it [07:34,  4.33it/s]\u001b[A\n","993it [07:34,  4.33it/s]\u001b[A\n","994it [07:34,  4.32it/s]\u001b[A\n","995it [07:34,  4.33it/s]\u001b[A\n","996it [07:35,  4.32it/s]\u001b[A\n","997it [07:35,  4.32it/s]\u001b[A\n","998it [07:35,  4.32it/s]\u001b[A\n","999it [07:35,  4.33it/s]\u001b[A\n","1000it [07:36,  4.33it/s]\u001b[A\n","1001it [07:36,  4.33it/s]\u001b[A\n","1002it [07:36,  4.33it/s]\u001b[A\n","1003it [07:36,  4.33it/s]\u001b[A\n","1004it [07:37,  4.33it/s]\u001b[A\n","1005it [07:37,  4.33it/s]\u001b[A\n","1006it [07:37,  4.32it/s]\u001b[A\n","1007it [07:37,  4.32it/s]\u001b[A\n","1008it [07:37,  4.33it/s]\u001b[A\n","1009it [07:38,  4.33it/s]\u001b[A\n","1010it [07:38,  4.33it/s]\u001b[A\n","1011it [07:38,  4.33it/s]\u001b[A\n","1012it [07:38,  4.33it/s]\u001b[A\n","1013it [07:39,  4.33it/s]\u001b[A\n","1014it [07:39,  4.33it/s]\u001b[A\n","1015it [07:39,  4.33it/s]\u001b[A\n","1016it [07:39,  4.32it/s]\u001b[A\n","1017it [07:40,  4.32it/s]\u001b[A\n","1018it [07:40,  4.33it/s]\u001b[A\n","1019it [07:40,  4.33it/s]\u001b[A\n","1020it [07:40,  4.33it/s]\u001b[A\n","1021it [07:40,  4.33it/s]\u001b[A\n","1022it [07:41,  4.33it/s]\u001b[A\n","1023it [07:41,  4.32it/s]\u001b[A\n","1024it [07:41,  4.32it/s]\u001b[A\n","1025it [07:41,  4.33it/s]\u001b[A\n","1026it [07:42,  4.33it/s]\u001b[A\n","1027it [07:42,  4.32it/s]\u001b[A\n","1028it [07:42,  4.33it/s]\u001b[A\n","1029it [07:42,  4.33it/s]\u001b[A\n","1030it [07:43,  4.33it/s]\u001b[A\n","1031it [07:43,  4.33it/s]\u001b[A\n","1032it [07:43,  4.33it/s]\u001b[A\n","1033it [07:43,  4.33it/s]\u001b[A\n","1034it [07:43,  4.33it/s]\u001b[A\n","1035it [07:44,  4.33it/s]\u001b[A\n","1036it [07:44,  4.33it/s]\u001b[A\n","1037it [07:44,  4.32it/s]\u001b[A\n","1038it [07:44,  4.33it/s]\u001b[A\n","1039it [07:45,  4.32it/s]\u001b[A\n","1040it [07:45,  4.32it/s]\u001b[A\n","1041it [07:45,  4.32it/s]\u001b[A\n","1042it [07:45,  4.33it/s]\u001b[A\n","1043it [07:46,  4.33it/s]\u001b[A\n","1044it [07:46,  4.33it/s]\u001b[A\n","1045it [07:46,  4.33it/s]\u001b[A\n","1046it [07:46,  4.33it/s]\u001b[A\n","1047it [07:46,  4.33it/s]\u001b[A\n","1048it [07:47,  4.33it/s]\u001b[A\n","1049it [07:47,  4.33it/s]\u001b[A\n","1050it [07:47,  4.32it/s]\u001b[A\n","1051it [07:47,  4.33it/s]\u001b[A\n","1052it [07:48,  4.33it/s]\u001b[A\n","1053it [07:48,  4.33it/s]\u001b[A\n","1054it [07:48,  4.33it/s]\u001b[A\n","1055it [07:48,  4.33it/s]\u001b[A\n","1056it [07:49,  4.33it/s]\u001b[A\n","1057it [07:49,  4.32it/s]\u001b[A\n","1058it [07:49,  4.32it/s]\u001b[A\n","1059it [07:49,  4.32it/s]\u001b[A\n","1060it [07:50,  4.32it/s]\u001b[A\n","1061it [07:50,  4.33it/s]\u001b[A\n","1062it [07:50,  4.33it/s]\u001b[A\n","1063it [07:50,  4.32it/s]\u001b[A\n","1064it [07:50,  4.33it/s]\u001b[A\n","1065it [07:51,  4.33it/s]\u001b[A\n","1066it [07:51,  4.33it/s]\u001b[A\n","1067it [07:51,  4.33it/s]\u001b[A\n","1068it [07:51,  4.33it/s]\u001b[A\n","1069it [07:52,  4.33it/s]\u001b[A\n","1070it [07:52,  4.32it/s]\u001b[A\n","1071it [07:52,  4.32it/s]\u001b[A\n","1072it [07:52,  4.32it/s]\u001b[A\n","1073it [07:53,  4.32it/s]\u001b[A\n","1074it [07:53,  4.32it/s]\u001b[A\n","1075it [07:53,  4.32it/s]\u001b[A\n","1076it [07:53,  4.33it/s]\u001b[A\n","1077it [07:53,  4.33it/s]\u001b[A\n","1078it [07:54,  4.33it/s]\u001b[A\n","1079it [07:54,  4.32it/s]\u001b[A\n","1080it [07:54,  4.32it/s]\u001b[A\n","1081it [07:54,  4.32it/s]\u001b[A\n","1082it [07:55,  4.32it/s]\u001b[A\n","1083it [07:55,  4.32it/s]\u001b[A\n","1084it [07:55,  4.32it/s]\u001b[A\n","1085it [07:55,  4.32it/s]\u001b[A\n","1086it [07:56,  4.33it/s]\u001b[A\n","1087it [07:56,  4.33it/s]\u001b[A\n","1088it [07:56,  4.33it/s]\u001b[A\n","1089it [07:56,  4.33it/s]\u001b[A\n","1090it [07:56,  4.32it/s]\u001b[A\n","1091it [07:57,  4.33it/s]\u001b[A\n","1092it [07:57,  4.32it/s]\u001b[A\n","1093it [07:57,  4.32it/s]\u001b[A\n","1094it [07:57,  4.32it/s]\u001b[A\n","1095it [07:58,  4.33it/s]\u001b[A\n","1096it [07:58,  4.32it/s]\u001b[A\n","1097it [07:58,  4.33it/s]\u001b[A\n","1098it [07:58,  4.33it/s]\u001b[A\n","1099it [07:59,  4.33it/s]\u001b[A\n","1100it [07:59,  4.33it/s]\u001b[A\n","1101it [07:59,  4.33it/s]\u001b[A\n","1102it [07:59,  4.32it/s]\u001b[A\n","1103it [07:59,  4.32it/s]\u001b[A\n","1104it [08:00,  4.32it/s]\u001b[A\n","1105it [08:00,  4.32it/s]\u001b[A\n","1106it [08:00,  4.32it/s]\u001b[A\n","1107it [08:00,  4.32it/s]\u001b[A\n","1108it [08:01,  4.32it/s]\u001b[A\n","1109it [08:01,  4.32it/s]\u001b[A\n","Epoch:  70% 174/250 [09:24<02:07,  1.67s/it]\n","1111it [08:43, 12.76s/it]\u001b[A\n","1112it [08:43,  9.00s/it]\u001b[A\n","1113it [08:44,  6.37s/it]\u001b[A\n","1114it [08:44,  4.53s/it]\u001b[A\n","1115it [08:44,  3.24s/it]\u001b[A\n","1116it [08:44,  2.34s/it]\u001b[A\n","1117it [08:44,  1.71s/it]\u001b[A\n","1118it [08:45,  1.26s/it]\u001b[A\n","1119it [08:45,  1.05it/s]\u001b[A\n","1120it [08:45,  1.36it/s]\u001b[A\n","1121it [08:45,  1.71it/s]\u001b[A\n","1122it [08:46,  2.09it/s]\u001b[A\n","1123it [08:46,  2.47it/s]\u001b[A\n","1124it [08:46,  2.84it/s]\u001b[A\n","1125it [08:46,  3.16it/s]\u001b[A\n","1126it [08:47,  3.44it/s]\u001b[A\n","1127it [08:47,  3.66it/s]\u001b[A\n","1128it [08:47,  3.84it/s]\u001b[A\n","1129it [08:47,  3.97it/s]\u001b[A\n","1130it [08:47,  4.07it/s]\u001b[A\n","1131it [08:48,  4.14it/s]\u001b[A\n","1132it [08:48,  4.20it/s]\u001b[A\n","1133it [08:48,  4.23it/s]\u001b[A\n","1134it [08:48,  4.26it/s]\u001b[A\n","1135it [08:49,  4.28it/s]\u001b[A\n","1136it [08:49,  4.29it/s]\u001b[A\n","1137it [08:49,  4.30it/s]\u001b[A\n","1138it [08:49,  4.31it/s]\u001b[A\n","1139it [08:50,  4.31it/s]\u001b[A\n","1140it [08:50,  4.32it/s]\u001b[A\n","1141it [08:50,  4.32it/s]\u001b[A\n","1142it [08:50,  4.32it/s]\u001b[A\n","1143it [08:50,  4.32it/s]\u001b[A\n","1144it [08:51,  4.32it/s]\u001b[A\n","1145it [08:51,  4.33it/s]\u001b[A\n","1146it [08:51,  4.33it/s]\u001b[A\n","1147it [08:51,  4.33it/s]\u001b[A\n","1148it [08:52,  4.33it/s]\u001b[A\n","1149it [08:52,  4.33it/s]\u001b[A\n","1150it [08:52,  4.33it/s]\u001b[A\n","1151it [08:52,  4.33it/s]\u001b[A\n","1152it [08:53,  4.32it/s]\u001b[A\n","1153it [08:53,  4.33it/s]\u001b[A\n","1154it [08:53,  4.33it/s]\u001b[A\n","1155it [08:53,  4.33it/s]\u001b[A\n","1156it [08:53,  4.33it/s]\u001b[A\n","1157it [08:54,  4.33it/s]\u001b[A\n","1158it [08:54,  4.33it/s]\u001b[A\n","1159it [08:54,  4.33it/s]\u001b[A\n","1160it [08:54,  4.33it/s]\u001b[A\n","1161it [08:55,  4.33it/s]\u001b[A\n","1162it [08:55,  4.32it/s]\u001b[A\n","1163it [08:55,  4.33it/s]\u001b[A\n","1164it [08:55,  4.33it/s]\u001b[A\n","1165it [08:56,  4.32it/s]\u001b[A\n","1166it [08:56,  4.33it/s]\u001b[A\n","1167it [08:56,  4.33it/s]\u001b[A\n","1168it [08:56,  4.33it/s]\u001b[A\n","1169it [08:56,  4.33it/s]\u001b[A\n","1170it [08:57,  4.33it/s]\u001b[A\n","1171it [08:57,  4.33it/s]\u001b[A\n","1172it [08:57,  4.33it/s]\u001b[A\n","1173it [08:57,  4.33it/s]\u001b[A\n","1174it [08:58,  4.32it/s]\u001b[A\n","1175it [08:58,  4.32it/s]\u001b[A\n","1176it [08:58,  4.32it/s]\u001b[A\n","1177it [08:58,  4.32it/s]\u001b[A\n","1178it [08:59,  4.32it/s]\u001b[A\n","1179it [08:59,  4.33it/s]\u001b[A\n","1180it [08:59,  4.33it/s]\u001b[A\n","1181it [08:59,  4.33it/s]\u001b[A\n","1182it [08:59,  4.32it/s]\u001b[A\n","1183it [09:00,  4.33it/s]\u001b[A\n","1184it [09:00,  4.32it/s]\u001b[A\n","1185it [09:00,  4.32it/s]\u001b[A\n","1186it [09:00,  4.32it/s]\u001b[A\n","1187it [09:01,  4.32it/s]\u001b[A\n","1188it [09:01,  4.33it/s]\u001b[A\n","1189it [09:01,  4.33it/s]\u001b[A\n","1190it [09:01,  4.33it/s]\u001b[A\n","1191it [09:02,  4.33it/s]\u001b[A\n","1192it [09:02,  4.33it/s]\u001b[A\n","1193it [09:02,  4.33it/s]\u001b[A\n","1194it [09:02,  4.33it/s]\u001b[A\n","1195it [09:02,  4.32it/s]\u001b[A\n","1196it [09:03,  4.33it/s]\u001b[A\n","1197it [09:03,  4.33it/s]\u001b[A\n","1198it [09:03,  4.33it/s]\u001b[A\n","1199it [09:03,  4.33it/s]\u001b[A\n","1200it [09:04,  4.33it/s]\u001b[A\n","1201it [09:04,  4.33it/s]\u001b[A\n","1202it [09:04,  4.32it/s]\u001b[A\n","1203it [09:04,  4.32it/s]\u001b[A\n","1204it [09:05,  4.33it/s]\u001b[A\n","1205it [09:05,  4.33it/s]\u001b[A\n","1206it [09:05,  4.33it/s]\u001b[A\n","1207it [09:05,  4.32it/s]\u001b[A\n","1208it [09:05,  4.32it/s]\u001b[A\n","1209it [09:06,  4.33it/s]\u001b[A\n","1210it [09:06,  4.33it/s]\u001b[A\n","1211it [09:06,  4.33it/s]\u001b[A\n","1212it [09:06,  4.33it/s]\u001b[A\n","1213it [09:07,  4.33it/s]\u001b[A\n","1214it [09:07,  4.33it/s]\u001b[A\n","1215it [09:07,  4.33it/s]\u001b[A\n","1216it [09:07,  4.33it/s]\u001b[A\n","1217it [09:08,  4.33it/s]\u001b[A\n","1218it [09:08,  4.32it/s]\u001b[A\n","1219it [09:08,  4.33it/s]\u001b[A\n","1220it [09:08,  4.33it/s]\u001b[A\n","1221it [09:08,  4.33it/s]\u001b[A\n","1222it [09:09,  4.33it/s]\u001b[A\n","1223it [09:09,  4.33it/s]\u001b[A\n","1224it [09:09,  4.33it/s]\u001b[A\n","1225it [09:09,  4.33it/s]\u001b[A\n","1226it [09:10,  4.33it/s]\u001b[A\n","1227it [09:10,  4.33it/s]\u001b[A\n","1228it [09:10,  4.33it/s]\u001b[A\n","1229it [09:10,  4.32it/s]\u001b[A\n","1230it [09:11,  4.32it/s]\u001b[A\n","1231it [09:11,  4.32it/s]\u001b[A\n","1232it [09:11,  4.33it/s]\u001b[A\n","1233it [09:11,  4.33it/s]\u001b[A\n","1234it [09:11,  4.32it/s]\u001b[A\n","1235it [09:12,  4.32it/s]\u001b[A\n","1236it [09:12,  4.32it/s]\u001b[A\n","1237it [09:12,  4.32it/s]\u001b[A\n","1238it [09:12,  4.32it/s]\u001b[A\n","1239it [09:13,  4.32it/s]\u001b[A\n","1240it [09:13,  4.32it/s]\u001b[A\n","1241it [09:13,  4.32it/s]\u001b[A\n","1242it [09:13,  4.32it/s]\u001b[A\n","1243it [09:14,  4.33it/s]\u001b[A\n","1244it [09:14,  4.33it/s]\u001b[A\n","1245it [09:14,  4.33it/s]\u001b[A\n","1246it [09:14,  4.33it/s]\u001b[A\n","1247it [09:15,  4.32it/s]\u001b[A\n","1248it [09:15,  4.32it/s]\u001b[A\n","1249it [09:15,  4.32it/s]\u001b[A\n","1250it [09:15,  4.32it/s]\u001b[A\n","1251it [09:15,  4.32it/s]\u001b[A\n","1252it [09:16,  4.32it/s]\u001b[A\n","1253it [09:16,  4.32it/s]\u001b[A\n","1254it [09:16,  4.32it/s]\u001b[A\n","1255it [09:16,  4.32it/s]\u001b[A\n","1256it [09:17,  4.32it/s]\u001b[A\n","1257it [09:17,  4.32it/s]\u001b[A\n","1258it [09:17,  4.32it/s]\u001b[A\n","1259it [09:17,  4.32it/s]\u001b[A\n","1260it [09:18,  4.32it/s]\u001b[A\n","1261it [09:18,  4.32it/s]\u001b[A\n","1262it [09:18,  4.33it/s]\u001b[A\n","1263it [09:18,  4.32it/s]\u001b[A\n","1264it [09:18,  4.32it/s]\u001b[A\n","1265it [09:19,  4.32it/s]\u001b[A\n","1266it [09:19,  4.32it/s]\u001b[A\n","1267it [09:19,  4.33it/s]\u001b[A\n","1268it [09:19,  4.33it/s]\u001b[A\n","1269it [09:20,  4.33it/s]\u001b[A\n","1270it [09:20,  4.33it/s]\u001b[A\n","1271it [09:20,  4.33it/s]\u001b[A\n","1272it [09:20,  4.33it/s]\u001b[A\n","1273it [09:21,  4.32it/s]\u001b[A\n","1274it [09:21,  4.32it/s]\u001b[A\n","1275it [09:21,  4.33it/s]\u001b[A\n","1276it [09:21,  4.33it/s]\u001b[A\n","1277it [09:21,  4.33it/s]\u001b[A\n","1278it [09:22,  4.33it/s]\u001b[A\n","1279it [09:22,  4.33it/s]\u001b[A\n","1280it [09:22,  4.33it/s]\u001b[A\n","1281it [09:22,  4.32it/s]\u001b[A\n","1282it [09:23,  4.33it/s]\u001b[A\n","1283it [09:23,  4.32it/s]\u001b[A\n","1284it [09:23,  4.32it/s]\u001b[A\n","1285it [09:23,  4.33it/s]\u001b[A\n","1286it [09:24,  4.33it/s]\u001b[A\n","1287it [09:24,  4.32it/s]\u001b[A\n","1288it [09:24,  4.33it/s]\u001b[A\n","1289it [09:24,  4.33it/s]\u001b[A\n","1290it [09:24,  4.33it/s]\u001b[A\n","1291it [09:25,  4.33it/s]\u001b[A\n","1292it [09:25,  4.33it/s]\u001b[A\n","1293it [09:25,  4.33it/s]\u001b[A\n","1294it [09:25,  4.33it/s]\u001b[A\n","Epoch:  80% 199/250 [10:48<01:25,  1.67s/it]\n","1296it [10:08, 12.76s/it]\u001b[A\n","1297it [10:08,  9.00s/it]\u001b[A\n","1298it [10:08,  6.37s/it]\u001b[A\n","1299it [10:08,  4.53s/it]\u001b[A\n","1300it [10:09,  3.24s/it]\u001b[A\n","1301it [10:09,  2.34s/it]\u001b[A\n","1302it [10:09,  1.71s/it]\u001b[A\n","1303it [10:09,  1.26s/it]\u001b[A\n","1304it [10:09,  1.05it/s]\u001b[A\n","1305it [10:10,  1.36it/s]\u001b[A\n","1306it [10:10,  1.71it/s]\u001b[A\n","1307it [10:10,  2.09it/s]\u001b[A\n","1308it [10:10,  2.47it/s]\u001b[A\n","1309it [10:11,  2.83it/s]\u001b[A\n","1310it [10:11,  3.16it/s]\u001b[A\n","1311it [10:11,  3.44it/s]\u001b[A\n","1312it [10:11,  3.67it/s]\u001b[A\n","1313it [10:12,  3.84it/s]\u001b[A\n","1314it [10:12,  3.98it/s]\u001b[A\n","1315it [10:12,  4.07it/s]\u001b[A\n","1316it [10:12,  4.14it/s]\u001b[A\n","1317it [10:12,  4.20it/s]\u001b[A\n","1318it [10:13,  4.24it/s]\u001b[A\n","1319it [10:13,  4.26it/s]\u001b[A\n","1320it [10:13,  4.28it/s]\u001b[A\n","1321it [10:13,  4.30it/s]\u001b[A\n","1322it [10:14,  4.30it/s]\u001b[A\n","1323it [10:14,  4.31it/s]\u001b[A\n","1324it [10:14,  4.32it/s]\u001b[A\n","1325it [10:14,  4.32it/s]\u001b[A\n","1326it [10:15,  4.31it/s]\u001b[A\n","1327it [10:15,  4.31it/s]\u001b[A\n","1328it [10:15,  4.32it/s]\u001b[A\n","1329it [10:15,  4.32it/s]\u001b[A\n","1330it [10:15,  4.32it/s]\u001b[A\n","1331it [10:16,  4.32it/s]\u001b[A\n","1332it [10:16,  4.32it/s]\u001b[A\n","1333it [10:16,  4.32it/s]\u001b[A\n","1334it [10:16,  4.33it/s]\u001b[A\n","1335it [10:17,  4.32it/s]\u001b[A\n","1336it [10:17,  4.32it/s]\u001b[A\n","1337it [10:17,  4.33it/s]\u001b[A\n","1338it [10:17,  4.33it/s]\u001b[A\n","1339it [10:18,  4.32it/s]\u001b[A\n","1340it [10:18,  4.33it/s]\u001b[A\n","1341it [10:18,  4.33it/s]\u001b[A\n","1342it [10:18,  4.33it/s]\u001b[A\n","1343it [10:18,  4.33it/s]\u001b[A\n","1344it [10:19,  4.33it/s]\u001b[A\n","1345it [10:19,  4.32it/s]\u001b[A\n","1346it [10:19,  4.32it/s]\u001b[A\n","1347it [10:19,  4.33it/s]\u001b[A\n","1348it [10:20,  4.32it/s]\u001b[A\n","1349it [10:20,  4.32it/s]\u001b[A\n","1350it [10:20,  4.33it/s]\u001b[A\n","1351it [10:20,  4.33it/s]\u001b[A\n","1352it [10:21,  4.33it/s]\u001b[A\n","1353it [10:21,  4.33it/s]\u001b[A\n","1354it [10:21,  4.33it/s]\u001b[A\n","1355it [10:21,  4.33it/s]\u001b[A\n","1356it [10:21,  4.33it/s]\u001b[A\n","1357it [10:22,  4.33it/s]\u001b[A\n","1358it [10:22,  4.33it/s]\u001b[A\n","1359it [10:22,  4.33it/s]\u001b[A\n","1360it [10:22,  4.32it/s]\u001b[A\n","1361it [10:23,  4.33it/s]\u001b[A\n","1362it [10:23,  4.33it/s]\u001b[A\n","1363it [10:23,  4.33it/s]\u001b[A\n","1364it [10:23,  4.33it/s]\u001b[A\n","1365it [10:24,  4.33it/s]\u001b[A\n","1366it [10:24,  4.33it/s]\u001b[A\n","1367it [10:24,  4.33it/s]\u001b[A\n","1368it [10:24,  4.33it/s]\u001b[A\n","1369it [10:24,  4.33it/s]\u001b[A\n","1370it [10:25,  4.33it/s]\u001b[A\n","1371it [10:25,  4.33it/s]\u001b[A\n","1372it [10:25,  4.33it/s]\u001b[A\n","1373it [10:25,  4.33it/s]\u001b[A\n","1374it [10:26,  4.33it/s]\u001b[A\n","1375it [10:26,  4.33it/s]\u001b[A\n","1376it [10:26,  4.33it/s]\u001b[A\n","1377it [10:26,  4.33it/s]\u001b[A\n","1378it [10:27,  4.33it/s]\u001b[A\n","1379it [10:27,  4.33it/s]\u001b[A\n","1380it [10:27,  4.33it/s]\u001b[A\n","1381it [10:27,  4.32it/s]\u001b[A\n","1382it [10:27,  4.32it/s]\u001b[A\n","1383it [10:28,  4.32it/s]\u001b[A\n","1384it [10:28,  4.33it/s]\u001b[A\n","1385it [10:28,  4.33it/s]\u001b[A\n","1386it [10:28,  4.33it/s]\u001b[A\n","1387it [10:29,  4.33it/s]\u001b[A\n","1388it [10:29,  4.33it/s]\u001b[A\n","1389it [10:29,  4.33it/s]\u001b[A\n","1390it [10:29,  4.33it/s]\u001b[A\n","1391it [10:30,  4.32it/s]\u001b[A\n","1392it [10:30,  4.32it/s]\u001b[A\n","1393it [10:30,  4.32it/s]\u001b[A\n","1394it [10:30,  4.33it/s]\u001b[A\n","1395it [10:30,  4.33it/s]\u001b[A\n","1396it [10:31,  4.33it/s]\u001b[A\n","1397it [10:31,  4.33it/s]\u001b[A\n","1398it [10:31,  4.33it/s]\u001b[A\n","1399it [10:31,  4.33it/s]\u001b[A\n","1400it [10:32,  4.33it/s]\u001b[A\n","1401it [10:32,  4.33it/s]\u001b[A\n","1402it [10:32,  4.33it/s]\u001b[A\n","1403it [10:32,  4.33it/s]\u001b[A\n","1404it [10:33,  4.33it/s]\u001b[A\n","1405it [10:33,  4.33it/s]\u001b[A\n","1406it [10:33,  4.33it/s]\u001b[A\n","1407it [10:33,  4.33it/s]\u001b[A\n","1408it [10:33,  4.33it/s]\u001b[A\n","1409it [10:34,  4.33it/s]\u001b[A\n","1410it [10:34,  4.33it/s]\u001b[A\n","1411it [10:34,  4.33it/s]\u001b[A\n","1412it [10:34,  4.32it/s]\u001b[A\n","1413it [10:35,  4.33it/s]\u001b[A\n","1414it [10:35,  4.32it/s]\u001b[A\n","1415it [10:35,  4.32it/s]\u001b[A\n","1416it [10:35,  4.32it/s]\u001b[A\n","1417it [10:36,  4.33it/s]\u001b[A\n","1418it [10:36,  4.33it/s]\u001b[A\n","1419it [10:36,  4.33it/s]\u001b[A\n","1420it [10:36,  4.33it/s]\u001b[A\n","1421it [10:36,  4.33it/s]\u001b[A\n","1422it [10:37,  4.33it/s]\u001b[A\n","1423it [10:37,  4.33it/s]\u001b[A\n","1424it [10:37,  4.33it/s]\u001b[A\n","1425it [10:37,  4.32it/s]\u001b[A\n","1426it [10:38,  4.33it/s]\u001b[A\n","1427it [10:38,  4.33it/s]\u001b[A\n","1428it [10:38,  4.33it/s]\u001b[A\n","1429it [10:38,  4.33it/s]\u001b[A\n","1430it [10:39,  4.33it/s]\u001b[A\n","1431it [10:39,  4.33it/s]\u001b[A\n","1432it [10:39,  4.33it/s]\u001b[A\n","1433it [10:39,  4.33it/s]\u001b[A\n","1434it [10:40,  4.33it/s]\u001b[A\n","1435it [10:40,  4.33it/s]\u001b[A\n","1436it [10:40,  4.33it/s]\u001b[A\n","1437it [10:40,  4.33it/s]\u001b[A\n","1438it [10:40,  4.33it/s]\u001b[A\n","1439it [10:41,  4.33it/s]\u001b[A\n","1440it [10:41,  4.33it/s]\u001b[A\n","1441it [10:41,  4.33it/s]\u001b[A\n","1442it [10:41,  4.33it/s]\u001b[A\n","1443it [10:42,  4.33it/s]\u001b[A\n","1444it [10:42,  4.33it/s]\u001b[A\n","1445it [10:42,  4.33it/s]\u001b[A\n","1446it [10:42,  4.33it/s]\u001b[A\n","1447it [10:43,  4.33it/s]\u001b[A\n","1448it [10:43,  4.33it/s]\u001b[A\n","1449it [10:43,  4.33it/s]\u001b[A\n","1450it [10:43,  4.33it/s]\u001b[A\n","1451it [10:43,  4.33it/s]\u001b[A\n","1452it [10:44,  4.33it/s]\u001b[A\n","1453it [10:44,  4.33it/s]\u001b[A\n","1454it [10:44,  4.32it/s]\u001b[A\n","1455it [10:44,  4.32it/s]\u001b[A\n","1456it [10:45,  4.33it/s]\u001b[A\n","1457it [10:45,  4.33it/s]\u001b[A\n","1458it [10:45,  4.32it/s]\u001b[A\n","1459it [10:45,  4.33it/s]\u001b[A\n","1460it [10:46,  4.33it/s]\u001b[A\n","1461it [10:46,  4.33it/s]\u001b[A\n","1462it [10:46,  4.33it/s]\u001b[A\n","1463it [10:46,  4.33it/s]\u001b[A\n","1464it [10:46,  4.33it/s]\u001b[A\n","1465it [10:47,  4.33it/s]\u001b[A\n","1466it [10:47,  4.33it/s]\u001b[A\n","1467it [10:47,  4.33it/s]\u001b[A\n","1468it [10:47,  4.33it/s]\u001b[A\n","1469it [10:48,  4.33it/s]\u001b[A\n","1470it [10:48,  4.33it/s]\u001b[A\n","1471it [10:48,  4.33it/s]\u001b[A\n","1472it [10:48,  4.33it/s]\u001b[A\n","1473it [10:49,  4.33it/s]\u001b[A\n","1474it [10:49,  4.33it/s]\u001b[A\n","1475it [10:49,  4.33it/s]\u001b[A\n","1476it [10:49,  4.33it/s]\u001b[A\n","1477it [10:49,  4.33it/s]\u001b[A\n","1478it [10:50,  4.33it/s]\u001b[A\n","1479it [10:50,  4.33it/s]\u001b[A\n","Epoch:  90% 224/250 [12:12<00:43,  1.67s/it]\n","1481it [11:32, 12.73s/it]\u001b[A\n","1482it [11:32,  8.98s/it]\u001b[A\n","1483it [11:32,  6.35s/it]\u001b[A\n","1484it [11:33,  4.52s/it]\u001b[A\n","1485it [11:33,  3.23s/it]\u001b[A\n","1486it [11:33,  2.33s/it]\u001b[A\n","1487it [11:33,  1.70s/it]\u001b[A\n","1488it [11:34,  1.26s/it]\u001b[A\n","1489it [11:34,  1.05it/s]\u001b[A\n","1490it [11:34,  1.36it/s]\u001b[A\n","1491it [11:34,  1.71it/s]\u001b[A\n","1492it [11:35,  2.09it/s]\u001b[A\n","1493it [11:35,  2.48it/s]\u001b[A\n","1494it [11:35,  2.84it/s]\u001b[A\n","1495it [11:35,  3.17it/s]\u001b[A\n","1496it [11:35,  3.44it/s]\u001b[A\n","1497it [11:36,  3.67it/s]\u001b[A\n","1498it [11:36,  3.85it/s]\u001b[A\n","1499it [11:36,  3.98it/s]\u001b[A\n","1500it [11:36,  4.08it/s]\u001b[A\n","1501it [11:37,  4.15it/s]\u001b[A\n","1502it [11:37,  4.20it/s]\u001b[A\n","1503it [11:37,  4.24it/s]\u001b[A\n","1504it [11:37,  4.27it/s]\u001b[A\n","1505it [11:38,  4.29it/s]\u001b[A\n","1506it [11:38,  4.30it/s]\u001b[A\n","1507it [11:38,  4.31it/s]\u001b[A\n","1508it [11:38,  4.31it/s]\u001b[A\n","1509it [11:38,  4.32it/s]\u001b[A\n","1510it [11:39,  4.32it/s]\u001b[A\n","1511it [11:39,  4.33it/s]\u001b[A\n","1512it [11:39,  4.33it/s]\u001b[A\n","1513it [11:39,  4.33it/s]\u001b[A\n","1514it [11:40,  4.33it/s]\u001b[A\n","1515it [11:40,  4.33it/s]\u001b[A\n","1516it [11:40,  4.33it/s]\u001b[A\n","1517it [11:40,  4.33it/s]\u001b[A\n","1518it [11:41,  4.33it/s]\u001b[A\n","1519it [11:41,  4.33it/s]\u001b[A\n","1520it [11:41,  4.33it/s]\u001b[A\n","1521it [11:41,  4.33it/s]\u001b[A\n","1522it [11:41,  4.33it/s]\u001b[A\n","1523it [11:42,  4.33it/s]\u001b[A\n","1524it [11:42,  4.33it/s]\u001b[A\n","1525it [11:42,  4.33it/s]\u001b[A\n","1526it [11:42,  4.33it/s]\u001b[A\n","1527it [11:43,  4.33it/s]\u001b[A\n","1528it [11:43,  4.33it/s]\u001b[A\n","1529it [11:43,  4.32it/s]\u001b[A\n","1530it [11:43,  4.32it/s]\u001b[A\n","1531it [11:44,  4.33it/s]\u001b[A\n","1532it [11:44,  4.33it/s]\u001b[A\n","1533it [11:44,  4.33it/s]\u001b[A\n","1534it [11:44,  4.33it/s]\u001b[A\n","1535it [11:44,  4.33it/s]\u001b[A\n","1536it [11:45,  4.33it/s]\u001b[A\n","1537it [11:45,  4.33it/s]\u001b[A\n","1538it [11:45,  4.33it/s]\u001b[A\n","1539it [11:45,  4.33it/s]\u001b[A\n","1540it [11:46,  4.33it/s]\u001b[A\n","1541it [11:46,  4.33it/s]\u001b[A\n","1542it [11:46,  4.33it/s]\u001b[A\n","1543it [11:46,  4.33it/s]\u001b[A\n","1544it [11:47,  4.33it/s]\u001b[A\n","1545it [11:47,  4.33it/s]\u001b[A\n","1546it [11:47,  4.33it/s]\u001b[A\n","1547it [11:47,  4.33it/s]\u001b[A\n","1548it [11:47,  4.33it/s]\u001b[A\n","1549it [11:48,  4.33it/s]\u001b[A\n","1550it [11:48,  4.33it/s]\u001b[A\n","1551it [11:48,  4.33it/s]\u001b[A\n","1552it [11:48,  4.33it/s]\u001b[A\n","1553it [11:49,  4.33it/s]\u001b[A\n","1554it [11:49,  4.33it/s]\u001b[A\n","1555it [11:49,  4.33it/s]\u001b[A\n","1556it [11:49,  4.33it/s]\u001b[A\n","1557it [11:50,  4.33it/s]\u001b[A\n","1558it [11:50,  4.33it/s]\u001b[A\n","1559it [11:50,  4.33it/s]\u001b[A\n","1560it [11:50,  4.33it/s]\u001b[A\n","1561it [11:50,  4.33it/s]\u001b[A\n","1562it [11:51,  4.33it/s]\u001b[A\n","1563it [11:51,  4.33it/s]\u001b[A\n","1564it [11:51,  4.33it/s]\u001b[A\n","1565it [11:51,  4.33it/s]\u001b[A\n","1566it [11:52,  4.33it/s]\u001b[A\n","1567it [11:52,  4.33it/s]\u001b[A\n","1568it [11:52,  4.33it/s]\u001b[A\n","1569it [11:52,  4.33it/s]\u001b[A\n","1570it [11:53,  4.33it/s]\u001b[A\n","1571it [11:53,  4.33it/s]\u001b[A\n","1572it [11:53,  4.33it/s]\u001b[A\n","1573it [11:53,  4.33it/s]\u001b[A\n","1574it [11:53,  4.33it/s]\u001b[A\n","1575it [11:54,  4.33it/s]\u001b[A\n","1576it [11:54,  4.33it/s]\u001b[A\n","1577it [11:54,  4.33it/s]\u001b[A\n","1578it [11:54,  4.33it/s]\u001b[A\n","1579it [11:55,  4.33it/s]\u001b[A\n","1580it [11:55,  4.33it/s]\u001b[A\n","1581it [11:55,  4.33it/s]\u001b[A\n","1582it [11:55,  4.33it/s]\u001b[A\n","1583it [11:56,  4.33it/s]\u001b[A\n","1584it [11:56,  4.33it/s]\u001b[A\n","1585it [11:56,  4.33it/s]\u001b[A\n","1586it [11:56,  4.33it/s]\u001b[A\n","1587it [11:56,  4.33it/s]\u001b[A\n","1588it [11:57,  4.33it/s]\u001b[A\n","1589it [11:57,  4.33it/s]\u001b[A\n","1590it [11:57,  4.33it/s]\u001b[A\n","1591it [11:57,  4.33it/s]\u001b[A\n","1592it [11:58,  4.33it/s]\u001b[A\n","1593it [11:58,  4.33it/s]\u001b[A\n","1594it [11:58,  4.33it/s]\u001b[A\n","1595it [11:58,  4.33it/s]\u001b[A\n","1596it [11:59,  4.33it/s]\u001b[A\n","1597it [11:59,  4.33it/s]\u001b[A\n","1598it [11:59,  4.33it/s]\u001b[A\n","1599it [11:59,  4.33it/s]\u001b[A\n","1600it [11:59,  4.33it/s]\u001b[A\n","1601it [12:00,  4.33it/s]\u001b[A\n","1602it [12:00,  4.33it/s]\u001b[A\n","1603it [12:00,  4.33it/s]\u001b[A\n","1604it [12:00,  4.33it/s]\u001b[A\n","1605it [12:01,  4.33it/s]\u001b[A\n","1606it [12:01,  4.33it/s]\u001b[A\n","1607it [12:01,  4.33it/s]\u001b[A\n","1608it [12:01,  4.33it/s]\u001b[A\n","1609it [12:02,  4.33it/s]\u001b[A\n","1610it [12:02,  4.33it/s]\u001b[A\n","1611it [12:02,  4.33it/s]\u001b[A\n","1612it [12:02,  4.33it/s]\u001b[A\n","1613it [12:02,  4.33it/s]\u001b[A\n","1614it [12:03,  4.33it/s]\u001b[A\n","1615it [12:03,  4.33it/s]\u001b[A\n","1616it [12:03,  4.33it/s]\u001b[A\n","1617it [12:03,  4.33it/s]\u001b[A\n","1618it [12:04,  4.33it/s]\u001b[A\n","1619it [12:04,  4.33it/s]\u001b[A\n","1620it [12:04,  4.33it/s]\u001b[A\n","1621it [12:04,  4.33it/s]\u001b[A\n","1622it [12:05,  4.33it/s]\u001b[A\n","1623it [12:05,  4.33it/s]\u001b[A\n","1624it [12:05,  4.33it/s]\u001b[A\n","1625it [12:05,  4.33it/s]\u001b[A\n","1626it [12:06,  4.33it/s]\u001b[A\n","1627it [12:06,  4.33it/s]\u001b[A\n","1628it [12:06,  4.33it/s]\u001b[A\n","1629it [12:06,  4.33it/s]\u001b[A\n","1630it [12:06,  4.33it/s]\u001b[A\n","1631it [12:07,  4.33it/s]\u001b[A\n","1632it [12:07,  4.33it/s]\u001b[A\n","1633it [12:07,  4.33it/s]\u001b[A\n","1634it [12:07,  4.33it/s]\u001b[A\n","1635it [12:08,  4.33it/s]\u001b[A\n","1636it [12:08,  4.32it/s]\u001b[A\n","1637it [12:08,  4.33it/s]\u001b[A\n","1638it [12:08,  4.33it/s]\u001b[A\n","1639it [12:09,  4.33it/s]\u001b[A\n","1640it [12:09,  4.33it/s]\u001b[A\n","1641it [12:09,  4.33it/s]\u001b[A\n","1642it [12:09,  4.33it/s]\u001b[A\n","1643it [12:09,  4.33it/s]\u001b[A\n","1644it [12:10,  4.33it/s]\u001b[A\n","1645it [12:10,  4.33it/s]\u001b[A\n","1646it [12:10,  4.32it/s]\u001b[A\n","1647it [12:10,  4.33it/s]\u001b[A\n","1648it [12:11,  4.33it/s]\u001b[A\n","1649it [12:11,  4.33it/s]\u001b[A\n","1650it [12:11,  4.33it/s]\u001b[A\n","1651it [12:11,  4.33it/s]\u001b[A\n","1652it [12:12,  4.33it/s]\u001b[A\n","1653it [12:12,  4.33it/s]\u001b[A\n","1654it [12:12,  4.33it/s]\u001b[A\n","1655it [12:12,  4.33it/s]\u001b[A\n","1656it [12:12,  4.33it/s]\u001b[A\n","1657it [12:13,  4.33it/s]\u001b[A\n","1658it [12:13,  4.33it/s]\u001b[A\n","1659it [12:13,  4.33it/s]\u001b[A\n","1660it [12:13,  4.33it/s]\u001b[A\n","1661it [12:14,  4.33it/s]\u001b[A\n","1662it [12:14,  4.33it/s]\u001b[A\n","1663it [12:14,  4.33it/s]\u001b[A\n","1664it [12:14,  4.33it/s]\u001b[A\n","Epoch: 100% 249/250 [13:37<00:01,  1.67s/it]\n","1666it [12:56, 12.73s/it]\u001b[A\n","1667it [12:57,  8.98s/it]\u001b[A\n","1668it [12:57,  6.36s/it]\u001b[A\n","1669it [12:57,  4.52s/it]\u001b[A\n","1670it [12:57,  3.23s/it]\u001b[A\n","1671it [12:58,  2.33s/it]\u001b[A\n","1672it [12:58,  1.70s/it]\u001b[A\n","1673it [12:58,  1.26s/it]\u001b[A\n","1674it [12:58,  1.05it/s]\u001b[A\n","1675it [12:58,  1.36it/s]\u001b[A\n","1676it [12:59,  1.71it/s]\u001b[A\n","1677it [12:59,  2.09it/s]\u001b[A\n","1678it [12:59,  2.48it/s]\u001b[A\n","1679it [12:59,  2.84it/s]\u001b[A\n","1680it [13:00,  3.17it/s]\u001b[A\n","1681it [13:00,  3.44it/s]\u001b[A\n","1682it [13:00,  3.67it/s]\u001b[A\n","1683it [13:00,  3.84it/s]\u001b[A\n","1684it [13:01,  3.97it/s]\u001b[A\n","1685it [13:01,  4.07it/s]\u001b[A\n","1686it [13:01,  4.15it/s]\u001b[A\n","1687it [13:01,  4.20it/s]\u001b[A\n","1688it [13:01,  4.24it/s]\u001b[A\n","1689it [13:02,  4.27it/s]\u001b[A\n","1690it [13:02,  4.29it/s]\u001b[A\n","1691it [13:02,  4.30it/s]\u001b[A\n","1692it [13:02,  4.31it/s]\u001b[A\n","1693it [13:03,  4.32it/s]\u001b[A\n","1694it [13:03,  4.32it/s]\u001b[A\n","1695it [13:03,  4.32it/s]\u001b[A\n","1696it [13:03,  4.33it/s]\u001b[A\n","1697it [13:04,  4.33it/s]\u001b[A\n","1698it [13:04,  4.33it/s]\u001b[A\n","1699it [13:04,  4.33it/s]\u001b[A\n","1700it [13:04,  4.33it/s]\u001b[A\n","1701it [13:04,  4.33it/s]\u001b[A\n","1702it [13:05,  4.33it/s]\u001b[A\n","1703it [13:05,  4.33it/s]\u001b[A\n","1704it [13:05,  4.33it/s]\u001b[A\n","1705it [13:05,  4.33it/s]\u001b[A\n","1706it [13:06,  4.33it/s]\u001b[A\n","1707it [13:06,  4.33it/s]\u001b[A\n","1708it [13:06,  4.33it/s]\u001b[A\n","1709it [13:06,  4.33it/s]\u001b[A\n","1710it [13:07,  4.33it/s]\u001b[A\n","1711it [13:07,  4.33it/s]\u001b[A\n","1712it [13:07,  4.33it/s]\u001b[A\n","1713it [13:07,  4.33it/s]\u001b[A\n","1714it [13:07,  4.33it/s]\u001b[A\n","1715it [13:08,  4.33it/s]\u001b[A\n","1716it [13:08,  4.33it/s]\u001b[A\n","1717it [13:08,  4.33it/s]\u001b[A\n","1718it [13:08,  4.33it/s]\u001b[A\n","1719it [13:09,  4.33it/s]\u001b[A\n","1720it [13:09,  4.33it/s]\u001b[A\n","1721it [13:09,  4.33it/s]\u001b[A\n","1722it [13:09,  4.33it/s]\u001b[A\n","1723it [13:10,  4.33it/s]\u001b[A\n","1724it [13:10,  4.33it/s]\u001b[A\n","1725it [13:10,  4.33it/s]\u001b[A\n","1726it [13:10,  4.33it/s]\u001b[A\n","1727it [13:10,  4.33it/s]\u001b[A\n","1728it [13:11,  4.33it/s]\u001b[A\n","1729it [13:11,  4.33it/s]\u001b[A\n","1730it [13:11,  4.33it/s]\u001b[A\n","1731it [13:11,  4.33it/s]\u001b[A\n","1732it [13:12,  4.33it/s]\u001b[A\n","1733it [13:12,  4.33it/s]\u001b[A\n","1734it [13:12,  4.33it/s]\u001b[A\n","1735it [13:12,  4.33it/s]\u001b[A\n","1736it [13:13,  4.33it/s]\u001b[A\n","1737it [13:13,  4.33it/s]\u001b[A\n","1738it [13:13,  4.33it/s]\u001b[A\n","1739it [13:13,  4.33it/s]\u001b[A\n","1740it [13:14,  4.33it/s]\u001b[A\n","1741it [13:14,  4.33it/s]\u001b[A\n","1742it [13:14,  4.33it/s]\u001b[A\n","1743it [13:14,  4.33it/s]\u001b[A\n","1744it [13:14,  4.33it/s]\u001b[A\n","1745it [13:15,  4.33it/s]\u001b[A\n","1746it [13:15,  4.33it/s]\u001b[A\n","1747it [13:15,  4.33it/s]\u001b[A\n","1748it [13:15,  4.33it/s]\u001b[A\n","1749it [13:16,  4.33it/s]\u001b[A\n","1750it [13:16,  4.33it/s]\u001b[A\n","1751it [13:16,  4.33it/s]\u001b[A\n","1752it [13:16,  4.33it/s]\u001b[A\n","1753it [13:17,  4.33it/s]\u001b[A\n","1754it [13:17,  4.33it/s]\u001b[A\n","1755it [13:17,  4.33it/s]\u001b[A\n","1756it [13:17,  4.33it/s]\u001b[A\n","1757it [13:17,  4.33it/s]\u001b[A\n","1758it [13:18,  4.33it/s]\u001b[A\n","1759it [13:18,  4.33it/s]\u001b[A\n","1760it [13:18,  4.33it/s]\u001b[A\n","1761it [13:18,  4.33it/s]\u001b[A\n","1762it [13:19,  4.33it/s]\u001b[A\n","1763it [13:19,  4.33it/s]\u001b[A\n","1764it [13:19,  4.33it/s]\u001b[A\n","1765it [13:19,  4.33it/s]\u001b[A\n","1766it [13:20,  4.33it/s]\u001b[A\n","1767it [13:20,  4.33it/s]\u001b[A\n","1768it [13:20,  4.33it/s]\u001b[A\n","1769it [13:20,  4.33it/s]\u001b[A\n","1770it [13:20,  4.33it/s]\u001b[A\n","1771it [13:21,  4.33it/s]\u001b[A\n","1772it [13:21,  4.33it/s]\u001b[A\n","1773it [13:21,  4.33it/s]\u001b[A\n","1774it [13:21,  4.33it/s]\u001b[A\n","1775it [13:22,  4.33it/s]\u001b[A\n","1776it [13:22,  4.33it/s]\u001b[A\n","1777it [13:22,  4.33it/s]\u001b[A\n","1778it [13:22,  4.33it/s]\u001b[A\n","1779it [13:23,  4.33it/s]\u001b[A\n","1780it [13:23,  4.33it/s]\u001b[A\n","1781it [13:23,  4.33it/s]\u001b[A\n","1782it [13:23,  4.33it/s]\u001b[A\n","1783it [13:23,  4.33it/s]\u001b[A\n","1784it [13:24,  4.33it/s]\u001b[A\n","1785it [13:24,  4.33it/s]\u001b[A\n","1786it [13:24,  4.33it/s]\u001b[A\n","1787it [13:24,  4.33it/s]\u001b[A\n","1788it [13:25,  4.33it/s]\u001b[A\n","1789it [13:25,  4.33it/s]\u001b[A\n","1790it [13:25,  4.33it/s]\u001b[A\n","1791it [13:25,  4.33it/s]\u001b[A\n","1792it [13:26,  4.33it/s]\u001b[A\n","1793it [13:26,  4.33it/s]\u001b[A\n","1794it [13:26,  4.33it/s]\u001b[A\n","1795it [13:26,  4.33it/s]\u001b[A\n","1796it [13:26,  4.33it/s]\u001b[A\n","1797it [13:27,  4.33it/s]\u001b[A\n","1798it [13:27,  4.33it/s]\u001b[A\n","1799it [13:27,  4.33it/s]\u001b[A\n","1800it [13:27,  4.33it/s]\u001b[A\n","1801it [13:28,  4.33it/s]\u001b[A\n","1802it [13:28,  4.33it/s]\u001b[A\n","1803it [13:28,  4.33it/s]\u001b[A\n","1804it [13:28,  4.33it/s]\u001b[A\n","1805it [13:29,  4.33it/s]\u001b[A\n","1806it [13:29,  4.33it/s]\u001b[A\n","1807it [13:29,  4.33it/s]\u001b[A\n","1808it [13:29,  4.33it/s]\u001b[A\n","1809it [13:29,  4.33it/s]\u001b[A\n","1810it [13:30,  4.33it/s]\u001b[A\n","1811it [13:30,  4.33it/s]\u001b[A\n","1812it [13:30,  4.33it/s]\u001b[A\n","1813it [13:30,  4.33it/s]\u001b[A\n","1814it [13:31,  4.33it/s]\u001b[A\n","1815it [13:31,  4.33it/s]\u001b[A\n","1816it [13:31,  4.33it/s]\u001b[A\n","1817it [13:31,  4.33it/s]\u001b[A\n","1818it [13:32,  4.33it/s]\u001b[A\n","1819it [13:32,  4.33it/s]\u001b[A\n","1820it [13:32,  4.33it/s]\u001b[A\n","1821it [13:32,  4.33it/s]\u001b[A\n","1822it [13:32,  4.33it/s]\u001b[A\n","1823it [13:33,  4.33it/s]\u001b[A\n","1824it [13:33,  4.33it/s]\u001b[A\n","1825it [13:33,  4.33it/s]\u001b[A\n","1826it [13:33,  4.33it/s]\u001b[A\n","1827it [13:34,  4.33it/s]\u001b[A\n","1828it [13:34,  4.33it/s]\u001b[A\n","1829it [13:34,  4.33it/s]\u001b[A\n","1830it [13:34,  4.33it/s]\u001b[A\n","1831it [13:35,  4.33it/s]\u001b[A\n","1832it [13:35,  4.33it/s]\u001b[A\n","1833it [13:35,  4.33it/s]\u001b[A\n","1834it [13:35,  4.33it/s]\u001b[A\n","1835it [13:35,  4.33it/s]\u001b[A\n","1836it [13:36,  4.33it/s]\u001b[A\n","1837it [13:36,  4.33it/s]\u001b[A\n","1838it [13:36,  4.33it/s]\u001b[A\n","1839it [13:36,  4.33it/s]\u001b[A\n","1840it [13:37,  4.33it/s]\u001b[A\n","1841it [13:37,  4.33it/s]\u001b[A\n","1842it [13:37,  4.33it/s]\u001b[A\n","1843it [13:37,  4.33it/s]\u001b[A\n","1844it [13:38,  4.33it/s]\u001b[A\n","1845it [13:38,  4.33it/s]\u001b[A\n","1846it [13:38,  4.33it/s]\u001b[A\n","1847it [13:38,  4.33it/s]\u001b[A\n","1848it [13:38,  4.33it/s]\u001b[A\n","1849it [13:39,  4.33it/s]\u001b[A\n","Epoch: 100% 250/250 [14:21<00:00,  3.45s/it]\n","03/14/2022 09:48:59 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/14/2022 09:49:12 - INFO - __main__ -   *** Validate ***\n","\n","1851it [13:52,  4.18s/it]\u001b[A\n","1852it [13:53,  2.99s/it]\u001b[A\n","1853it [13:53,  2.16s/it]\u001b[A\n","1854it [13:53,  1.58s/it]\u001b[A\n","1855it [13:53,  1.18s/it]\u001b[A\n","1856it [13:53,  1.12it/s]\u001b[A\n","1857it [13:54,  1.44it/s]\u001b[A\n","1858it [13:54,  1.80it/s]\u001b[A\n","1859it [13:54,  2.18it/s]\u001b[A\n","1860it [13:54,  2.56it/s]\u001b[A\n","1861it [13:55,  2.92it/s]\u001b[A\n","1862it [13:55,  3.24it/s]\u001b[A\n","1863it [13:55,  3.50it/s]\u001b[A\n","1864it [13:55,  3.71it/s]\u001b[A\n","1865it [13:56,  3.88it/s]\u001b[A\n","1866it [13:56,  4.00it/s]\u001b[A\n","1867it [13:56,  4.10it/s]\u001b[A\n","1868it [13:56,  4.16it/s]\u001b[A\n","1869it [13:56,  4.21it/s]\u001b[A\n","1870it [13:57,  4.25it/s]\u001b[A\n","1871it [13:57,  4.27it/s]\u001b[A\n","1872it [13:57,  4.29it/s]\u001b[A\n","1873it [13:57,  4.30it/s]\u001b[A\n","1874it [13:58,  4.31it/s]\u001b[A\n","1875it [13:58,  4.31it/s]\u001b[A\n","1876it [13:58,  4.32it/s]\u001b[A\n","1877it [13:58,  4.32it/s]\u001b[A\n","1878it [13:59,  4.32it/s]\u001b[A\n","1879it [13:59,  4.32it/s]\u001b[A\n","1880it [13:59,  4.33it/s]\u001b[A\n","1881it [13:59,  4.33it/s]\u001b[A\n","1882it [13:59,  4.33it/s]\u001b[A\n","1883it [14:00,  4.33it/s]\u001b[A\n","1884it [14:00,  4.33it/s]\u001b[A\n","1885it [14:00,  4.33it/s]\u001b[A\n","1886it [14:00,  4.33it/s]\u001b[A\n","1887it [14:01,  4.33it/s]\u001b[A\n","1888it [14:01,  4.33it/s]\u001b[A\n","1889it [14:01,  4.33it/s]\u001b[A\n","1890it [14:01,  4.33it/s]\u001b[A\n","1891it [14:02,  4.33it/s]\u001b[A\n","1892it [14:02,  4.33it/s]\u001b[A\n","1893it [14:02,  4.33it/s]\u001b[A\n","1894it [14:02,  4.33it/s]\u001b[A\n","1895it [14:02,  4.33it/s]\u001b[A\n","1896it [14:03,  4.33it/s]\u001b[A\n","1897it [14:03,  4.33it/s]\u001b[A\n","1898it [14:03,  4.33it/s]\u001b[A\n","1899it [14:03,  4.33it/s]\u001b[A\n","1900it [14:04,  4.33it/s]\u001b[A\n","1901it [14:04,  4.33it/s]\u001b[A\n","1902it [14:04,  4.33it/s]\u001b[A\n","1903it [14:04,  4.33it/s]\u001b[A\n","1904it [14:05,  4.33it/s]\u001b[A\n","1905it [14:05,  4.33it/s]\u001b[A\n","1906it [14:05,  4.33it/s]\u001b[A\n","1907it [14:05,  4.33it/s]\u001b[A\n","1908it [14:05,  4.33it/s]\u001b[A\n","1909it [14:06,  4.33it/s]\u001b[A\n","1910it [14:06,  4.33it/s]\u001b[A\n","1911it [14:06,  4.33it/s]\u001b[A\n","1912it [14:06,  4.33it/s]\u001b[A\n","1913it [14:07,  4.33it/s]\u001b[A\n","1914it [14:07,  4.33it/s]\u001b[A\n","1915it [14:07,  4.33it/s]\u001b[A\n","1916it [14:07,  4.33it/s]\u001b[A\n","1917it [14:08,  4.33it/s]\u001b[A\n","1918it [14:08,  4.32it/s]\u001b[A\n","1919it [14:08,  4.32it/s]\u001b[A\n","1920it [14:08,  4.33it/s]\u001b[A\n","1921it [14:08,  4.33it/s]\u001b[A\n","1922it [14:09,  4.33it/s]\u001b[A\n","1923it [14:09,  4.33it/s]\u001b[A\n","1924it [14:09,  4.33it/s]\u001b[A\n","1925it [14:09,  4.33it/s]\u001b[A\n","1926it [14:10,  4.33it/s]\u001b[A\n","1927it [14:10,  4.33it/s]\u001b[A\n","1928it [14:10,  4.33it/s]\u001b[A\n","1929it [14:10,  4.33it/s]\u001b[A\n","1930it [14:11,  4.33it/s]\u001b[A\n","1931it [14:11,  4.33it/s]\u001b[A\n","1932it [14:11,  4.33it/s]\u001b[A\n","1933it [14:11,  4.33it/s]\u001b[A\n","1934it [14:11,  4.33it/s]\u001b[A\n","1935it [14:12,  4.33it/s]\u001b[A\n","1936it [14:12,  4.33it/s]\u001b[A\n","1937it [14:12,  4.33it/s]\u001b[A\n","1938it [14:12,  4.33it/s]\u001b[A\n","1939it [14:13,  4.33it/s]\u001b[A\n","1940it [14:13,  4.33it/s]\u001b[A\n","1941it [14:13,  4.33it/s]\u001b[A\n","1942it [14:13,  4.33it/s]\u001b[A\n","1943it [14:14,  4.33it/s]\u001b[A\n","1944it [14:14,  4.33it/s]\u001b[A\n","1945it [14:14,  4.33it/s]\u001b[A\n","1946it [14:14,  4.33it/s]\u001b[A\n","1947it [14:14,  4.33it/s]\u001b[A\n","1948it [14:15,  4.33it/s]\u001b[A\n","1949it [14:15,  4.33it/s]\u001b[A\n","1950it [14:15,  4.33it/s]\u001b[A\n","1951it [14:15,  4.33it/s]\u001b[A\n","1952it [14:16,  4.33it/s]\u001b[A\n","1953it [14:16,  4.33it/s]\u001b[A\n","1954it [14:16,  4.33it/s]\u001b[A\n","1955it [14:16,  4.33it/s]\u001b[A\n","1956it [14:17,  4.33it/s]\u001b[A\n","1957it [14:17,  4.33it/s]\u001b[A\n","1958it [14:17,  4.33it/s]\u001b[A\n","1959it [14:17,  4.33it/s]\u001b[A\n","1960it [14:17,  4.33it/s]\u001b[A\n","1961it [14:18,  4.33it/s]\u001b[A\n","1962it [14:18,  4.33it/s]\u001b[A\n","1963it [14:18,  4.33it/s]\u001b[A\n","1964it [14:18,  4.33it/s]\u001b[A\n","1965it [14:19,  4.33it/s]\u001b[A\n","1966it [14:19,  4.33it/s]\u001b[A\n","1967it [14:19,  4.33it/s]\u001b[A\n","1968it [14:19,  4.33it/s]\u001b[A\n","1969it [14:20,  4.33it/s]\u001b[A\n","1970it [14:20,  4.33it/s]\u001b[A\n","1971it [14:20,  4.33it/s]\u001b[A\n","1972it [14:20,  4.33it/s]\u001b[A\n","1973it [14:20,  4.33it/s]\u001b[A\n","1974it [14:21,  4.33it/s]\u001b[A\n","1975it [14:21,  4.33it/s]\u001b[A\n","1976it [14:21,  4.33it/s]\u001b[A\n","1977it [14:21,  4.33it/s]\u001b[A\n","1978it [14:22,  4.33it/s]\u001b[A\n","1979it [14:22,  4.33it/s]\u001b[A\n","1980it [14:22,  4.33it/s]\u001b[A\n","1981it [14:22,  4.33it/s]\u001b[A\n","1982it [14:23,  4.32it/s]\u001b[A\n","1983it [14:23,  4.32it/s]\u001b[A\n","1984it [14:23,  4.33it/s]\u001b[A\n","1985it [14:23,  4.32it/s]\u001b[A\n","1986it [14:23,  4.32it/s]\u001b[A\n","1987it [14:24,  4.33it/s]\u001b[A\n","1988it [14:24,  4.33it/s]\u001b[A\n","1989it [14:24,  4.33it/s]\u001b[A\n","1990it [14:24,  4.33it/s]\u001b[A\n","1991it [14:25,  4.33it/s]\u001b[A\n","1992it [14:25,  4.33it/s]\u001b[A\n","1993it [14:25,  4.33it/s]\u001b[A\n","1994it [14:25,  4.33it/s]\u001b[A\n","1995it [14:26,  4.33it/s]\u001b[A\n","1996it [14:26,  4.33it/s]\u001b[A\n","1997it [14:26,  4.33it/s]\u001b[A\n","1998it [14:26,  4.33it/s]\u001b[A\n","1999it [14:26,  4.33it/s]\u001b[A\n","2000it [14:27,  4.33it/s]\u001b[A\n","2001it [14:27,  4.33it/s]\u001b[A\n","2002it [14:27,  4.33it/s]\u001b[A\n","2003it [14:27,  4.33it/s]\u001b[A\n","2004it [14:28,  4.33it/s]\u001b[A\n","2005it [14:28,  4.33it/s]\u001b[A\n","2006it [14:28,  4.33it/s]\u001b[A\n","2007it [14:28,  4.33it/s]\u001b[A\n","2008it [14:29,  4.33it/s]\u001b[A\n","2009it [14:29,  4.33it/s]\u001b[A\n","2010it [14:29,  4.33it/s]\u001b[A\n","2011it [14:29,  4.33it/s]\u001b[A\n","2012it [14:29,  4.33it/s]\u001b[A\n","2013it [14:30,  4.33it/s]\u001b[A\n","2014it [14:30,  4.33it/s]\u001b[A\n","2015it [14:30,  4.33it/s]\u001b[A\n","2016it [14:30,  4.33it/s]\u001b[A\n","2017it [14:31,  4.33it/s]\u001b[A\n","2018it [14:31,  4.33it/s]\u001b[A\n","2019it [14:31,  4.33it/s]\u001b[A\n","2020it [14:31,  4.33it/s]\u001b[A\n","2021it [14:32,  4.33it/s]\u001b[A\n","2022it [14:32,  4.33it/s]\u001b[A\n","2023it [14:32,  4.33it/s]\u001b[A\n","2024it [14:32,  4.33it/s]\u001b[A\n","2025it [14:32,  4.33it/s]\u001b[A\n","2026it [14:33,  4.33it/s]\u001b[A\n","2027it [14:33,  4.33it/s]\u001b[A\n","2028it [14:33,  4.33it/s]\u001b[A\n","2029it [14:33,  4.33it/s]\u001b[A\n","2030it [14:34,  4.33it/s]\u001b[A\n","2031it [14:34,  4.33it/s]\u001b[A\n","2032it [14:34,  4.33it/s]\u001b[A\n","2033it [14:34,  4.33it/s]\u001b[A\n","2034it [14:35,  4.33it/s]\u001b[A\n","2035it [14:35,  4.33it/s]\u001b[A03/14/2022 09:49:55 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/14/2022 09:49:55 - INFO - __main__ -     eval_loss = 4.215115070343018\n","03/14/2022 09:49:55 - INFO - __main__ -     eval_auroc = 0.937250018119812\n","03/14/2022 09:49:55 - INFO - __main__ -     eval_recall = 1.0\n","03/14/2022 09:49:55 - INFO - __main__ -     eval_f1 = 0.39370080828666687\n","03/14/2022 09:49:55 - INFO - root -   *** Test ***\n","\n","2036it [14:35,  4.26it/s]\u001b[A\n","2037it [14:35,  4.28it/s]\u001b[A\n","2038it [14:36,  4.29it/s]\u001b[A\n","2039it [14:36,  4.30it/s]\u001b[A\n","2040it [14:36,  4.31it/s]\u001b[A\n","2041it [14:36,  4.31it/s]\u001b[A\n","2042it [14:36,  4.32it/s]\u001b[A\n","2043it [14:37,  4.32it/s]\u001b[A\n","2044it [14:37,  4.32it/s]\u001b[A\n","2045it [14:37,  4.33it/s]\u001b[A\n","2046it [14:37,  4.33it/s]\u001b[A\n","2047it [14:38,  4.33it/s]\u001b[A\n","2048it [14:38,  4.33it/s]\u001b[A\n","2049it [14:38,  4.33it/s]\u001b[A\n","2050it [14:38,  4.33it/s]\u001b[A\n","2051it [14:39,  4.33it/s]\u001b[A\n","2052it [14:39,  4.33it/s]\u001b[A\n","2053it [14:39,  4.33it/s]\u001b[A\n","2054it [14:39,  4.33it/s]\u001b[A\n","2055it [14:39,  4.33it/s]\u001b[A\n","2056it [14:40,  4.33it/s]\u001b[A\n","2057it [14:40,  4.33it/s]\u001b[A\n","2058it [14:40,  4.33it/s]\u001b[A\n","2059it [14:40,  4.33it/s]\u001b[A\n","2060it [14:41,  4.33it/s]\u001b[A\n","2061it [14:41,  4.33it/s]\u001b[A\n","2062it [14:41,  4.33it/s]\u001b[A\n","2063it [14:41,  4.33it/s]\u001b[A\n","2064it [14:42,  4.33it/s]\u001b[A\n","2065it [14:42,  4.32it/s]\u001b[A\n","2066it [14:42,  4.32it/s]\u001b[A\n","2067it [14:42,  4.33it/s]\u001b[A\n","2068it [14:42,  4.33it/s]\u001b[A\n","2069it [14:43,  4.33it/s]\u001b[A\n","2070it [14:43,  4.33it/s]\u001b[A\n","2071it [14:43,  4.33it/s]\u001b[A\n","2072it [14:43,  4.33it/s]\u001b[A\n","2073it [14:44,  4.33it/s]\u001b[A\n","2074it [14:44,  4.33it/s]\u001b[A\n","2075it [14:44,  4.33it/s]\u001b[A\n","2076it [14:44,  4.33it/s]\u001b[A\n","2077it [14:45,  4.33it/s]\u001b[A\n","2078it [14:45,  4.33it/s]\u001b[A\n","2079it [14:45,  4.33it/s]\u001b[A\n","2080it [14:45,  4.33it/s]\u001b[A\n","2081it [14:45,  4.33it/s]\u001b[A\n","2082it [14:46,  4.33it/s]\u001b[A\n","2083it [14:46,  4.33it/s]\u001b[A\n","2084it [14:46,  4.33it/s]\u001b[A\n","2085it [14:46,  4.33it/s]\u001b[A\n","2086it [14:47,  4.33it/s]\u001b[A\n","2087it [14:47,  4.33it/s]\u001b[A\n","2088it [14:47,  4.33it/s]\u001b[A\n","2089it [14:47,  4.33it/s]\u001b[A\n","2090it [14:48,  4.33it/s]\u001b[A\n","2091it [14:48,  4.33it/s]\u001b[A\n","2092it [14:48,  4.33it/s]\u001b[A\n","2093it [14:48,  4.33it/s]\u001b[A\n","2094it [14:48,  4.33it/s]\u001b[A\n","2095it [14:49,  4.33it/s]\u001b[A\n","2096it [14:49,  4.33it/s]\u001b[A\n","2097it [14:49,  4.33it/s]\u001b[A\n","2098it [14:49,  4.33it/s]\u001b[A\n","2099it [14:50,  4.33it/s]\u001b[A\n","2100it [14:50,  4.33it/s]\u001b[A\n","2101it [14:50,  4.33it/s]\u001b[A\n","2102it [14:50,  4.33it/s]\u001b[A\n","2103it [14:51,  4.33it/s]\u001b[A\n","2104it [14:51,  4.33it/s]\u001b[A\n","2105it [14:51,  4.33it/s]\u001b[A\n","2106it [14:51,  4.33it/s]\u001b[A\n","2107it [14:51,  4.33it/s]\u001b[A\n","2108it [14:52,  4.33it/s]\u001b[A\n","2109it [14:52,  4.33it/s]\u001b[A\n","2110it [14:52,  4.33it/s]\u001b[A\n","2111it [14:52,  4.33it/s]\u001b[A\n","2112it [14:53,  4.33it/s]\u001b[A\n","2113it [14:53,  4.33it/s]\u001b[A\n","2114it [14:53,  4.33it/s]\u001b[A\n","2115it [14:53,  4.33it/s]\u001b[A\n","2116it [14:54,  4.33it/s]\u001b[A\n","2117it [14:54,  4.33it/s]\u001b[A\n","2118it [14:54,  4.32it/s]\u001b[A\n","2119it [14:54,  4.33it/s]\u001b[A\n","2120it [14:54,  4.33it/s]\u001b[A\n","2121it [14:55,  4.33it/s]\u001b[A\n","2122it [14:55,  4.33it/s]\u001b[A\n","2123it [14:55,  4.33it/s]\u001b[A\n","2124it [14:55,  4.33it/s]\u001b[A\n","2125it [14:56,  4.33it/s]\u001b[A\n","2126it [14:56,  4.33it/s]\u001b[A\n","2127it [14:56,  4.33it/s]\u001b[A\n","2128it [14:56,  4.33it/s]\u001b[A\n","2129it [14:57,  4.33it/s]\u001b[A\n","2130it [14:57,  4.33it/s]\u001b[A\n","2131it [14:57,  4.33it/s]\u001b[A\n","2132it [14:57,  4.33it/s]\u001b[A\n","2133it [14:57,  4.33it/s]\u001b[A\n","2134it [14:58,  4.33it/s]\u001b[A\n","2135it [14:58,  4.33it/s]\u001b[A\n","2136it [14:58,  4.33it/s]\u001b[A\n","2137it [14:58,  4.33it/s]\u001b[A\n","2138it [14:59,  4.33it/s]\u001b[A\n","2139it [14:59,  4.33it/s]\u001b[A\n","2140it [14:59,  4.33it/s]\u001b[A\n","2141it [14:59,  4.33it/s]\u001b[A\n","2142it [15:00,  4.33it/s]\u001b[A\n","2143it [15:00,  4.33it/s]\u001b[A\n","2144it [15:00,  4.33it/s]\u001b[A\n","2145it [15:00,  4.33it/s]\u001b[A\n","2146it [15:00,  4.33it/s]\u001b[A\n","2147it [15:01,  4.33it/s]\u001b[A\n","2148it [15:01,  4.33it/s]\u001b[A\n","2149it [15:01,  4.33it/s]\u001b[A\n","2150it [15:01,  4.32it/s]\u001b[A\n","2151it [15:02,  4.32it/s]\u001b[A\n","2152it [15:02,  4.33it/s]\u001b[A\n","2153it [15:02,  4.33it/s]\u001b[A\n","2154it [15:02,  4.33it/s]\u001b[A\n","2155it [15:03,  4.33it/s]\u001b[A\n","2156it [15:03,  4.33it/s]\u001b[A\n","2157it [15:03,  4.33it/s]\u001b[A\n","2158it [15:03,  4.33it/s]\u001b[A\n","2159it [15:03,  4.33it/s]\u001b[A\n","2160it [15:04,  4.33it/s]\u001b[A\n","2161it [15:04,  4.33it/s]\u001b[A\n","2162it [15:04,  4.33it/s]\u001b[A\n","2163it [15:04,  4.33it/s]\u001b[A\n","2164it [15:05,  4.33it/s]\u001b[A\n","2165it [15:05,  4.33it/s]\u001b[A\n","2166it [15:05,  4.33it/s]\u001b[A\n","2167it [15:05,  4.33it/s]\u001b[A\n","2168it [15:06,  4.33it/s]\u001b[A\n","2169it [15:06,  4.33it/s]\u001b[A\n","2170it [15:06,  4.33it/s]\u001b[A\n","2171it [15:06,  4.33it/s]\u001b[A\n","2172it [15:06,  4.33it/s]\u001b[A\n","2173it [15:07,  4.33it/s]\u001b[A\n","2174it [15:07,  4.33it/s]\u001b[A\n","2175it [15:07,  4.33it/s]\u001b[A\n","2176it [15:07,  4.33it/s]\u001b[A\n","2177it [15:08,  4.33it/s]\u001b[A\n","2178it [15:08,  4.33it/s]\u001b[A\n","2179it [15:08,  4.33it/s]\u001b[A\n","2180it [15:08,  4.33it/s]\u001b[A\n","2181it [15:09,  4.33it/s]\u001b[A\n","2182it [15:09,  4.33it/s]\u001b[A\n","2183it [15:09,  4.33it/s]\u001b[A\n","2184it [15:09,  4.32it/s]\u001b[A\n","2185it [15:09,  4.33it/s]\u001b[A\n","2186it [15:10,  4.33it/s]\u001b[A\n","2187it [15:10,  4.33it/s]\u001b[A\n","2188it [15:10,  4.33it/s]\u001b[A\n","2189it [15:10,  4.33it/s]\u001b[A\n","2190it [15:11,  4.33it/s]\u001b[A\n","2191it [15:11,  4.33it/s]\u001b[A\n","2192it [15:11,  4.33it/s]\u001b[A\n","2193it [15:11,  4.33it/s]\u001b[A\n","2194it [15:12,  4.33it/s]\u001b[A\n","2195it [15:12,  4.33it/s]\u001b[A\n","2196it [15:12,  4.33it/s]\u001b[A\n","2197it [15:12,  4.33it/s]\u001b[A\n","2198it [15:12,  4.33it/s]\u001b[A\n","2199it [15:13,  4.33it/s]\u001b[A\n","2200it [15:13,  4.33it/s]\u001b[A\n","2201it [15:13,  4.33it/s]\u001b[A\n","2202it [15:13,  4.33it/s]\u001b[A\n","2203it [15:14,  4.32it/s]\u001b[A\n","2204it [15:14,  4.32it/s]\u001b[A\n","2205it [15:14,  4.33it/s]\u001b[A\n","2206it [15:14,  4.33it/s]\u001b[A\n","2207it [15:15,  4.33it/s]\u001b[A\n","2208it [15:15,  4.33it/s]\u001b[A\n","2209it [15:15,  4.33it/s]\u001b[A\n","2210it [15:15,  4.33it/s]\u001b[A\n","2211it [15:15,  4.33it/s]\u001b[A\n","2212it [15:16,  4.33it/s]\u001b[A\n","2213it [15:16,  4.33it/s]\u001b[A\n","2214it [15:16,  4.33it/s]\u001b[A\n","2215it [15:16,  4.33it/s]\u001b[A\n","2216it [15:17,  4.33it/s]\u001b[A\n","2217it [15:17,  4.33it/s]\u001b[A\n","2218it [15:17,  4.33it/s]\u001b[A\n","2219it [15:17,  4.33it/s]\u001b[A\n","2220it [15:18,  4.32it/s]\u001b[A\n","2221it [15:18,  4.33it/s]\u001b[A\n","2222it [15:18,  4.33it/s]\u001b[A\n","2223it [15:18,  4.32it/s]\u001b[A\n","2224it [15:18,  4.32it/s]\u001b[A\n","2225it [15:19,  4.33it/s]\u001b[A\n","2226it [15:19,  4.32it/s]\u001b[A\n","2227it [15:19,  4.32it/s]\u001b[A\n","2228it [15:19,  4.33it/s]\u001b[A\n","2229it [15:20,  4.33it/s]\u001b[A\n","2230it [15:20,  4.33it/s]\u001b[A\n","2231it [15:20,  4.33it/s]\u001b[A\n","2232it [15:20,  4.33it/s]\u001b[A\n","2233it [15:21,  4.33it/s]\u001b[A\n","2234it [15:21,  4.33it/s]\u001b[A\n","2235it [15:21,  4.33it/s]\u001b[A\n","2236it [15:21,  4.33it/s]\u001b[A\n","2237it [15:21,  4.33it/s]\u001b[A\n","2238it [15:22,  4.33it/s]\u001b[A\n","2239it [15:22,  4.33it/s]\u001b[A\n","2240it [15:22,  4.33it/s]\u001b[A\n","2241it [15:22,  4.33it/s]\u001b[A\n","2242it [15:23,  4.32it/s]\u001b[A\n","2243it [15:23,  4.33it/s]\u001b[A\n","2244it [15:23,  4.33it/s]\u001b[A\n","2245it [15:23,  4.33it/s]\u001b[A\n","2246it [15:24,  4.33it/s]\u001b[A\n","2247it [15:24,  4.33it/s]\u001b[A\n","2248it [15:24,  4.33it/s]\u001b[A\n","2249it [15:24,  4.33it/s]\u001b[A\n","2250it [15:24,  4.33it/s]\u001b[A\n","2251it [15:25,  4.33it/s]\u001b[A\n","2252it [15:25,  4.33it/s]\u001b[A\n","2253it [15:25,  4.33it/s]\u001b[A\n","2254it [15:25,  4.33it/s]\u001b[A\n","2255it [15:26,  4.33it/s]\u001b[A\n","2256it [15:26,  4.33it/s]\u001b[A\n","2257it [15:26,  4.33it/s]\u001b[A\n","2258it [15:26,  4.33it/s]\u001b[A\n","2259it [15:27,  4.33it/s]\u001b[A\n","2260it [15:27,  4.33it/s]\u001b[A\n","2261it [15:27,  4.33it/s]\u001b[A\n","2262it [15:27,  4.33it/s]\u001b[A\n","2263it [15:27,  4.33it/s]\u001b[A\n","2264it [15:28,  4.33it/s]\u001b[A\n","2265it [15:28,  4.33it/s]\u001b[A\n","2266it [15:28,  4.33it/s]\u001b[A\n","2267it [15:28,  4.33it/s]\u001b[A\n","2268it [15:29,  4.33it/s]\u001b[A\n","2269it [15:29,  4.33it/s]\u001b[A\n","2270it [15:29,  4.33it/s]\u001b[A\n","2271it [15:29,  4.33it/s]\u001b[A\n","2272it [15:30,  4.33it/s]\u001b[A\n","2273it [15:30,  4.33it/s]\u001b[A\n","2274it [15:30,  4.33it/s]\u001b[A\n","2275it [15:30,  4.33it/s]\u001b[A\n","2276it [15:30,  4.33it/s]\u001b[A\n","2277it [15:31,  4.33it/s]\u001b[A\n","2278it [15:31,  4.33it/s]\u001b[A\n","2279it [15:31,  4.33it/s]\u001b[A\n","2280it [15:31,  4.33it/s]\u001b[A\n","2281it [15:32,  4.33it/s]\u001b[A\n","2282it [15:32,  4.33it/s]\u001b[A\n","2283it [15:32,  4.33it/s]\u001b[A\n","2284it [15:32,  4.33it/s]\u001b[A\n","2285it [15:33,  4.33it/s]\u001b[A\n","2286it [15:33,  4.33it/s]\u001b[A\n","2287it [15:33,  4.33it/s]\u001b[A\n","2288it [15:33,  4.33it/s]\u001b[A\n","2289it [15:33,  4.33it/s]\u001b[A\n","2290it [15:34,  4.33it/s]\u001b[A\n","2291it [15:34,  4.33it/s]\u001b[A\n","2292it [15:34,  4.33it/s]\u001b[A\n","2293it [15:34,  4.33it/s]\u001b[A\n","2294it [15:35,  4.33it/s]\u001b[A\n","2295it [15:35,  4.33it/s]\u001b[A\n","2296it [15:35,  4.33it/s]\u001b[A\n","2297it [15:35,  4.33it/s]\u001b[A\n","2298it [15:36,  4.33it/s]\u001b[A\n","2299it [15:36,  4.33it/s]\u001b[A\n","2300it [15:36,  4.33it/s]\u001b[A\n","2301it [15:36,  4.33it/s]\u001b[A\n","2302it [15:36,  4.33it/s]\u001b[A\n","2303it [15:37,  4.33it/s]\u001b[A\n","2304it [15:37,  4.33it/s]\u001b[A\n","2305it [15:37,  4.33it/s]\u001b[A\n","2306it [15:37,  4.33it/s]\u001b[A\n","2307it [15:38,  4.33it/s]\u001b[A\n","2308it [15:38,  4.33it/s]\u001b[A\n","2309it [15:38,  4.33it/s]\u001b[A\n","2310it [15:38,  4.33it/s]\u001b[A\n","2311it [15:39,  4.33it/s]\u001b[A\n","2312it [15:39,  4.33it/s]\u001b[A\n","2313it [15:39,  4.33it/s]\u001b[A\n","2314it [15:39,  4.33it/s]\u001b[A\n","2315it [15:40,  4.33it/s]\u001b[A\n","2316it [15:40,  4.33it/s]\u001b[A\n","2317it [15:40,  4.33it/s]\u001b[A\n","2318it [15:40,  4.33it/s]\u001b[A\n","2319it [15:40,  4.33it/s]\u001b[A\n","2320it [15:41,  4.33it/s]\u001b[A\n","2321it [15:41,  4.33it/s]\u001b[A\n","2322it [15:41,  4.33it/s]\u001b[A\n","2323it [15:41,  4.33it/s]\u001b[A\n","2324it [15:42,  4.33it/s]\u001b[A\n","2325it [15:42,  4.33it/s]\u001b[A\n","2326it [15:42,  4.33it/s]\u001b[A\n","2327it [15:42,  4.33it/s]\u001b[A\n","2328it [15:43,  4.33it/s]\u001b[A\n","2329it [15:43,  4.33it/s]\u001b[A\n","2330it [15:43,  4.33it/s]\u001b[A\n","2331it [15:43,  4.33it/s]\u001b[A\n","2332it [15:43,  4.33it/s]\u001b[A\n","2333it [15:44,  4.33it/s]\u001b[A\n","2334it [15:44,  4.33it/s]\u001b[A\n","2335it [15:44,  4.33it/s]\u001b[A\n","2336it [15:44,  4.33it/s]\u001b[A\n","2337it [15:45,  4.33it/s]\u001b[A\n","2338it [15:45,  4.33it/s]\u001b[A\n","2339it [15:45,  4.33it/s]\u001b[A\n","2340it [15:45,  4.33it/s]\u001b[A\n","2341it [15:46,  4.33it/s]\u001b[A\n","2342it [15:46,  4.33it/s]\u001b[A\n","2343it [15:46,  4.33it/s]\u001b[A\n","2344it [15:46,  4.33it/s]\u001b[A\n","2345it [15:46,  4.33it/s]\u001b[A\n","2346it [15:47,  4.33it/s]\u001b[A\n","2347it [15:47,  4.33it/s]\u001b[A\n","2348it [15:47,  4.33it/s]\u001b[A\n","2349it [15:47,  4.33it/s]\u001b[A\n","2350it [15:48,  4.33it/s]\u001b[A\n","2351it [15:48,  4.33it/s]\u001b[A\n","2352it [15:48,  4.33it/s]\u001b[A\n","2353it [15:48,  4.33it/s]\u001b[A\n","2354it [15:49,  4.33it/s]\u001b[A03/14/2022 09:51:09 - INFO - __main__ -   ***** Test results spoilers *****\n","03/14/2022 09:51:09 - INFO - __main__ -     eval_loss = 5.07611083984375\n","03/14/2022 09:51:09 - INFO - __main__ -     eval_auroc = 0.852401077747345\n","03/14/2022 09:51:09 - INFO - __main__ -     eval_recall = 0.9583333134651184\n","03/14/2022 09:51:09 - INFO - __main__ -     eval_f1 = 0.2266009896993637\n","03/14/2022 09:51:09 - INFO - filelock -   Lock 140578843819600 acquired on log.lock\n","03/14/2022 09:51:09 - INFO - filelock -   Lock 140578843819600 released on log.lock\n","2354it [15:49,  2.48it/s]\n"]}]},{"cell_type":"code","source":["!source env/bin/activate; python tools/ensemble.py --condition \"{'tag': 'yes_no_ensemble', 'task_name': 'spoilers', 'few_shot_type': 'prompt-demo'}\" --n_models 20"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T49oI5hLkC3d","executionInfo":{"status":"ok","timestamp":1647253078620,"user_tz":420,"elapsed":5845,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"}},"outputId":"f21b669f-075a-4774-9eed-58de7fb8f2a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model 0 | val: mean +- std: 94.4 +- 0.0 | test: mean +- std: 88.4 (0.0) (median 88.4) / 95.8 +- 0.0 (median 95.8)\n","Model 1 | val: mean +- std: 94.4 +- 0.0 | test: mean +- std: 90.3 (0.0) (median 90.3) / 95.8 +- 0.0 (median 95.8)\n","Model 2 | val: mean +- std: 95.2 +- 0.0 | test: mean +- std: 89.7 (0.0) (median 89.7) / 95.8 +- 0.0 (median 95.8)\n","Model 3 | val: mean +- std: 88.0 +- 0.0 | test: mean +- std: 83.4 (0.0) (median 83.4) / 91.7 +- 0.0 (median 91.7)\n","Model 4 | val: mean +- std: 91.6 +- 0.0 | test: mean +- std: 85.1 (0.0) (median 85.1) / 95.8 +- 0.0 (median 95.8)\n","Model 5 | val: mean +- std: 86.5 +- 0.0 | test: mean +- std: 86.7 (0.0) (median 86.7) / 75.0 +- 0.0 (median 75.0)\n","Model 6 | val: mean +- std: 87.7 +- 0.0 | test: mean +- std: 84.4 (0.0) (median 84.4) / 100.0 +- 0.0 (median 100.0)\n","Model 7 | val: mean +- std: 93.1 +- 0.0 | test: mean +- std: 90.2 (0.0) (median 90.2) / 83.3 +- 0.0 (median 83.3)\n","Model 8 | val: mean +- std: 86.0 +- 0.0 | test: mean +- std: 81.8 (0.0) (median 81.8) / 95.8 +- 0.0 (median 95.8)\n","Model 9 | val: mean +- std: 89.8 +- 0.0 | test: mean +- std: 83.6 (0.0) (median 83.6) / 95.8 +- 0.0 (median 95.8)\n","Model 10 | val: mean +- std: 89.9 +- 0.0 | test: mean +- std: 79.1 (0.0) (median 79.1) / 75.0 +- 0.0 (median 75.0)\n","Model 11 | val: mean +- std: 93.6 +- 0.0 | test: mean +- std: 89.2 (0.0) (median 89.2) / 91.7 +- 0.0 (median 91.7)\n","Model 12 | val: mean +- std: 96.0 +- 0.0 | test: mean +- std: 93.4 (0.0) (median 93.4) / 83.3 +- 0.0 (median 83.3)\n","Model 13 | val: mean +- std: 87.5 +- 0.0 | test: mean +- std: 87.2 (0.0) (median 87.2) / 87.5 +- 0.0 (median 87.5)\n","Model 14 | val: mean +- std: 85.3 +- 0.0 | test: mean +- std: 77.1 (0.0) (median 77.1) / 95.8 +- 0.0 (median 95.8)\n","Model 15 | val: mean +- std: 92.0 +- 0.0 | test: mean +- std: 83.3 (0.0) (median 83.3) / 83.3 +- 0.0 (median 83.3)\n","Model 16 | val: mean +- std: 85.3 +- 0.0 | test: mean +- std: 77.5 (0.0) (median 77.5) / 95.8 +- 0.0 (median 95.8)\n","Model 17 | val: mean +- std: 88.3 +- 0.0 | test: mean +- std: 82.4 (0.0) (median 82.4) / 91.7 +- 0.0 (median 91.7)\n","Model 18 | val: mean +- std: 87.3 +- 0.0 | test: mean +- std: 82.4 (0.0) (median 82.4) / 83.3 +- 0.0 (median 83.3)\n","Model 19 | val: mean +- std: 93.7 +- 0.0 | test: mean +- std: 85.2 (0.0) (median 85.2) / 95.8 +- 0.0 (median 95.8)\n","mean +- std: 78.4 (0.0) (median 78.4) / 91.7 (0.0) (median 91.7)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"39wlurug4F2j"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"run.ipynb","provenance":[],"authorship_tag":"ABX9TyMPtKj7hmaeRRvum0Li3nMz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}