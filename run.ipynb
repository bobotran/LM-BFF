{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20451,"status":"ok","timestamp":1696996114708,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"K0FWZuLPwwJL","outputId":"001bc2bb-b917-4114-d2e8-979370c70559"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1264,"status":"ok","timestamp":1696996115970,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"XiHSG1Okw2Zp","outputId":"83c5ee8e-6dbf-4fc5-ebfb-56499724c695"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Spoiler Detection/LM-BFF\n","\u001b[0m\u001b[01;34mauto_label_mapping\u001b[0m/        log.lock\n","\u001b[01;34mauto_template\u001b[0m/             \u001b[01;34mnew_env\u001b[0m/\n","average_logits.py          \u001b[01;34mold_ensemble_predict_results\u001b[0m/\n","copy_ensemble_predicts.sh  README.md\n","count_tokens.ipynb         requirements.txt\n","count_tokens.py            \u001b[01;34mresult\u001b[0m/\n","\u001b[01;34mdata\u001b[0m/                      run_experiment.sh\n","discord_log                run.ipynb\n","eda.ipynb                  run_multiple.sh\n","\u001b[01;34mensemble_predict_results\u001b[0m/  run.py\n","ensemble_predict.sh        \u001b[01;34mspoilers_auto_label_mapping\u001b[0m/\n","\u001b[01;34menv\u001b[0m/                       \u001b[01;34mspoilers_auto_template\u001b[0m/\n","evaluate.ipynb             \u001b[01;34msrc\u001b[0m/\n","evaluate.sh                template_search.sh\n","\u001b[01;34mfigs\u001b[0m/                      tmp.txt\n","label_search.sh            \u001b[01;34mtools\u001b[0m/\n","LICENSE                    train_individual.sh\n","log                        uncertain_log\n"]}],"source":["%cd \"/content/drive/MyDrive/Spoiler Detection/LM-BFF\"\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1353,"status":"ok","timestamp":1696917407840,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"6GerUwQjwnUg","outputId":"6846c7be-608b-4c1b-e4a5-621edacbeba8"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/bin/python\n"]}],"source":["\n","!source env/bin/activate; which python"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":191462,"status":"ok","timestamp":1696996310517,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"mrerFuJg9Lcp","outputId":"f7db616b-3492-4b0b-9ee8-a32915ed8a0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n","\r0% [Connecting to security.ubuntu.com (91.189.91.82)] [Connected to cloud.r-pro\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","\r0% [2 InRelease 14.2 kB/119 kB 12%] [Connecting to security.ubuntu.com (91.189.\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","\r0% [2 InRelease 30.1 kB/119 kB 25%] [Connecting to security.ubuntu.com (91.189.\r0% [2 InRelease 33.0 kB/119 kB 28%] [Connecting to security.ubuntu.com (91.189.\r                                                                               \rHit:4 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","\r0% [2 InRelease 85.1 kB/119 kB 72%] [Connecting to security.ubuntu.com (91.189.\r0% [Connecting to security.ubuntu.com (91.189.91.82)] [Connecting to ppa.launch\r                                                                               \rGet:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n","Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,343 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,269 kB]\n","Get:12 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,004 kB]\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,081 kB]\n","Fetched 5,038 kB in 8s (599 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libpython3.8 libpython3.8-dev libpython3.8-minimal libpython3.8-stdlib\n","  mailcap mime-support python3.8 python3.8-distutils python3.8-lib2to3\n","  python3.8-minimal\n","Suggested packages:\n","  binfmt-support\n","The following NEW packages will be installed:\n","  libpython3.8 libpython3.8-dev libpython3.8-minimal libpython3.8-stdlib\n","  mailcap mime-support python3.8 python3.8-dev python3.8-distutils\n","  python3.8-lib2to3 python3.8-minimal python3.8-venv\n","0 upgraded, 12 newly installed, 0 to remove and 18 not upgraded.\n","Need to get 16.4 MB of archives.\n","After this operation, 50.2 MB of additional disk space will be used.\n","Get:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-minimal amd64 3.8.18-1+jammy1 [794 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n","Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-minimal amd64 3.8.18-1+jammy1 [2,024 kB]\n","Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-stdlib amd64 3.8.18-1+jammy1 [1,815 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n","Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8 amd64 3.8.18-1+jammy1 [1,800 kB]\n","Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-dev amd64 3.8.18-1+jammy1 [4,386 kB]\n","Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8 amd64 3.8.18-1+jammy1 [438 kB]\n","Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-dev amd64 3.8.18-1+jammy1 [500 kB]\n","Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-lib2to3 all 3.8.18-1+jammy1 [126 kB]\n","Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-distutils all 3.8.18-1+jammy1 [193 kB]\n","Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-venv amd64 3.8.18-1+jammy1 [4,311 kB]\n","Fetched 16.4 MB in 1s (12.7 MB/s)\n","Selecting previously unselected package libpython3.8-minimal:amd64.\n","(Reading database ... 120875 files and directories currently installed.)\n","Preparing to unpack .../00-libpython3.8-minimal_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking libpython3.8-minimal:amd64 (3.8.18-1+jammy1) ...\n","Selecting previously unselected package python3.8-minimal.\n","Preparing to unpack .../01-python3.8-minimal_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking python3.8-minimal (3.8.18-1+jammy1) ...\n","Selecting previously unselected package mailcap.\n","Preparing to unpack .../02-mailcap_3.70+nmu1ubuntu1_all.deb ...\n","Unpacking mailcap (3.70+nmu1ubuntu1) ...\n","Selecting previously unselected package mime-support.\n","Preparing to unpack .../03-mime-support_3.66_all.deb ...\n","Unpacking mime-support (3.66) ...\n","Selecting previously unselected package libpython3.8-stdlib:amd64.\n","Preparing to unpack .../04-libpython3.8-stdlib_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking libpython3.8-stdlib:amd64 (3.8.18-1+jammy1) ...\n","Selecting previously unselected package libpython3.8:amd64.\n","Preparing to unpack .../05-libpython3.8_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking libpython3.8:amd64 (3.8.18-1+jammy1) ...\n","Selecting previously unselected package libpython3.8-dev:amd64.\n","Preparing to unpack .../06-libpython3.8-dev_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking libpython3.8-dev:amd64 (3.8.18-1+jammy1) ...\n","Selecting previously unselected package python3.8.\n","Preparing to unpack .../07-python3.8_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking python3.8 (3.8.18-1+jammy1) ...\n","Selecting previously unselected package python3.8-dev.\n","Preparing to unpack .../08-python3.8-dev_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking python3.8-dev (3.8.18-1+jammy1) ...\n","Selecting previously unselected package python3.8-lib2to3.\n","Preparing to unpack .../09-python3.8-lib2to3_3.8.18-1+jammy1_all.deb ...\n","Unpacking python3.8-lib2to3 (3.8.18-1+jammy1) ...\n","Selecting previously unselected package python3.8-distutils.\n","Preparing to unpack .../10-python3.8-distutils_3.8.18-1+jammy1_all.deb ...\n","Unpacking python3.8-distutils (3.8.18-1+jammy1) ...\n","Selecting previously unselected package python3.8-venv.\n","Preparing to unpack .../11-python3.8-venv_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking python3.8-venv (3.8.18-1+jammy1) ...\n","Setting up libpython3.8-minimal:amd64 (3.8.18-1+jammy1) ...\n","Setting up python3.8-lib2to3 (3.8.18-1+jammy1) ...\n","Setting up python3.8-minimal (3.8.18-1+jammy1) ...\n","Setting up python3.8-distutils (3.8.18-1+jammy1) ...\n","Setting up mailcap (3.70+nmu1ubuntu1) ...\n","Setting up mime-support (3.66) ...\n","Setting up libpython3.8-stdlib:amd64 (3.8.18-1+jammy1) ...\n","Setting up python3.8 (3.8.18-1+jammy1) ...\n","Setting up libpython3.8:amd64 (3.8.18-1+jammy1) ...\n","Setting up python3.8-venv (3.8.18-1+jammy1) ...\n","Setting up libpython3.8-dev:amd64 (3.8.18-1+jammy1) ...\n","Setting up python3.8-dev (3.8.18-1+jammy1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Requirement already satisfied: pip in ./env/lib/python3.8/site-packages (23.2.1)\n"]}],"source":["!apt-get update\n","!apt-get install python3.8-dev python3.8-venv\n","!source env/bin/activate; python -m pip install --upgrade pip"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47919,"status":"ok","timestamp":1696993526186,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"ybVVjGD2Bscy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"29e84de1-81c9-4bc3-c5cc-1c8674429341"},"outputs":[{"output_type":"stream","name":"stdout","text":["^C\n"]}],"source":["!find . -type f -name '*.lock' -delete"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":398,"status":"ok","timestamp":1696995404434,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"RtJDwadFYfn4","outputId":"0f384206-34b0-42f5-d5ca-110a7faa7cd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Oct 11 03:36:44 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    24W / 300W |      0MiB / 16384MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28977708,"status":"ok","timestamp":1674464547105,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":480},"id":"_CAWPaTjYUxt","outputId":"68d7584f-700f-4997-aad5-149ee9ec807d"},"outputs":[{"name":"stdout","output_type":"stream","text":["01/23/2023 01:18:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","01/23/2023 01:18:46 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/pred_v1.0_batch32', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=128, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=4500, warmup_steps=0, logging_dir='runs/Jan23_01-18-46_b3dd4534b7aa', logging_first_step=False, logging_steps=225, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=225, dataloader_num_workers=0, past_index=-1, run_name='result/pred_v1.0_batch32', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=True, save_logit_dir='result/pred_v1.0_batch32', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","01/23/2023 01:18:46 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","01/23/2023 01:18:48 - INFO - filelock -   Lock 139692952398192 acquired on /root/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748.lock\n","Downloading: 100% 482/482 [00:00<00:00, 505kB/s]\n","01/23/2023 01:18:49 - INFO - filelock -   Lock 139692952398192 released on /root/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748.lock\n","01/23/2023 01:18:51 - INFO - filelock -   Lock 139692952419680 acquired on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n","Downloading: 100% 899k/899k [00:01<00:00, 811kB/s]\n","01/23/2023 01:18:53 - INFO - filelock -   Lock 139692952419680 released on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n","01/23/2023 01:18:54 - INFO - filelock -   Lock 139692952396320 acquired on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n","Downloading: 100% 456k/456k [00:00<00:00, 508kB/s]\n","01/23/2023 01:18:56 - INFO - filelock -   Lock 139692952396320 released on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n","01/23/2023 01:18:56 - INFO - src.dataset -   Label 0 to word ƒ†relevant (4249)\n","01/23/2023 01:18:56 - INFO - src.dataset -   Label 1 to word ƒ†irrelevant (21821)\n","01/23/2023 01:18:56 - INFO - src.dataset -   Total num_sample for mode train: 1\n","01/23/2023 01:18:56 - INFO - src.dataset -   Creating/loading examples from dataset file at data/unlabeled_v1.0.1\n","01/23/2023 01:18:56 - INFO - filelock -   Lock 139692952397568 acquired on data/unlabeled_v1.0.1/cached_train_RobertaTokenizer_512_spoilers.lock\n","01/23/2023 01:18:56 - INFO - src.dataset -   Creating features from dataset file at data/unlabeled_v1.0.1\n","01/23/2023 01:18:57 - INFO - src.dataset -   Saving features into cached file data/unlabeled_v1.0.1/cached_train_RobertaTokenizer_512_spoilers [took 0.040 s]\n","01/23/2023 01:18:57 - INFO - filelock -   Lock 139692952397568 released on data/unlabeled_v1.0.1/cached_train_RobertaTokenizer_512_spoilers.lock\n","01/23/2023 01:19:00 - INFO - src.dataset -   Label 0 to word ƒ†relevant (4249)\n","01/23/2023 01:19:00 - INFO - src.dataset -   Label 1 to word ƒ†irrelevant (21821)\n","01/23/2023 01:19:00 - INFO - src.dataset -   Total num_sample for mode test: 1\n","01/23/2023 01:19:00 - INFO - src.dataset -   Creating/loading examples from dataset file at data/unlabeled_v1.0.1\n","01/23/2023 01:19:00 - INFO - filelock -   Lock 139692952397568 acquired on data/unlabeled_v1.0.1/cached_test_RobertaTokenizer_512_spoilers.lock\n","01/23/2023 01:19:00 - INFO - src.dataset -   Creating features from dataset file at data/unlabeled_v1.0.1\n","01/23/2023 01:19:05 - INFO - src.dataset -   Saving features into cached file data/unlabeled_v1.0.1/cached_test_RobertaTokenizer_512_spoilers [took 1.399 s]\n","01/23/2023 01:19:05 - INFO - filelock -   Lock 139692952397568 released on data/unlabeled_v1.0.1/cached_test_RobertaTokenizer_512_spoilers.lock\n","01/23/2023 01:20:45 - INFO - src.dataset -   *** Example ***\n","01/23/2023 01:20:45 - INFO - src.dataset -   guid: test-0\n","01/23/2023 01:20:45 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 2387, 12535, 35, 5414, 24979, 16, 164, 7, 7960, 255, 8615, 225, 6, 7572, 4270, 6, 12235, 281, 6, 20, 755, 36671, 8, 13943, 25704, 32, 164, 7, 1597, 4, 4594, 39508, 16, 164, 7, 304, 15522, 428, 7026, 4, 248, 2055, 226, 5457, 344, 16, 8748, 39679, 32653, 4, 85, 21, 50264, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=None, mask_pos=[53], label_word_list=None)\n","01/23/2023 01:20:45 - INFO - src.dataset -   text: <s>My predictions: Kings Landing is going to Burn Tommen, Margery, Loras, The High Sparrow and Cersei are going to die. Eruron is going to use Dragonbinder. R + L = J is CONFIRMED. It was<mask>.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n","01/23/2023 01:31:51 - INFO - root -   *** Test ***\n","100% 1978/1978 [7:30:13<00:00,  9.85s/it]/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","01/23/2023 09:02:18 - INFO - __main__ -   ***** Test results spoilers *****\n","01/23/2023 09:02:18 - INFO - filelock -   Lock 139678203678528 acquired on log.lock\n","01/23/2023 09:02:19 - INFO - filelock -   Lock 139678203678528 released on log.lock\n","100% 1978/1978 [7:30:14<00:00, 13.66s/it]\n"]}],"source":["# Divide unlabeled set (test.csv) into chunks of 260000\n","!source env/bin/activate; TAG=\"pred_v1.0_batch32\" TYPE=prompt TASK=spoilers BS=32 LR=1e-5 SEED=21 MODEL=roberta-large bash evaluate.sh \\\n"," \"--template *cls**sent_0*._It_was*mask*.*sep+* --mapping {0:'relevant',1:'irrelevant'} --first_sent_limit 502 --other_sent_limit 502\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304,"status":"ok","timestamp":1696918972819,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"h1BUI2Tl6RaP","outputId":"ee0b283c-5353-46a3-f7ea-811b485c0d39"},"outputs":[{"name":"stdout","output_type":"stream","text":["rm: cannot remove '/content/drive/MyDrive/Spoiler Detection/spoiler-detection/data/unlabeled_v1.0.1/cached_\\*': No such file or directory\n"]}],"source":["# !rm \"data/unlabeled_v1.0.1/test.csv\"\n","# !rm \"data/unlabeled_v1.0.1/chunk_*\"\"\n","!rm \"data/unlabeled_v1.0.1/cached_*\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzfD7O9-gjLA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696996310969,"user_tz":420,"elapsed":466,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"}},"outputId":"89d48d89-892a-437a-d1dd-b4280887b15a"},"outputs":[{"output_type":"stream","name":"stdout","text":["#!/bin/bash\n","for template_id in {49..50}\n","do\n","    # for seed in 13 21 42 87 100\n","    for seed in 21\n","    do\n","        # To save time, we fix these hyper-parameters\n","        bs=32\n","        lr=1e-5\n","        \n","        TAG=irrelevant-relevant-template-ensemble-predict\n","        TYPE=prompt\n","        TASK=spoilers\n","        BS=$bs\n","        LR=$lr\n","        SEED=$seed\n","        MODEL=roberta-large\n","        TEMPLATE_ID=$template_id\n","    \n","        K=3615\n","        NUM_EPOCHS=20\n","        # Training steps\n","        MAX_STEP=$(($K * 2 / $BS * $NUM_EPOCHS))\n","        \n","        # Validation steps\n","        EVAL_STEP=$(($K * 2 / $BS * 1))\n","        LOG_STEP=$(($EVAL_STEP / 10))\n","        \n","        TASK_EXTRA=\"\"\n","\n","        MAPPING=\"{0:'relevant',1:'irrelevant'}\"\n","        \n","        REAL_BS=2\n","        GS=$(expr $BS / $REAL_BS)\n","        \n","        DATA_DIR=\"data/unlabeled_v1.0.1\"\n","        \n","        CHECKPOINT=result/$template_id/\n","        \n","        # Since we only use dev performance here, use --no_predict to skip testing\n","        python run.py \\\n","          --resume_from $CHECKPOINT \\\n","          --save_logit \\\n","          --save_logit_dir result/autolabeled_$template_id \\\n","          --task_name $TASK \\\n","          --data_dir $DATA_DIR \\\n","          --do_predict \\\n","          --do_eval \\\n","          --model_name_or_path $MODEL \\\n","          --few_shot_type $TYPE \\\n","          --num_k $K \\\n","          --max_seq_length 512 \\\n","          --per_device_eval_batch_size 128 \\\n","          --gradient_accumulation_steps $GS \\\n","          --learning_rate $LR \\\n","          --max_steps $MAX_STEP \\\n","          --logging_steps $EVAL_STEP \\\n","          --eval_steps $EVAL_STEP \\\n","          --num_train_epochs 0 \\\n","          --output_dir result/autolabeled_$template_id \\\n","          --seed $SEED \\\n","          --tag $TAG \\\n","          --mapping $MAPPING \\\n","          --max_steps $MAX_STEP \\\n","          --template_path spoilers_auto_template/irrelevant_relevant/16-$seed.txt \\\n","          --template_id $template_id \\\n","          --first_sent_limit 502 \\\n","          --other_sent_limit 502\n","    done\n","done\n"]}],"source":["!cat ensemble_predict.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18BdLqK-XWkh","outputId":"09e2ff77-38d6-41ec-fd3b-3fb47ac4089a"},"outputs":[{"output_type":"stream","name":"stdout","text":["10/11/2023 04:07:26 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/irrelevant_relevant/16-21.txt\n","10/11/2023 04:07:26 - INFO - __main__ -   Specify load the 49-th template: *cls**sent_0*‚ñÅat*mask*.*sep+*\n","10/11/2023 04:07:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","10/11/2023 04:07:26 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/autolabeled_49', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=128, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=4500, warmup_steps=0, logging_dir='runs/Oct11_04-07-25_26edf46c0760', logging_first_step=False, logging_steps=225, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=225, dataloader_num_workers=0, past_index=-1, run_name='result/autolabeled_49', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=True, save_logit_dir='result/autolabeled_49', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","10/11/2023 04:07:26 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","10/11/2023 04:07:28 - INFO - filelock -   Lock 134565055098064 acquired on /root/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748.lock\n","Downloading: 100% 482/482 [00:00<00:00, 366kB/s]\n","10/11/2023 04:07:29 - INFO - filelock -   Lock 134565055098064 released on /root/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748.lock\n","10/11/2023 04:07:29 - INFO - filelock -   Lock 134565055049680 acquired on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n","Downloading: 100% 899k/899k [00:00<00:00, 2.09MB/s]\n","10/11/2023 04:07:30 - INFO - filelock -   Lock 134565055049680 released on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n","10/11/2023 04:07:31 - INFO - filelock -   Lock 134565055048768 acquired on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n","Downloading: 100% 456k/456k [00:00<00:00, 1.32MB/s]\n","10/11/2023 04:07:32 - INFO - filelock -   Lock 134565055048768 released on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n","10/11/2023 04:07:32 - INFO - src.dataset -   Label 0 to word ƒ†relevant (4249)\n","10/11/2023 04:07:32 - INFO - src.dataset -   Label 1 to word ƒ†irrelevant (21821)\n","10/11/2023 04:07:32 - INFO - src.dataset -   Total num_sample for mode train: 1\n","10/11/2023 04:07:32 - INFO - src.dataset -   Creating/loading examples from dataset file at data/unlabeled_v1.0.1\n","10/11/2023 04:07:32 - INFO - filelock -   Lock 134565055049680 acquired on data/unlabeled_v1.0.1/cached_train_RobertaTokenizer_512_spoilers.lock\n","10/11/2023 04:07:33 - INFO - src.dataset -   Loading features from cached file data/unlabeled_v1.0.1/cached_train_RobertaTokenizer_512_spoilers [took 1.263 s]\n","10/11/2023 04:07:33 - INFO - filelock -   Lock 134565055049680 released on data/unlabeled_v1.0.1/cached_train_RobertaTokenizer_512_spoilers.lock\n","10/11/2023 04:07:36 - INFO - src.dataset -   Label 0 to word ƒ†relevant (4249)\n","10/11/2023 04:07:36 - INFO - src.dataset -   Label 1 to word ƒ†irrelevant (21821)\n","10/11/2023 04:07:36 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","10/11/2023 04:07:36 - INFO - src.dataset -   Creating/loading examples from dataset file at data/unlabeled_v1.0.1\n","10/11/2023 04:07:36 - INFO - filelock -   Lock 134565055049680 acquired on data/unlabeled_v1.0.1/cached_dev_RobertaTokenizer_512_spoilers.lock\n","10/11/2023 04:07:38 - INFO - src.dataset -   Loading features from cached file data/unlabeled_v1.0.1/cached_dev_RobertaTokenizer_512_spoilers [took 1.700 s]\n","10/11/2023 04:07:38 - INFO - filelock -   Lock 134565055049680 released on data/unlabeled_v1.0.1/cached_dev_RobertaTokenizer_512_spoilers.lock\n","10/11/2023 04:07:39 - INFO - src.dataset -   *** Example ***\n","10/11/2023 04:07:39 - INFO - src.dataset -   guid: dev-0\n","10/11/2023 04:07:39 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 771, 523, 642, 38, 122, 33, 7036, 14, 51, 351, 17, 27, 90, 28, 37698, 24, 7, 18220, 42, 191, 4, 8133, 44412, 268, 4, 3421, 7, 8402, 9, 5, 2664, 636, 12266, 8, 471, 13, 127, 3627, 4, 48584, 10172, 415, 50264, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[43], label_word_list=None)\n","10/11/2023 04:07:39 - INFO - src.dataset -   text: <s>Welp I now have confirmation that they won‚Äôt be uploading it to Hulu this season. Motherfuckers. Time to dust of the tricorne and head for my ship.‚ñÅat<mask>.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n","10/11/2023 04:07:45 - INFO - src.dataset -   Label 0 to word ƒ†relevant (4249)\n","10/11/2023 04:07:45 - INFO - src.dataset -   Label 1 to word ƒ†irrelevant (21821)\n","10/11/2023 04:07:45 - INFO - src.dataset -   Total num_sample for mode test: 1\n","10/11/2023 04:07:45 - INFO - src.dataset -   Creating/loading examples from dataset file at data/unlabeled_v1.0.1\n","10/11/2023 04:07:45 - INFO - filelock -   Lock 134565055049440 acquired on data/unlabeled_v1.0.1/cached_test_RobertaTokenizer_512_spoilers.lock\n","10/11/2023 04:07:50 - INFO - src.dataset -   Loading features from cached file data/unlabeled_v1.0.1/cached_test_RobertaTokenizer_512_spoilers [took 5.003 s]\n","10/11/2023 04:07:50 - INFO - filelock -   Lock 134565055049440 released on data/unlabeled_v1.0.1/cached_test_RobertaTokenizer_512_spoilers.lock\n","10/11/2023 04:09:47 - INFO - src.dataset -   *** Example ***\n","10/11/2023 04:09:47 - INFO - src.dataset -   guid: test-0\n","10/11/2023 04:09:47 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 17, 48, 2522, 38810, 9406, 3366, 17, 46, 1174, 3309, 1755, 1000, 846, 967, 10321, 6738, 11433, 17841, 4958, 48584, 10172, 415, 50264, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=None, mask_pos=[23], label_word_list=None)\n","10/11/2023 04:09:47 - INFO - src.dataset -   text: <s>‚ÄúOur hextech dream‚Äù‚Ä¶ JayceXViktor shipping incoming üò´‚ñÅat<mask>.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","10/11/2023 04:19:44 - INFO - __main__ -   *** Validate ***\n","100% 22/22 [01:30<00:00,  2.78s/it]/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","10/11/2023 04:21:23 - INFO - __main__ -   ***** Eval results spoilers *****\n","10/11/2023 04:21:23 - INFO - __main__ -     eval_loss = 0.3966015577316284\n","10/11/2023 04:21:23 - INFO - __main__ -     eval_ap = 0.8786135315895081\n","10/11/2023 04:21:23 - INFO - __main__ -     eval_auroc = 0.9111727476119995\n","10/11/2023 04:21:23 - INFO - __main__ -     eval_recall = 0.8402710556983948\n","10/11/2023 04:21:23 - INFO - __main__ -     eval_f1 = 0.7937813997268677\n","10/11/2023 04:21:24 - INFO - root -   *** Test ***\n","1972it [2:06:03,  3.83s/it]"]}],"source":["# Divide unlabeled set (test.csv) into chunks of 260000\n","!source env/bin/activate; bash ensemble_predict.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1538000,"status":"ok","timestamp":1693297440978,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"CGOL-CmaAO9p","outputId":"aaed5b27-c605-41ab-ac5f-924fa8892714"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/training_args.py:337: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  warnings.warn(\n","08/29/2023 07:19:48 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/29/2023 07:19:48 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/stuffed_v1.0_batch32', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=900, warmup_steps=0, logging_dir='runs/Aug29_07-19-48_2b7e1c624e4f', logging_first_step=False, logging_steps=22, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=225, dataloader_num_workers=0, past_index=-1, run_name='result/stuffed_v1.0_batch32', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=True, save_logit_dir='result/stuffed_v1.0_batch32', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","08/29/2023 07:19:48 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","08/29/2023 07:19:50 - INFO - filelock -   Lock 134345648430096 acquired on /root/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748.lock\n","Downloading: 100% 482/482 [00:00<00:00, 388kB/s]\n","08/29/2023 07:19:50 - INFO - filelock -   Lock 134345648430096 released on /root/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748.lock\n","08/29/2023 07:19:51 - INFO - filelock -   Lock 134345648365280 acquired on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n","Downloading: 100% 899k/899k [00:00<00:00, 2.11MB/s]\n","08/29/2023 07:19:52 - INFO - filelock -   Lock 134345648365280 released on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n","08/29/2023 07:19:52 - INFO - filelock -   Lock 134345648483104 acquired on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n","Downloading: 100% 456k/456k [00:00<00:00, 1.35MB/s]\n","08/29/2023 07:19:53 - INFO - filelock -   Lock 134345648483104 released on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n","08/29/2023 07:19:54 - INFO - src.dataset -   Label 0 to word ƒ†relevant (4249)\n","08/29/2023 07:19:54 - INFO - src.dataset -   Label 1 to word ƒ†irrelevant (21821)\n","08/29/2023 07:19:54 - INFO - src.dataset -   Total num_sample for mode train: 1\n","08/29/2023 07:19:54 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/stuffed_binary_v1.0/3615-21\n","08/29/2023 07:19:55 - INFO - filelock -   Lock 134345648395360 acquired on data/k-shot/spoilers/stuffed_binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","08/29/2023 07:19:55 - INFO - src.dataset -   Creating features from dataset file at data/k-shot/spoilers/stuffed_binary_v1.0/3615-21\n","08/29/2023 07:19:58 - INFO - src.dataset -   Saving features into cached file data/k-shot/spoilers/stuffed_binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers [took 0.284 s]\n","08/29/2023 07:19:58 - INFO - filelock -   Lock 134345648395360 released on data/k-shot/spoilers/stuffed_binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","08/29/2023 07:20:00 - INFO - src.dataset -   Label 0 to word ƒ†relevant (4249)\n","08/29/2023 07:20:00 - INFO - src.dataset -   Label 1 to word ƒ†irrelevant (21821)\n","08/29/2023 07:20:00 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","08/29/2023 07:20:00 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/stuffed_binary_v1.0/3615-21\n","08/29/2023 07:20:00 - INFO - filelock -   Lock 134345648395360 acquired on data/k-shot/spoilers/stuffed_binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","08/29/2023 07:20:00 - INFO - src.dataset -   Creating features from dataset file at data/k-shot/spoilers/stuffed_binary_v1.0/3615-21\n","08/29/2023 07:20:03 - INFO - src.dataset -   Saving features into cached file data/k-shot/spoilers/stuffed_binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers [took 0.388 s]\n","08/29/2023 07:20:03 - INFO - filelock -   Lock 134345648395360 released on data/k-shot/spoilers/stuffed_binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","Token indices sequence length is longer than the specified maximum sequence length for this model (866 > 512). Running this sequence through the model will result in indexing errors\n","08/29/2023 07:20:03 - INFO - src.dataset -   *** Example ***\n","08/29/2023 07:20:03 - INFO - src.dataset -   guid: dev-0\n","08/29/2023 07:20:03 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 10975, 45331, 42645, 15565, 642, 38, 122, 33, 7036, 14, 51, 351, 17, 27, 90, 28, 37698, 24, 7, 18220, 42, 191, 4, 8133, 44412, 268, 4, 3421, 7, 8402, 9, 5, 2664, 636, 12266, 8, 471, 13, 127, 3627, 7586, 646, 47977, 42645, 6280, 281, 5870, 18013, 6629, 88, 840, 21777, 108, 179, 18, 9067, 1080, 7, 3190, 10, 10062, 3895, 3344, 19, 10, 5044, 2001, 10062, 3895, 3344, 4, 229, 11993, 2636, 4542, 28555, 16819, 5, 3895, 6, 28555, 16819, 6696, 5, 3895, 8, 5712, 15028, 4, 832, 471, 5712, 15, 24803, 5781, 281, 4793, 8, 6280, 281, 5870, 606, 878, 11, 5, 9067, 1080, 8499, 71, 37, 23154, 229, 11993, 2636, 18066, 4, 229, 11993, 2636, 3026, 6280, 281, 5870, 7, 489, 961, 66, 9, 5, 9067, 1080, 8499, 4, 6280, 281, 5870, 817, 70, 9, 5, 82, 54, 58, 59, 7, 2914, 5, 9067, 1080, 929, 1004, 409, 4, 264, 13328, 5, 9067, 1080, 8499, 456, 7, 1649, 62, 15, 229, 11993, 2636, 8, 28555, 16819, 4, 264, 3681, 14, 51, 214, 202, 11, 5, 276, 737, 51, 58, 137, 4, 255, 10936, 8810, 6990, 28555, 16819, 114, 37, 115, 492, 123, 55, 657, 2949, 4, 28555, 16819, 3681, 14, 255, 10936, 8810, 34, 1224, 88, 10, 430, 621, 4, 832, 2549, 34, 1714, 2156, 700, 34, 41, 5567, 4506, 8, 39, 6718, 16, 45, 5, 276, 25, 137, 4, 18222, 1023, 5602, 4265, 255, 10936, 8810, 16, 5921, 139, 1295, 6, 8, 423, 1516, 7758, 8, 1072, 7, 3318, 255, 10936, 8810, 19, 11471, 2225, 4, 229, 11786, 605, 18879, 5792, 11, 5, 9067, 1080, 8499, 8, 6990, 5, 130, 2786, 114, 229, 11993, 2636, 16, 11, 5, 929, 4, 229, 11786, 605, 18879, 8, 255, 10936, 8810, 32, 314, 1937, 11, 5, 9067, 1080, 8499, 77, 18222, 1023, 5602, 8, 28555, 16819, 1656, 66, 9, 5, 929, 4, 732, 4151, 8, 229, 11993, 2636, 192, 18222, 1023, 5602, 8, 28555, 16819, 36596, 15, 229, 11786, 605, 18879, 8, 255, 10936, 8810, 4, 252, 218, 75, 206, 51, 33, 1348, 42058, 648, 6, 53, 51, 236, 7, 192, 114, 51, 33, 4, 732, 4151, 8, 229, 11993, 2636, 1962, 11, 6, 10010, 192, 14, 229, 11786, 605, 18879, 8, 255, 10936, 8810, 32, 1826, 1420, 6, 20479, 8, 255, 10936, 8810, 16, 20479, 229, 11786, 605, 18879, 18, 5397, 4, 572, 1576, 28555, 16819, 224, 14, 1826, 1420, 16, 15, 5, 78, 1248, 6, 20479, 16, 15, 5, 371, 1248, 6, 8, 2099, 16, 15, 5, 1998, 1248, 6, 229, 11993, 2636, 33947, 4, 229, 11786, 605, 18879, 172, 7441, 14, 79, 8, 255, 10936, 8810, 58, 95, 35204, 19, 106, 4, 732, 4151, 19101, 28555, 16819, 6, 229, 11993, 2636, 6, 8, 18222, 1023, 5602, 7, 310, 10, 177, 79, 156, 11, 5, 9513, 8766, 18003, 2009, 4, 28555, 16819, 8, 229, 11993, 2636, 2991, 5, 10021, 6, 61, 817, 732, 4151, 5074, 4, 18222, 1023, 5602, 924, 773, 6, 98, 229, 11993, 2636, 8, 28555, 4, 85, 21, 50264, 4, 2, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[506], label_word_list=None)\n","08/29/2023 07:20:03 - INFO - src.dataset -   text: <s>[Comment]: Welp I now have confirmation that they won‚Äôt be uploading it to Hulu this season. Motherfuckers. Time to dust of the tricorne and head for my ship.. [Summary]: Hayasaka sneaks into Shuchi'in's Student Council to replace a instant coffee mix with a decaf instant coffee mix. Kaguya serves Miyuki the coffee, Miyuki drinks the coffee and falls asleep. His head falls on Kaguyas shoulder and Hayasaka comes running in the Student Council Room after he hears Kaguya shout. Kaguya tells Hayasaka to keep everyone out of the Student Council Room. Hayasaka makes all of the people who were about to enter the Student Council room turn away. She enters the Student Council Room again to check up on Kaguya and Miyuki. She sees that they're still in the same position they were before. Tsubasa asks Miyuki if he could give him more love advice. Miyuki sees that Tsubasa has turned into a different person. His hair has changed,he has an earring and his personality is not the same as before. Ishigami thinks Tsubasa is gloating, and later gets mad and wants to tie Tsubasa with toilet paper. Kashiwagi walks in the Student Council Room and asks the three boys if Kaguya is in the room. Kashiwagi and Tsubasa are left alone in the Student Council Room when Ishigami and Miyuki walk out of the room. Chika and Kaguya see Ishigami and Miyuki sneaking on Kashiwagi and Tsubasa. They don't think they have reached Nirvana yet, but they want to see if they have. Chika and Kaguya join in,they see that Kashiwagi and Tsubasa are holding hands, kissing and Tsubasa is kissing Kashiwagi's neck. After hearing Miyuki say that holding hands is on the first date, kissing is on the third date, and sex is on the fifth date, Kaguya collapses. Kashiwagi then reveals that she and Tsubasa were just messing with them. Chika invites Miyuki, Kaguya, and Ishigami to play a game she made in the Tabletop Gaming Club. Miyuki and Kaguya decline the invitation, which makes Chika sad. Ishigami shows interest, so Kaguya and Miy. It was<mask>.</s><pad><pad><pad>\n","08/29/2023 07:20:20 - INFO - src.dataset -   Label 0 to word ƒ†relevant (4249)\n","08/29/2023 07:20:20 - INFO - src.dataset -   Label 1 to word ƒ†irrelevant (21821)\n","08/29/2023 07:20:20 - INFO - src.dataset -   Total num_sample for mode test: 1\n","08/29/2023 07:20:20 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/stuffed_binary_v1.0/3615-21\n","08/29/2023 07:20:20 - INFO - filelock -   Lock 134345648395552 acquired on data/k-shot/spoilers/stuffed_binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers.lock\n","08/29/2023 07:20:20 - INFO - src.dataset -   Creating features from dataset file at data/k-shot/spoilers/stuffed_binary_v1.0/3615-21\n","08/29/2023 07:20:22 - INFO - src.dataset -   Saving features into cached file data/k-shot/spoilers/stuffed_binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers [took 0.320 s]\n","08/29/2023 07:20:22 - INFO - filelock -   Lock 134345648395552 released on data/k-shot/spoilers/stuffed_binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers.lock\n","08/29/2023 07:20:22 - INFO - src.dataset -   *** Example ***\n","08/29/2023 07:20:22 - INFO - src.dataset -   guid: test-0\n","08/29/2023 07:20:22 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 10975, 45331, 42645, 3347, 3996, 24, 4, 152, 3238, 21, 169, 357, 4, 38, 2638, 9925, 1253, 8725, 3860, 7, 2807, 227, 5, 232, 8, 1003, 73, 21290, 284, 4, 1534, 14, 12355, 8810, 37, 21, 35510, 145, 2997, 7, 116, 38, 794, 10, 14061, 15, 69, 15401, 4, 289, 10987, 21, 24282, 7735, 4, 264, 1449, 79, 965, 75, 49, 10295, 8, 172, 161, 79, 29, 202, 5, 8501, 9, 5, 2658, 19006, 4, 264, 332, 14, 70, 5, 1462, 3878, 74, 3999, 2854, 19, 18038, 53, 99, 59, 5, 32440, 6988, 661, 4, 1876, 9, 106, 58, 683, 12260, 2580, 50, 11, 5, 92, 2658, 19006, 8, 51, 1507, 19, 5, 910, 16250, 596, 74, 70, 5, 1462, 3878, 54, 129, 1467, 14, 9187, 3033, 624, 5, 6347, 1032, 13, 5, 1079, 9, 9187, 61, 1654, 106, 11, 42, 1068, 6, 17161, 102, 23737, 642, 14963, 53, 99, 38, 5324, 4, 497, 155, 35, 844, 1236, 12001, 161, 51, 1381, 13, 107, 546, 13, 5274, 4, 10426, 35, 5, 204, 12, 180, 498, 330, 1588, 4, 497, 262, 35, 4390, 289, 10987, 161, 51, 3033, 11, 5, 751, 232, 4, 152, 5072, 3137, 506, 21871, 14, 51, 1381, 15166, 4, 20, 129, 1219, 1368, 10987, 74, 33, 10, 1528, 1219, 7, 213, 74, 28, 7, 2097, 5, 910, 16250, 4, 3771, 31360, 4795, 16, 98, 3953, 24, 95, 14236, 162, 9, 821, 28261, 4795, 4, 38, 21, 2818, 821, 10810, 74, 9, 26, 4, 6553, 47, 192, 14, 1369, 116, 1586, 4691, 16, 25672, 7, 206, 381, 2558, 16, 164, 7, 912, 23, 42, 477, 4, 381, 2558, 16, 533, 7, 8439, 5, 1445, 232, 4, 21198, 2420, 11, 39996, 1026, 16, 10, 1856, 7, 70, 9, 106, 4356, 686, 4, 5363, 10720, 1223, 5101, 13, 99, 37, 222, 21, 3127, 17758, 4, 1223, 5101, 16, 35304, 744, 9287, 23, 70, 498, 38, 5170, 77, 37, 581, 1597, 4, 3791, 5, 3980, 58, 2343, 59, 501, 14200, 498, 1640, 118, 11590, 43, 20, 169, 26968, 4242, 17200, 4102, 5, 6529, 21, 5, 275, 169, 939, 115, 9, 5207, 7, 28, 2781, 4, 290, 73, 698, 13, 162, 4832, 322, 646, 47977, 42645, 20, 363, 137, 6, 150, 144, 82, 32, 15028, 6, 289, 10987, 25391, 105, 44388, 5363, 11596, 12265, 37566, 18, 8235, 9, 10, 16219, 301, 11, 5, 6291, 7, 486, 123, 66, 13, 10, 529, 4, 2285, 39, 3834, 571, 1879, 1033, 6, 37, 1411, 66, 7, 972, 289, 10987, 8, 12355, 8810, 22277, 7043, 282, 6, 54, 3291, 106, 62, 7, 2078, 15, 20050, 22277, 7043, 282, 18, 1881, 8, 49, 92, 6529, 19, 103, 7031, 1073, 43060, 31, 1127, 607, 4, 289, 10987, 708, 15, 8197, 381, 2558, 15630, 2403, 18, 708, 13, 18038, 6, 53, 78, 51, 40, 240, 55, 4181, 4, 12355, 8810, 16, 1320, 11, 6, 53, 5363, 16, 2273, 59, 5, 9123, 6, 187, 190, 114, 51, 912, 123, 6, 5, 232, 40, 202, 4157, 18864, 2071, 8, 381, 2558, 40, 129, 4, 85, 21, 50264, 4, 2, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[506], label_word_list=None)\n","08/29/2023 07:20:22 - INFO - src.dataset -   text: <s>[Comment]: Finally watched it. This episode was way better. I loved Jeans inner struggle to choose between the world and himself/future family. Is that Mikasa he was imagining being married to? I saw a scar on her cheek. Hange was kinda weird. She claims she isn't their superior and then says shes still the commander of the survey corps. She points that all the dead soldiers wouldnt agree with genocide but what about the yeagerist. Many of them were once cadets or in the new survey corps and they agreed with the rumbling why would all the dead soldiers who only knew that humanity lived within the walls fight for the rest of humanity which forced them in this situation, Kinda nitpicky but what I noticed. At 3:20 jean says they tried for years looking for answers. Years: the 4-year timeskip. At 7:57 Hange says they lived in the outside world. This basically comfirms that they tried diplomacy. The only reason hange would have a true reason to go would be to prevent the rumbling. Magaths argument is so weak it just reminds me of gabis argument. I was hoping gabi would of said. Did you see that happen? Armin is naive to think Eren is going to stop at this point. Eren is likely to destroy the entire world. Pieck in titan form is a threat to all of them im sure. Jean kicking Reiner for what he did was truly satisfying. Reiner is spitting death flags at all times I wonder when he'll die. Though the trees were shown about 140000 times(i counted) The way Yelena tore apart the alliance was the best way i could of hoped to be delivered. 8/10 for me :). [Summary]: The night before, while most people are asleep, Hange Zo√´ interrupts Jean Kirschtein's fantasy of a pleasant life in the interior to call him out for a meeting. Despite his misgivings, he goes out to meet Hange and Mikasa Ackermann, who brings them up to speed on Levi Ackermann's condition and their new alliance with some stragglers from Marley. Hange plans on stopping Eren Jaeger's plans for genocide, but first they will need more allies. Mikasa is immediately in, but Jean is concerned about the aftermath, since even if they stop him, the world will still hate Eldians and Eren will only. It was<mask>.</s><pad><pad><pad>\n","08/29/2023 07:20:29 - INFO - filelock -   Lock 134345233524624 acquired on /root/.cache/torch/transformers/2339ac1858323405dffff5156947669fed6f63a0c34cfab35bda4f78791893d2.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536.lock\n","Downloading: 100% 1.43G/1.43G [00:13<00:00, 108MB/s]\n","08/29/2023 07:20:42 - INFO - filelock -   Lock 134345233524624 released on /root/.cache/torch/transformers/2339ac1858323405dffff5156947669fed6f63a0c34cfab35bda4f78791893d2.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536.lock\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","08/29/2023 07:21:04 - INFO - src.trainer -   ***** Running training *****\n","08/29/2023 07:21:04 - INFO - src.trainer -     Num examples = 7229\n","08/29/2023 07:21:04 - INFO - src.trainer -     Num Epochs = 4\n","08/29/2023 07:21:04 - INFO - src.trainer -     Instantaneous batch size per device = 2\n","08/29/2023 07:21:04 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 32\n","08/29/2023 07:21:04 - INFO - src.trainer -     Gradient Accumulation steps = 16\n","08/29/2023 07:21:04 - INFO - src.trainer -     Total optimization steps = 900\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:   0% 0/4 [00:00<?, ?it/s]08/29/2023 07:22:27 - INFO - src.trainer -   Train loss: 0.7858632694591176\n","08/29/2023 07:23:47 - INFO - src.trainer -   Train loss: 0.7331790057095614\n","08/29/2023 07:25:07 - INFO - src.trainer -   Train loss: 0.5683494914661754\n","08/29/2023 07:26:27 - INFO - src.trainer -   Train loss: 0.4821499911221591\n","08/29/2023 07:27:47 - INFO - src.trainer -   Train loss: 0.5000872178511186\n","08/29/2023 07:29:07 - INFO - src.trainer -   Train loss: 0.45122632113370026\n","08/29/2023 07:30:27 - INFO - src.trainer -   Train loss: 0.4091675498268821\n","08/29/2023 07:31:47 - INFO - src.trainer -   Train loss: 0.3937194130637429\n","08/29/2023 07:33:07 - INFO - src.trainer -   Train loss: 0.4257822903719815\n","08/29/2023 07:34:27 - INFO - src.trainer -   Train loss: 0.38256315751509234\n","\n","  0% 0/169 [00:00<?, ?it/s]\u001b[A\n","  1% 2/169 [00:00<00:42,  3.92it/s]\u001b[A\n","  2% 3/169 [00:01<00:55,  3.01it/s]\u001b[A\n","  2% 4/169 [00:01<01:03,  2.59it/s]\u001b[A\n","  3% 5/169 [00:02<01:09,  2.36it/s]\u001b[A\n","  4% 6/169 [00:02<01:13,  2.23it/s]\u001b[A\n","  4% 7/169 [00:03<01:15,  2.14it/s]\u001b[A\n","  5% 8/169 [00:03<01:17,  2.08it/s]\u001b[A\n","  5% 9/169 [00:04<01:18,  2.04it/s]\u001b[A\n","  6% 10/169 [00:04<01:19,  2.01it/s]\u001b[A\n","  7% 11/169 [00:05<01:19,  2.00it/s]\u001b[A\n","  7% 12/169 [00:05<01:19,  1.98it/s]\u001b[A\n","  8% 13/169 [00:06<01:19,  1.97it/s]\u001b[A\n","  8% 14/169 [00:06<01:18,  1.97it/s]\u001b[A\n","  9% 15/169 [00:07<01:18,  1.96it/s]\u001b[A\n","  9% 16/169 [00:07<01:18,  1.96it/s]\u001b[A\n"," 10% 17/169 [00:08<01:17,  1.96it/s]\u001b[A\n"," 11% 18/169 [00:08<01:17,  1.96it/s]\u001b[A\n"," 11% 19/169 [00:09<01:16,  1.96it/s]\u001b[A\n"," 12% 20/169 [00:09<01:16,  1.96it/s]\u001b[A\n"," 12% 21/169 [00:10<01:15,  1.96it/s]\u001b[A\n"," 13% 22/169 [00:10<01:15,  1.96it/s]\u001b[A\n"," 14% 23/169 [00:11<01:14,  1.96it/s]\u001b[A\n"," 14% 24/169 [00:11<01:14,  1.96it/s]\u001b[A\n"," 15% 25/169 [00:12<01:13,  1.96it/s]\u001b[A\n"," 15% 26/169 [00:12<01:13,  1.96it/s]\u001b[A\n"," 16% 27/169 [00:13<01:12,  1.96it/s]\u001b[A\n"," 17% 28/169 [00:13<01:12,  1.96it/s]\u001b[A\n"," 17% 29/169 [00:14<01:11,  1.96it/s]\u001b[A\n"," 18% 30/169 [00:14<01:11,  1.96it/s]\u001b[A\n"," 18% 31/169 [00:15<01:10,  1.96it/s]\u001b[A\n"," 19% 32/169 [00:15<01:09,  1.96it/s]\u001b[A\n"," 20% 33/169 [00:16<01:09,  1.96it/s]\u001b[A\n"," 20% 34/169 [00:16<01:08,  1.96it/s]\u001b[A\n"," 21% 35/169 [00:17<01:08,  1.96it/s]\u001b[A\n"," 21% 36/169 [00:17<01:07,  1.96it/s]\u001b[A\n"," 22% 37/169 [00:18<01:07,  1.96it/s]\u001b[A\n"," 22% 38/169 [00:18<01:07,  1.96it/s]\u001b[A\n"," 23% 39/169 [00:19<01:06,  1.95it/s]\u001b[A\n"," 24% 40/169 [00:19<01:05,  1.96it/s]\u001b[A\n"," 24% 41/169 [00:20<01:05,  1.95it/s]\u001b[A\n"," 25% 42/169 [00:20<01:04,  1.95it/s]\u001b[A\n"," 25% 43/169 [00:21<01:04,  1.96it/s]\u001b[A\n"," 26% 44/169 [00:21<01:03,  1.96it/s]\u001b[A\n"," 27% 45/169 [00:22<01:03,  1.96it/s]\u001b[A\n"," 27% 46/169 [00:23<01:02,  1.96it/s]\u001b[A\n"," 28% 47/169 [00:23<01:02,  1.96it/s]\u001b[A\n"," 28% 48/169 [00:24<01:01,  1.96it/s]\u001b[A\n"," 29% 49/169 [00:24<01:01,  1.96it/s]\u001b[A\n"," 30% 50/169 [00:25<01:00,  1.96it/s]\u001b[A\n"," 30% 51/169 [00:25<01:00,  1.96it/s]\u001b[A\n"," 31% 52/169 [00:26<00:59,  1.96it/s]\u001b[A\n"," 31% 53/169 [00:26<00:59,  1.96it/s]\u001b[A\n"," 32% 54/169 [00:27<00:58,  1.96it/s]\u001b[A\n"," 33% 55/169 [00:27<00:58,  1.96it/s]\u001b[A\n"," 33% 56/169 [00:28<00:57,  1.96it/s]\u001b[A\n"," 34% 57/169 [00:28<00:57,  1.96it/s]\u001b[A\n"," 34% 58/169 [00:29<00:56,  1.96it/s]\u001b[A\n"," 35% 59/169 [00:29<00:56,  1.96it/s]\u001b[A\n"," 36% 60/169 [00:30<00:55,  1.96it/s]\u001b[A\n"," 36% 61/169 [00:30<00:55,  1.96it/s]\u001b[A\n"," 37% 62/169 [00:31<00:54,  1.96it/s]\u001b[A\n"," 37% 63/169 [00:31<00:54,  1.96it/s]\u001b[A\n"," 38% 64/169 [00:32<00:53,  1.96it/s]\u001b[A\n"," 38% 65/169 [00:32<00:53,  1.96it/s]\u001b[A\n"," 39% 66/169 [00:33<00:52,  1.96it/s]\u001b[A\n"," 40% 67/169 [00:33<00:52,  1.96it/s]\u001b[A\n"," 40% 68/169 [00:34<00:51,  1.96it/s]\u001b[A\n"," 41% 69/169 [00:34<00:51,  1.96it/s]\u001b[A\n"," 41% 70/169 [00:35<00:50,  1.96it/s]\u001b[A\n"," 42% 71/169 [00:35<00:50,  1.96it/s]\u001b[A\n"," 43% 72/169 [00:36<00:49,  1.96it/s]\u001b[A\n"," 43% 73/169 [00:36<00:49,  1.96it/s]\u001b[A\n"," 44% 74/169 [00:37<00:48,  1.96it/s]\u001b[A\n"," 44% 75/169 [00:37<00:48,  1.95it/s]\u001b[A\n"," 45% 76/169 [00:38<00:47,  1.96it/s]\u001b[A\n"," 46% 77/169 [00:38<00:47,  1.96it/s]\u001b[A\n"," 46% 78/169 [00:39<00:46,  1.96it/s]\u001b[A\n"," 47% 79/169 [00:39<00:46,  1.95it/s]\u001b[A\n"," 47% 80/169 [00:40<00:45,  1.95it/s]\u001b[A\n"," 48% 81/169 [00:40<00:45,  1.95it/s]\u001b[A\n"," 49% 82/169 [00:41<00:44,  1.96it/s]\u001b[A\n"," 49% 83/169 [00:41<00:43,  1.96it/s]\u001b[A\n"," 50% 84/169 [00:42<00:43,  1.96it/s]\u001b[A\n"," 50% 85/169 [00:42<00:42,  1.96it/s]\u001b[A\n"," 51% 86/169 [00:43<00:42,  1.96it/s]\u001b[A\n"," 51% 87/169 [00:43<00:41,  1.96it/s]\u001b[A\n"," 52% 88/169 [00:44<00:41,  1.96it/s]\u001b[A\n"," 53% 89/169 [00:44<00:40,  1.96it/s]\u001b[A\n"," 53% 90/169 [00:45<00:40,  1.96it/s]\u001b[A\n"," 54% 91/169 [00:45<00:39,  1.96it/s]\u001b[A\n"," 54% 92/169 [00:46<00:39,  1.96it/s]\u001b[A\n"," 55% 93/169 [00:47<00:38,  1.96it/s]\u001b[A\n"," 56% 94/169 [00:47<00:38,  1.95it/s]\u001b[A\n"," 56% 95/169 [00:48<00:37,  1.95it/s]\u001b[A\n"," 57% 96/169 [00:48<00:37,  1.96it/s]\u001b[A\n"," 57% 97/169 [00:49<00:36,  1.96it/s]\u001b[A\n"," 58% 98/169 [00:49<00:36,  1.96it/s]\u001b[A\n"," 59% 99/169 [00:50<00:35,  1.96it/s]\u001b[A\n"," 59% 100/169 [00:50<00:35,  1.96it/s]\u001b[A\n"," 60% 101/169 [00:51<00:34,  1.96it/s]\u001b[A\n"," 60% 102/169 [00:51<00:34,  1.96it/s]\u001b[A\n"," 61% 103/169 [00:52<00:33,  1.96it/s]\u001b[A\n"," 62% 104/169 [00:52<00:33,  1.96it/s]\u001b[A\n"," 62% 105/169 [00:53<00:32,  1.96it/s]\u001b[A\n"," 63% 106/169 [00:53<00:32,  1.96it/s]\u001b[A\n"," 63% 107/169 [00:54<00:31,  1.96it/s]\u001b[A\n"," 64% 108/169 [00:54<00:31,  1.96it/s]\u001b[A\n"," 64% 109/169 [00:55<00:30,  1.96it/s]\u001b[A\n"," 65% 110/169 [00:55<00:30,  1.96it/s]\u001b[A\n"," 66% 111/169 [00:56<00:29,  1.96it/s]\u001b[A\n"," 66% 112/169 [00:56<00:29,  1.96it/s]\u001b[A\n"," 67% 113/169 [00:57<00:28,  1.96it/s]\u001b[A\n"," 67% 114/169 [00:57<00:28,  1.96it/s]\u001b[A\n"," 68% 115/169 [00:58<00:27,  1.96it/s]\u001b[A\n"," 69% 116/169 [00:58<00:27,  1.96it/s]\u001b[A\n"," 69% 117/169 [00:59<00:26,  1.96it/s]\u001b[A\n"," 70% 118/169 [00:59<00:26,  1.96it/s]\u001b[A\n"," 70% 119/169 [01:00<00:25,  1.96it/s]\u001b[A\n"," 71% 120/169 [01:00<00:25,  1.96it/s]\u001b[A\n"," 72% 121/169 [01:01<00:24,  1.96it/s]\u001b[A\n"," 72% 122/169 [01:01<00:23,  1.96it/s]\u001b[A\n"," 73% 123/169 [01:02<00:23,  1.96it/s]\u001b[A\n"," 73% 124/169 [01:02<00:22,  1.96it/s]\u001b[A\n"," 74% 125/169 [01:03<00:22,  1.96it/s]\u001b[A\n"," 75% 126/169 [01:03<00:21,  1.96it/s]\u001b[A\n"," 75% 127/169 [01:04<00:21,  1.96it/s]\u001b[A\n"," 76% 128/169 [01:04<00:20,  1.96it/s]\u001b[A\n"," 76% 129/169 [01:05<00:20,  1.96it/s]\u001b[A\n"," 77% 130/169 [01:05<00:19,  1.96it/s]\u001b[A\n"," 78% 131/169 [01:06<00:19,  1.96it/s]\u001b[A\n"," 78% 132/169 [01:06<00:18,  1.96it/s]\u001b[A\n"," 79% 133/169 [01:07<00:18,  1.96it/s]\u001b[A\n"," 79% 134/169 [01:07<00:17,  1.96it/s]\u001b[A\n"," 80% 135/169 [01:08<00:17,  1.96it/s]\u001b[A\n"," 80% 136/169 [01:08<00:16,  1.96it/s]\u001b[A\n"," 81% 137/169 [01:09<00:16,  1.96it/s]\u001b[A\n"," 82% 138/169 [01:10<00:15,  1.96it/s]\u001b[A\n"," 82% 139/169 [01:10<00:15,  1.96it/s]\u001b[A\n"," 83% 140/169 [01:11<00:14,  1.96it/s]\u001b[A\n"," 83% 141/169 [01:11<00:14,  1.96it/s]\u001b[A\n"," 84% 142/169 [01:12<00:13,  1.96it/s]\u001b[A\n"," 85% 143/169 [01:12<00:13,  1.96it/s]\u001b[A\n"," 85% 144/169 [01:13<00:12,  1.96it/s]\u001b[A\n"," 86% 145/169 [01:13<00:12,  1.96it/s]\u001b[A\n"," 86% 146/169 [01:14<00:11,  1.96it/s]\u001b[A\n"," 87% 147/169 [01:14<00:11,  1.96it/s]\u001b[A\n"," 88% 148/169 [01:15<00:10,  1.96it/s]\u001b[A\n"," 88% 149/169 [01:15<00:10,  1.96it/s]\u001b[A\n"," 89% 150/169 [01:16<00:09,  1.96it/s]\u001b[A\n"," 89% 151/169 [01:16<00:09,  1.96it/s]\u001b[A\n"," 90% 152/169 [01:17<00:08,  1.96it/s]\u001b[A\n"," 91% 153/169 [01:17<00:08,  1.96it/s]\u001b[A\n"," 91% 154/169 [01:18<00:07,  1.96it/s]\u001b[A\n"," 92% 155/169 [01:18<00:07,  1.96it/s]\u001b[A\n"," 92% 156/169 [01:19<00:06,  1.96it/s]\u001b[A\n"," 93% 157/169 [01:19<00:06,  1.96it/s]\u001b[A\n"," 93% 158/169 [01:20<00:05,  1.96it/s]\u001b[A\n"," 94% 159/169 [01:20<00:05,  1.96it/s]\u001b[A\n"," 95% 160/169 [01:21<00:04,  1.96it/s]\u001b[A\n"," 95% 161/169 [01:21<00:04,  1.96it/s]\u001b[A\n"," 96% 162/169 [01:22<00:03,  1.96it/s]\u001b[A\n"," 96% 163/169 [01:22<00:03,  1.96it/s]\u001b[A\n"," 97% 164/169 [01:23<00:02,  1.96it/s]\u001b[A\n"," 98% 165/169 [01:23<00:02,  1.96it/s]\u001b[A\n"," 98% 166/169 [01:24<00:01,  1.96it/s]\u001b[A\n"," 99% 167/169 [01:24<00:01,  1.96it/s]\u001b[A\n"," 99% 168/169 [01:25<00:00,  1.95it/s]\u001b[A\n","100% 169/169 [01:25<00:00,  2.22it/s]\u001b[A08/29/2023 07:36:12 - INFO - src.trainer -   Best dev result: 0.9123060703277588\n","Epoch:  25% 1/4 [15:16<45:50, 916.99s/it]08/29/2023 07:37:23 - INFO - src.trainer -   Train loss: 0.3503653786399148\n","08/29/2023 07:38:42 - INFO - src.trainer -   Train loss: 0.3816888982599432\n","08/29/2023 07:40:02 - INFO - src.trainer -   Train loss: 0.33757157759232953\n","08/29/2023 07:41:21 - INFO - src.trainer -   Train loss: 0.3520452325994318\n","08/29/2023 07:42:41 - INFO - src.trainer -   Train loss: 0.2939085526899858\n","08/29/2023 07:44:01 - INFO - src.trainer -   Train loss: 0.3400358720259233\n","08/29/2023 07:45:20 - INFO - src.trainer -   Train loss: 0.3095876520330256\n","08/29/2023 07:46:40 - INFO - src.trainer -   Train loss: 0.2943663163618608\n","08/29/2023 07:47:59 - INFO - src.trainer -   Train loss: 0.3595393787730824\n","08/29/2023 07:49:19 - INFO - src.trainer -   Train loss: 0.32119334827769885\n","\n","170it [15:09, 247.52s/it]            \u001b[A\n","171it [15:10, 173.42s/it]\u001b[A\n","172it [15:10, 121.55s/it]\u001b[A\n","173it [15:11, 85.24s/it] \u001b[A\n","174it [15:11, 59.82s/it]\u001b[A\n","175it [15:12, 42.03s/it]\u001b[A\n","176it [15:12, 29.57s/it]\u001b[A\n","177it [15:13, 20.85s/it]\u001b[A\n","178it [15:13, 14.75s/it]\u001b[A\n","179it [15:14, 10.48s/it]\u001b[A\n","180it [15:14,  7.49s/it]\u001b[A\n","181it [15:15,  5.40s/it]\u001b[A\n","182it [15:15,  3.93s/it]\u001b[A\n","183it [15:16,  2.90s/it]\u001b[A\n","184it [15:16,  2.19s/it]\u001b[A\n","185it [15:17,  1.68s/it]\u001b[A\n","186it [15:17,  1.33s/it]\u001b[A\n","187it [15:18,  1.09s/it]\u001b[A\n","188it [15:18,  1.09it/s]\u001b[A\n","189it [15:19,  1.26it/s]\u001b[A\n","190it [15:19,  1.41it/s]\u001b[A\n","191it [15:20,  1.54it/s]\u001b[A\n","192it [15:20,  1.64it/s]\u001b[A\n","193it [15:21,  1.73it/s]\u001b[A\n","194it [15:21,  1.79it/s]\u001b[A\n","195it [15:22,  1.84it/s]\u001b[A\n","196it [15:22,  1.87it/s]\u001b[A\n","197it [15:23,  1.90it/s]\u001b[A\n","198it [15:23,  1.91it/s]\u001b[A\n","199it [15:24,  1.93it/s]\u001b[A\n","200it [15:24,  1.94it/s]\u001b[A\n","201it [15:25,  1.94it/s]\u001b[A\n","202it [15:26,  1.95it/s]\u001b[A\n","203it [15:26,  1.95it/s]\u001b[A\n","204it [15:27,  1.95it/s]\u001b[A\n","205it [15:27,  1.95it/s]\u001b[A\n","206it [15:28,  1.95it/s]\u001b[A\n","207it [15:28,  1.96it/s]\u001b[A\n","208it [15:29,  1.96it/s]\u001b[A\n","209it [15:29,  1.95it/s]\u001b[A\n","210it [15:30,  1.95it/s]\u001b[A\n","211it [15:30,  1.95it/s]\u001b[A\n","212it [15:31,  1.95it/s]\u001b[A\n","213it [15:31,  1.95it/s]\u001b[A\n","214it [15:32,  1.95it/s]\u001b[A\n","215it [15:32,  1.95it/s]\u001b[A\n","216it [15:33,  1.95it/s]\u001b[A\n","217it [15:33,  1.95it/s]\u001b[A\n","218it [15:34,  1.95it/s]\u001b[A\n","219it [15:34,  1.96it/s]\u001b[A\n","220it [15:35,  1.96it/s]\u001b[A\n","221it [15:35,  1.96it/s]\u001b[A\n","222it [15:36,  1.96it/s]\u001b[A\n","223it [15:36,  1.96it/s]\u001b[A\n","224it [15:37,  1.96it/s]\u001b[A\n","225it [15:37,  1.95it/s]\u001b[A\n","226it [15:38,  1.96it/s]\u001b[A\n","227it [15:38,  1.96it/s]\u001b[A\n","228it [15:39,  1.96it/s]\u001b[A\n","229it [15:39,  1.96it/s]\u001b[A\n","230it [15:40,  1.95it/s]\u001b[A\n","231it [15:40,  1.95it/s]\u001b[A\n","232it [15:41,  1.95it/s]\u001b[A\n","233it [15:41,  1.95it/s]\u001b[A\n","234it [15:42,  1.96it/s]\u001b[A\n","235it [15:42,  1.95it/s]\u001b[A\n","236it [15:43,  1.96it/s]\u001b[A\n","237it [15:43,  1.96it/s]\u001b[A\n","238it [15:44,  1.96it/s]\u001b[A\n","239it [15:44,  1.96it/s]\u001b[A\n","240it [15:45,  1.96it/s]\u001b[A\n","241it [15:45,  1.96it/s]\u001b[A\n","242it [15:46,  1.96it/s]\u001b[A\n","243it [15:46,  1.96it/s]\u001b[A\n","244it [15:47,  1.96it/s]\u001b[A\n","245it [15:48,  1.96it/s]\u001b[A\n","246it [15:48,  1.96it/s]\u001b[A\n","247it [15:49,  1.96it/s]\u001b[A\n","248it [15:49,  1.96it/s]\u001b[A\n","249it [15:50,  1.96it/s]\u001b[A\n","250it [15:50,  1.96it/s]\u001b[A\n","251it [15:51,  1.95it/s]\u001b[A\n","252it [15:51,  1.95it/s]\u001b[A\n","253it [15:52,  1.95it/s]\u001b[A\n","254it [15:52,  1.95it/s]\u001b[A\n","255it [15:53,  1.95it/s]\u001b[A\n","256it [15:53,  1.96it/s]\u001b[A\n","257it [15:54,  1.96it/s]\u001b[A\n","258it [15:54,  1.96it/s]\u001b[A\n","259it [15:55,  1.96it/s]\u001b[A\n","260it [15:55,  1.96it/s]\u001b[A\n","261it [15:56,  1.96it/s]\u001b[A\n","262it [15:56,  1.96it/s]\u001b[A\n","263it [15:57,  1.96it/s]\u001b[A\n","264it [15:57,  1.95it/s]\u001b[A\n","265it [15:58,  1.96it/s]\u001b[A\n","266it [15:58,  1.96it/s]\u001b[A\n","267it [15:59,  1.96it/s]\u001b[A\n","268it [15:59,  1.96it/s]\u001b[A\n","269it [16:00,  1.95it/s]\u001b[A\n","270it [16:00,  1.95it/s]\u001b[A\n","271it [16:01,  1.96it/s]\u001b[A\n","272it [16:01,  1.96it/s]\u001b[A\n","273it [16:02,  1.96it/s]\u001b[A\n","274it [16:02,  1.96it/s]\u001b[A\n","275it [16:03,  1.96it/s]\u001b[A\n","276it [16:03,  1.96it/s]\u001b[A\n","277it [16:04,  1.96it/s]\u001b[A\n","278it [16:04,  1.96it/s]\u001b[A\n","279it [16:05,  1.95it/s]\u001b[A\n","280it [16:05,  1.95it/s]\u001b[A\n","281it [16:06,  1.95it/s]\u001b[A\n","282it [16:06,  1.95it/s]\u001b[A\n","283it [16:07,  1.96it/s]\u001b[A\n","284it [16:07,  1.96it/s]\u001b[A\n","285it [16:08,  1.96it/s]\u001b[A\n","286it [16:08,  1.96it/s]\u001b[A\n","287it [16:09,  1.96it/s]\u001b[A\n","288it [16:09,  1.96it/s]\u001b[A\n","289it [16:10,  1.96it/s]\u001b[A\n","290it [16:11,  1.96it/s]\u001b[A\n","291it [16:11,  1.96it/s]\u001b[A\n","292it [16:12,  1.96it/s]\u001b[A\n","293it [16:12,  1.96it/s]\u001b[A\n","294it [16:13,  1.96it/s]\u001b[A\n","295it [16:13,  1.95it/s]\u001b[A\n","296it [16:14,  1.96it/s]\u001b[A\n","297it [16:14,  1.95it/s]\u001b[A\n","298it [16:15,  1.96it/s]\u001b[A\n","299it [16:15,  1.96it/s]\u001b[A\n","300it [16:16,  1.96it/s]\u001b[A\n","301it [16:16,  1.96it/s]\u001b[A\n","302it [16:17,  1.96it/s]\u001b[A\n","303it [16:17,  1.96it/s]\u001b[A\n","304it [16:18,  1.96it/s]\u001b[A\n","305it [16:18,  1.96it/s]\u001b[A\n","306it [16:19,  1.96it/s]\u001b[A\n","307it [16:19,  1.96it/s]\u001b[A\n","308it [16:20,  1.96it/s]\u001b[A\n","309it [16:20,  1.96it/s]\u001b[A\n","310it [16:21,  1.96it/s]\u001b[A\n","311it [16:21,  1.96it/s]\u001b[A\n","312it [16:22,  1.96it/s]\u001b[A\n","313it [16:22,  1.96it/s]\u001b[A\n","314it [16:23,  1.96it/s]\u001b[A\n","315it [16:23,  1.96it/s]\u001b[A\n","316it [16:24,  1.96it/s]\u001b[A\n","317it [16:24,  1.96it/s]\u001b[A\n","318it [16:25,  1.96it/s]\u001b[A\n","319it [16:25,  1.96it/s]\u001b[A\n","320it [16:26,  1.96it/s]\u001b[A\n","321it [16:26,  1.96it/s]\u001b[A\n","322it [16:27,  1.96it/s]\u001b[A\n","323it [16:27,  1.96it/s]\u001b[A\n","324it [16:28,  1.96it/s]\u001b[A\n","325it [16:28,  1.96it/s]\u001b[A\n","326it [16:29,  1.96it/s]\u001b[A\n","327it [16:29,  1.96it/s]\u001b[A\n","328it [16:30,  1.96it/s]\u001b[A\n","329it [16:30,  1.96it/s]\u001b[A\n","330it [16:31,  1.96it/s]\u001b[A\n","331it [16:31,  1.96it/s]\u001b[A\n","332it [16:32,  1.96it/s]\u001b[A\n","333it [16:32,  1.96it/s]\u001b[A\n","334it [16:33,  1.96it/s]\u001b[A\n","335it [16:34,  1.96it/s]\u001b[A\n","336it [16:34,  1.96it/s]\u001b[A\n","337it [16:35,  1.96it/s]\u001b[A\n","338it [16:35,  2.22it/s]\u001b[A08/29/2023 07:51:21 - INFO - src.trainer -   Best dev result: 0.9158944487571716\n","Epoch:  50% 2/4 [30:26<30:29, 914.74s/it]08/29/2023 07:52:14 - INFO - src.trainer -   Train loss: 0.31057600541548297\n","08/29/2023 07:53:34 - INFO - src.trainer -   Train loss: 0.20794469659978693\n","08/29/2023 07:54:54 - INFO - src.trainer -   Train loss: 0.15058482776988635\n","08/29/2023 07:56:13 - INFO - src.trainer -   Train loss: 0.2534311467950994\n","08/29/2023 07:57:33 - INFO - src.trainer -   Train loss: 0.2112516923384233\n","08/29/2023 07:58:52 - INFO - src.trainer -   Train loss: 0.2152952714399858\n","08/29/2023 08:00:12 - INFO - src.trainer -   Train loss: 0.21046239679509943\n","08/29/2023 08:01:32 - INFO - src.trainer -   Train loss: 0.23582874644886365\n","08/29/2023 08:02:51 - INFO - src.trainer -   Train loss: 0.18710743297230115\n","08/29/2023 08:04:11 - INFO - src.trainer -   Train loss: 0.22021692449396307\n","\n","339it [30:20, 247.80s/it]\u001b[A\n","340it [30:20, 173.61s/it]\u001b[A\n","341it [30:21, 121.68s/it]\u001b[A\n","342it [30:21, 85.33s/it] \u001b[A\n","343it [30:22, 59.88s/it]\u001b[A\n","344it [30:22, 42.07s/it]\u001b[A\n","345it [30:23, 29.60s/it]\u001b[A\n","346it [30:23, 20.88s/it]\u001b[A\n","347it [30:24, 14.77s/it]\u001b[A\n","348it [30:24, 10.49s/it]\u001b[A\n","349it [30:25,  7.50s/it]\u001b[A\n","350it [30:25,  5.40s/it]\u001b[A\n","351it [30:26,  3.93s/it]\u001b[A\n","352it [30:26,  2.91s/it]\u001b[A\n","353it [30:27,  2.19s/it]\u001b[A\n","354it [30:27,  1.69s/it]\u001b[A\n","355it [30:28,  1.33s/it]\u001b[A\n","356it [30:28,  1.09s/it]\u001b[A\n","357it [30:29,  1.09it/s]\u001b[A\n","358it [30:29,  1.26it/s]\u001b[A\n","359it [30:30,  1.41it/s]\u001b[A\n","360it [30:31,  1.54it/s]\u001b[A\n","361it [30:31,  1.64it/s]\u001b[A\n","362it [30:32,  1.73it/s]\u001b[A\n","363it [30:32,  1.79it/s]\u001b[A\n","364it [30:33,  1.84it/s]\u001b[A\n","365it [30:33,  1.87it/s]\u001b[A\n","366it [30:34,  1.89it/s]\u001b[A\n","367it [30:34,  1.91it/s]\u001b[A\n","368it [30:35,  1.92it/s]\u001b[A\n","369it [30:35,  1.93it/s]\u001b[A\n","370it [30:36,  1.94it/s]\u001b[A\n","371it [30:36,  1.94it/s]\u001b[A\n","372it [30:37,  1.95it/s]\u001b[A\n","373it [30:37,  1.95it/s]\u001b[A\n","374it [30:38,  1.95it/s]\u001b[A\n","375it [30:38,  1.95it/s]\u001b[A\n","376it [30:39,  1.95it/s]\u001b[A\n","377it [30:39,  1.95it/s]\u001b[A\n","378it [30:40,  1.95it/s]\u001b[A\n","379it [30:40,  1.95it/s]\u001b[A\n","380it [30:41,  1.96it/s]\u001b[A\n","381it [30:41,  1.96it/s]\u001b[A\n","382it [30:42,  1.96it/s]\u001b[A\n","383it [30:42,  1.96it/s]\u001b[A\n","384it [30:43,  1.96it/s]\u001b[A\n","385it [30:43,  1.96it/s]\u001b[A\n","386it [30:44,  1.95it/s]\u001b[A\n","387it [30:44,  1.95it/s]\u001b[A\n","388it [30:45,  1.95it/s]\u001b[A\n","389it [30:45,  1.95it/s]\u001b[A\n","390it [30:46,  1.95it/s]\u001b[A\n","391it [30:46,  1.95it/s]\u001b[A\n","392it [30:47,  1.96it/s]\u001b[A\n","393it [30:47,  1.96it/s]\u001b[A\n","394it [30:48,  1.96it/s]\u001b[A\n","395it [30:48,  1.96it/s]\u001b[A\n","396it [30:49,  1.96it/s]\u001b[A\n","397it [30:49,  1.96it/s]\u001b[A\n","398it [30:50,  1.96it/s]\u001b[A\n","399it [30:50,  1.96it/s]\u001b[A\n","400it [30:51,  1.96it/s]\u001b[A\n","401it [30:51,  1.96it/s]\u001b[A\n","402it [30:52,  1.96it/s]\u001b[A\n","403it [30:53,  1.96it/s]\u001b[A\n","404it [30:53,  1.96it/s]\u001b[A\n","405it [30:54,  1.96it/s]\u001b[A\n","406it [30:54,  1.96it/s]\u001b[A\n","407it [30:55,  1.96it/s]\u001b[A\n","408it [30:55,  1.96it/s]\u001b[A\n","409it [30:56,  1.96it/s]\u001b[A\n","410it [30:56,  1.96it/s]\u001b[A\n","411it [30:57,  1.96it/s]\u001b[A\n","412it [30:57,  1.96it/s]\u001b[A\n","413it [30:58,  1.96it/s]\u001b[A\n","414it [30:58,  1.96it/s]\u001b[A\n","415it [30:59,  1.96it/s]\u001b[A\n","416it [30:59,  1.96it/s]\u001b[A\n","417it [31:00,  1.95it/s]\u001b[A\n","418it [31:00,  1.95it/s]\u001b[A\n","419it [31:01,  1.95it/s]\u001b[A\n","420it [31:01,  1.95it/s]\u001b[A\n","421it [31:02,  1.95it/s]\u001b[A\n","422it [31:02,  1.95it/s]\u001b[A\n","423it [31:03,  1.95it/s]\u001b[A\n","424it [31:03,  1.96it/s]\u001b[A\n","425it [31:04,  1.96it/s]\u001b[A\n","426it [31:04,  1.96it/s]\u001b[A\n","427it [31:05,  1.96it/s]\u001b[A\n","428it [31:05,  1.96it/s]\u001b[A\n","429it [31:06,  1.96it/s]\u001b[A\n","430it [31:06,  1.96it/s]\u001b[A\n","431it [31:07,  1.96it/s]\u001b[A\n","432it [31:07,  1.96it/s]\u001b[A\n","433it [31:08,  1.96it/s]\u001b[A\n","434it [31:08,  1.96it/s]\u001b[A\n","435it [31:09,  1.95it/s]\u001b[A\n","436it [31:09,  1.96it/s]\u001b[A\n","437it [31:10,  1.96it/s]\u001b[A\n","438it [31:10,  1.96it/s]\u001b[A\n","439it [31:11,  1.96it/s]\u001b[A\n","440it [31:11,  1.96it/s]\u001b[A\n","441it [31:12,  1.96it/s]\u001b[A\n","442it [31:12,  1.96it/s]\u001b[A\n","443it [31:13,  1.96it/s]\u001b[A\n","444it [31:13,  1.96it/s]\u001b[A\n","445it [31:14,  1.96it/s]\u001b[A\n","446it [31:14,  1.96it/s]\u001b[A\n","447it [31:15,  1.96it/s]\u001b[A\n","448it [31:16,  1.96it/s]\u001b[A\n","449it [31:16,  1.96it/s]\u001b[A\n","450it [31:17,  1.96it/s]\u001b[A\n","451it [31:17,  1.96it/s]\u001b[A\n","452it [31:18,  1.96it/s]\u001b[A\n","453it [31:18,  1.96it/s]\u001b[A\n","454it [31:19,  1.96it/s]\u001b[A\n","455it [31:19,  1.96it/s]\u001b[A\n","456it [31:20,  1.96it/s]\u001b[A\n","457it [31:20,  1.95it/s]\u001b[A\n","458it [31:21,  1.95it/s]\u001b[A\n","459it [31:21,  1.95it/s]\u001b[A\n","460it [31:22,  1.95it/s]\u001b[A\n","461it [31:22,  1.96it/s]\u001b[A\n","462it [31:23,  1.96it/s]\u001b[A\n","463it [31:23,  1.96it/s]\u001b[A\n","464it [31:24,  1.96it/s]\u001b[A\n","465it [31:24,  1.96it/s]\u001b[A\n","466it [31:25,  1.96it/s]\u001b[A\n","467it [31:25,  1.96it/s]\u001b[A\n","468it [31:26,  1.96it/s]\u001b[A\n","469it [31:26,  1.96it/s]\u001b[A\n","470it [31:27,  1.96it/s]\u001b[A\n","471it [31:27,  1.96it/s]\u001b[A\n","472it [31:28,  1.96it/s]\u001b[A\n","473it [31:28,  1.96it/s]\u001b[A\n","474it [31:29,  1.96it/s]\u001b[A\n","475it [31:29,  1.96it/s]\u001b[A\n","476it [31:30,  1.96it/s]\u001b[A\n","477it [31:30,  1.96it/s]\u001b[A\n","478it [31:31,  1.96it/s]\u001b[A\n","479it [31:31,  1.96it/s]\u001b[A\n","480it [31:32,  1.95it/s]\u001b[A\n","481it [31:32,  1.95it/s]\u001b[A\n","482it [31:33,  1.95it/s]\u001b[A\n","483it [31:33,  1.95it/s]\u001b[A\n","484it [31:34,  1.95it/s]\u001b[A\n","485it [31:34,  1.95it/s]\u001b[A\n","486it [31:35,  1.95it/s]\u001b[A\n","487it [31:35,  1.96it/s]\u001b[A\n","488it [31:36,  1.96it/s]\u001b[A\n","489it [31:36,  1.96it/s]\u001b[A\n","490it [31:37,  1.96it/s]\u001b[A\n","491it [31:37,  1.96it/s]\u001b[A\n","492it [31:38,  1.96it/s]\u001b[A\n","493it [31:39,  1.96it/s]\u001b[A\n","494it [31:39,  1.96it/s]\u001b[A\n","495it [31:40,  1.96it/s]\u001b[A\n","496it [31:40,  1.96it/s]\u001b[A\n","497it [31:41,  1.96it/s]\u001b[A\n","498it [31:41,  1.96it/s]\u001b[A\n","499it [31:42,  1.96it/s]\u001b[A\n","500it [31:42,  1.96it/s]\u001b[A\n","501it [31:43,  1.96it/s]\u001b[A\n","502it [31:43,  1.96it/s]\u001b[A\n","503it [31:44,  1.96it/s]\u001b[A\n","504it [31:44,  1.96it/s]\u001b[A\n","505it [31:45,  1.96it/s]\u001b[A\n","506it [31:45,  1.96it/s]\u001b[A\n","507it [31:45,  2.22it/s]\u001b[A08/29/2023 08:06:31 - INFO - src.trainer -   Best dev result: 0.9170293807983398\n","Epoch:  75% 3/4 [45:38<15:14, 914.07s/it]08/29/2023 08:07:09 - INFO - src.trainer -   Train loss: 0.18468544699928977\n","08/29/2023 08:08:28 - INFO - src.trainer -   Train loss: 0.14100785688920456\n","08/29/2023 08:09:47 - INFO - src.trainer -   Train loss: 0.11686983975497159\n","08/29/2023 08:11:07 - INFO - src.trainer -   Train loss: 0.1657166914506392\n","08/29/2023 08:12:26 - INFO - src.trainer -   Train loss: 0.09993050315163353\n","08/29/2023 08:13:46 - INFO - src.trainer -   Train loss: 0.11989593505859375\n","08/29/2023 08:15:05 - INFO - src.trainer -   Train loss: 0.11414614590731534\n","08/29/2023 08:16:25 - INFO - src.trainer -   Train loss: 0.10520518909801137\n","08/29/2023 08:17:44 - INFO - src.trainer -   Train loss: 0.09545343572443182\n","08/29/2023 08:19:04 - INFO - src.trainer -   Train loss: 0.1400909423828125\n","\n","508it [45:31, 247.96s/it]\u001b[A\n","509it [45:31, 173.73s/it]\u001b[A\n","510it [45:32, 121.76s/it]\u001b[A\n","511it [45:32, 85.39s/it] \u001b[A\n","512it [45:33, 59.92s/it]\u001b[A\n","513it [45:34, 42.10s/it]\u001b[A\n","514it [45:34, 29.62s/it]\u001b[A\n","515it [45:35, 20.89s/it]\u001b[A\n","516it [45:35, 14.78s/it]\u001b[A\n","517it [45:36, 10.50s/it]\u001b[A\n","518it [45:36,  7.50s/it]\u001b[A\n","519it [45:37,  5.40s/it]\u001b[A\n","520it [45:37,  3.94s/it]\u001b[A\n","521it [45:38,  2.91s/it]\u001b[A\n","522it [45:38,  2.19s/it]\u001b[A\n","523it [45:39,  1.69s/it]\u001b[A\n","524it [45:39,  1.33s/it]\u001b[A\n","525it [45:40,  1.09s/it]\u001b[A\n","526it [45:40,  1.09it/s]\u001b[A\n","527it [45:41,  1.26it/s]\u001b[A\n","528it [45:41,  1.41it/s]\u001b[A\n","529it [45:42,  1.54it/s]\u001b[A\n","530it [45:42,  1.65it/s]\u001b[A\n","531it [45:43,  1.73it/s]\u001b[A\n","532it [45:43,  1.79it/s]\u001b[A\n","533it [45:44,  1.84it/s]\u001b[A\n","534it [45:44,  1.87it/s]\u001b[A\n","535it [45:45,  1.90it/s]\u001b[A\n","536it [45:45,  1.91it/s]\u001b[A\n","537it [45:46,  1.93it/s]\u001b[A\n","538it [45:46,  1.94it/s]\u001b[A\n","539it [45:47,  1.94it/s]\u001b[A\n","540it [45:47,  1.94it/s]\u001b[A\n","541it [45:48,  1.95it/s]\u001b[A\n","542it [45:48,  1.95it/s]\u001b[A\n","543it [45:49,  1.95it/s]\u001b[A\n","544it [45:49,  1.95it/s]\u001b[A\n","545it [45:50,  1.95it/s]\u001b[A\n","546it [45:50,  1.96it/s]\u001b[A\n","547it [45:51,  1.96it/s]\u001b[A\n","548it [45:51,  1.96it/s]\u001b[A\n","549it [45:52,  1.96it/s]\u001b[A\n","550it [45:52,  1.96it/s]\u001b[A\n","551it [45:53,  1.95it/s]\u001b[A\n","552it [45:53,  1.95it/s]\u001b[A\n","553it [45:54,  1.95it/s]\u001b[A\n","554it [45:54,  1.95it/s]\u001b[A\n","555it [45:55,  1.95it/s]\u001b[A\n","556it [45:55,  1.95it/s]\u001b[A\n","557it [45:56,  1.95it/s]\u001b[A\n","558it [45:57,  1.95it/s]\u001b[A\n","559it [45:57,  1.95it/s]\u001b[A\n","560it [45:58,  1.95it/s]\u001b[A\n","561it [45:58,  1.95it/s]\u001b[A\n","562it [45:59,  1.96it/s]\u001b[A\n","563it [45:59,  1.96it/s]\u001b[A\n","564it [46:00,  1.96it/s]\u001b[A\n","565it [46:00,  1.96it/s]\u001b[A\n","566it [46:01,  1.96it/s]\u001b[A\n","567it [46:01,  1.96it/s]\u001b[A\n","568it [46:02,  1.96it/s]\u001b[A\n","569it [46:02,  1.96it/s]\u001b[A\n","570it [46:03,  1.96it/s]\u001b[A\n","571it [46:03,  1.96it/s]\u001b[A\n","572it [46:04,  1.96it/s]\u001b[A\n","573it [46:04,  1.96it/s]\u001b[A\n","574it [46:05,  1.96it/s]\u001b[A\n","575it [46:05,  1.96it/s]\u001b[A\n","576it [46:06,  1.96it/s]\u001b[A\n","577it [46:06,  1.96it/s]\u001b[A\n","578it [46:07,  1.96it/s]\u001b[A\n","579it [46:07,  1.95it/s]\u001b[A\n","580it [46:08,  1.96it/s]\u001b[A\n","581it [46:08,  1.95it/s]\u001b[A\n","582it [46:09,  1.95it/s]\u001b[A\n","583it [46:09,  1.96it/s]\u001b[A\n","584it [46:10,  1.96it/s]\u001b[A\n","585it [46:10,  1.96it/s]\u001b[A\n","586it [46:11,  1.96it/s]\u001b[A\n","587it [46:11,  1.96it/s]\u001b[A\n","588it [46:12,  1.96it/s]\u001b[A\n","589it [46:12,  1.96it/s]\u001b[A\n","590it [46:13,  1.96it/s]\u001b[A\n","591it [46:13,  1.95it/s]\u001b[A\n","592it [46:14,  1.95it/s]\u001b[A\n","593it [46:14,  1.95it/s]\u001b[A\n","594it [46:15,  1.96it/s]\u001b[A\n","595it [46:15,  1.96it/s]\u001b[A\n","596it [46:16,  1.95it/s]\u001b[A\n","597it [46:16,  1.96it/s]\u001b[A\n","598it [46:17,  1.95it/s]\u001b[A\n","599it [46:17,  1.96it/s]\u001b[A\n","600it [46:18,  1.96it/s]\u001b[A\n","601it [46:19,  1.96it/s]\u001b[A\n","602it [46:19,  1.96it/s]\u001b[A\n","603it [46:20,  1.96it/s]\u001b[A\n","604it [46:20,  1.96it/s]\u001b[A\n","605it [46:21,  1.96it/s]\u001b[A\n","606it [46:21,  1.96it/s]\u001b[A\n","607it [46:22,  1.96it/s]\u001b[A\n","608it [46:22,  1.96it/s]\u001b[A\n","609it [46:23,  1.96it/s]\u001b[A\n","610it [46:23,  1.96it/s]\u001b[A\n","611it [46:24,  1.96it/s]\u001b[A\n","612it [46:24,  1.96it/s]\u001b[A\n","613it [46:25,  1.96it/s]\u001b[A\n","614it [46:25,  1.95it/s]\u001b[A\n","615it [46:26,  1.95it/s]\u001b[A\n","616it [46:26,  1.95it/s]\u001b[A\n","617it [46:27,  1.95it/s]\u001b[A\n","618it [46:27,  1.95it/s]\u001b[A\n","619it [46:28,  1.95it/s]\u001b[A\n","620it [46:28,  1.96it/s]\u001b[A\n","621it [46:29,  1.96it/s]\u001b[A\n","622it [46:29,  1.96it/s]\u001b[A\n","623it [46:30,  1.96it/s]\u001b[A\n","624it [46:30,  1.96it/s]\u001b[A\n","625it [46:31,  1.96it/s]\u001b[A\n","626it [46:31,  1.96it/s]\u001b[A\n","627it [46:32,  1.96it/s]\u001b[A\n","628it [46:32,  1.96it/s]\u001b[A\n","629it [46:33,  1.96it/s]\u001b[A\n","630it [46:33,  1.96it/s]\u001b[A\n","631it [46:34,  1.96it/s]\u001b[A\n","632it [46:34,  1.96it/s]\u001b[A\n","633it [46:35,  1.96it/s]\u001b[A\n","634it [46:35,  1.96it/s]\u001b[A\n","635it [46:36,  1.96it/s]\u001b[A\n","636it [46:36,  1.96it/s]\u001b[A\n","637it [46:37,  1.96it/s]\u001b[A\n","638it [46:37,  1.96it/s]\u001b[A\n","639it [46:38,  1.96it/s]\u001b[A\n","640it [46:38,  1.96it/s]\u001b[A\n","641it [46:39,  1.96it/s]\u001b[A\n","642it [46:39,  1.96it/s]\u001b[A\n","643it [46:40,  1.96it/s]\u001b[A\n","644it [46:40,  1.96it/s]\u001b[A\n","645it [46:41,  1.96it/s]\u001b[A\n","646it [46:42,  1.96it/s]\u001b[A\n","647it [46:42,  1.96it/s]\u001b[A\n","648it [46:43,  1.96it/s]\u001b[A\n","649it [46:43,  1.96it/s]\u001b[A\n","650it [46:44,  1.96it/s]\u001b[A\n","651it [46:44,  1.96it/s]\u001b[A\n","652it [46:45,  1.96it/s]\u001b[A\n","653it [46:45,  1.96it/s]\u001b[A\n","654it [46:46,  1.96it/s]\u001b[A\n","655it [46:46,  1.96it/s]\u001b[A\n","656it [46:47,  1.96it/s]\u001b[A\n","657it [46:47,  1.96it/s]\u001b[A\n","658it [46:48,  1.96it/s]\u001b[A\n","659it [46:48,  1.96it/s]\u001b[A\n","660it [46:49,  1.96it/s]\u001b[A\n","661it [46:49,  1.96it/s]\u001b[A\n","662it [46:50,  1.96it/s]\u001b[A\n","663it [46:50,  1.96it/s]\u001b[A\n","664it [46:51,  1.96it/s]\u001b[A\n","665it [46:51,  1.96it/s]\u001b[A\n","666it [46:52,  1.96it/s]\u001b[A\n","667it [46:52,  1.96it/s]\u001b[A\n","668it [46:53,  1.96it/s]\u001b[A\n","669it [46:53,  1.96it/s]\u001b[A\n","670it [46:54,  1.96it/s]\u001b[A\n","671it [46:54,  1.96it/s]\u001b[A\n","672it [46:55,  1.96it/s]\u001b[A\n","673it [46:55,  1.96it/s]\u001b[A\n","674it [46:56,  1.96it/s]\u001b[A\n","675it [46:56,  1.96it/s]\u001b[A\n","Epoch: 100% 4/4 [1:00:41<00:00, 910.38s/it]\n","08/29/2023 08:21:46 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","08/29/2023 08:21:58 - INFO - __main__ -   *** Validate ***\n","\n","677it [47:13,  5.15s/it]\u001b[A\n","678it [47:13,  3.76s/it]\u001b[A\n","679it [47:14,  2.78s/it]\u001b[A\n","680it [47:14,  2.10s/it]\u001b[A\n","681it [47:15,  1.62s/it]\u001b[A\n","682it [47:15,  1.29s/it]\u001b[A\n","683it [47:16,  1.05s/it]\u001b[A\n","684it [47:16,  1.12it/s]\u001b[A\n","685it [47:17,  1.29it/s]\u001b[A\n","686it [47:17,  1.44it/s]\u001b[A\n","687it [47:18,  1.57it/s]\u001b[A\n","688it [47:18,  1.67it/s]\u001b[A\n","689it [47:19,  1.75it/s]\u001b[A\n","690it [47:19,  1.81it/s]\u001b[A\n","691it [47:20,  1.85it/s]\u001b[A\n","692it [47:20,  1.89it/s]\u001b[A\n","693it [47:21,  1.91it/s]\u001b[A\n","694it [47:21,  1.93it/s]\u001b[A\n","695it [47:22,  1.94it/s]\u001b[A\n","696it [47:22,  1.95it/s]\u001b[A\n","697it [47:23,  1.95it/s]\u001b[A\n","698it [47:23,  1.96it/s]\u001b[A\n","699it [47:24,  1.96it/s]\u001b[A\n","700it [47:24,  1.96it/s]\u001b[A\n","701it [47:25,  1.96it/s]\u001b[A\n","702it [47:25,  1.97it/s]\u001b[A\n","703it [47:26,  1.97it/s]\u001b[A\n","704it [47:26,  1.97it/s]\u001b[A\n","705it [47:27,  1.97it/s]\u001b[A\n","706it [47:27,  1.97it/s]\u001b[A\n","707it [47:28,  1.97it/s]\u001b[A\n","708it [47:28,  1.97it/s]\u001b[A\n","709it [47:29,  1.97it/s]\u001b[A\n","710it [47:29,  1.97it/s]\u001b[A\n","711it [47:30,  1.97it/s]\u001b[A\n","712it [47:31,  1.97it/s]\u001b[A\n","713it [47:31,  1.97it/s]\u001b[A\n","714it [47:32,  1.97it/s]\u001b[A\n","715it [47:32,  1.97it/s]\u001b[A\n","716it [47:33,  1.97it/s]\u001b[A\n","717it [47:33,  1.97it/s]\u001b[A\n","718it [47:34,  1.97it/s]\u001b[A\n","719it [47:34,  1.97it/s]\u001b[A\n","720it [47:35,  1.97it/s]\u001b[A\n","721it [47:35,  1.97it/s]\u001b[A\n","722it [47:36,  1.97it/s]\u001b[A\n","723it [47:36,  1.97it/s]\u001b[A\n","724it [47:37,  1.97it/s]\u001b[A\n","725it [47:37,  1.97it/s]\u001b[A\n","726it [47:38,  1.97it/s]\u001b[A\n","727it [47:38,  1.97it/s]\u001b[A\n","728it [47:39,  1.97it/s]\u001b[A\n","729it [47:39,  1.97it/s]\u001b[A\n","730it [47:40,  1.97it/s]\u001b[A\n","731it [47:40,  1.96it/s]\u001b[A\n","732it [47:41,  1.96it/s]\u001b[A\n","733it [47:41,  1.96it/s]\u001b[A\n","734it [47:42,  1.96it/s]\u001b[A\n","735it [47:42,  1.96it/s]\u001b[A\n","736it [47:43,  1.96it/s]\u001b[A\n","737it [47:43,  1.96it/s]\u001b[A\n","738it [47:44,  1.96it/s]\u001b[A\n","739it [47:44,  1.96it/s]\u001b[A\n","740it [47:45,  1.96it/s]\u001b[A\n","741it [47:45,  1.96it/s]\u001b[A\n","742it [47:46,  1.96it/s]\u001b[A\n","743it [47:46,  1.96it/s]\u001b[A\n","744it [47:47,  1.96it/s]\u001b[A\n","745it [47:47,  1.96it/s]\u001b[A\n","746it [47:48,  1.96it/s]\u001b[A\n","747it [47:48,  1.96it/s]\u001b[A\n","748it [47:49,  1.96it/s]\u001b[A\n","749it [47:49,  1.96it/s]\u001b[A\n","750it [47:50,  1.96it/s]\u001b[A\n","751it [47:50,  1.96it/s]\u001b[A\n","752it [47:51,  1.96it/s]\u001b[A\n","753it [47:51,  1.97it/s]\u001b[A\n","754it [47:52,  1.96it/s]\u001b[A\n","755it [47:52,  1.96it/s]\u001b[A\n","756it [47:53,  1.96it/s]\u001b[A\n","757it [47:53,  1.96it/s]\u001b[A\n","758it [47:54,  1.96it/s]\u001b[A\n","759it [47:54,  1.96it/s]\u001b[A\n","760it [47:55,  1.96it/s]\u001b[A\n","761it [47:55,  1.96it/s]\u001b[A\n","762it [47:56,  1.96it/s]\u001b[A\n","763it [47:56,  1.96it/s]\u001b[A\n","764it [47:57,  1.96it/s]\u001b[A\n","765it [47:57,  1.96it/s]\u001b[A\n","766it [47:58,  1.96it/s]\u001b[A\n","767it [47:59,  1.96it/s]\u001b[A\n","768it [47:59,  1.96it/s]\u001b[A\n","769it [48:00,  1.96it/s]\u001b[A\n","770it [48:00,  1.96it/s]\u001b[A\n","771it [48:01,  1.96it/s]\u001b[A\n","772it [48:01,  1.96it/s]\u001b[A\n","773it [48:02,  1.96it/s]\u001b[A\n","774it [48:02,  1.96it/s]\u001b[A\n","775it [48:03,  1.96it/s]\u001b[A\n","776it [48:03,  1.96it/s]\u001b[A\n","777it [48:04,  1.96it/s]\u001b[A\n","778it [48:04,  1.96it/s]\u001b[A\n","779it [48:05,  1.96it/s]\u001b[A\n","780it [48:05,  1.97it/s]\u001b[A\n","781it [48:06,  1.97it/s]\u001b[A\n","782it [48:06,  1.96it/s]\u001b[A\n","783it [48:07,  1.96it/s]\u001b[A\n","784it [48:07,  1.96it/s]\u001b[A\n","785it [48:08,  1.96it/s]\u001b[A\n","786it [48:08,  1.96it/s]\u001b[A\n","787it [48:09,  1.96it/s]\u001b[A\n","788it [48:09,  1.96it/s]\u001b[A\n","789it [48:10,  1.96it/s]\u001b[A\n","790it [48:10,  1.96it/s]\u001b[A\n","791it [48:11,  1.96it/s]\u001b[A\n","792it [48:11,  1.96it/s]\u001b[A\n","793it [48:12,  1.96it/s]\u001b[A\n","794it [48:12,  1.96it/s]\u001b[A\n","795it [48:13,  1.96it/s]\u001b[A\n","796it [48:13,  1.96it/s]\u001b[A\n","797it [48:14,  1.96it/s]\u001b[A\n","798it [48:14,  1.96it/s]\u001b[A\n","799it [48:15,  1.96it/s]\u001b[A\n","800it [48:15,  1.96it/s]\u001b[A\n","801it [48:16,  1.96it/s]\u001b[A\n","802it [48:16,  1.96it/s]\u001b[A\n","803it [48:17,  1.96it/s]\u001b[A\n","804it [48:17,  1.96it/s]\u001b[A\n","805it [48:18,  1.97it/s]\u001b[A\n","806it [48:18,  1.96it/s]\u001b[A\n","807it [48:19,  1.96it/s]\u001b[A\n","808it [48:19,  1.97it/s]\u001b[A\n","809it [48:20,  1.97it/s]\u001b[A\n","810it [48:20,  1.96it/s]\u001b[A\n","811it [48:21,  1.96it/s]\u001b[A\n","812it [48:21,  1.96it/s]\u001b[A\n","813it [48:22,  1.96it/s]\u001b[A\n","814it [48:22,  1.96it/s]\u001b[A\n","815it [48:23,  1.96it/s]\u001b[A\n","816it [48:23,  1.96it/s]\u001b[A\n","817it [48:24,  1.96it/s]\u001b[A\n","818it [48:25,  1.96it/s]\u001b[A\n","819it [48:25,  1.96it/s]\u001b[A\n","820it [48:26,  1.96it/s]\u001b[A\n","821it [48:26,  1.96it/s]\u001b[A\n","822it [48:27,  1.96it/s]\u001b[A\n","823it [48:27,  1.96it/s]\u001b[A\n","824it [48:28,  1.96it/s]\u001b[A\n","825it [48:28,  1.96it/s]\u001b[A\n","826it [48:29,  1.96it/s]\u001b[A\n","827it [48:29,  1.96it/s]\u001b[A\n","828it [48:30,  1.96it/s]\u001b[A\n","829it [48:30,  1.97it/s]\u001b[A\n","830it [48:31,  1.97it/s]\u001b[A\n","831it [48:31,  1.97it/s]\u001b[A\n","832it [48:32,  1.97it/s]\u001b[A\n","833it [48:32,  1.97it/s]\u001b[A\n","834it [48:33,  1.97it/s]\u001b[A\n","835it [48:33,  1.97it/s]\u001b[A\n","836it [48:34,  1.97it/s]\u001b[A\n","837it [48:34,  1.97it/s]\u001b[A\n","838it [48:35,  1.97it/s]\u001b[A\n","839it [48:35,  1.97it/s]\u001b[A\n","840it [48:36,  1.96it/s]\u001b[A\n","841it [48:36,  1.96it/s]\u001b[A\n","842it [48:37,  1.97it/s]\u001b[A\n","843it [48:37,  1.96it/s]\u001b[A\n","844it [48:38,  1.96it/s]\u001b[A\n","845it [48:38,  2.23it/s]\u001b[A08/29/2023 08:23:24 - INFO - __main__ -   ***** Eval results spoilers *****\n","08/29/2023 08:23:24 - INFO - __main__ -     eval_loss = 0.4395201802253723\n","08/29/2023 08:23:24 - INFO - __main__ -     eval_ap = 0.8826854825019836\n","08/29/2023 08:23:24 - INFO - __main__ -     eval_auroc = 0.9170293807983398\n","08/29/2023 08:23:24 - INFO - __main__ -     eval_recall = 0.8354308009147644\n","08/29/2023 08:23:24 - INFO - __main__ -     eval_f1 = 0.7928341627120972\n","08/29/2023 08:23:24 - INFO - root -   *** Test ***\n","\n","846it [48:39,  2.12it/s]\u001b[A\n","847it [48:39,  2.07it/s]\u001b[A\n","848it [48:40,  2.04it/s]\u001b[A\n","849it [48:40,  2.02it/s]\u001b[A\n","850it [48:41,  2.00it/s]\u001b[A\n","851it [48:41,  1.99it/s]\u001b[A\n","852it [48:42,  1.98it/s]\u001b[A\n","853it [48:42,  1.97it/s]\u001b[A\n","854it [48:43,  1.97it/s]\u001b[A\n","855it [48:43,  1.97it/s]\u001b[A\n","856it [48:44,  1.96it/s]\u001b[A\n","857it [48:44,  1.96it/s]\u001b[A\n","858it [48:45,  1.96it/s]\u001b[A\n","859it [48:45,  1.96it/s]\u001b[A\n","860it [48:46,  1.96it/s]\u001b[A\n","861it [48:46,  1.96it/s]\u001b[A\n","862it [48:47,  1.96it/s]\u001b[A\n","863it [48:47,  1.96it/s]\u001b[A\n","864it [48:48,  1.96it/s]\u001b[A\n","865it [48:48,  1.96it/s]\u001b[A\n","866it [48:49,  1.96it/s]\u001b[A\n","867it [48:49,  1.96it/s]\u001b[A\n","868it [48:50,  1.96it/s]\u001b[A\n","869it [48:50,  1.96it/s]\u001b[A\n","870it [48:51,  1.96it/s]\u001b[A\n","871it [48:51,  1.96it/s]\u001b[A\n","872it [48:52,  1.96it/s]\u001b[A\n","873it [48:52,  1.96it/s]\u001b[A\n","874it [48:53,  1.96it/s]\u001b[A\n","875it [48:53,  1.96it/s]\u001b[A\n","876it [48:54,  1.96it/s]\u001b[A\n","877it [48:54,  1.96it/s]\u001b[A\n","878it [48:55,  1.96it/s]\u001b[A\n","879it [48:55,  1.96it/s]\u001b[A\n","880it [48:56,  1.96it/s]\u001b[A\n","881it [48:56,  1.96it/s]\u001b[A\n","882it [48:57,  1.96it/s]\u001b[A\n","883it [48:57,  1.96it/s]\u001b[A\n","884it [48:58,  1.96it/s]\u001b[A\n","885it [48:58,  1.96it/s]\u001b[A\n","886it [48:59,  1.96it/s]\u001b[A\n","887it [48:59,  1.96it/s]\u001b[A\n","888it [49:00,  1.96it/s]\u001b[A\n","889it [49:00,  1.96it/s]\u001b[A\n","890it [49:01,  1.96it/s]\u001b[A\n","891it [49:02,  1.96it/s]\u001b[A\n","892it [49:02,  1.96it/s]\u001b[A\n","893it [49:03,  1.96it/s]\u001b[A\n","894it [49:03,  1.96it/s]\u001b[A\n","895it [49:04,  1.96it/s]\u001b[A\n","896it [49:04,  1.96it/s]\u001b[A\n","897it [49:05,  1.96it/s]\u001b[A\n","898it [49:05,  1.96it/s]\u001b[A\n","899it [49:06,  1.96it/s]\u001b[A\n","900it [49:06,  1.96it/s]\u001b[A\n","901it [49:07,  1.96it/s]\u001b[A\n","902it [49:07,  1.96it/s]\u001b[A\n","903it [49:08,  1.96it/s]\u001b[A\n","904it [49:08,  1.96it/s]\u001b[A\n","905it [49:09,  1.96it/s]\u001b[A\n","906it [49:09,  1.96it/s]\u001b[A\n","907it [49:10,  1.96it/s]\u001b[A\n","908it [49:10,  1.96it/s]\u001b[A\n","909it [49:11,  1.96it/s]\u001b[A\n","910it [49:11,  1.96it/s]\u001b[A\n","911it [49:12,  1.96it/s]\u001b[A\n","912it [49:12,  1.96it/s]\u001b[A\n","913it [49:13,  1.96it/s]\u001b[A\n","914it [49:13,  1.96it/s]\u001b[A08/29/2023 08:23:59 - INFO - __main__ -   ***** Test results spoilers *****\n","08/29/2023 08:23:59 - INFO - __main__ -     eval_loss = 0.43993130326271057\n","08/29/2023 08:23:59 - INFO - __main__ -     eval_ap = 0.9111554026603699\n","08/29/2023 08:23:59 - INFO - __main__ -     eval_auroc = 0.9183704257011414\n","08/29/2023 08:23:59 - INFO - __main__ -     eval_recall = 0.8191056847572327\n","08/29/2023 08:23:59 - INFO - __main__ -     eval_f1 = 0.8133198618888855\n","08/29/2023 08:23:59 - INFO - filelock -   Lock 134345229193168 acquired on log.lock\n","08/29/2023 08:24:00 - INFO - filelock -   Lock 134345229193168 released on log.lock\n","914it [49:14,  3.23s/it]\n"]}],"source":["!source env/bin/activate; TAG=\"stuffed_v1.0_batch32\" TYPE=prompt TASK=spoilers BS=32 LR=1e-5 SEED=21 MODEL=roberta-large bash run_experiment.sh \\\n"," \"--template *cls**sent_0*._It_was*mask*.*sep+* --mapping {0:'relevant',1:'irrelevant'} --first_sent_limit 502 --other_sent_limit 502\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2VkaNYu18D-","outputId":"dc688065-6dc8-4baa-c41e-662a1dbafc32"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n"," 15% 410/2698 [11:10<1:02:40,  1.64s/it]\u001b[A\n"," 15% 411/2698 [11:12<1:02:43,  1.65s/it]\u001b[A\n"," 15% 412/2698 [11:14<1:02:29,  1.64s/it]\u001b[A\n"," 15% 413/2698 [11:15<1:02:27,  1.64s/it]\u001b[A\n"," 15% 414/2698 [11:17<1:02:33,  1.64s/it]\u001b[A\n"," 15% 415/2698 [11:19<1:02:27,  1.64s/it]\u001b[A\n"," 15% 416/2698 [11:20<1:02:27,  1.64s/it]\u001b[A\n"," 15% 417/2698 [11:22<1:02:31,  1.64s/it]\u001b[A\n"," 15% 418/2698 [11:24<1:02:24,  1.64s/it]\u001b[A\n"," 16% 419/2698 [11:25<1:02:24,  1.64s/it]\u001b[A\n"," 16% 420/2698 [11:27<1:02:30,  1.65s/it]\u001b[A\n"," 16% 421/2698 [11:29<1:02:14,  1.64s/it]\u001b[A\n"," 16% 422/2698 [11:30<1:02:14,  1.64s/it]\u001b[A\n"," 16% 423/2698 [11:32<1:02:08,  1.64s/it]\u001b[A\n"," 16% 424/2698 [11:33<1:02:06,  1.64s/it]\u001b[A\n"," 16% 425/2698 [11:35<1:02:12,  1.64s/it]\u001b[A\n"," 16% 426/2698 [11:37<1:02:11,  1.64s/it]\u001b[A\n"," 16% 427/2698 [11:38<1:02:10,  1.64s/it]\u001b[A\n"," 16% 428/2698 [11:40<1:02:17,  1.65s/it]\u001b[A\n"," 16% 429/2698 [11:42<1:02:09,  1.64s/it]\u001b[A\n"," 16% 430/2698 [11:43<1:02:02,  1.64s/it]\u001b[A\n"," 16% 431/2698 [11:45<1:02:05,  1.64s/it]\u001b[A\n"," 16% 432/2698 [11:47<1:02:09,  1.65s/it]\u001b[A\n"," 16% 433/2698 [11:48<1:02:07,  1.65s/it]\u001b[A\n"," 16% 434/2698 [11:50<1:02:02,  1.64s/it]\u001b[A\n"," 16% 435/2698 [11:52<1:02:03,  1.65s/it]\u001b[A\n"," 16% 436/2698 [11:53<1:01:57,  1.64s/it]\u001b[A\n"," 16% 437/2698 [11:55<1:02:03,  1.65s/it]\u001b[A\n"," 16% 438/2698 [11:56<1:01:54,  1.64s/it]\u001b[A\n"," 16% 439/2698 [11:58<1:01:55,  1.64s/it]\u001b[A\n"," 16% 440/2698 [12:00<1:01:51,  1.64s/it]\u001b[A\n"," 16% 441/2698 [12:01<1:01:59,  1.65s/it]\u001b[A\n"," 16% 442/2698 [12:03<1:02:03,  1.65s/it]\u001b[A\n"," 16% 443/2698 [12:05<1:02:13,  1.66s/it]\u001b[A\n"," 16% 444/2698 [12:06<1:02:15,  1.66s/it]\u001b[A\n"," 16% 445/2698 [12:08<1:02:16,  1.66s/it]\u001b[A\n"," 17% 446/2698 [12:10<1:02:16,  1.66s/it]\u001b[A\n"," 17% 447/2698 [12:11<1:02:11,  1.66s/it]\u001b[A\n"," 17% 448/2698 [12:13<1:02:07,  1.66s/it]\u001b[A\n"," 17% 449/2698 [12:15<1:02:02,  1.66s/it]\u001b[A\n"," 17% 450/2698 [12:16<1:01:58,  1.65s/it]\u001b[A\n"," 17% 451/2698 [12:18<1:01:57,  1.65s/it]\u001b[A\n"," 17% 452/2698 [12:20<1:01:52,  1.65s/it]\u001b[A\n"," 17% 453/2698 [12:21<1:01:52,  1.65s/it]\u001b[A\n"," 17% 454/2698 [12:23<1:01:35,  1.65s/it]\u001b[A\n"," 17% 455/2698 [12:25<1:01:38,  1.65s/it]\u001b[A\n"," 17% 456/2698 [12:26<1:01:38,  1.65s/it]\u001b[A\n"," 17% 457/2698 [12:28<1:01:34,  1.65s/it]\u001b[A\n"," 17% 458/2698 [12:30<1:01:34,  1.65s/it]\u001b[A\n"," 17% 459/2698 [12:31<1:01:26,  1.65s/it]\u001b[A\n"," 17% 460/2698 [12:33<1:01:23,  1.65s/it]\u001b[A\n"," 17% 461/2698 [12:34<1:01:28,  1.65s/it]\u001b[A\n"," 17% 462/2698 [12:36<1:01:32,  1.65s/it]\u001b[A\n"," 17% 463/2698 [12:38<1:01:19,  1.65s/it]\u001b[A\n"," 17% 464/2698 [12:39<1:01:19,  1.65s/it]\u001b[A\n"," 17% 465/2698 [12:41<1:01:16,  1.65s/it]\u001b[A\n"," 17% 466/2698 [12:43<1:01:19,  1.65s/it]\u001b[A\n"," 17% 467/2698 [12:44<1:01:18,  1.65s/it]\u001b[A\n"," 17% 468/2698 [12:46<1:01:13,  1.65s/it]\u001b[A\n"," 17% 469/2698 [12:48<1:01:18,  1.65s/it]\u001b[A\n"," 17% 470/2698 [12:49<1:01:12,  1.65s/it]\u001b[A\n"," 17% 471/2698 [12:51<1:01:11,  1.65s/it]\u001b[A\n"," 17% 472/2698 [12:53<1:01:10,  1.65s/it]\u001b[A\n"," 18% 473/2698 [12:54<1:01:08,  1.65s/it]\u001b[A\n"," 18% 474/2698 [12:56<1:01:01,  1.65s/it]\u001b[A\n"," 18% 475/2698 [12:58<1:01:05,  1.65s/it]\u001b[A\n"," 18% 476/2698 [12:59<1:01:05,  1.65s/it]\u001b[A\n"," 18% 477/2698 [13:01<1:01:00,  1.65s/it]\u001b[A\n"," 18% 478/2698 [13:02<1:00:52,  1.65s/it]\u001b[A\n"," 18% 479/2698 [13:04<1:00:50,  1.65s/it]\u001b[A\n"," 18% 480/2698 [13:06<1:00:41,  1.64s/it]\u001b[A\n"," 18% 481/2698 [13:07<1:00:40,  1.64s/it]\u001b[A\n"," 18% 482/2698 [13:09<1:00:41,  1.64s/it]\u001b[A\n"," 18% 483/2698 [13:11<1:00:49,  1.65s/it]\u001b[A\n"," 18% 484/2698 [13:12<1:00:51,  1.65s/it]\u001b[A\n"," 18% 485/2698 [13:14<1:00:42,  1.65s/it]\u001b[A\n"," 18% 486/2698 [13:16<1:00:43,  1.65s/it]\u001b[A\n"," 18% 487/2698 [13:17<1:00:35,  1.64s/it]\u001b[A\n"," 18% 488/2698 [13:19<1:00:32,  1.64s/it]\u001b[A\n"," 18% 489/2698 [13:21<1:00:30,  1.64s/it]\u001b[A\n"," 18% 490/2698 [13:22<1:00:41,  1.65s/it]\u001b[A\n"," 18% 491/2698 [13:24<1:00:34,  1.65s/it]\u001b[A\n"," 18% 492/2698 [13:26<1:00:34,  1.65s/it]\u001b[A\n"," 18% 493/2698 [13:27<1:00:31,  1.65s/it]\u001b[A\n"," 18% 494/2698 [13:29<1:00:26,  1.65s/it]\u001b[A\n"," 18% 495/2698 [13:30<1:00:33,  1.65s/it]\u001b[A\n"," 18% 496/2698 [13:32<1:00:33,  1.65s/it]\u001b[A\n"," 18% 497/2698 [13:34<1:00:30,  1.65s/it]\u001b[A\n"," 18% 498/2698 [13:35<1:00:29,  1.65s/it]\u001b[A\n"," 18% 499/2698 [13:37<1:00:19,  1.65s/it]\u001b[A\n"," 19% 500/2698 [13:39<1:00:15,  1.65s/it]\u001b[A\n"," 19% 501/2698 [13:40<1:00:18,  1.65s/it]\u001b[A\n"," 19% 502/2698 [13:42<1:00:13,  1.65s/it]\u001b[A\n"," 19% 503/2698 [13:44<1:00:15,  1.65s/it]\u001b[A\n"," 19% 504/2698 [13:45<1:00:14,  1.65s/it]\u001b[A\n"," 19% 505/2698 [13:47<1:00:08,  1.65s/it]\u001b[A\n"," 19% 506/2698 [13:49<1:00:10,  1.65s/it]\u001b[A\n"," 19% 507/2698 [13:50<1:00:03,  1.64s/it]\u001b[A\n"," 19% 508/2698 [13:52<59:53,  1.64s/it]  \u001b[A\n"," 19% 509/2698 [13:54<59:50,  1.64s/it]\u001b[A\n"," 19% 510/2698 [13:55<59:46,  1.64s/it]\u001b[A\n"," 19% 511/2698 [13:57<59:45,  1.64s/it]\u001b[A\n"," 19% 512/2698 [13:58<59:41,  1.64s/it]\u001b[A\n"," 19% 513/2698 [14:00<59:37,  1.64s/it]\u001b[A\n"," 19% 514/2698 [14:02<59:40,  1.64s/it]\u001b[A\n"," 19% 515/2698 [14:03<59:36,  1.64s/it]\u001b[A\n"," 19% 516/2698 [14:05<59:40,  1.64s/it]\u001b[A\n"," 19% 517/2698 [14:07<59:38,  1.64s/it]\u001b[A\n"," 19% 518/2698 [14:08<59:36,  1.64s/it]\u001b[A\n"," 19% 519/2698 [14:10<59:27,  1.64s/it]\u001b[A\n"," 19% 520/2698 [14:12<59:21,  1.64s/it]\u001b[A\n"," 19% 521/2698 [14:13<59:18,  1.63s/it]\u001b[A\n"," 19% 522/2698 [14:15<59:09,  1.63s/it]\u001b[A\n"," 19% 523/2698 [14:16<59:11,  1.63s/it]\u001b[A\n"," 19% 524/2698 [14:18<59:07,  1.63s/it]\u001b[A\n"," 19% 525/2698 [14:20<59:05,  1.63s/it]\u001b[A\n"," 19% 526/2698 [14:21<59:04,  1.63s/it]\u001b[A\n"," 20% 527/2698 [14:23<59:06,  1.63s/it]\u001b[A\n"," 20% 528/2698 [14:25<59:14,  1.64s/it]\u001b[A\n"," 20% 529/2698 [14:26<59:17,  1.64s/it]\u001b[A\n"," 20% 530/2698 [14:28<59:13,  1.64s/it]\u001b[A\n"," 20% 531/2698 [14:30<59:15,  1.64s/it]\u001b[A\n"," 20% 532/2698 [14:31<59:09,  1.64s/it]\u001b[A\n"," 20% 533/2698 [14:33<59:04,  1.64s/it]\u001b[A\n"," 20% 534/2698 [14:34<59:05,  1.64s/it]\u001b[A\n"," 20% 535/2698 [14:36<59:00,  1.64s/it]\u001b[A\n"," 20% 536/2698 [14:38<58:53,  1.63s/it]\u001b[A\n"," 20% 537/2698 [14:39<58:50,  1.63s/it]\u001b[A\n"," 20% 538/2698 [14:41<58:58,  1.64s/it]\u001b[A\n"," 20% 539/2698 [14:43<59:00,  1.64s/it]\u001b[A\n"," 20% 540/2698 [14:44<58:55,  1.64s/it]\u001b[A\n"," 20% 541/2698 [14:46<58:52,  1.64s/it]\u001b[A\n"," 20% 542/2698 [14:48<58:50,  1.64s/it]\u001b[A\n"," 20% 543/2698 [14:49<58:47,  1.64s/it]\u001b[A\n"," 20% 544/2698 [14:51<58:41,  1.63s/it]\u001b[A\n"," 20% 545/2698 [14:52<58:47,  1.64s/it]\u001b[A\n"," 20% 546/2698 [14:54<58:47,  1.64s/it]\u001b[A\n"," 20% 547/2698 [14:56<58:45,  1.64s/it]\u001b[A\n"," 20% 548/2698 [14:57<58:43,  1.64s/it]\u001b[A\n"," 20% 549/2698 [14:59<58:41,  1.64s/it]\u001b[A\n"," 20% 550/2698 [15:01<58:37,  1.64s/it]\u001b[A\n"," 20% 551/2698 [15:02<58:34,  1.64s/it]\u001b[A\n"," 20% 552/2698 [15:04<58:39,  1.64s/it]\u001b[A\n"," 20% 553/2698 [15:06<58:48,  1.64s/it]\u001b[A\n"," 21% 554/2698 [15:07<58:39,  1.64s/it]\u001b[A\n"," 21% 555/2698 [15:09<58:38,  1.64s/it]\u001b[A\n"," 21% 556/2698 [15:10<58:40,  1.64s/it]\u001b[A\n"," 21% 557/2698 [15:12<58:31,  1.64s/it]\u001b[A\n"," 21% 558/2698 [15:14<58:23,  1.64s/it]\u001b[A\n"," 21% 559/2698 [15:15<58:22,  1.64s/it]\u001b[A\n"," 21% 560/2698 [15:17<58:24,  1.64s/it]\u001b[A\n"," 21% 561/2698 [15:19<58:18,  1.64s/it]\u001b[A\n"," 21% 562/2698 [15:20<58:15,  1.64s/it]\u001b[A\n"," 21% 563/2698 [15:22<58:11,  1.64s/it]\u001b[A\n"," 21% 564/2698 [15:24<58:05,  1.63s/it]\u001b[A\n"," 21% 565/2698 [15:25<58:02,  1.63s/it]\u001b[A\n"," 21% 566/2698 [15:27<58:01,  1.63s/it]\u001b[A\n"," 21% 567/2698 [15:28<58:11,  1.64s/it]\u001b[A\n"," 21% 568/2698 [15:30<58:06,  1.64s/it]\u001b[A\n"," 21% 569/2698 [15:32<58:07,  1.64s/it]\u001b[A\n"," 21% 570/2698 [15:33<58:08,  1.64s/it]\u001b[A\n"," 21% 571/2698 [15:35<58:00,  1.64s/it]\u001b[A\n"," 21% 572/2698 [15:37<58:08,  1.64s/it]\u001b[A\n"," 21% 573/2698 [15:38<58:20,  1.65s/it]\u001b[A\n"," 21% 574/2698 [15:40<58:22,  1.65s/it]\u001b[A\n"," 21% 575/2698 [15:42<58:27,  1.65s/it]\u001b[A\n"," 21% 576/2698 [15:43<58:25,  1.65s/it]\u001b[A\n"," 21% 577/2698 [15:45<58:19,  1.65s/it]\u001b[A\n"," 21% 578/2698 [15:47<58:21,  1.65s/it]\u001b[A\n"," 21% 579/2698 [15:48<58:17,  1.65s/it]\u001b[A\n"," 21% 580/2698 [15:50<58:06,  1.65s/it]\u001b[A\n"," 22% 581/2698 [15:52<58:02,  1.64s/it]\u001b[A\n"," 22% 582/2698 [15:53<58:13,  1.65s/it]\u001b[A\n"," 22% 583/2698 [15:55<58:18,  1.65s/it]\u001b[A\n"," 22% 584/2698 [15:57<58:20,  1.66s/it]\u001b[A\n"," 22% 585/2698 [15:58<58:17,  1.66s/it]\u001b[A\n"," 22% 586/2698 [16:00<58:00,  1.65s/it]\u001b[A\n"," 22% 587/2698 [16:01<58:01,  1.65s/it]\u001b[A\n"," 22% 588/2698 [16:03<58:03,  1.65s/it]\u001b[A\n"," 22% 589/2698 [16:05<58:01,  1.65s/it]\u001b[A\n"," 22% 590/2698 [16:06<58:03,  1.65s/it]\u001b[A\n"," 22% 591/2698 [16:08<57:58,  1.65s/it]\u001b[A\n"," 22% 592/2698 [16:10<57:57,  1.65s/it]\u001b[A\n"," 22% 593/2698 [16:11<58:01,  1.65s/it]\u001b[A\n"," 22% 594/2698 [16:13<58:00,  1.65s/it]\u001b[A\n"," 22% 595/2698 [16:15<57:52,  1.65s/it]\u001b[A\n"," 22% 596/2698 [16:16<57:46,  1.65s/it]\u001b[A\n"," 22% 597/2698 [16:18<57:37,  1.65s/it]\u001b[A\n"," 22% 598/2698 [16:20<57:30,  1.64s/it]\u001b[A\n"," 22% 599/2698 [16:21<57:30,  1.64s/it]\u001b[A\n"," 22% 600/2698 [16:23<57:30,  1.64s/it]\u001b[A\n"," 22% 601/2698 [16:25<57:27,  1.64s/it]\u001b[A\n"," 22% 602/2698 [16:26<57:26,  1.64s/it]\u001b[A\n"," 22% 603/2698 [16:28<57:30,  1.65s/it]\u001b[A\n"," 22% 604/2698 [16:29<57:29,  1.65s/it]\u001b[A\n"," 22% 605/2698 [16:31<57:23,  1.65s/it]\u001b[A\n"," 22% 606/2698 [16:33<57:15,  1.64s/it]\u001b[A\n"," 22% 607/2698 [16:34<57:14,  1.64s/it]\u001b[A\n"," 23% 608/2698 [16:36<57:17,  1.64s/it]\u001b[A\n"," 23% 609/2698 [16:38<57:23,  1.65s/it]\u001b[A\n"," 23% 610/2698 [16:39<57:28,  1.65s/it]\u001b[A\n"," 23% 611/2698 [16:41<57:18,  1.65s/it]\u001b[A\n"," 23% 612/2698 [16:43<57:23,  1.65s/it]\u001b[A\n"," 23% 613/2698 [16:44<57:18,  1.65s/it]\u001b[A\n"," 23% 614/2698 [16:46<57:12,  1.65s/it]\u001b[A\n"," 23% 615/2698 [16:48<57:03,  1.64s/it]\u001b[A\n"," 23% 616/2698 [16:49<57:09,  1.65s/it]\u001b[A\n"," 23% 617/2698 [16:51<57:06,  1.65s/it]\u001b[A\n"," 23% 618/2698 [16:53<57:08,  1.65s/it]\u001b[A\n"," 23% 619/2698 [16:54<56:59,  1.64s/it]\u001b[A\n"," 23% 620/2698 [16:56<57:05,  1.65s/it]\u001b[A\n"," 23% 621/2698 [16:57<56:59,  1.65s/it]\u001b[A\n"," 23% 622/2698 [16:59<56:53,  1.64s/it]\u001b[A\n"," 23% 623/2698 [17:01<56:52,  1.64s/it]\u001b[A\n"," 23% 624/2698 [17:02<56:56,  1.65s/it]\u001b[A\n"," 23% 625/2698 [17:04<56:55,  1.65s/it]\u001b[A\n"," 23% 626/2698 [17:06<56:59,  1.65s/it]\u001b[A\n"," 23% 627/2698 [17:07<56:59,  1.65s/it]\u001b[A\n"," 23% 628/2698 [17:09<56:56,  1.65s/it]\u001b[A\n"," 23% 629/2698 [17:11<56:51,  1.65s/it]\u001b[A\n"," 23% 630/2698 [17:12<56:50,  1.65s/it]\u001b[A\n"," 23% 631/2698 [17:14<56:38,  1.64s/it]\u001b[A\n"," 23% 632/2698 [17:16<56:32,  1.64s/it]\u001b[A\n"," 23% 633/2698 [17:17<56:30,  1.64s/it]\u001b[A\n"," 23% 634/2698 [17:19<56:33,  1.64s/it]\u001b[A\n"," 24% 635/2698 [17:21<56:33,  1.64s/it]\u001b[A\n"," 24% 636/2698 [17:22<56:34,  1.65s/it]\u001b[A\n"," 24% 637/2698 [17:24<56:38,  1.65s/it]\u001b[A\n"," 24% 638/2698 [17:25<56:29,  1.65s/it]\u001b[A\n"," 24% 639/2698 [17:27<56:25,  1.64s/it]\u001b[A\n"," 24% 640/2698 [17:29<56:29,  1.65s/it]\u001b[A\n"," 24% 641/2698 [17:30<56:29,  1.65s/it]\u001b[A\n"," 24% 642/2698 [17:32<56:27,  1.65s/it]\u001b[A\n"," 24% 643/2698 [17:34<56:26,  1.65s/it]\u001b[A\n"," 24% 644/2698 [17:35<56:23,  1.65s/it]\u001b[A\n"," 24% 645/2698 [17:37<56:20,  1.65s/it]\u001b[A\n"," 24% 646/2698 [17:39<56:23,  1.65s/it]\u001b[A\n"," 24% 647/2698 [17:40<56:21,  1.65s/it]\u001b[A\n"," 24% 648/2698 [17:42<56:17,  1.65s/it]\u001b[A\n"," 24% 649/2698 [17:44<56:15,  1.65s/it]\u001b[A\n"," 24% 650/2698 [17:45<56:11,  1.65s/it]\u001b[A\n"," 24% 651/2698 [17:47<56:15,  1.65s/it]\u001b[A\n"," 24% 652/2698 [17:49<56:21,  1.65s/it]\u001b[A\n"," 24% 653/2698 [17:50<56:12,  1.65s/it]\u001b[A\n"," 24% 654/2698 [17:52<56:06,  1.65s/it]\u001b[A\n"," 24% 655/2698 [17:53<56:11,  1.65s/it]\u001b[A\n"," 24% 656/2698 [17:55<56:12,  1.65s/it]\u001b[A\n"," 24% 657/2698 [17:57<56:11,  1.65s/it]\u001b[A\n"," 24% 658/2698 [17:58<56:13,  1.65s/it]\u001b[A\n"," 24% 659/2698 [18:00<56:14,  1.66s/it]\u001b[A\n"," 24% 660/2698 [18:02<56:02,  1.65s/it]\u001b[A\n"," 24% 661/2698 [18:03<55:53,  1.65s/it]\u001b[A\n"," 25% 662/2698 [18:05<55:51,  1.65s/it]\u001b[A\n"," 25% 663/2698 [18:07<55:48,  1.65s/it]\u001b[A\n"," 25% 664/2698 [18:08<55:43,  1.64s/it]\u001b[A\n"," 25% 665/2698 [18:10<55:42,  1.64s/it]\u001b[A\n"," 25% 666/2698 [18:12<55:46,  1.65s/it]\u001b[A\n"," 25% 667/2698 [18:13<55:44,  1.65s/it]\u001b[A\n"," 25% 668/2698 [18:15<55:34,  1.64s/it]\u001b[A\n"," 25% 669/2698 [18:17<55:29,  1.64s/it]\u001b[A\n"," 25% 670/2698 [18:18<55:23,  1.64s/it]\u001b[A\n"," 25% 671/2698 [18:20<55:20,  1.64s/it]\u001b[A\n"," 25% 672/2698 [18:21<55:26,  1.64s/it]\u001b[A\n"," 25% 673/2698 [18:23<55:31,  1.64s/it]\u001b[A\n"," 25% 674/2698 [18:25<55:31,  1.65s/it]\u001b[A\n"," 25% 675/2698 [18:26<55:25,  1.64s/it]\u001b[A\n"," 25% 676/2698 [18:28<55:20,  1.64s/it]\u001b[A\n"," 25% 677/2698 [18:30<55:24,  1.65s/it]\u001b[A\n"," 25% 678/2698 [18:31<55:14,  1.64s/it]\u001b[A\n"," 25% 679/2698 [18:33<55:08,  1.64s/it]\u001b[A\n"," 25% 680/2698 [18:35<55:08,  1.64s/it]\u001b[A\n"," 25% 681/2698 [18:36<55:05,  1.64s/it]\u001b[A\n"," 25% 682/2698 [18:38<55:01,  1.64s/it]\u001b[A\n"," 25% 683/2698 [18:39<54:56,  1.64s/it]\u001b[A\n"," 25% 684/2698 [18:41<55:08,  1.64s/it]\u001b[A\n"," 25% 685/2698 [18:43<55:05,  1.64s/it]\u001b[A\n"," 25% 686/2698 [18:44<55:01,  1.64s/it]\u001b[A\n"," 25% 687/2698 [18:46<54:47,  1.63s/it]\u001b[A\n"," 26% 688/2698 [18:48<54:42,  1.63s/it]\u001b[A\n"," 26% 689/2698 [18:49<54:34,  1.63s/it]\u001b[A\n"," 26% 690/2698 [18:51<54:31,  1.63s/it]\u001b[A\n"," 26% 691/2698 [18:53<54:27,  1.63s/it]\u001b[A\n"," 26% 692/2698 [18:54<54:39,  1.63s/it]\u001b[A\n"," 26% 693/2698 [18:56<54:42,  1.64s/it]\u001b[A\n"," 26% 694/2698 [18:57<54:37,  1.64s/it]\u001b[A\n"," 26% 695/2698 [18:59<54:33,  1.63s/it]\u001b[A\n"," 26% 696/2698 [19:01<54:33,  1.64s/it]\u001b[A\n"," 26% 697/2698 [19:02<54:34,  1.64s/it]\u001b[A\n"," 26% 698/2698 [19:04<54:31,  1.64s/it]\u001b[A\n"," 26% 699/2698 [19:06<54:27,  1.63s/it]\u001b[A\n"," 26% 700/2698 [19:07<54:27,  1.64s/it]\u001b[A\n"," 26% 701/2698 [19:09<54:24,  1.63s/it]\u001b[A\n"," 26% 702/2698 [19:11<54:30,  1.64s/it]\u001b[A\n"," 26% 703/2698 [19:12<54:22,  1.64s/it]\u001b[A\n"," 26% 704/2698 [19:14<54:20,  1.64s/it]\u001b[A\n"," 26% 705/2698 [19:15<54:19,  1.64s/it]\u001b[A\n"," 26% 706/2698 [19:17<54:17,  1.64s/it]\u001b[A\n"," 26% 707/2698 [19:19<54:12,  1.63s/it]\u001b[A\n"," 26% 708/2698 [19:20<54:03,  1.63s/it]\u001b[A\n"," 26% 709/2698 [19:22<54:01,  1.63s/it]\u001b[A\n"," 26% 710/2698 [19:24<53:58,  1.63s/it]\u001b[A\n"," 26% 711/2698 [19:25<53:55,  1.63s/it]\u001b[A\n"," 26% 712/2698 [19:27<53:53,  1.63s/it]\u001b[A\n"," 26% 713/2698 [19:28<53:51,  1.63s/it]\u001b[A\n"," 26% 714/2698 [19:30<53:59,  1.63s/it]\u001b[A\n"," 27% 715/2698 [19:32<53:54,  1.63s/it]\u001b[A\n"," 27% 716/2698 [19:33<53:56,  1.63s/it]\u001b[A\n"," 27% 717/2698 [19:35<53:55,  1.63s/it]\u001b[A\n"," 27% 718/2698 [19:37<53:48,  1.63s/it]\u001b[A\n"," 27% 719/2698 [19:38<53:42,  1.63s/it]\u001b[A\n"," 27% 720/2698 [19:40<53:45,  1.63s/it]\u001b[A\n"," 27% 721/2698 [19:42<53:47,  1.63s/it]\u001b[A\n"," 27% 722/2698 [19:43<53:47,  1.63s/it]\u001b[A\n"," 27% 723/2698 [19:45<53:50,  1.64s/it]\u001b[A\n"," 27% 724/2698 [19:46<53:48,  1.64s/it]\u001b[A\n"," 27% 725/2698 [19:48<53:40,  1.63s/it]\u001b[A\n"," 27% 726/2698 [19:50<53:34,  1.63s/it]\u001b[A\n"," 27% 727/2698 [19:51<53:29,  1.63s/it]\u001b[A\n"," 27% 728/2698 [19:53<53:30,  1.63s/it]\u001b[A\n"," 27% 729/2698 [19:55<53:27,  1.63s/it]\u001b[A\n"," 27% 730/2698 [19:56<53:29,  1.63s/it]\u001b[A\n"," 27% 731/2698 [19:58<53:27,  1.63s/it]\u001b[A\n"," 27% 732/2698 [19:59<53:27,  1.63s/it]\u001b[A\n"," 27% 733/2698 [20:01<53:24,  1.63s/it]\u001b[A\n"," 27% 734/2698 [20:03<53:27,  1.63s/it]\u001b[A\n"," 27% 735/2698 [20:04<53:18,  1.63s/it]\u001b[A\n"," 27% 736/2698 [20:06<53:16,  1.63s/it]\u001b[A\n"," 27% 737/2698 [20:08<53:17,  1.63s/it]\u001b[A\n"," 27% 738/2698 [20:09<53:15,  1.63s/it]\u001b[A\n"," 27% 739/2698 [20:11<53:11,  1.63s/it]\u001b[A\n"," 27% 740/2698 [20:13<53:21,  1.64s/it]\u001b[A\n"," 27% 741/2698 [20:14<53:16,  1.63s/it]\u001b[A\n"," 28% 742/2698 [20:16<53:19,  1.64s/it]\u001b[A\n"," 28% 743/2698 [20:17<53:19,  1.64s/it]\u001b[A\n"," 28% 744/2698 [20:19<53:27,  1.64s/it]\u001b[A\n"," 28% 745/2698 [20:21<53:16,  1.64s/it]\u001b[A\n"," 28% 746/2698 [20:22<53:20,  1.64s/it]\u001b[A\n"," 28% 747/2698 [20:24<53:11,  1.64s/it]\u001b[A\n"," 28% 748/2698 [20:26<53:17,  1.64s/it]\u001b[A\n"," 28% 749/2698 [20:27<53:16,  1.64s/it]\u001b[A\n"," 28% 750/2698 [20:29<53:19,  1.64s/it]\u001b[A\n"," 28% 751/2698 [20:31<53:18,  1.64s/it]\u001b[A\n"," 28% 752/2698 [20:32<53:23,  1.65s/it]\u001b[A\n"," 28% 753/2698 [20:34<53:15,  1.64s/it]\u001b[A\n"," 28% 754/2698 [20:36<53:21,  1.65s/it]\u001b[A\n"," 28% 755/2698 [20:37<53:15,  1.64s/it]\u001b[A\n"," 28% 756/2698 [20:39<53:15,  1.65s/it]\u001b[A\n"," 28% 757/2698 [20:40<53:13,  1.65s/it]\u001b[A\n"," 28% 758/2698 [20:42<53:12,  1.65s/it]\u001b[A\n"," 28% 759/2698 [20:44<53:14,  1.65s/it]\u001b[A\n"," 28% 760/2698 [20:45<53:10,  1.65s/it]\u001b[A\n"," 28% 761/2698 [20:47<53:09,  1.65s/it]\u001b[A\n"," 28% 762/2698 [20:49<53:04,  1.64s/it]\u001b[A\n"," 28% 763/2698 [20:50<52:54,  1.64s/it]\u001b[A\n"," 28% 764/2698 [20:52<52:47,  1.64s/it]\u001b[A\n"," 28% 765/2698 [20:54<52:48,  1.64s/it]\u001b[A\n"," 28% 766/2698 [20:55<52:39,  1.64s/it]\u001b[A\n"," 28% 767/2698 [20:57<52:30,  1.63s/it]\u001b[A\n"," 28% 768/2698 [20:58<52:30,  1.63s/it]\u001b[A\n"," 29% 769/2698 [21:00<52:33,  1.63s/it]\u001b[A\n"," 29% 770/2698 [21:02<52:34,  1.64s/it]\u001b[A\n"," 29% 771/2698 [21:03<52:34,  1.64s/it]\u001b[A\n"," 29% 772/2698 [21:05<52:36,  1.64s/it]\u001b[A\n"," 29% 773/2698 [21:07<52:33,  1.64s/it]\u001b[A\n"," 29% 774/2698 [21:08<52:27,  1.64s/it]\u001b[A\n"," 29% 775/2698 [21:10<52:24,  1.64s/it]\u001b[A\n"," 29% 776/2698 [21:12<52:31,  1.64s/it]\u001b[A\n"," 29% 777/2698 [21:13<52:25,  1.64s/it]\u001b[A\n"," 29% 778/2698 [21:15<52:28,  1.64s/it]\u001b[A\n"," 29% 779/2698 [21:17<52:33,  1.64s/it]\u001b[A\n"," 29% 780/2698 [21:18<52:42,  1.65s/it]\u001b[A\n"," 29% 781/2698 [21:20<52:35,  1.65s/it]\u001b[A\n"," 29% 782/2698 [21:21<52:41,  1.65s/it]\u001b[A\n"," 29% 783/2698 [21:23<52:37,  1.65s/it]\u001b[A\n"," 29% 784/2698 [21:25<52:31,  1.65s/it]\u001b[A\n"," 29% 785/2698 [21:26<52:33,  1.65s/it]\u001b[A\n"," 29% 786/2698 [21:28<52:22,  1.64s/it]\u001b[A\n"," 29% 787/2698 [21:30<52:12,  1.64s/it]\u001b[A\n"," 29% 788/2698 [21:31<52:09,  1.64s/it]\u001b[A\n"," 29% 789/2698 [21:33<52:05,  1.64s/it]\u001b[A\n"," 29% 790/2698 [21:35<52:08,  1.64s/it]\u001b[A\n"," 29% 791/2698 [21:36<52:02,  1.64s/it]\u001b[A\n"," 29% 792/2698 [21:38<52:00,  1.64s/it]\u001b[A\n"," 29% 793/2698 [21:40<52:03,  1.64s/it]\u001b[A\n"," 29% 794/2698 [21:41<52:06,  1.64s/it]\u001b[A\n"," 29% 795/2698 [21:43<52:08,  1.64s/it]\u001b[A\n"," 30% 796/2698 [21:44<52:07,  1.64s/it]\u001b[A\n"," 30% 797/2698 [21:46<52:06,  1.64s/it]\u001b[A\n"," 30% 798/2698 [21:48<52:06,  1.65s/it]\u001b[A\n"," 30% 799/2698 [21:49<52:03,  1.64s/it]\u001b[A\n"," 30% 800/2698 [21:51<51:52,  1.64s/it]\u001b[A\n"," 30% 801/2698 [21:53<51:52,  1.64s/it]\u001b[A\n"," 30% 802/2698 [21:54<51:55,  1.64s/it]\u001b[A\n"," 30% 803/2698 [21:56<51:53,  1.64s/it]\u001b[A\n"," 30% 804/2698 [21:58<51:52,  1.64s/it]\u001b[A\n"," 30% 805/2698 [21:59<51:56,  1.65s/it]\u001b[A\n"," 30% 806/2698 [22:01<51:59,  1.65s/it]\u001b[A\n"," 30% 807/2698 [22:03<51:53,  1.65s/it]\u001b[A\n"," 30% 808/2698 [22:04<51:54,  1.65s/it]\u001b[A\n"," 30% 809/2698 [22:06<51:55,  1.65s/it]\u001b[A\n"," 30% 810/2698 [22:08<51:58,  1.65s/it]\u001b[A\n"," 30% 811/2698 [22:09<51:56,  1.65s/it]\u001b[A\n"," 30% 812/2698 [22:11<51:49,  1.65s/it]\u001b[A\n"," 30% 813/2698 [22:12<51:48,  1.65s/it]\u001b[A\n"," 30% 814/2698 [22:14<51:45,  1.65s/it]\u001b[A\n"," 30% 815/2698 [22:16<51:42,  1.65s/it]\u001b[A\n"," 30% 816/2698 [22:17<51:40,  1.65s/it]\u001b[A\n"," 30% 817/2698 [22:19<51:41,  1.65s/it]\u001b[A\n"," 30% 818/2698 [22:21<51:40,  1.65s/it]\u001b[A\n"," 30% 819/2698 [22:22<51:29,  1.64s/it]\u001b[A\n"," 30% 820/2698 [22:24<51:32,  1.65s/it]\u001b[A\n"," 30% 821/2698 [22:26<51:34,  1.65s/it]\u001b[A\n"," 30% 822/2698 [22:27<51:27,  1.65s/it]\u001b[A\n"," 31% 823/2698 [22:29<51:19,  1.64s/it]\u001b[A\n"," 31% 824/2698 [22:31<51:16,  1.64s/it]\u001b[A\n"," 31% 825/2698 [22:32<51:19,  1.64s/it]\u001b[A\n"," 31% 826/2698 [22:34<51:16,  1.64s/it]\u001b[A\n"," 31% 827/2698 [22:36<51:24,  1.65s/it]\u001b[A\n"," 31% 828/2698 [22:37<51:20,  1.65s/it]\u001b[A\n"," 31% 829/2698 [22:39<51:22,  1.65s/it]\u001b[A\n"," 31% 830/2698 [22:40<51:15,  1.65s/it]\u001b[A\n"," 31% 831/2698 [22:42<51:07,  1.64s/it]\u001b[A\n"," 31% 832/2698 [22:44<51:05,  1.64s/it]\u001b[A\n"," 31% 833/2698 [22:45<51:06,  1.64s/it]\u001b[A\n"," 31% 834/2698 [22:47<51:09,  1.65s/it]\u001b[A\n"," 31% 835/2698 [22:49<51:13,  1.65s/it]\u001b[A\n"," 31% 836/2698 [22:50<51:10,  1.65s/it]\u001b[A\n"," 31% 837/2698 [22:52<51:06,  1.65s/it]\u001b[A\n"," 31% 838/2698 [22:54<50:57,  1.64s/it]\u001b[A\n"," 31% 839/2698 [22:55<51:00,  1.65s/it]\u001b[A\n"," 31% 840/2698 [22:57<50:55,  1.64s/it]\u001b[A\n"," 31% 841/2698 [22:59<51:01,  1.65s/it]\u001b[A\n"," 31% 842/2698 [23:00<51:01,  1.65s/it]\u001b[A\n"," 31% 843/2698 [23:02<51:07,  1.65s/it]\u001b[A\n"," 31% 844/2698 [23:04<50:57,  1.65s/it]\u001b[A\n"," 31% 845/2698 [23:05<50:44,  1.64s/it]\u001b[A\n"," 31% 846/2698 [23:07<50:38,  1.64s/it]\u001b[A\n"," 31% 847/2698 [23:08<50:31,  1.64s/it]\u001b[A\n"," 31% 848/2698 [23:10<50:27,  1.64s/it]\u001b[A\n"," 31% 849/2698 [23:12<50:30,  1.64s/it]\u001b[A\n"," 32% 850/2698 [23:13<50:31,  1.64s/it]\u001b[A\n"," 32% 851/2698 [23:15<50:18,  1.63s/it]\u001b[A\n"," 32% 852/2698 [23:17<50:23,  1.64s/it]\u001b[A\n"," 32% 853/2698 [23:18<50:30,  1.64s/it]\u001b[A\n"," 32% 854/2698 [23:20<50:28,  1.64s/it]\u001b[A\n"," 32% 855/2698 [23:22<50:23,  1.64s/it]\u001b[A\n"," 32% 856/2698 [23:23<50:23,  1.64s/it]\u001b[A\n"," 32% 857/2698 [23:25<50:18,  1.64s/it]\u001b[A\n"," 32% 858/2698 [23:26<50:12,  1.64s/it]\u001b[A\n"," 32% 859/2698 [23:28<50:10,  1.64s/it]\u001b[A\n"," 32% 860/2698 [23:30<50:13,  1.64s/it]\u001b[A\n"," 32% 861/2698 [23:31<50:09,  1.64s/it]\u001b[A\n"," 32% 862/2698 [23:33<49:59,  1.63s/it]\u001b[A\n"," 32% 863/2698 [23:35<49:55,  1.63s/it]\u001b[A\n"," 32% 864/2698 [23:36<49:57,  1.63s/it]\u001b[A\n"," 32% 865/2698 [23:38<50:06,  1.64s/it]\u001b[A\n"," 32% 866/2698 [23:40<50:06,  1.64s/it]\u001b[A\n"," 32% 867/2698 [23:41<50:05,  1.64s/it]\u001b[A\n"," 32% 868/2698 [23:43<50:05,  1.64s/it]\u001b[A\n"," 32% 869/2698 [23:44<49:55,  1.64s/it]\u001b[A\n"," 32% 870/2698 [23:46<49:53,  1.64s/it]\u001b[A\n"," 32% 871/2698 [23:48<49:44,  1.63s/it]\u001b[A\n"," 32% 872/2698 [23:49<49:47,  1.64s/it]\u001b[A\n"," 32% 873/2698 [23:51<49:42,  1.63s/it]\u001b[A\n"," 32% 874/2698 [23:53<49:32,  1.63s/it]\u001b[A\n"," 32% 875/2698 [23:54<49:29,  1.63s/it]\u001b[A\n"," 32% 876/2698 [23:56<49:32,  1.63s/it]\u001b[A\n"," 33% 877/2698 [23:58<49:26,  1.63s/it]\u001b[A\n"," 33% 878/2698 [23:59<49:24,  1.63s/it]\u001b[A\n"," 33% 879/2698 [24:01<49:26,  1.63s/it]\u001b[A\n"," 33% 880/2698 [24:02<49:26,  1.63s/it]\u001b[A\n"," 33% 881/2698 [24:04<49:23,  1.63s/it]\u001b[A\n"," 33% 882/2698 [24:06<49:28,  1.63s/it]\u001b[A\n"," 33% 883/2698 [24:07<49:23,  1.63s/it]\u001b[A\n"," 33% 884/2698 [24:09<49:21,  1.63s/it]\u001b[A\n"," 33% 885/2698 [24:11<49:15,  1.63s/it]\u001b[A\n"," 33% 886/2698 [24:12<49:10,  1.63s/it]\u001b[A\n"," 33% 887/2698 [24:14<49:08,  1.63s/it]\u001b[A\n"," 33% 888/2698 [24:15<49:16,  1.63s/it]\u001b[A\n"," 33% 889/2698 [24:17<49:15,  1.63s/it]\u001b[A\n"," 33% 890/2698 [24:19<49:07,  1.63s/it]\u001b[A\n"," 33% 891/2698 [24:20<49:08,  1.63s/it]\u001b[A\n"," 33% 892/2698 [24:22<49:11,  1.63s/it]\u001b[A\n"," 33% 893/2698 [24:24<49:15,  1.64s/it]\u001b[A\n"," 33% 894/2698 [24:25<49:09,  1.64s/it]\u001b[A\n"," 33% 895/2698 [24:27<49:02,  1.63s/it]\u001b[A\n"," 33% 896/2698 [24:29<49:03,  1.63s/it]\u001b[A\n"," 33% 897/2698 [24:30<49:00,  1.63s/it]\u001b[A\n"," 33% 898/2698 [24:32<48:51,  1.63s/it]\u001b[A\n"," 33% 899/2698 [24:33<48:52,  1.63s/it]\u001b[A\n"," 33% 900/2698 [24:35<48:51,  1.63s/it]\u001b[A\n"," 33% 901/2698 [24:37<48:53,  1.63s/it]\u001b[A\n"," 33% 902/2698 [24:38<48:47,  1.63s/it]\u001b[A\n"," 33% 903/2698 [24:40<48:50,  1.63s/it]\u001b[A\n"," 34% 904/2698 [24:42<48:46,  1.63s/it]\u001b[A\n"," 34% 905/2698 [24:43<48:46,  1.63s/it]\u001b[A\n"," 34% 906/2698 [24:45<48:39,  1.63s/it]\u001b[A\n"," 34% 907/2698 [24:46<48:31,  1.63s/it]\u001b[A\n"," 34% 908/2698 [24:48<48:29,  1.63s/it]\u001b[A\n"," 34% 909/2698 [24:50<48:41,  1.63s/it]\u001b[A\n"," 34% 910/2698 [24:51<48:36,  1.63s/it]\u001b[A\n"," 34% 911/2698 [24:53<48:47,  1.64s/it]\u001b[A\n"," 34% 912/2698 [24:55<48:43,  1.64s/it]\u001b[A\n"," 34% 913/2698 [24:56<48:46,  1.64s/it]\u001b[A\n"," 34% 914/2698 [24:58<48:47,  1.64s/it]\u001b[A\n"," 34% 915/2698 [25:00<48:43,  1.64s/it]\u001b[A\n"," 34% 916/2698 [25:01<48:37,  1.64s/it]\u001b[A\n"," 34% 917/2698 [25:03<48:36,  1.64s/it]\u001b[A\n"," 34% 918/2698 [25:04<48:32,  1.64s/it]\u001b[A\n"," 34% 919/2698 [25:06<48:38,  1.64s/it]\u001b[A\n"," 34% 920/2698 [25:08<48:40,  1.64s/it]\u001b[A\n"," 34% 921/2698 [25:09<48:39,  1.64s/it]\u001b[A\n"," 34% 922/2698 [25:11<48:36,  1.64s/it]\u001b[A\n"," 34% 923/2698 [25:13<48:42,  1.65s/it]\u001b[A\n"," 34% 924/2698 [25:14<48:39,  1.65s/it]\u001b[A\n"," 34% 925/2698 [25:16<48:39,  1.65s/it]\u001b[A\n"," 34% 926/2698 [25:18<48:40,  1.65s/it]\u001b[A\n"," 34% 927/2698 [25:19<48:38,  1.65s/it]\u001b[A\n"," 34% 928/2698 [25:21<48:29,  1.64s/it]\u001b[A\n"," 34% 929/2698 [25:23<48:31,  1.65s/it]\u001b[A\n"," 34% 930/2698 [25:24<48:28,  1.65s/it]\u001b[A\n"," 35% 931/2698 [25:26<48:21,  1.64s/it]\u001b[A\n"," 35% 932/2698 [25:27<48:16,  1.64s/it]\u001b[A\n"," 35% 933/2698 [25:29<48:15,  1.64s/it]\u001b[A\n"," 35% 934/2698 [25:31<48:22,  1.65s/it]\u001b[A\n"," 35% 935/2698 [25:32<48:14,  1.64s/it]\u001b[A\n"," 35% 936/2698 [25:34<48:16,  1.64s/it]\u001b[A\n"," 35% 937/2698 [25:36<48:19,  1.65s/it]\u001b[A\n"," 35% 938/2698 [25:37<48:15,  1.65s/it]\u001b[A\n"," 35% 939/2698 [25:39<48:16,  1.65s/it]\u001b[A\n"," 35% 940/2698 [25:41<48:14,  1.65s/it]\u001b[A\n"," 35% 941/2698 [25:42<48:14,  1.65s/it]\u001b[A\n"," 35% 942/2698 [25:44<48:16,  1.65s/it]\u001b[A\n"," 35% 943/2698 [25:46<48:16,  1.65s/it]\u001b[A\n"," 35% 944/2698 [25:47<48:12,  1.65s/it]\u001b[A\n"," 35% 945/2698 [25:49<48:20,  1.65s/it]\u001b[A\n"," 35% 946/2698 [25:51<48:14,  1.65s/it]\u001b[A\n"," 35% 947/2698 [25:52<48:16,  1.65s/it]\u001b[A\n"," 35% 948/2698 [25:54<48:13,  1.65s/it]\u001b[A\n"," 35% 949/2698 [25:56<48:16,  1.66s/it]\u001b[A\n"," 35% 950/2698 [25:57<48:11,  1.65s/it]\u001b[A\n"," 35% 951/2698 [25:59<48:09,  1.65s/it]\u001b[A\n"," 35% 952/2698 [26:01<48:07,  1.65s/it]\u001b[A\n"," 35% 953/2698 [26:02<48:03,  1.65s/it]\u001b[A\n"," 35% 954/2698 [26:04<48:02,  1.65s/it]\u001b[A\n"," 35% 955/2698 [26:05<48:01,  1.65s/it]\u001b[A\n"," 35% 956/2698 [26:07<47:52,  1.65s/it]\u001b[A\n"," 35% 957/2698 [26:09<47:48,  1.65s/it]\u001b[A\n"," 36% 958/2698 [26:10<47:47,  1.65s/it]\u001b[A\n"," 36% 959/2698 [26:12<47:53,  1.65s/it]\u001b[A\n"," 36% 960/2698 [26:14<47:45,  1.65s/it]\u001b[A\n"," 36% 961/2698 [26:15<47:41,  1.65s/it]\u001b[A\n"," 36% 962/2698 [26:17<47:47,  1.65s/it]\u001b[A\n"," 36% 963/2698 [26:19<47:49,  1.65s/it]\u001b[A\n"," 36% 964/2698 [26:20<47:40,  1.65s/it]\u001b[A\n"," 36% 965/2698 [26:22<47:45,  1.65s/it]\u001b[A\n"," 36% 966/2698 [26:24<47:37,  1.65s/it]\u001b[A\n"," 36% 967/2698 [26:25<47:28,  1.65s/it]\u001b[A\n"," 36% 968/2698 [26:27<47:27,  1.65s/it]\u001b[A\n"," 36% 969/2698 [26:29<47:22,  1.64s/it]\u001b[A\n"," 36% 970/2698 [26:30<47:27,  1.65s/it]\u001b[A\n"," 36% 971/2698 [26:32<47:28,  1.65s/it]\u001b[A\n"," 36% 972/2698 [26:33<47:27,  1.65s/it]\u001b[A\n"," 36% 973/2698 [26:35<47:27,  1.65s/it]\u001b[A\n"," 36% 974/2698 [26:37<47:25,  1.65s/it]\u001b[A\n"," 36% 975/2698 [26:38<47:25,  1.65s/it]\u001b[A\n"," 36% 976/2698 [26:40<47:19,  1.65s/it]\u001b[A\n"," 36% 977/2698 [26:42<47:16,  1.65s/it]\u001b[A\n"," 36% 978/2698 [26:43<47:16,  1.65s/it]\u001b[A\n"," 36% 979/2698 [26:45<47:21,  1.65s/it]\u001b[A\n"," 36% 980/2698 [26:47<47:20,  1.65s/it]\u001b[A\n"," 36% 981/2698 [26:48<47:20,  1.65s/it]\u001b[A\n"," 36% 982/2698 [26:50<47:12,  1.65s/it]\u001b[A\n"," 36% 983/2698 [26:52<47:09,  1.65s/it]\u001b[A\n"," 36% 984/2698 [26:53<46:59,  1.65s/it]\u001b[A\n"," 37% 985/2698 [26:55<47:05,  1.65s/it]\u001b[A\n"," 37% 986/2698 [26:57<47:06,  1.65s/it]\u001b[A\n"," 37% 987/2698 [26:58<47:00,  1.65s/it]\u001b[A\n"," 37% 988/2698 [27:00<46:56,  1.65s/it]\u001b[A\n"," 37% 989/2698 [27:02<46:58,  1.65s/it]\u001b[A\n"," 37% 990/2698 [27:03<46:53,  1.65s/it]\u001b[A\n"," 37% 991/2698 [27:05<46:44,  1.64s/it]\u001b[A\n"," 37% 992/2698 [27:06<46:43,  1.64s/it]\u001b[A\n"," 37% 993/2698 [27:08<46:36,  1.64s/it]\u001b[A\n"," 37% 994/2698 [27:10<46:40,  1.64s/it]\u001b[A\n"," 37% 995/2698 [27:11<46:44,  1.65s/it]\u001b[A\n"," 37% 996/2698 [27:13<46:44,  1.65s/it]\u001b[A\n"," 37% 997/2698 [27:15<46:35,  1.64s/it]\u001b[A\n"," 37% 998/2698 [27:16<46:32,  1.64s/it]\u001b[A\n"," 37% 999/2698 [27:18<46:31,  1.64s/it]\u001b[A\n"," 37% 1000/2698 [27:20<46:27,  1.64s/it]\u001b[A\n"," 37% 1001/2698 [27:21<46:25,  1.64s/it]\u001b[A\n"," 37% 1002/2698 [27:23<46:24,  1.64s/it]\u001b[A\n"," 37% 1003/2698 [27:25<46:24,  1.64s/it]\u001b[A\n"," 37% 1004/2698 [27:26<46:24,  1.64s/it]\u001b[A\n"," 37% 1005/2698 [27:28<46:12,  1.64s/it]\u001b[A\n"," 37% 1006/2698 [27:29<46:17,  1.64s/it]\u001b[A\n"," 37% 1007/2698 [27:31<46:13,  1.64s/it]\u001b[A\n"," 37% 1008/2698 [27:33<46:06,  1.64s/it]\u001b[A\n"," 37% 1009/2698 [27:34<46:06,  1.64s/it]\u001b[A\n"," 37% 1010/2698 [27:36<46:02,  1.64s/it]\u001b[A\n"," 37% 1011/2698 [27:38<46:03,  1.64s/it]\u001b[A\n"," 38% 1012/2698 [27:39<46:11,  1.64s/it]\u001b[A\n"," 38% 1013/2698 [27:41<46:05,  1.64s/it]\u001b[A\n"," 38% 1014/2698 [27:43<46:01,  1.64s/it]\u001b[A\n"," 38% 1015/2698 [27:44<45:53,  1.64s/it]\u001b[A\n"," 38% 1016/2698 [27:46<45:52,  1.64s/it]\u001b[A\n"," 38% 1017/2698 [27:47<45:56,  1.64s/it]\u001b[A\n"," 38% 1018/2698 [27:49<45:54,  1.64s/it]\u001b[A\n"," 38% 1019/2698 [27:51<45:53,  1.64s/it]\u001b[A\n"," 38% 1020/2698 [27:52<45:45,  1.64s/it]\u001b[A\n"," 38% 1021/2698 [27:54<45:48,  1.64s/it]\u001b[A\n"," 38% 1022/2698 [27:56<45:47,  1.64s/it]\u001b[A\n"," 38% 1023/2698 [27:57<45:43,  1.64s/it]\u001b[A\n"," 38% 1024/2698 [27:59<45:39,  1.64s/it]\u001b[A\n"," 38% 1025/2698 [28:01<45:38,  1.64s/it]\u001b[A\n"," 38% 1026/2698 [28:02<45:41,  1.64s/it]\u001b[A\n"," 38% 1027/2698 [28:04<45:36,  1.64s/it]\u001b[A\n"," 38% 1028/2698 [28:05<45:36,  1.64s/it]\u001b[A\n"," 38% 1029/2698 [28:07<45:41,  1.64s/it]\u001b[A\n"," 38% 1030/2698 [28:09<45:31,  1.64s/it]\u001b[A\n"," 38% 1031/2698 [28:10<45:29,  1.64s/it]\u001b[A\n"," 38% 1032/2698 [28:12<45:25,  1.64s/it]\u001b[A\n"," 38% 1033/2698 [28:14<45:28,  1.64s/it]\u001b[A\n"," 38% 1034/2698 [28:15<45:27,  1.64s/it]\u001b[A\n"," 38% 1035/2698 [28:17<45:28,  1.64s/it]\u001b[A\n"," 38% 1036/2698 [28:19<45:27,  1.64s/it]\u001b[A\n"," 38% 1037/2698 [28:20<45:25,  1.64s/it]\u001b[A\n"," 38% 1038/2698 [28:22<45:19,  1.64s/it]\u001b[A\n"," 39% 1039/2698 [28:24<45:15,  1.64s/it]\u001b[A\n"," 39% 1040/2698 [28:25<45:12,  1.64s/it]\u001b[A\n"," 39% 1041/2698 [28:27<45:08,  1.63s/it]\u001b[A\n"," 39% 1042/2698 [28:28<45:08,  1.64s/it]\u001b[A\n"," 39% 1043/2698 [28:30<45:08,  1.64s/it]\u001b[A\n"," 39% 1044/2698 [28:32<45:01,  1.63s/it]\u001b[A\n"," 39% 1045/2698 [28:33<44:57,  1.63s/it]\u001b[A\n"," 39% 1046/2698 [28:35<44:53,  1.63s/it]\u001b[A\n"," 39% 1047/2698 [28:37<44:53,  1.63s/it]\u001b[A\n"," 39% 1048/2698 [28:38<44:48,  1.63s/it]\u001b[A\n"," 39% 1049/2698 [28:40<44:51,  1.63s/it]\u001b[A\n"," 39% 1050/2698 [28:41<44:51,  1.63s/it]\u001b[A\n"," 39% 1051/2698 [28:43<44:49,  1.63s/it]\u001b[A\n"," 39% 1052/2698 [28:45<44:54,  1.64s/it]\u001b[A\n"," 39% 1053/2698 [28:46<44:56,  1.64s/it]\u001b[A\n"," 39% 1054/2698 [28:48<44:51,  1.64s/it]\u001b[A\n"," 39% 1055/2698 [28:50<44:43,  1.63s/it]\u001b[A\n"," 39% 1056/2698 [28:51<44:40,  1.63s/it]\u001b[A\n"," 39% 1057/2698 [28:53<44:39,  1.63s/it]\u001b[A\n"," 39% 1058/2698 [28:55<44:36,  1.63s/it]\u001b[A\n"," 39% 1059/2698 [28:56<44:34,  1.63s/it]\u001b[A\n"," 39% 1060/2698 [28:58<44:36,  1.63s/it]\u001b[A\n"," 39% 1061/2698 [28:59<44:37,  1.64s/it]\u001b[A\n"," 39% 1062/2698 [29:01<44:34,  1.63s/it]\u001b[A\n"," 39% 1063/2698 [29:03<44:33,  1.64s/it]\u001b[A\n"," 39% 1064/2698 [29:04<44:30,  1.63s/it]\u001b[A\n"," 39% 1065/2698 [29:06<44:33,  1.64s/it]\u001b[A\n"," 40% 1066/2698 [29:08<44:30,  1.64s/it]\u001b[A\n"," 40% 1067/2698 [29:09<44:25,  1.63s/it]\u001b[A\n"," 40% 1068/2698 [29:11<44:30,  1.64s/it]\u001b[A\n"," 40% 1069/2698 [29:13<44:30,  1.64s/it]\u001b[A\n"," 40% 1070/2698 [29:14<44:23,  1.64s/it]\u001b[A\n"," 40% 1071/2698 [29:16<44:17,  1.63s/it]\u001b[A\n"," 40% 1072/2698 [29:17<44:10,  1.63s/it]\u001b[A\n"," 40% 1073/2698 [29:19<44:03,  1.63s/it]\u001b[A\n"," 40% 1074/2698 [29:21<44:00,  1.63s/it]\u001b[A\n"," 40% 1075/2698 [29:22<44:03,  1.63s/it]\u001b[A\n"," 40% 1076/2698 [29:24<44:04,  1.63s/it]\u001b[A\n"," 40% 1077/2698 [29:26<44:07,  1.63s/it]\u001b[A\n"," 40% 1078/2698 [29:27<44:09,  1.64s/it]\u001b[A\n"," 40% 1079/2698 [29:29<44:14,  1.64s/it]\u001b[A\n"," 40% 1080/2698 [29:31<44:08,  1.64s/it]\u001b[A\n"," 40% 1081/2698 [29:32<44:19,  1.64s/it]\u001b[A\n"," 40% 1082/2698 [29:34<44:14,  1.64s/it]\u001b[A\n"," 40% 1083/2698 [29:35<44:17,  1.65s/it]\u001b[A\n"," 40% 1084/2698 [29:37<44:15,  1.65s/it]\u001b[A\n"," 40% 1085/2698 [29:39<44:12,  1.64s/it]\u001b[A\n"," 40% 1086/2698 [29:40<44:14,  1.65s/it]\u001b[A\n"," 40% 1087/2698 [29:42<44:17,  1.65s/it]\u001b[A\n"," 40% 1088/2698 [29:44<44:24,  1.66s/it]\u001b[A\n"," 40% 1089/2698 [29:45<44:24,  1.66s/it]\u001b[A\n"," 40% 1090/2698 [29:47<44:21,  1.66s/it]\u001b[A\n"," 40% 1091/2698 [29:49<44:20,  1.66s/it]\u001b[A\n"," 40% 1092/2698 [29:50<44:13,  1.65s/it]\u001b[A\n"," 41% 1093/2698 [29:52<44:09,  1.65s/it]\u001b[A\n"," 41% 1094/2698 [29:54<44:11,  1.65s/it]\u001b[A\n"," 41% 1095/2698 [29:55<44:19,  1.66s/it]\u001b[A\n"," 41% 1096/2698 [29:57<44:17,  1.66s/it]\u001b[A\n"," 41% 1097/2698 [29:59<44:10,  1.66s/it]\u001b[A\n"," 41% 1098/2698 [30:00<44:07,  1.65s/it]\u001b[A\n"," 41% 1099/2698 [30:02<44:00,  1.65s/it]\u001b[A\n"," 41% 1100/2698 [30:04<43:58,  1.65s/it]\u001b[A\n"," 41% 1101/2698 [30:05<43:54,  1.65s/it]\u001b[A\n"," 41% 1102/2698 [30:07<43:50,  1.65s/it]\u001b[A\n"," 41% 1103/2698 [30:09<43:52,  1.65s/it]\u001b[A\n"," 41% 1104/2698 [30:10<43:41,  1.64s/it]\u001b[A\n"," 41% 1105/2698 [30:12<43:36,  1.64s/it]\u001b[A\n"," 41% 1106/2698 [30:13<43:30,  1.64s/it]\u001b[A\n"," 41% 1107/2698 [30:15<43:34,  1.64s/it]\u001b[A\n"," 41% 1108/2698 [30:17<43:30,  1.64s/it]\u001b[A\n"," 41% 1109/2698 [30:18<43:33,  1.64s/it]\u001b[A\n"," 41% 1110/2698 [30:20<43:30,  1.64s/it]\u001b[A\n"," 41% 1111/2698 [30:22<43:26,  1.64s/it]\u001b[A\n"," 41% 1112/2698 [30:23<43:19,  1.64s/it]\u001b[A\n"," 41% 1113/2698 [30:25<43:23,  1.64s/it]\u001b[A\n"," 41% 1114/2698 [30:27<43:18,  1.64s/it]\u001b[A\n"," 41% 1115/2698 [30:28<43:16,  1.64s/it]\u001b[A\n"," 41% 1116/2698 [30:30<43:19,  1.64s/it]\u001b[A\n"," 41% 1117/2698 [30:31<43:13,  1.64s/it]\u001b[A\n"," 41% 1118/2698 [30:33<43:23,  1.65s/it]\u001b[A\n"," 41% 1119/2698 [30:35<43:15,  1.64s/it]\u001b[A\n"," 42% 1120/2698 [30:36<43:18,  1.65s/it]\u001b[A\n"," 42% 1121/2698 [30:38<43:10,  1.64s/it]\u001b[A\n"," 42% 1122/2698 [30:40<43:13,  1.65s/it]\u001b[A\n"," 42% 1123/2698 [30:41<43:07,  1.64s/it]\u001b[A\n"," 42% 1124/2698 [30:43<43:09,  1.65s/it]\u001b[A\n"," 42% 1125/2698 [30:45<43:02,  1.64s/it]\u001b[A\n"," 42% 1126/2698 [30:46<42:58,  1.64s/it]\u001b[A\n"," 42% 1127/2698 [30:48<42:56,  1.64s/it]\u001b[A\n"," 42% 1128/2698 [30:50<42:53,  1.64s/it]\u001b[A\n"," 42% 1129/2698 [30:51<42:47,  1.64s/it]\u001b[A\n"," 42% 1130/2698 [30:53<42:48,  1.64s/it]\u001b[A\n"," 42% 1131/2698 [30:54<42:44,  1.64s/it]\u001b[A\n"," 42% 1132/2698 [30:56<42:49,  1.64s/it]\u001b[A\n"," 42% 1133/2698 [30:58<42:50,  1.64s/it]\u001b[A\n"," 42% 1134/2698 [30:59<42:51,  1.64s/it]\u001b[A\n"," 42% 1135/2698 [31:01<42:50,  1.64s/it]\u001b[A\n"," 42% 1136/2698 [31:03<42:51,  1.65s/it]\u001b[A\n"," 42% 1137/2698 [31:04<42:48,  1.65s/it]\u001b[A\n"," 42% 1138/2698 [31:06<42:44,  1.64s/it]\u001b[A\n"," 42% 1139/2698 [31:08<42:37,  1.64s/it]\u001b[A\n"," 42% 1140/2698 [31:09<42:37,  1.64s/it]\u001b[A\n"," 42% 1141/2698 [31:11<42:38,  1.64s/it]\u001b[A\n"," 42% 1142/2698 [31:13<42:42,  1.65s/it]\u001b[A\n"," 42% 1143/2698 [31:14<42:39,  1.65s/it]\u001b[A\n"," 42% 1144/2698 [31:16<42:34,  1.64s/it]\u001b[A\n"," 42% 1145/2698 [31:17<42:31,  1.64s/it]\u001b[A\n"," 42% 1146/2698 [31:19<42:34,  1.65s/it]\u001b[A\n"," 43% 1147/2698 [31:21<42:38,  1.65s/it]\u001b[A\n"," 43% 1148/2698 [31:22<42:35,  1.65s/it]\u001b[A\n"," 43% 1149/2698 [31:24<42:31,  1.65s/it]\u001b[A\n"," 43% 1150/2698 [31:26<42:26,  1.64s/it]\u001b[A\n"," 43% 1151/2698 [31:27<42:27,  1.65s/it]\u001b[A\n"," 43% 1152/2698 [31:29<42:26,  1.65s/it]\u001b[A\n"," 43% 1153/2698 [31:31<42:14,  1.64s/it]\u001b[A\n"," 43% 1154/2698 [31:32<42:17,  1.64s/it]\u001b[A\n"," 43% 1155/2698 [31:34<42:15,  1.64s/it]\u001b[A\n"," 43% 1156/2698 [31:36<42:13,  1.64s/it]\u001b[A\n"," 43% 1157/2698 [31:37<42:21,  1.65s/it]\u001b[A\n"," 43% 1158/2698 [31:39<42:18,  1.65s/it]\u001b[A\n"," 43% 1159/2698 [31:41<42:14,  1.65s/it]\u001b[A\n"," 43% 1160/2698 [31:42<42:14,  1.65s/it]\u001b[A\n"," 43% 1161/2698 [31:44<42:12,  1.65s/it]\u001b[A\n"," 43% 1162/2698 [31:45<42:05,  1.64s/it]\u001b[A\n"," 43% 1163/2698 [31:47<42:08,  1.65s/it]\u001b[A\n"," 43% 1164/2698 [31:49<42:06,  1.65s/it]\u001b[A\n"," 43% 1165/2698 [31:50<42:07,  1.65s/it]\u001b[A\n"," 43% 1166/2698 [31:52<41:58,  1.64s/it]\u001b[A\n"," 43% 1167/2698 [31:54<41:58,  1.65s/it]\u001b[A\n"," 43% 1168/2698 [31:55<42:01,  1.65s/it]\u001b[A\n"," 43% 1169/2698 [31:57<42:01,  1.65s/it]\u001b[A\n"," 43% 1170/2698 [31:59<42:00,  1.65s/it]\u001b[A\n"," 43% 1171/2698 [32:00<41:55,  1.65s/it]\u001b[A\n"," 43% 1172/2698 [32:02<41:58,  1.65s/it]\u001b[A\n"," 43% 1173/2698 [32:04<41:57,  1.65s/it]\u001b[A\n"," 44% 1174/2698 [32:05<41:52,  1.65s/it]\u001b[A\n"," 44% 1175/2698 [32:07<41:45,  1.64s/it]\u001b[A\n"," 44% 1176/2698 [32:09<41:38,  1.64s/it]\u001b[A\n"," 44% 1177/2698 [32:10<41:39,  1.64s/it]\u001b[A\n"," 44% 1178/2698 [32:12<41:35,  1.64s/it]\u001b[A\n"," 44% 1179/2698 [32:13<41:29,  1.64s/it]\u001b[A\n"," 44% 1180/2698 [32:15<41:31,  1.64s/it]\u001b[A\n"," 44% 1181/2698 [32:17<41:27,  1.64s/it]\u001b[A\n"," 44% 1182/2698 [32:18<41:16,  1.63s/it]\u001b[A\n"," 44% 1183/2698 [32:20<41:15,  1.63s/it]\u001b[A\n"," 44% 1184/2698 [32:22<41:10,  1.63s/it]\u001b[A\n"," 44% 1185/2698 [32:23<41:07,  1.63s/it]\u001b[A\n"," 44% 1186/2698 [32:25<41:07,  1.63s/it]\u001b[A\n"," 44% 1187/2698 [32:27<41:04,  1.63s/it]\u001b[A\n"," 44% 1188/2698 [32:28<41:03,  1.63s/it]\u001b[A\n"," 44% 1189/2698 [32:30<40:59,  1.63s/it]\u001b[A\n"," 44% 1190/2698 [32:31<41:01,  1.63s/it]\u001b[A\n"," 44% 1191/2698 [32:33<41:00,  1.63s/it]\u001b[A\n"," 44% 1192/2698 [32:35<40:59,  1.63s/it]\u001b[A\n"," 44% 1193/2698 [32:36<40:55,  1.63s/it]\u001b[A\n"," 44% 1194/2698 [32:38<40:53,  1.63s/it]\u001b[A\n"," 44% 1195/2698 [32:40<40:52,  1.63s/it]\u001b[A\n"," 44% 1196/2698 [32:41<40:53,  1.63s/it]\u001b[A\n"," 44% 1197/2698 [32:43<40:53,  1.63s/it]\u001b[A\n"," 44% 1198/2698 [32:44<40:50,  1.63s/it]\u001b[A\n"," 44% 1199/2698 [32:46<40:49,  1.63s/it]\u001b[A\n"," 44% 1200/2698 [32:48<40:45,  1.63s/it]\u001b[A\n"," 45% 1201/2698 [32:49<40:50,  1.64s/it]\u001b[A\n"," 45% 1202/2698 [32:51<40:45,  1.63s/it]\u001b[A\n"," 45% 1203/2698 [32:53<40:51,  1.64s/it]\u001b[A\n"," 45% 1204/2698 [32:54<40:48,  1.64s/it]\u001b[A\n"," 45% 1205/2698 [32:56<40:47,  1.64s/it]\u001b[A\n"," 45% 1206/2698 [32:58<40:40,  1.64s/it]\u001b[A\n"," 45% 1207/2698 [32:59<40:39,  1.64s/it]\u001b[A\n"," 45% 1208/2698 [33:01<40:33,  1.63s/it]\u001b[A\n"," 45% 1209/2698 [33:02<40:28,  1.63s/it]\u001b[A\n"," 45% 1210/2698 [33:04<40:25,  1.63s/it]\u001b[A\n"," 45% 1211/2698 [33:06<40:23,  1.63s/it]\u001b[A\n"," 45% 1212/2698 [33:07<40:23,  1.63s/it]\u001b[A\n"," 45% 1213/2698 [33:09<40:21,  1.63s/it]\u001b[A\n"," 45% 1214/2698 [33:11<40:19,  1.63s/it]\u001b[A\n"," 45% 1215/2698 [33:12<40:17,  1.63s/it]\u001b[A\n"," 45% 1216/2698 [33:14<40:21,  1.63s/it]\u001b[A\n"," 45% 1217/2698 [33:16<40:22,  1.64s/it]\u001b[A\n"," 45% 1218/2698 [33:17<40:20,  1.64s/it]\u001b[A\n"," 45% 1219/2698 [33:19<40:15,  1.63s/it]\u001b[A\n"," 45% 1220/2698 [33:20<40:09,  1.63s/it]\u001b[A\n"," 45% 1221/2698 [33:22<40:22,  1.64s/it]\u001b[A\n"," 45% 1222/2698 [33:24<40:16,  1.64s/it]\u001b[A\n"," 45% 1223/2698 [33:25<40:15,  1.64s/it]\u001b[A\n"," 45% 1224/2698 [33:27<40:14,  1.64s/it]\u001b[A\n"," 45% 1225/2698 [33:29<40:06,  1.63s/it]\u001b[A\n"," 45% 1226/2698 [33:30<40:07,  1.64s/it]\u001b[A\n"," 45% 1227/2698 [33:32<40:05,  1.64s/it]\u001b[A\n"," 46% 1228/2698 [33:34<40:05,  1.64s/it]\u001b[A\n"," 46% 1229/2698 [33:35<40:04,  1.64s/it]\u001b[A\n"," 46% 1230/2698 [33:37<40:02,  1.64s/it]\u001b[A\n"," 46% 1231/2698 [33:38<39:59,  1.64s/it]\u001b[A\n"," 46% 1232/2698 [33:40<39:55,  1.63s/it]\u001b[A\n"," 46% 1233/2698 [33:42<39:52,  1.63s/it]\u001b[A\n"," 46% 1234/2698 [33:43<39:50,  1.63s/it]\u001b[A\n"," 46% 1235/2698 [33:45<39:45,  1.63s/it]\u001b[A\n"," 46% 1236/2698 [33:47<39:49,  1.63s/it]\u001b[A\n"," 46% 1237/2698 [33:48<39:47,  1.63s/it]\u001b[A\n"," 46% 1238/2698 [33:50<39:47,  1.64s/it]\u001b[A\n"," 46% 1239/2698 [33:51<39:41,  1.63s/it]\u001b[A\n"," 46% 1240/2698 [33:53<39:43,  1.63s/it]\u001b[A\n"," 46% 1241/2698 [33:55<39:42,  1.64s/it]\u001b[A\n"," 46% 1242/2698 [33:56<39:42,  1.64s/it]\u001b[A\n"," 46% 1243/2698 [33:58<39:39,  1.64s/it]\u001b[A\n"," 46% 1244/2698 [34:00<39:41,  1.64s/it]\u001b[A\n"," 46% 1245/2698 [34:01<39:34,  1.63s/it]\u001b[A\n"," 46% 1246/2698 [34:03<39:39,  1.64s/it]\u001b[A\n"," 46% 1247/2698 [34:05<39:35,  1.64s/it]\u001b[A\n"," 46% 1248/2698 [34:06<39:39,  1.64s/it]\u001b[A\n"," 46% 1249/2698 [34:08<39:39,  1.64s/it]\u001b[A\n"," 46% 1250/2698 [34:10<39:39,  1.64s/it]\u001b[A\n"," 46% 1251/2698 [34:11<39:36,  1.64s/it]\u001b[A\n"," 46% 1252/2698 [34:13<39:41,  1.65s/it]\u001b[A\n"," 46% 1253/2698 [34:14<39:38,  1.65s/it]\u001b[A\n"," 46% 1254/2698 [34:16<39:39,  1.65s/it]\u001b[A\n"," 47% 1255/2698 [34:18<39:30,  1.64s/it]\u001b[A\n"," 47% 1256/2698 [34:19<39:34,  1.65s/it]\u001b[A\n"," 47% 1257/2698 [34:21<39:36,  1.65s/it]\u001b[A\n"," 47% 1258/2698 [34:23<39:36,  1.65s/it]\u001b[A\n"," 47% 1259/2698 [34:24<39:34,  1.65s/it]\u001b[A\n"," 47% 1260/2698 [34:26<39:29,  1.65s/it]\u001b[A\n"," 47% 1261/2698 [34:28<39:23,  1.64s/it]\u001b[A\n"," 47% 1262/2698 [34:29<39:27,  1.65s/it]\u001b[A\n"," 47% 1263/2698 [34:31<39:29,  1.65s/it]\u001b[A\n"," 47% 1264/2698 [34:33<39:35,  1.66s/it]\u001b[A\n"," 47% 1265/2698 [34:34<39:31,  1.66s/it]\u001b[A\n"," 47% 1266/2698 [34:36<39:25,  1.65s/it]\u001b[A\n"," 47% 1267/2698 [34:38<39:24,  1.65s/it]\u001b[A\n"," 47% 1268/2698 [34:39<39:22,  1.65s/it]\u001b[A\n"," 47% 1269/2698 [34:41<39:20,  1.65s/it]\u001b[A\n"," 47% 1270/2698 [34:43<39:21,  1.65s/it]\u001b[A\n"," 47% 1271/2698 [34:44<39:18,  1.65s/it]\u001b[A\n"," 47% 1272/2698 [34:46<39:12,  1.65s/it]\u001b[A\n"," 47% 1273/2698 [34:47<39:09,  1.65s/it]\u001b[A\n"," 47% 1274/2698 [34:49<39:14,  1.65s/it]\u001b[A\n"," 47% 1275/2698 [34:51<39:11,  1.65s/it]\u001b[A\n"," 47% 1276/2698 [34:52<39:03,  1.65s/it]\u001b[A\n"," 47% 1277/2698 [34:54<39:02,  1.65s/it]\u001b[A\n"," 47% 1278/2698 [34:56<38:58,  1.65s/it]\u001b[A\n"," 47% 1279/2698 [34:57<38:56,  1.65s/it]\u001b[A\n"," 47% 1280/2698 [34:59<38:51,  1.64s/it]\u001b[A\n"," 47% 1281/2698 [35:01<38:51,  1.65s/it]\u001b[A\n"," 48% 1282/2698 [35:02<38:55,  1.65s/it]\u001b[A\n"," 48% 1283/2698 [35:04<38:52,  1.65s/it]\u001b[A\n"," 48% 1284/2698 [35:06<38:54,  1.65s/it]\u001b[A\n"," 48% 1285/2698 [35:07<38:49,  1.65s/it]\u001b[A\n"," 48% 1286/2698 [35:09<38:53,  1.65s/it]\u001b[A\n"," 48% 1287/2698 [35:11<38:49,  1.65s/it]\u001b[A\n"," 48% 1288/2698 [35:12<38:47,  1.65s/it]\u001b[A\n"," 48% 1289/2698 [35:14<38:42,  1.65s/it]\u001b[A\n"," 48% 1290/2698 [35:15<38:38,  1.65s/it]\u001b[A\n"," 48% 1291/2698 [35:17<38:43,  1.65s/it]\u001b[A\n"," 48% 1292/2698 [35:19<38:40,  1.65s/it]\u001b[A\n"," 48% 1293/2698 [35:20<38:36,  1.65s/it]\u001b[A\n"," 48% 1294/2698 [35:22<38:31,  1.65s/it]\u001b[A\n"," 48% 1295/2698 [35:24<38:34,  1.65s/it]\u001b[A\n"," 48% 1296/2698 [35:25<38:34,  1.65s/it]\u001b[A\n"," 48% 1297/2698 [35:27<38:26,  1.65s/it]\u001b[A\n"," 48% 1298/2698 [35:29<38:29,  1.65s/it]\u001b[A\n"," 48% 1299/2698 [35:30<38:30,  1.65s/it]\u001b[A\n"," 48% 1300/2698 [35:32<38:27,  1.65s/it]\u001b[A\n"," 48% 1301/2698 [35:34<38:29,  1.65s/it]\u001b[A\n"," 48% 1302/2698 [35:35<38:21,  1.65s/it]\u001b[A\n"," 48% 1303/2698 [35:37<38:22,  1.65s/it]\u001b[A\n"," 48% 1304/2698 [35:39<38:16,  1.65s/it]\u001b[A\n"," 48% 1305/2698 [35:40<38:17,  1.65s/it]\u001b[A\n"," 48% 1306/2698 [35:42<38:08,  1.64s/it]\u001b[A\n"," 48% 1307/2698 [35:44<38:08,  1.65s/it]\u001b[A\n"," 48% 1308/2698 [35:45<38:00,  1.64s/it]\u001b[A\n"," 49% 1309/2698 [35:47<37:57,  1.64s/it]\u001b[A\n"," 49% 1310/2698 [35:48<37:53,  1.64s/it]\u001b[A\n"," 49% 1311/2698 [35:50<37:57,  1.64s/it]\u001b[A\n"," 49% 1312/2698 [35:52<38:06,  1.65s/it]\u001b[A\n"," 49% 1313/2698 [35:53<37:59,  1.65s/it]\u001b[A\n"," 49% 1314/2698 [35:55<37:58,  1.65s/it]\u001b[A\n"," 49% 1315/2698 [35:57<37:55,  1.65s/it]\u001b[A\n"," 49% 1316/2698 [35:58<37:57,  1.65s/it]\u001b[A\n"," 49% 1317/2698 [36:00<37:57,  1.65s/it]\u001b[A\n"," 49% 1318/2698 [36:02<37:53,  1.65s/it]\u001b[A\n"," 49% 1319/2698 [36:03<37:43,  1.64s/it]\u001b[A\n"," 49% 1320/2698 [36:05<37:41,  1.64s/it]\u001b[A\n"," 49% 1321/2698 [36:07<37:40,  1.64s/it]\u001b[A\n"," 49% 1322/2698 [36:08<37:43,  1.64s/it]\u001b[A\n"," 49% 1323/2698 [36:10<37:43,  1.65s/it]\u001b[A\n"," 49% 1324/2698 [36:11<37:40,  1.65s/it]\u001b[A\n"," 49% 1325/2698 [36:13<37:31,  1.64s/it]\u001b[A\n"," 49% 1326/2698 [36:15<37:32,  1.64s/it]\u001b[A\n"," 49% 1327/2698 [36:16<37:33,  1.64s/it]\u001b[A\n"," 49% 1328/2698 [36:18<37:25,  1.64s/it]\u001b[A\n"," 49% 1329/2698 [36:20<37:24,  1.64s/it]\u001b[A\n"," 49% 1330/2698 [36:21<37:19,  1.64s/it]\u001b[A\n"," 49% 1331/2698 [36:23<37:27,  1.64s/it]\u001b[A\n"," 49% 1332/2698 [36:25<37:23,  1.64s/it]\u001b[A\n"," 49% 1333/2698 [36:26<37:26,  1.65s/it]\u001b[A\n"," 49% 1334/2698 [36:28<37:22,  1.64s/it]\u001b[A\n"," 49% 1335/2698 [36:30<37:24,  1.65s/it]\u001b[A\n"," 50% 1336/2698 [36:31<37:22,  1.65s/it]\u001b[A\n"," 50% 1337/2698 [36:33<37:21,  1.65s/it]\u001b[A\n"," 50% 1338/2698 [36:34<37:19,  1.65s/it]\u001b[A\n"," 50% 1339/2698 [36:36<37:18,  1.65s/it]\u001b[A\n"," 50% 1340/2698 [36:38<37:13,  1.64s/it]\u001b[A\n"," 50% 1341/2698 [36:39<37:17,  1.65s/it]\u001b[A\n"," 50% 1342/2698 [36:41<37:15,  1.65s/it]\u001b[A\n"," 50% 1343/2698 [36:43<37:07,  1.64s/it]\u001b[A\n"," 50% 1344/2698 [36:44<37:04,  1.64s/it]\u001b[A\n"," 50% 1345/2698 [36:46<36:58,  1.64s/it]\u001b[A\n"," 50% 1346/2698 [36:48<36:59,  1.64s/it]\u001b[A\n"," 50% 1347/2698 [36:49<37:00,  1.64s/it]\u001b[A\n"," 50% 1348/2698 [36:51<37:00,  1.65s/it]\u001b[A\n"," 50% 1349/2698 [36:53<36:56,  1.64s/it]\u001b[A\n"," 50% 1350/2698 [36:54<36:50,  1.64s/it]\u001b[A\n"," 50% 1351/2698 [36:56<36:46,  1.64s/it]\u001b[A\n"," 50% 1352/2698 [36:57<36:41,  1.64s/it]\u001b[A\n"," 50% 1353/2698 [36:59<36:36,  1.63s/it]\u001b[A\n"," 50% 1354/2698 [37:01<36:34,  1.63s/it]\u001b[A\n"," 50% 1355/2698 [37:02<36:34,  1.63s/it]\u001b[A\n"," 50% 1356/2698 [37:04<36:32,  1.63s/it]\u001b[A\n"," 50% 1357/2698 [37:06<36:30,  1.63s/it]\u001b[A\n"," 50% 1358/2698 [37:07<36:30,  1.63s/it]\u001b[A\n"," 50% 1359/2698 [37:09<36:32,  1.64s/it]\u001b[A\n"," 50% 1360/2698 [37:11<36:28,  1.64s/it]\u001b[A\n"," 50% 1361/2698 [37:12<36:27,  1.64s/it]\u001b[A\n"," 50% 1362/2698 [37:14<36:21,  1.63s/it]\u001b[A\n"," 51% 1363/2698 [37:15<36:14,  1.63s/it]\u001b[A\n"," 51% 1364/2698 [37:17<36:20,  1.63s/it]\u001b[A\n"," 51% 1365/2698 [37:19<36:17,  1.63s/it]\u001b[A\n"," 51% 1366/2698 [37:20<36:12,  1.63s/it]\u001b[A\n"," 51% 1367/2698 [37:22<36:12,  1.63s/it]\u001b[A\n"," 51% 1368/2698 [37:24<36:07,  1.63s/it]\u001b[A\n"," 51% 1369/2698 [37:25<36:03,  1.63s/it]\u001b[A\n"," 51% 1370/2698 [37:27<36:06,  1.63s/it]\u001b[A\n"," 51% 1371/2698 [37:28<36:10,  1.64s/it]\u001b[A\n"," 51% 1372/2698 [37:30<36:10,  1.64s/it]\u001b[A\n"," 51% 1373/2698 [37:32<36:07,  1.64s/it]\u001b[A\n"," 51% 1374/2698 [37:33<36:04,  1.63s/it]\u001b[A\n"," 51% 1375/2698 [37:35<36:03,  1.64s/it]\u001b[A\n"," 51% 1376/2698 [37:37<36:03,  1.64s/it]\u001b[A\n"," 51% 1377/2698 [37:38<36:01,  1.64s/it]\u001b[A\n"," 51% 1378/2698 [37:40<35:55,  1.63s/it]\u001b[A\n"," 51% 1379/2698 [37:42<35:51,  1.63s/it]\u001b[A\n"," 51% 1380/2698 [37:43<35:51,  1.63s/it]\u001b[A\n"," 51% 1381/2698 [37:45<35:53,  1.64s/it]\u001b[A\n"," 51% 1382/2698 [37:46<35:49,  1.63s/it]\u001b[A\n"," 51% 1383/2698 [37:48<35:42,  1.63s/it]\u001b[A\n"," 51% 1384/2698 [37:50<35:35,  1.63s/it]\u001b[A\n"," 51% 1385/2698 [37:51<35:36,  1.63s/it]\u001b[A\n"," 51% 1386/2698 [37:53<35:31,  1.62s/it]\u001b[A\n"," 51% 1387/2698 [37:55<35:28,  1.62s/it]\u001b[A\n"," 51% 1388/2698 [37:56<35:29,  1.63s/it]\u001b[A\n"," 51% 1389/2698 [37:58<35:30,  1.63s/it]\u001b[A\n"," 52% 1390/2698 [37:59<35:35,  1.63s/it]\u001b[A\n"," 52% 1391/2698 [38:01<35:34,  1.63s/it]\u001b[A\n"," 52% 1392/2698 [38:03<35:27,  1.63s/it]\u001b[A\n"," 52% 1393/2698 [38:04<35:25,  1.63s/it]\u001b[A\n"," 52% 1394/2698 [38:06<35:21,  1.63s/it]\u001b[A\n"," 52% 1395/2698 [38:08<35:18,  1.63s/it]\u001b[A\n"," 52% 1396/2698 [38:09<35:15,  1.62s/it]\u001b[A\n"," 52% 1397/2698 [38:11<35:20,  1.63s/it]\u001b[A\n"," 52% 1398/2698 [38:13<35:24,  1.63s/it]\u001b[A\n"," 52% 1399/2698 [38:14<35:27,  1.64s/it]\u001b[A\n"," 52% 1400/2698 [38:16<35:21,  1.63s/it]\u001b[A\n"," 52% 1401/2698 [38:17<35:18,  1.63s/it]\u001b[A\n"," 52% 1402/2698 [38:19<35:21,  1.64s/it]\u001b[A\n"," 52% 1403/2698 [38:21<35:21,  1.64s/it]\u001b[A\n"," 52% 1404/2698 [38:22<35:14,  1.63s/it]\u001b[A\n"," 52% 1405/2698 [38:24<35:12,  1.63s/it]\u001b[A\n"," 52% 1406/2698 [38:26<35:08,  1.63s/it]\u001b[A\n"," 52% 1407/2698 [38:27<35:03,  1.63s/it]\u001b[A\n"," 52% 1408/2698 [38:29<34:59,  1.63s/it]\u001b[A\n"," 52% 1409/2698 [38:30<35:02,  1.63s/it]\u001b[A\n"," 52% 1410/2698 [38:32<35:02,  1.63s/it]\u001b[A\n"," 52% 1411/2698 [38:34<35:01,  1.63s/it]\u001b[A\n"," 52% 1412/2698 [38:35<34:59,  1.63s/it]\u001b[A\n"," 52% 1413/2698 [38:37<34:59,  1.63s/it]\u001b[A\n"," 52% 1414/2698 [38:39<34:59,  1.63s/it]\u001b[A\n"," 52% 1415/2698 [38:40<35:01,  1.64s/it]\u001b[A\n"," 52% 1416/2698 [38:42<34:59,  1.64s/it]\u001b[A\n"," 53% 1417/2698 [38:44<35:01,  1.64s/it]\u001b[A\n"," 53% 1418/2698 [38:45<34:59,  1.64s/it]\u001b[A\n"," 53% 1419/2698 [38:47<34:58,  1.64s/it]\u001b[A\n"," 53% 1420/2698 [38:49<35:01,  1.64s/it]\u001b[A\n"," 53% 1421/2698 [38:50<34:57,  1.64s/it]\u001b[A\n"," 53% 1422/2698 [38:52<34:57,  1.64s/it]\u001b[A\n"," 53% 1423/2698 [38:53<34:51,  1.64s/it]\u001b[A\n"," 53% 1424/2698 [38:55<34:49,  1.64s/it]\u001b[A\n"," 53% 1425/2698 [38:57<34:49,  1.64s/it]\u001b[A\n"," 53% 1426/2698 [38:58<34:56,  1.65s/it]\u001b[A\n"," 53% 1427/2698 [39:00<34:51,  1.65s/it]\u001b[A\n"," 53% 1428/2698 [39:02<34:47,  1.64s/it]\u001b[A\n"," 53% 1429/2698 [39:03<34:43,  1.64s/it]\u001b[A\n"," 53% 1430/2698 [39:05<34:45,  1.64s/it]\u001b[A\n"," 53% 1431/2698 [39:07<34:41,  1.64s/it]\u001b[A\n"," 53% 1432/2698 [39:08<34:43,  1.65s/it]\u001b[A\n"," 53% 1433/2698 [39:10<34:41,  1.65s/it]\u001b[A\n"," 53% 1434/2698 [39:12<34:39,  1.65s/it]\u001b[A\n"," 53% 1435/2698 [39:13<34:41,  1.65s/it]\u001b[A\n"," 53% 1436/2698 [39:15<34:36,  1.65s/it]\u001b[A\n"," 53% 1437/2698 [39:16<34:36,  1.65s/it]\u001b[A\n"," 53% 1438/2698 [39:18<34:29,  1.64s/it]\u001b[A\n"," 53% 1439/2698 [39:20<34:24,  1.64s/it]\u001b[A\n"," 53% 1440/2698 [39:21<34:23,  1.64s/it]\u001b[A\n"," 53% 1441/2698 [39:23<34:16,  1.64s/it]\u001b[A\n"," 53% 1442/2698 [39:25<34:17,  1.64s/it]\u001b[A\n"," 53% 1443/2698 [39:26<34:16,  1.64s/it]\u001b[A\n"," 54% 1444/2698 [39:28<34:21,  1.64s/it]\u001b[A\n"," 54% 1445/2698 [39:30<34:21,  1.65s/it]\u001b[A\n"," 54% 1446/2698 [39:31<34:16,  1.64s/it]\u001b[A\n"," 54% 1447/2698 [39:33<34:15,  1.64s/it]\u001b[A\n"," 54% 1448/2698 [39:35<34:15,  1.64s/it]\u001b[A\n"," 54% 1449/2698 [39:36<34:10,  1.64s/it]\u001b[A\n"," 54% 1450/2698 [39:38<34:11,  1.64s/it]\u001b[A\n"," 54% 1451/2698 [39:39<34:11,  1.64s/it]\u001b[A\n"," 54% 1452/2698 [39:41<34:12,  1.65s/it]\u001b[A\n"," 54% 1453/2698 [39:43<34:11,  1.65s/it]\u001b[A\n"," 54% 1454/2698 [39:44<34:10,  1.65s/it]\u001b[A\n"," 54% 1455/2698 [39:46<34:11,  1.65s/it]\u001b[A\n"," 54% 1456/2698 [39:48<34:05,  1.65s/it]\u001b[A\n"," 54% 1457/2698 [39:49<34:04,  1.65s/it]\u001b[A\n"," 54% 1458/2698 [39:51<34:00,  1.65s/it]\u001b[A\n"," 54% 1459/2698 [39:53<33:57,  1.64s/it]\u001b[A\n"," 54% 1460/2698 [39:54<33:56,  1.64s/it]\u001b[A\n"," 54% 1461/2698 [39:56<33:56,  1.65s/it]\u001b[A\n"," 54% 1462/2698 [39:58<33:51,  1.64s/it]\u001b[A\n"," 54% 1463/2698 [39:59<33:46,  1.64s/it]\u001b[A\n"," 54% 1464/2698 [40:01<33:46,  1.64s/it]\u001b[A\n"," 54% 1465/2698 [40:02<33:48,  1.65s/it]\u001b[A\n"," 54% 1466/2698 [40:04<33:40,  1.64s/it]\u001b[A\n"," 54% 1467/2698 [40:06<33:36,  1.64s/it]\u001b[A\n"," 54% 1468/2698 [40:07<33:38,  1.64s/it]\u001b[A\n"," 54% 1469/2698 [40:09<33:43,  1.65s/it]\u001b[A\n"," 54% 1470/2698 [40:11<33:41,  1.65s/it]\u001b[A\n"," 55% 1471/2698 [40:12<33:43,  1.65s/it]\u001b[A\n"," 55% 1472/2698 [40:14<33:38,  1.65s/it]\u001b[A\n"," 55% 1473/2698 [40:16<33:34,  1.64s/it]\u001b[A\n"," 55% 1474/2698 [40:17<33:33,  1.65s/it]\u001b[A\n"," 55% 1475/2698 [40:19<33:24,  1.64s/it]\u001b[A\n"," 55% 1476/2698 [40:21<33:24,  1.64s/it]\u001b[A\n"," 55% 1477/2698 [40:22<33:25,  1.64s/it]\u001b[A\n"," 55% 1478/2698 [40:24<33:19,  1.64s/it]\u001b[A\n"," 55% 1479/2698 [40:25<33:20,  1.64s/it]\u001b[A\n"," 55% 1480/2698 [40:27<33:21,  1.64s/it]\u001b[A\n"," 55% 1481/2698 [40:29<33:22,  1.65s/it]\u001b[A\n"," 55% 1482/2698 [40:30<33:19,  1.64s/it]\u001b[A\n"," 55% 1483/2698 [40:32<33:16,  1.64s/it]\u001b[A\n"," 55% 1484/2698 [40:34<33:13,  1.64s/it]\u001b[A\n"," 55% 1485/2698 [40:35<33:15,  1.65s/it]\u001b[A\n"," 55% 1486/2698 [40:37<33:11,  1.64s/it]\u001b[A\n"," 55% 1487/2698 [40:39<33:12,  1.65s/it]\u001b[A\n"," 55% 1488/2698 [40:40<33:11,  1.65s/it]\u001b[A\n"," 55% 1489/2698 [40:42<33:08,  1.64s/it]\u001b[A\n"," 55% 1490/2698 [40:44<33:05,  1.64s/it]\u001b[A\n"," 55% 1491/2698 [40:45<33:06,  1.65s/it]\u001b[A\n"," 55% 1492/2698 [40:47<33:06,  1.65s/it]\u001b[A\n"," 55% 1493/2698 [40:49<33:02,  1.65s/it]\u001b[A\n"," 55% 1494/2698 [40:50<32:59,  1.64s/it]\u001b[A\n"," 55% 1495/2698 [40:52<32:56,  1.64s/it]\u001b[A\n"," 55% 1496/2698 [40:53<32:54,  1.64s/it]\u001b[A\n"," 55% 1497/2698 [40:55<32:55,  1.64s/it]\u001b[A\n"," 56% 1498/2698 [40:57<32:50,  1.64s/it]\u001b[A\n"," 56% 1499/2698 [40:58<32:49,  1.64s/it]\u001b[A\n"," 56% 1500/2698 [41:00<32:48,  1.64s/it]\u001b[A\n"," 56% 1501/2698 [41:02<32:47,  1.64s/it]\u001b[A\n"," 56% 1502/2698 [41:03<32:53,  1.65s/it]\u001b[A\n"," 56% 1503/2698 [41:05<32:41,  1.64s/it]\u001b[A\n"," 56% 1504/2698 [41:07<32:43,  1.64s/it]\u001b[A\n"," 56% 1505/2698 [41:08<32:44,  1.65s/it]\u001b[A\n"," 56% 1506/2698 [41:10<32:42,  1.65s/it]\u001b[A\n"," 56% 1507/2698 [41:12<32:38,  1.64s/it]\u001b[A\n"," 56% 1508/2698 [41:13<32:36,  1.64s/it]\u001b[A\n"," 56% 1509/2698 [41:15<32:37,  1.65s/it]\u001b[A\n"," 56% 1510/2698 [41:16<32:38,  1.65s/it]\u001b[A\n"," 56% 1511/2698 [41:18<32:32,  1.65s/it]\u001b[A\n"," 56% 1512/2698 [41:20<32:23,  1.64s/it]\u001b[A\n"," 56% 1513/2698 [41:21<32:23,  1.64s/it]\u001b[A\n"," 56% 1514/2698 [41:23<32:17,  1.64s/it]\u001b[A\n"," 56% 1515/2698 [41:25<32:20,  1.64s/it]\u001b[A\n"," 56% 1516/2698 [41:26<32:20,  1.64s/it]\u001b[A\n"," 56% 1517/2698 [41:28<32:20,  1.64s/it]\u001b[A\n"," 56% 1518/2698 [41:30<32:14,  1.64s/it]\u001b[A\n"," 56% 1519/2698 [41:31<32:10,  1.64s/it]\u001b[A\n"," 56% 1520/2698 [41:33<32:07,  1.64s/it]\u001b[A\n"," 56% 1521/2698 [41:34<32:00,  1.63s/it]\u001b[A\n"," 56% 1522/2698 [41:36<32:01,  1.63s/it]\u001b[A\n"," 56% 1523/2698 [41:38<31:56,  1.63s/it]\u001b[A\n"," 56% 1524/2698 [41:39<31:54,  1.63s/it]\u001b[A\n"," 57% 1525/2698 [41:41<31:54,  1.63s/it]\u001b[A\n"," 57% 1526/2698 [41:43<31:51,  1.63s/it]\u001b[A\n"," 57% 1527/2698 [41:44<31:50,  1.63s/it]\u001b[A\n"," 57% 1528/2698 [41:46<31:48,  1.63s/it]\u001b[A\n"," 57% 1529/2698 [41:48<31:50,  1.63s/it]\u001b[A\n"," 57% 1530/2698 [41:49<31:46,  1.63s/it]\u001b[A\n"," 57% 1531/2698 [41:51<31:40,  1.63s/it]\u001b[A\n"," 57% 1532/2698 [41:52<31:39,  1.63s/it]\u001b[A\n"," 57% 1533/2698 [41:54<31:39,  1.63s/it]\u001b[A\n"," 57% 1534/2698 [41:56<31:38,  1.63s/it]\u001b[A\n"," 57% 1535/2698 [41:57<31:35,  1.63s/it]\u001b[A\n"," 57% 1536/2698 [41:59<31:32,  1.63s/it]\u001b[A\n"," 57% 1537/2698 [42:01<31:28,  1.63s/it]\u001b[A\n"," 57% 1538/2698 [42:02<31:32,  1.63s/it]\u001b[A\n"," 57% 1539/2698 [42:04<31:30,  1.63s/it]\u001b[A\n"," 57% 1540/2698 [42:05<31:29,  1.63s/it]\u001b[A\n"," 57% 1541/2698 [42:07<31:30,  1.63s/it]\u001b[A\n"," 57% 1542/2698 [42:09<31:29,  1.63s/it]\u001b[A\n"," 57% 1543/2698 [42:10<31:29,  1.64s/it]\u001b[A\n"," 57% 1544/2698 [42:12<31:28,  1.64s/it]\u001b[A\n"," 57% 1545/2698 [42:14<31:24,  1.63s/it]\u001b[A\n"," 57% 1546/2698 [42:15<31:19,  1.63s/it]\u001b[A\n"," 57% 1547/2698 [42:17<31:21,  1.63s/it]\u001b[A\n"," 57% 1548/2698 [42:19<31:21,  1.64s/it]\u001b[A\n"," 57% 1549/2698 [42:20<31:13,  1.63s/it]\u001b[A\n"," 57% 1550/2698 [42:22<31:08,  1.63s/it]\u001b[A\n"," 57% 1551/2698 [42:23<31:08,  1.63s/it]\u001b[A\n"," 58% 1552/2698 [42:25<31:04,  1.63s/it]\u001b[A\n"," 58% 1553/2698 [42:27<31:03,  1.63s/it]\u001b[A\n"," 58% 1554/2698 [42:28<31:04,  1.63s/it]\u001b[A\n"," 58% 1555/2698 [42:30<31:05,  1.63s/it]\u001b[A\n"," 58% 1556/2698 [42:32<31:03,  1.63s/it]\u001b[A\n"," 58% 1557/2698 [42:33<31:04,  1.63s/it]\u001b[A\n"," 58% 1558/2698 [42:35<31:07,  1.64s/it]\u001b[A\n"," 58% 1559/2698 [42:37<31:05,  1.64s/it]\u001b[A\n"," 58% 1560/2698 [42:38<31:02,  1.64s/it]\u001b[A\n"," 58% 1561/2698 [42:40<30:54,  1.63s/it]\u001b[A\n"," 58% 1562/2698 [42:41<30:51,  1.63s/it]\u001b[A\n"," 58% 1563/2698 [42:43<30:48,  1.63s/it]\u001b[A\n"," 58% 1564/2698 [42:45<30:43,  1.63s/it]\u001b[A\n"," 58% 1565/2698 [42:46<30:39,  1.62s/it]\u001b[A\n"," 58% 1566/2698 [42:48<30:38,  1.62s/it]\u001b[A\n"," 58% 1567/2698 [42:50<30:41,  1.63s/it]\u001b[A\n"," 58% 1568/2698 [42:51<30:40,  1.63s/it]\u001b[A\n"," 58% 1569/2698 [42:53<30:40,  1.63s/it]\u001b[A\n"," 58% 1570/2698 [42:54<30:40,  1.63s/it]\u001b[A\n"," 58% 1571/2698 [42:56<30:40,  1.63s/it]\u001b[A\n"," 58% 1572/2698 [42:58<30:36,  1.63s/it]\u001b[A\n"," 58% 1573/2698 [42:59<30:34,  1.63s/it]\u001b[A\n"," 58% 1574/2698 [43:01<30:35,  1.63s/it]\u001b[A\n"," 58% 1575/2698 [43:03<30:33,  1.63s/it]\u001b[A\n"," 58% 1576/2698 [43:04<30:34,  1.64s/it]\u001b[A\n"," 58% 1577/2698 [43:06<30:31,  1.63s/it]\u001b[A\n"," 58% 1578/2698 [43:07<30:30,  1.63s/it]\u001b[A\n"," 59% 1579/2698 [43:09<30:26,  1.63s/it]\u001b[A\n"," 59% 1580/2698 [43:11<30:21,  1.63s/it]\u001b[A\n"," 59% 1581/2698 [43:12<30:20,  1.63s/it]\u001b[A\n"," 59% 1582/2698 [43:14<30:17,  1.63s/it]\u001b[A\n"," 59% 1583/2698 [43:16<30:16,  1.63s/it]\u001b[A\n"," 59% 1584/2698 [43:17<30:17,  1.63s/it]\u001b[A\n"," 59% 1585/2698 [43:19<30:17,  1.63s/it]\u001b[A\n"," 59% 1586/2698 [43:21<30:20,  1.64s/it]\u001b[A\n"," 59% 1587/2698 [43:22<30:22,  1.64s/it]\u001b[A\n"," 59% 1588/2698 [43:24<30:16,  1.64s/it]\u001b[A\n"," 59% 1589/2698 [43:25<30:17,  1.64s/it]\u001b[A\n"," 59% 1590/2698 [43:27<30:19,  1.64s/it]\u001b[A\n"," 59% 1591/2698 [43:29<30:21,  1.65s/it]\u001b[A\n"," 59% 1592/2698 [43:30<30:16,  1.64s/it]\u001b[A\n"," 59% 1593/2698 [43:32<30:18,  1.65s/it]\u001b[A\n"," 59% 1594/2698 [43:34<30:19,  1.65s/it]\u001b[A\n"," 59% 1595/2698 [43:35<30:20,  1.65s/it]\u001b[A\n"," 59% 1596/2698 [43:37<30:14,  1.65s/it]\u001b[A\n"," 59% 1597/2698 [43:39<30:15,  1.65s/it]\u001b[A\n"," 59% 1598/2698 [43:40<30:12,  1.65s/it]\u001b[A\n"," 59% 1599/2698 [43:42<30:11,  1.65s/it]\u001b[A\n"," 59% 1600/2698 [43:44<30:13,  1.65s/it]\u001b[A\n"," 59% 1601/2698 [43:45<30:11,  1.65s/it]\u001b[A\n"," 59% 1602/2698 [43:47<30:11,  1.65s/it]\u001b[A\n"," 59% 1603/2698 [43:49<30:13,  1.66s/it]\u001b[A\n"," 59% 1604/2698 [43:50<30:09,  1.65s/it]\u001b[A\n"," 59% 1605/2698 [43:52<30:08,  1.65s/it]\u001b[A\n"," 60% 1606/2698 [43:54<30:03,  1.65s/it]\u001b[A\n"," 60% 1607/2698 [43:55<29:56,  1.65s/it]\u001b[A\n"," 60% 1608/2698 [43:57<29:56,  1.65s/it]\u001b[A\n"," 60% 1609/2698 [43:58<29:53,  1.65s/it]\u001b[A\n"," 60% 1610/2698 [44:00<29:52,  1.65s/it]\u001b[A\n"," 60% 1611/2698 [44:02<29:47,  1.64s/it]\u001b[A\n"," 60% 1612/2698 [44:03<29:42,  1.64s/it]\u001b[A\n"," 60% 1613/2698 [44:05<29:45,  1.65s/it]\u001b[A\n"," 60% 1614/2698 [44:07<29:46,  1.65s/it]\u001b[A\n"," 60% 1615/2698 [44:08<29:47,  1.65s/it]\u001b[A\n"," 60% 1616/2698 [44:10<29:44,  1.65s/it]\u001b[A\n"," 60% 1617/2698 [44:12<29:42,  1.65s/it]\u001b[A\n"," 60% 1618/2698 [44:13<29:40,  1.65s/it]\u001b[A\n"," 60% 1619/2698 [44:15<29:39,  1.65s/it]\u001b[A\n"," 60% 1620/2698 [44:17<29:37,  1.65s/it]\u001b[A\n"," 60% 1621/2698 [44:18<29:39,  1.65s/it]\u001b[A\n"," 60% 1622/2698 [44:20<29:41,  1.66s/it]\u001b[A\n"," 60% 1623/2698 [44:22<29:41,  1.66s/it]\u001b[A\n"," 60% 1624/2698 [44:23<29:41,  1.66s/it]\u001b[A\n"," 60% 1625/2698 [44:25<29:34,  1.65s/it]\u001b[A\n"," 60% 1626/2698 [44:27<29:34,  1.66s/it]\u001b[A\n"," 60% 1627/2698 [44:28<29:31,  1.65s/it]\u001b[A\n"," 60% 1628/2698 [44:30<29:32,  1.66s/it]\u001b[A\n"," 60% 1629/2698 [44:31<29:29,  1.66s/it]\u001b[A\n"," 60% 1630/2698 [44:33<29:24,  1.65s/it]\u001b[A\n"," 60% 1631/2698 [44:35<29:21,  1.65s/it]\u001b[A\n"," 60% 1632/2698 [44:36<29:17,  1.65s/it]\u001b[A\n"," 61% 1633/2698 [44:38<29:17,  1.65s/it]\u001b[A\n"," 61% 1634/2698 [44:40<29:09,  1.64s/it]\u001b[A\n"," 61% 1635/2698 [44:41<29:09,  1.65s/it]\u001b[A\n"," 61% 1636/2698 [44:43<29:10,  1.65s/it]\u001b[A\n"," 61% 1637/2698 [44:45<29:11,  1.65s/it]\u001b[A\n"," 61% 1638/2698 [44:46<29:12,  1.65s/it]\u001b[A\n"," 61% 1639/2698 [44:48<29:08,  1.65s/it]\u001b[A\n"," 61% 1640/2698 [44:50<29:09,  1.65s/it]\u001b[A\n"," 61% 1641/2698 [44:51<29:09,  1.65s/it]\u001b[A\n"," 61% 1642/2698 [44:53<29:02,  1.65s/it]\u001b[A\n"," 61% 1643/2698 [44:55<29:00,  1.65s/it]\u001b[A\n"," 61% 1644/2698 [44:56<29:00,  1.65s/it]\u001b[A\n"," 61% 1645/2698 [44:58<29:00,  1.65s/it]\u001b[A\n"," 61% 1646/2698 [45:00<28:55,  1.65s/it]\u001b[A\n"," 61% 1647/2698 [45:01<28:50,  1.65s/it]\u001b[A\n"," 61% 1648/2698 [45:03<28:47,  1.65s/it]\u001b[A\n"," 61% 1649/2698 [45:04<28:47,  1.65s/it]\u001b[A\n"," 61% 1650/2698 [45:06<28:48,  1.65s/it]\u001b[A\n"," 61% 1651/2698 [45:08<28:47,  1.65s/it]\u001b[A\n"," 61% 1652/2698 [45:09<28:49,  1.65s/it]\u001b[A\n"," 61% 1653/2698 [45:11<28:49,  1.65s/it]\u001b[A\n"," 61% 1654/2698 [45:13<28:49,  1.66s/it]\u001b[A\n"," 61% 1655/2698 [45:14<28:43,  1.65s/it]\u001b[A\n"," 61% 1656/2698 [45:16<28:46,  1.66s/it]\u001b[A\n"," 61% 1657/2698 [45:18<28:43,  1.66s/it]\u001b[A\n"," 61% 1658/2698 [45:19<28:41,  1.66s/it]\u001b[A\n"," 61% 1659/2698 [45:21<28:36,  1.65s/it]\u001b[A\n"," 62% 1660/2698 [45:23<28:34,  1.65s/it]\u001b[A\n"," 62% 1661/2698 [45:24<28:31,  1.65s/it]\u001b[A\n"," 62% 1662/2698 [45:26<28:29,  1.65s/it]\u001b[A\n"," 62% 1663/2698 [45:28<28:29,  1.65s/it]\u001b[A\n"," 62% 1664/2698 [45:29<28:28,  1.65s/it]\u001b[A\n"," 62% 1665/2698 [45:31<28:24,  1.65s/it]\u001b[A\n"," 62% 1666/2698 [45:33<28:24,  1.65s/it]\u001b[A\n"," 62% 1667/2698 [45:34<28:20,  1.65s/it]\u001b[A\n"," 62% 1668/2698 [45:36<28:19,  1.65s/it]\u001b[A\n"," 62% 1669/2698 [45:38<28:19,  1.65s/it]\u001b[A\n"," 62% 1670/2698 [45:39<28:16,  1.65s/it]\u001b[A\n"," 62% 1671/2698 [45:41<28:21,  1.66s/it]\u001b[A\n"," 62% 1672/2698 [45:43<28:20,  1.66s/it]\u001b[A\n"," 62% 1673/2698 [45:44<28:17,  1.66s/it]\u001b[A\n"," 62% 1674/2698 [45:46<28:16,  1.66s/it]\u001b[A\n"," 62% 1675/2698 [45:47<28:13,  1.66s/it]\u001b[A\n"," 62% 1676/2698 [45:49<28:09,  1.65s/it]\u001b[A\n"," 62% 1677/2698 [45:51<28:02,  1.65s/it]\u001b[A\n"," 62% 1678/2698 [45:52<28:07,  1.65s/it]\u001b[A\n"," 62% 1679/2698 [45:54<28:06,  1.65s/it]\u001b[A\n"," 62% 1680/2698 [45:56<27:59,  1.65s/it]\u001b[A\n"," 62% 1681/2698 [45:57<27:55,  1.65s/it]\u001b[A\n"," 62% 1682/2698 [45:59<27:52,  1.65s/it]\u001b[A\n"," 62% 1683/2698 [46:01<27:50,  1.65s/it]\u001b[A\n"," 62% 1684/2698 [46:02<27:52,  1.65s/it]\u001b[A\n"," 62% 1685/2698 [46:04<27:47,  1.65s/it]\u001b[A\n"," 62% 1686/2698 [46:06<27:47,  1.65s/it]\u001b[A\n"," 63% 1687/2698 [46:07<27:41,  1.64s/it]\u001b[A\n"," 63% 1688/2698 [46:09<27:37,  1.64s/it]\u001b[A\n"," 63% 1689/2698 [46:10<27:34,  1.64s/it]\u001b[A\n"," 63% 1690/2698 [46:12<27:34,  1.64s/it]\u001b[A\n"," 63% 1691/2698 [46:14<27:33,  1.64s/it]\u001b[A\n"," 63% 1692/2698 [46:15<27:31,  1.64s/it]\u001b[A\n"," 63% 1693/2698 [46:17<27:34,  1.65s/it]\u001b[A\n"," 63% 1694/2698 [46:19<27:32,  1.65s/it]\u001b[A\n"," 63% 1695/2698 [46:20<27:35,  1.65s/it]\u001b[A\n"," 63% 1696/2698 [46:22<27:30,  1.65s/it]\u001b[A\n"," 63% 1697/2698 [46:24<27:26,  1.64s/it]\u001b[A\n"," 63% 1698/2698 [46:25<27:24,  1.64s/it]\u001b[A\n"," 63% 1699/2698 [46:27<27:16,  1.64s/it]\u001b[A\n"," 63% 1700/2698 [46:29<27:18,  1.64s/it]\u001b[A\n"," 63% 1701/2698 [46:30<27:12,  1.64s/it]\u001b[A\n"," 63% 1702/2698 [46:32<27:11,  1.64s/it]\u001b[A\n"," 63% 1703/2698 [46:33<27:08,  1.64s/it]\u001b[A\n"," 63% 1704/2698 [46:35<27:05,  1.64s/it]\u001b[A\n"," 63% 1705/2698 [46:37<26:59,  1.63s/it]\u001b[A\n"," 63% 1706/2698 [46:38<27:00,  1.63s/it]\u001b[A\n"," 63% 1707/2698 [46:40<26:58,  1.63s/it]\u001b[A\n"," 63% 1708/2698 [46:42<27:02,  1.64s/it]\u001b[A\n"," 63% 1709/2698 [46:43<27:02,  1.64s/it]\u001b[A\n"," 63% 1710/2698 [46:45<26:56,  1.64s/it]\u001b[A\n"," 63% 1711/2698 [46:47<26:56,  1.64s/it]\u001b[A\n"," 63% 1712/2698 [46:48<26:58,  1.64s/it]\u001b[A\n"," 63% 1713/2698 [46:50<26:51,  1.64s/it]\u001b[A\n"," 64% 1714/2698 [46:51<26:49,  1.64s/it]\u001b[A\n"," 64% 1715/2698 [46:53<26:45,  1.63s/it]\u001b[A\n"," 64% 1716/2698 [46:55<26:46,  1.64s/it]\u001b[A\n"," 64% 1717/2698 [46:56<26:44,  1.64s/it]\u001b[A\n"," 64% 1718/2698 [46:58<26:43,  1.64s/it]\u001b[A\n"," 64% 1719/2698 [47:00<26:40,  1.64s/it]\u001b[A\n"," 64% 1720/2698 [47:01<26:34,  1.63s/it]\u001b[A\n"," 64% 1721/2698 [47:03<26:33,  1.63s/it]\u001b[A\n"," 64% 1722/2698 [47:05<26:32,  1.63s/it]\u001b[A\n"," 64% 1723/2698 [47:06<26:27,  1.63s/it]\u001b[A\n"," 64% 1724/2698 [47:08<26:22,  1.62s/it]\u001b[A\n"," 64% 1725/2698 [47:09<26:21,  1.63s/it]\u001b[A\n"," 64% 1726/2698 [47:11<26:20,  1.63s/it]\u001b[A\n"," 64% 1727/2698 [47:13<26:21,  1.63s/it]\u001b[A\n"," 64% 1728/2698 [47:14<26:18,  1.63s/it]\u001b[A\n"," 64% 1729/2698 [47:16<26:16,  1.63s/it]\u001b[A\n"," 64% 1730/2698 [47:18<26:12,  1.62s/it]\u001b[A\n"," 64% 1731/2698 [47:19<26:12,  1.63s/it]\u001b[A\n"," 64% 1732/2698 [47:21<26:12,  1.63s/it]\u001b[A\n"," 64% 1733/2698 [47:22<26:10,  1.63s/it]\u001b[A\n"," 64% 1734/2698 [47:24<26:10,  1.63s/it]\u001b[A\n"," 64% 1735/2698 [47:26<26:07,  1.63s/it]\u001b[A\n"," 64% 1736/2698 [47:27<26:04,  1.63s/it]\u001b[A\n"," 64% 1737/2698 [47:29<26:02,  1.63s/it]\u001b[A\n"," 64% 1738/2698 [47:31<25:58,  1.62s/it]\u001b[A\n"," 64% 1739/2698 [47:32<25:56,  1.62s/it]\u001b[A\n"," 64% 1740/2698 [47:34<25:56,  1.62s/it]\u001b[A\n"," 65% 1741/2698 [47:35<25:52,  1.62s/it]\u001b[A\n"," 65% 1742/2698 [47:37<25:56,  1.63s/it]\u001b[A\n"," 65% 1743/2698 [47:39<25:50,  1.62s/it]\u001b[A\n"," 65% 1744/2698 [47:40<25:51,  1.63s/it]\u001b[A\n"," 65% 1745/2698 [47:42<25:46,  1.62s/it]\u001b[A\n"," 65% 1746/2698 [47:44<25:47,  1.63s/it]\u001b[A\n"," 65% 1747/2698 [47:45<25:44,  1.62s/it]\u001b[A\n"," 65% 1748/2698 [47:47<25:47,  1.63s/it]\u001b[A\n"," 65% 1749/2698 [47:48<25:44,  1.63s/it]\u001b[A\n"," 65% 1750/2698 [47:50<25:42,  1.63s/it]\u001b[A\n"," 65% 1751/2698 [47:52<25:39,  1.63s/it]\u001b[A\n"," 65% 1752/2698 [47:53<25:42,  1.63s/it]\u001b[A\n"," 65% 1753/2698 [47:55<25:40,  1.63s/it]\u001b[A\n"," 65% 1754/2698 [47:57<25:42,  1.63s/it]\u001b[A\n"," 65% 1755/2698 [47:58<25:40,  1.63s/it]\u001b[A\n"," 65% 1756/2698 [48:00<25:43,  1.64s/it]\u001b[A\n"," 65% 1757/2698 [48:02<25:41,  1.64s/it]\u001b[A\n"," 65% 1758/2698 [48:03<25:42,  1.64s/it]\u001b[A\n"," 65% 1759/2698 [48:05<25:40,  1.64s/it]\u001b[A\n"," 65% 1760/2698 [48:06<25:40,  1.64s/it]\u001b[A\n"," 65% 1761/2698 [48:08<25:39,  1.64s/it]\u001b[A\n"," 65% 1762/2698 [48:10<25:41,  1.65s/it]\u001b[A\n"," 65% 1763/2698 [48:11<25:38,  1.65s/it]\u001b[A\n"," 65% 1764/2698 [48:13<25:37,  1.65s/it]\u001b[A\n"," 65% 1765/2698 [48:15<25:34,  1.65s/it]\u001b[A\n"," 65% 1766/2698 [48:16<25:35,  1.65s/it]\u001b[A\n"," 65% 1767/2698 [48:18<25:34,  1.65s/it]\u001b[A\n"," 66% 1768/2698 [48:20<25:34,  1.65s/it]\u001b[A\n"," 66% 1769/2698 [48:21<25:36,  1.65s/it]\u001b[A\n"," 66% 1770/2698 [48:23<25:28,  1.65s/it]\u001b[A\n"," 66% 1771/2698 [48:25<25:26,  1.65s/it]\u001b[A\n"," 66% 1772/2698 [48:26<25:28,  1.65s/it]\u001b[A\n"," 66% 1773/2698 [48:28<25:25,  1.65s/it]\u001b[A\n"," 66% 1774/2698 [48:30<25:25,  1.65s/it]\u001b[A\n"," 66% 1775/2698 [48:31<25:24,  1.65s/it]\u001b[A\n"," 66% 1776/2698 [48:33<25:21,  1.65s/it]\u001b[A\n"," 66% 1777/2698 [48:34<25:18,  1.65s/it]\u001b[A\n"," 66% 1778/2698 [48:36<25:14,  1.65s/it]\u001b[A\n"," 66% 1779/2698 [48:38<25:09,  1.64s/it]\u001b[A\n"," 66% 1780/2698 [48:39<25:11,  1.65s/it]\u001b[A\n"," 66% 1781/2698 [48:41<25:10,  1.65s/it]\u001b[A\n"," 66% 1782/2698 [48:43<25:06,  1.64s/it]\u001b[A\n"," 66% 1783/2698 [48:44<25:04,  1.64s/it]\u001b[A\n"," 66% 1784/2698 [48:46<25:03,  1.65s/it]\u001b[A\n"," 66% 1785/2698 [48:48<25:01,  1.64s/it]\u001b[A\n"," 66% 1786/2698 [48:49<25:01,  1.65s/it]\u001b[A\n"," 66% 1787/2698 [48:51<25:00,  1.65s/it]\u001b[A\n"," 66% 1788/2698 [48:53<24:59,  1.65s/it]\u001b[A\n"," 66% 1789/2698 [48:54<24:57,  1.65s/it]\u001b[A\n"," 66% 1790/2698 [48:56<24:54,  1.65s/it]\u001b[A\n"," 66% 1791/2698 [48:58<24:51,  1.64s/it]\u001b[A\n"," 66% 1792/2698 [48:59<24:56,  1.65s/it]\u001b[A\n"," 66% 1793/2698 [49:01<24:52,  1.65s/it]\u001b[A\n"," 66% 1794/2698 [49:02<24:52,  1.65s/it]\u001b[A\n"," 67% 1795/2698 [49:04<24:51,  1.65s/it]\u001b[A\n"," 67% 1796/2698 [49:06<24:50,  1.65s/it]\u001b[A\n"," 67% 1797/2698 [49:07<24:44,  1.65s/it]\u001b[A\n"," 67% 1798/2698 [49:09<24:41,  1.65s/it]\u001b[A\n"," 67% 1799/2698 [49:11<24:37,  1.64s/it]\u001b[A\n"," 67% 1800/2698 [49:12<24:35,  1.64s/it]\u001b[A\n"," 67% 1801/2698 [49:14<24:35,  1.65s/it]\u001b[A\n"," 67% 1802/2698 [49:16<24:32,  1.64s/it]\u001b[A\n"," 67% 1803/2698 [49:17<24:29,  1.64s/it]\u001b[A\n"," 67% 1804/2698 [49:19<24:31,  1.65s/it]\u001b[A\n"," 67% 1805/2698 [49:21<24:35,  1.65s/it]\u001b[A\n"," 67% 1806/2698 [49:22<24:34,  1.65s/it]\u001b[A\n"," 67% 1807/2698 [49:24<24:31,  1.65s/it]\u001b[A\n"," 67% 1808/2698 [49:26<24:26,  1.65s/it]\u001b[A\n"," 67% 1809/2698 [49:27<24:27,  1.65s/it]\u001b[A\n"," 67% 1810/2698 [49:29<24:24,  1.65s/it]\u001b[A\n"," 67% 1811/2698 [49:30<24:21,  1.65s/it]\u001b[A\n"," 67% 1812/2698 [49:32<24:16,  1.64s/it]\u001b[A\n"," 67% 1813/2698 [49:34<24:15,  1.64s/it]\u001b[A\n"," 67% 1814/2698 [49:35<24:10,  1.64s/it]\u001b[A\n"," 67% 1815/2698 [49:37<24:08,  1.64s/it]\u001b[A\n"," 67% 1816/2698 [49:39<24:09,  1.64s/it]\u001b[A\n"," 67% 1817/2698 [49:40<24:09,  1.65s/it]\u001b[A\n"," 67% 1818/2698 [49:42<24:10,  1.65s/it]\u001b[A\n"," 67% 1819/2698 [49:44<24:07,  1.65s/it]\u001b[A\n"," 67% 1820/2698 [49:45<24:09,  1.65s/it]\u001b[A\n"," 67% 1821/2698 [49:47<24:07,  1.65s/it]\u001b[A\n"," 68% 1822/2698 [49:49<24:08,  1.65s/it]\u001b[A\n"," 68% 1823/2698 [49:50<24:02,  1.65s/it]\u001b[A\n"," 68% 1824/2698 [49:52<24:02,  1.65s/it]\u001b[A\n"," 68% 1825/2698 [49:54<24:01,  1.65s/it]\u001b[A\n"," 68% 1826/2698 [49:55<24:02,  1.65s/it]\u001b[A\n"," 68% 1827/2698 [49:57<23:58,  1.65s/it]\u001b[A\n"," 68% 1828/2698 [49:59<24:00,  1.66s/it]\u001b[A\n"," 68% 1829/2698 [50:00<23:54,  1.65s/it]\u001b[A\n"," 68% 1830/2698 [50:02<23:50,  1.65s/it]\u001b[A\n"," 68% 1831/2698 [50:03<23:50,  1.65s/it]\u001b[A\n"," 68% 1832/2698 [50:05<23:49,  1.65s/it]\u001b[A\n"," 68% 1833/2698 [50:07<23:48,  1.65s/it]\u001b[A\n"," 68% 1834/2698 [50:08<23:44,  1.65s/it]\u001b[A\n"," 68% 1835/2698 [50:10<23:42,  1.65s/it]\u001b[A\n"," 68% 1836/2698 [50:12<23:39,  1.65s/it]\u001b[A\n"," 68% 1837/2698 [50:13<23:41,  1.65s/it]\u001b[A\n"," 68% 1838/2698 [50:15<23:38,  1.65s/it]\u001b[A\n"," 68% 1839/2698 [50:17<23:40,  1.65s/it]\u001b[A\n"," 68% 1840/2698 [50:18<23:34,  1.65s/it]\u001b[A\n"," 68% 1841/2698 [50:20<23:34,  1.65s/it]\u001b[A\n"," 68% 1842/2698 [50:22<23:34,  1.65s/it]\u001b[A\n"," 68% 1843/2698 [50:23<23:34,  1.65s/it]\u001b[A\n"," 68% 1844/2698 [50:25<23:30,  1.65s/it]\u001b[A\n"," 68% 1845/2698 [50:27<23:28,  1.65s/it]\u001b[A\n"," 68% 1846/2698 [50:28<23:27,  1.65s/it]\u001b[A\n"," 68% 1847/2698 [50:30<23:26,  1.65s/it]\u001b[A\n"," 68% 1848/2698 [50:32<23:22,  1.65s/it]\u001b[A\n"," 69% 1849/2698 [50:33<23:19,  1.65s/it]\u001b[A\n"," 69% 1850/2698 [50:35<23:13,  1.64s/it]\u001b[A\n"," 69% 1851/2698 [50:36<23:13,  1.65s/it]\u001b[A\n"," 69% 1852/2698 [50:38<23:12,  1.65s/it]\u001b[A\n"," 69% 1853/2698 [50:40<23:10,  1.65s/it]\u001b[A\n"," 69% 1854/2698 [50:41<23:05,  1.64s/it]\u001b[A\n"," 69% 1855/2698 [50:43<23:04,  1.64s/it]\u001b[A\n"," 69% 1856/2698 [50:45<23:02,  1.64s/it]\u001b[A\n"," 69% 1857/2698 [50:46<22:58,  1.64s/it]\u001b[A\n"," 69% 1858/2698 [50:48<22:53,  1.64s/it]\u001b[A\n"," 69% 1859/2698 [50:50<22:51,  1.64s/it]\u001b[A\n"," 69% 1860/2698 [50:51<22:50,  1.64s/it]\u001b[A\n"," 69% 1861/2698 [50:53<22:52,  1.64s/it]\u001b[A\n"," 69% 1862/2698 [50:54<22:52,  1.64s/it]\u001b[A\n"," 69% 1863/2698 [50:56<22:49,  1.64s/it]\u001b[A\n"," 69% 1864/2698 [50:58<22:49,  1.64s/it]\u001b[A\n"," 69% 1865/2698 [50:59<22:49,  1.64s/it]\u001b[A\n"," 69% 1866/2698 [51:01<22:46,  1.64s/it]\u001b[A\n"," 69% 1867/2698 [51:03<22:42,  1.64s/it]\u001b[A\n"," 69% 1868/2698 [51:04<22:39,  1.64s/it]\u001b[A\n"," 69% 1869/2698 [51:06<22:38,  1.64s/it]\u001b[A\n"," 69% 1870/2698 [51:08<22:37,  1.64s/it]\u001b[A\n"," 69% 1871/2698 [51:09<22:33,  1.64s/it]\u001b[A\n"," 69% 1872/2698 [51:11<22:34,  1.64s/it]\u001b[A\n"," 69% 1873/2698 [51:13<22:31,  1.64s/it]\u001b[A\n"," 69% 1874/2698 [51:14<22:27,  1.64s/it]\u001b[A\n"," 69% 1875/2698 [51:16<22:27,  1.64s/it]\u001b[A\n"," 70% 1876/2698 [51:17<22:27,  1.64s/it]\u001b[A\n"," 70% 1877/2698 [51:19<22:24,  1.64s/it]\u001b[A\n"," 70% 1878/2698 [51:21<22:27,  1.64s/it]\u001b[A\n"," 70% 1879/2698 [51:22<22:20,  1.64s/it]\u001b[A\n"," 70% 1880/2698 [51:24<22:17,  1.63s/it]\u001b[A\n"," 70% 1881/2698 [51:26<22:15,  1.63s/it]\u001b[A\n"," 70% 1882/2698 [51:27<22:14,  1.64s/it]\u001b[A\n"," 70% 1883/2698 [51:29<22:11,  1.63s/it]\u001b[A\n"," 70% 1884/2698 [51:31<22:12,  1.64s/it]\u001b[A\n"," 70% 1885/2698 [51:32<22:09,  1.63s/it]\u001b[A\n"," 70% 1886/2698 [51:34<22:07,  1.63s/it]\u001b[A\n"," 70% 1887/2698 [51:35<22:05,  1.63s/it]\u001b[A\n"," 70% 1888/2698 [51:37<22:02,  1.63s/it]\u001b[A\n"," 70% 1889/2698 [51:39<22:04,  1.64s/it]\u001b[A\n"," 70% 1890/2698 [51:40<21:59,  1.63s/it]\u001b[A\n"," 70% 1891/2698 [51:42<21:56,  1.63s/it]\u001b[A\n"," 70% 1892/2698 [51:44<21:55,  1.63s/it]\u001b[A\n"," 70% 1893/2698 [51:45<21:54,  1.63s/it]\u001b[A\n"," 70% 1894/2698 [51:47<21:53,  1.63s/it]\u001b[A\n"," 70% 1895/2698 [51:49<21:53,  1.64s/it]\u001b[A\n"," 70% 1896/2698 [51:50<21:55,  1.64s/it]\u001b[A\n"," 70% 1897/2698 [51:52<21:49,  1.63s/it]\u001b[A\n"," 70% 1898/2698 [51:53<21:47,  1.63s/it]\u001b[A\n"," 70% 1899/2698 [51:55<21:45,  1.63s/it]\u001b[A\n"," 70% 1900/2698 [51:57<21:45,  1.64s/it]\u001b[A\n"," 70% 1901/2698 [51:58<21:44,  1.64s/it]\u001b[A\n"," 70% 1902/2698 [52:00<21:46,  1.64s/it]\u001b[A\n"," 71% 1903/2698 [52:02<21:38,  1.63s/it]\u001b[A\n"," 71% 1904/2698 [52:03<21:38,  1.64s/it]\u001b[A\n"," 71% 1905/2698 [52:05<21:38,  1.64s/it]\u001b[A\n"," 71% 1906/2698 [52:07<21:36,  1.64s/it]\u001b[A\n"," 71% 1907/2698 [52:08<21:32,  1.63s/it]\u001b[A\n"," 71% 1908/2698 [52:10<21:32,  1.64s/it]\u001b[A\n"," 71% 1909/2698 [52:11<21:28,  1.63s/it]\u001b[A\n"," 71% 1910/2698 [52:13<21:26,  1.63s/it]\u001b[A\n"," 71% 1911/2698 [52:15<21:26,  1.63s/it]\u001b[A\n"," 71% 1912/2698 [52:16<21:25,  1.64s/it]\u001b[A\n"," 71% 1913/2698 [52:18<21:23,  1.63s/it]\u001b[A\n"," 71% 1914/2698 [52:20<21:22,  1.64s/it]\u001b[A\n"," 71% 1915/2698 [52:21<21:22,  1.64s/it]\u001b[A\n"," 71% 1916/2698 [52:23<21:20,  1.64s/it]\u001b[A\n"," 71% 1917/2698 [52:24<21:18,  1.64s/it]\u001b[A\n"," 71% 1918/2698 [52:26<21:15,  1.63s/it]\u001b[A\n"," 71% 1919/2698 [52:28<21:13,  1.63s/it]\u001b[A\n"," 71% 1920/2698 [52:29<21:16,  1.64s/it]\u001b[A\n"," 71% 1921/2698 [52:31<21:17,  1.64s/it]\u001b[A\n"," 71% 1922/2698 [52:33<21:14,  1.64s/it]\u001b[A\n"," 71% 1923/2698 [52:34<21:14,  1.65s/it]\u001b[A\n"," 71% 1924/2698 [52:36<21:18,  1.65s/it]\u001b[A\n"," 71% 1925/2698 [52:38<21:13,  1.65s/it]\u001b[A\n"," 71% 1926/2698 [52:39<21:14,  1.65s/it]\u001b[A\n"," 71% 1927/2698 [52:41<21:11,  1.65s/it]\u001b[A\n"," 71% 1928/2698 [52:43<21:12,  1.65s/it]\u001b[A\n"," 71% 1929/2698 [52:44<21:07,  1.65s/it]\u001b[A\n"," 72% 1930/2698 [52:46<21:03,  1.65s/it]\u001b[A\n"," 72% 1931/2698 [52:48<21:05,  1.65s/it]\u001b[A\n"," 72% 1932/2698 [52:49<21:04,  1.65s/it]\u001b[A\n"," 72% 1933/2698 [52:51<21:01,  1.65s/it]\u001b[A\n"," 72% 1934/2698 [52:53<21:03,  1.65s/it]\u001b[A\n"," 72% 1935/2698 [52:54<21:00,  1.65s/it]\u001b[A\n"," 72% 1936/2698 [52:56<20:58,  1.65s/it]\u001b[A\n"," 72% 1937/2698 [52:57<20:55,  1.65s/it]\u001b[A\n"," 72% 1938/2698 [52:59<20:57,  1.65s/it]\u001b[A\n"," 72% 1939/2698 [53:01<20:56,  1.66s/it]\u001b[A\n"," 72% 1940/2698 [53:02<20:53,  1.65s/it]\u001b[A\n"," 72% 1941/2698 [53:04<20:50,  1.65s/it]\u001b[A\n"," 72% 1942/2698 [53:06<20:48,  1.65s/it]\u001b[A\n"," 72% 1943/2698 [53:07<20:47,  1.65s/it]\u001b[A\n"," 72% 1944/2698 [53:09<20:43,  1.65s/it]\u001b[A\n"," 72% 1945/2698 [53:11<20:41,  1.65s/it]\u001b[A\n"," 72% 1946/2698 [53:12<20:38,  1.65s/it]\u001b[A\n"," 72% 1947/2698 [53:14<20:35,  1.64s/it]\u001b[A\n"," 72% 1948/2698 [53:16<20:32,  1.64s/it]\u001b[A\n"," 72% 1949/2698 [53:17<20:30,  1.64s/it]\u001b[A\n"," 72% 1950/2698 [53:19<20:30,  1.65s/it]\u001b[A\n"," 72% 1951/2698 [53:21<20:27,  1.64s/it]\u001b[A\n"," 72% 1952/2698 [53:22<20:26,  1.64s/it]\u001b[A\n"," 72% 1953/2698 [53:24<20:27,  1.65s/it]\u001b[A\n"," 72% 1954/2698 [53:25<20:24,  1.65s/it]\u001b[A\n"," 72% 1955/2698 [53:27<20:23,  1.65s/it]\u001b[A\n"," 72% 1956/2698 [53:29<20:24,  1.65s/it]\u001b[A\n"," 73% 1957/2698 [53:30<20:21,  1.65s/it]\u001b[A\n"," 73% 1958/2698 [53:32<20:22,  1.65s/it]\u001b[A\n"," 73% 1959/2698 [53:34<20:19,  1.65s/it]\u001b[A\n"," 73% 1960/2698 [53:35<20:16,  1.65s/it]\u001b[A\n"," 73% 1961/2698 [53:37<20:14,  1.65s/it]\u001b[A\n"," 73% 1962/2698 [53:39<20:10,  1.65s/it]\u001b[A\n"," 73% 1963/2698 [53:40<20:10,  1.65s/it]\u001b[A\n"," 73% 1964/2698 [53:42<20:09,  1.65s/it]\u001b[A\n"," 73% 1965/2698 [53:44<20:06,  1.65s/it]\u001b[A\n"," 73% 1966/2698 [53:45<20:06,  1.65s/it]\u001b[A\n"," 73% 1967/2698 [53:47<20:05,  1.65s/it]\u001b[A\n"," 73% 1968/2698 [53:49<20:04,  1.65s/it]\u001b[A\n"," 73% 1969/2698 [53:50<20:01,  1.65s/it]\u001b[A\n"," 73% 1970/2698 [53:52<20:01,  1.65s/it]\u001b[A\n"," 73% 1971/2698 [53:54<20:00,  1.65s/it]\u001b[A\n"," 73% 1972/2698 [53:55<19:54,  1.65s/it]\u001b[A\n"," 73% 1973/2698 [53:57<19:52,  1.64s/it]\u001b[A\n"," 73% 1974/2698 [53:58<19:53,  1.65s/it]\u001b[A\n"," 73% 1975/2698 [54:00<19:52,  1.65s/it]\u001b[A\n"," 73% 1976/2698 [54:02<19:49,  1.65s/it]\u001b[A\n"," 73% 1977/2698 [54:03<19:48,  1.65s/it]\u001b[A\n"," 73% 1978/2698 [54:05<19:46,  1.65s/it]\u001b[A\n"," 73% 1979/2698 [54:07<19:44,  1.65s/it]\u001b[A\n"," 73% 1980/2698 [54:08<19:42,  1.65s/it]\u001b[A\n"," 73% 1981/2698 [54:10<19:39,  1.65s/it]\u001b[A\n"," 73% 1982/2698 [54:12<19:38,  1.65s/it]\u001b[A\n"," 73% 1983/2698 [54:13<19:35,  1.64s/it]\u001b[A\n"," 74% 1984/2698 [54:15<19:32,  1.64s/it]\u001b[A\n"," 74% 1985/2698 [54:17<19:32,  1.64s/it]\u001b[A\n"," 74% 1986/2698 [54:18<19:33,  1.65s/it]\u001b[A\n"," 74% 1987/2698 [54:20<19:30,  1.65s/it]\u001b[A\n"," 74% 1988/2698 [54:22<19:28,  1.65s/it]\u001b[A\n"," 74% 1989/2698 [54:23<19:27,  1.65s/it]\u001b[A\n"," 74% 1990/2698 [54:25<19:27,  1.65s/it]\u001b[A\n"," 74% 1991/2698 [54:26<19:26,  1.65s/it]\u001b[A\n"," 74% 1992/2698 [54:28<19:23,  1.65s/it]\u001b[A\n"," 74% 1993/2698 [54:30<19:22,  1.65s/it]\u001b[A\n"," 74% 1994/2698 [54:31<19:22,  1.65s/it]\u001b[A\n"," 74% 1995/2698 [54:33<19:19,  1.65s/it]\u001b[A\n"," 74% 1996/2698 [54:35<19:14,  1.65s/it]\u001b[A\n"," 74% 1997/2698 [54:36<19:12,  1.64s/it]\u001b[A\n"," 74% 1998/2698 [54:38<19:12,  1.65s/it]\u001b[A\n"," 74% 1999/2698 [54:40<19:07,  1.64s/it]\u001b[A\n"," 74% 2000/2698 [54:41<19:06,  1.64s/it]\u001b[A\n"," 74% 2001/2698 [54:43<19:06,  1.65s/it]\u001b[A\n"," 74% 2002/2698 [54:45<19:04,  1.64s/it]\u001b[A\n"," 74% 2003/2698 [54:46<19:03,  1.64s/it]\u001b[A\n"," 74% 2004/2698 [54:48<18:58,  1.64s/it]\u001b[A\n"," 74% 2005/2698 [54:49<18:59,  1.64s/it]\u001b[A\n"," 74% 2006/2698 [54:51<19:00,  1.65s/it]\u001b[A\n"," 74% 2007/2698 [54:53<18:56,  1.64s/it]\u001b[A\n"," 74% 2008/2698 [54:54<18:55,  1.65s/it]\u001b[A\n"," 74% 2009/2698 [54:56<18:51,  1.64s/it]\u001b[A\n"," 74% 2010/2698 [54:58<18:49,  1.64s/it]\u001b[A\n"," 75% 2011/2698 [54:59<18:49,  1.64s/it]\u001b[A\n"," 75% 2012/2698 [55:01<18:49,  1.65s/it]\u001b[A\n"," 75% 2013/2698 [55:03<18:47,  1.65s/it]\u001b[A\n"," 75% 2014/2698 [55:04<18:44,  1.64s/it]\u001b[A\n"," 75% 2015/2698 [55:06<18:42,  1.64s/it]\u001b[A\n"," 75% 2016/2698 [55:08<18:42,  1.65s/it]\u001b[A\n"," 75% 2017/2698 [55:09<18:40,  1.65s/it]\u001b[A\n"," 75% 2018/2698 [55:11<18:39,  1.65s/it]\u001b[A\n"," 75% 2019/2698 [55:13<18:35,  1.64s/it]\u001b[A\n"," 75% 2020/2698 [55:14<18:34,  1.64s/it]\u001b[A\n"," 75% 2021/2698 [55:16<18:34,  1.65s/it]\u001b[A\n"," 75% 2022/2698 [55:17<18:32,  1.65s/it]\u001b[A\n"," 75% 2023/2698 [55:19<18:29,  1.64s/it]\u001b[A\n"," 75% 2024/2698 [55:21<18:27,  1.64s/it]\u001b[A\n"," 75% 2025/2698 [55:22<18:23,  1.64s/it]\u001b[A\n"," 75% 2026/2698 [55:24<18:20,  1.64s/it]\u001b[A\n"," 75% 2027/2698 [55:26<18:17,  1.64s/it]\u001b[A\n"," 75% 2028/2698 [55:27<18:15,  1.63s/it]\u001b[A\n"," 75% 2029/2698 [55:29<18:12,  1.63s/it]\u001b[A\n"," 75% 2030/2698 [55:31<18:11,  1.63s/it]\u001b[A\n"," 75% 2031/2698 [55:32<18:08,  1.63s/it]\u001b[A\n"," 75% 2032/2698 [55:34<18:07,  1.63s/it]\u001b[A\n"," 75% 2033/2698 [55:35<18:09,  1.64s/it]\u001b[A\n"," 75% 2034/2698 [55:37<18:08,  1.64s/it]\u001b[A\n"," 75% 2035/2698 [55:39<18:07,  1.64s/it]\u001b[A\n"," 75% 2036/2698 [55:40<18:02,  1.64s/it]\u001b[A\n"," 76% 2037/2698 [55:42<17:59,  1.63s/it]\u001b[A\n"," 76% 2038/2698 [55:44<17:56,  1.63s/it]\u001b[A\n"," 76% 2039/2698 [55:45<17:54,  1.63s/it]\u001b[A\n"," 76% 2040/2698 [55:47<17:53,  1.63s/it]\u001b[A\n"," 76% 2041/2698 [55:48<17:48,  1.63s/it]\u001b[A\n"," 76% 2042/2698 [55:50<17:46,  1.63s/it]\u001b[A\n"," 76% 2043/2698 [55:52<17:44,  1.62s/it]\u001b[A\n"," 76% 2044/2698 [55:53<17:42,  1.62s/it]\u001b[A\n"," 76% 2045/2698 [55:55<17:42,  1.63s/it]\u001b[A\n"," 76% 2046/2698 [55:57<17:42,  1.63s/it]\u001b[A\n"," 76% 2047/2698 [55:58<17:41,  1.63s/it]\u001b[A\n"," 76% 2048/2698 [56:00<17:37,  1.63s/it]\u001b[A\n"," 76% 2049/2698 [56:02<17:37,  1.63s/it]\u001b[A\n"," 76% 2050/2698 [56:03<17:37,  1.63s/it]\u001b[A\n"," 76% 2051/2698 [56:05<17:34,  1.63s/it]\u001b[A\n"," 76% 2052/2698 [56:06<17:31,  1.63s/it]\u001b[A\n"," 76% 2053/2698 [56:08<17:31,  1.63s/it]\u001b[A\n"," 76% 2054/2698 [56:10<17:30,  1.63s/it]\u001b[A\n"," 76% 2055/2698 [56:11<17:28,  1.63s/it]\u001b[A\n"," 76% 2056/2698 [56:13<17:23,  1.63s/it]\u001b[A\n"," 76% 2057/2698 [56:15<17:21,  1.63s/it]\u001b[A\n"," 76% 2058/2698 [56:16<17:17,  1.62s/it]\u001b[A\n"," 76% 2059/2698 [56:18<17:17,  1.62s/it]\u001b[A\n"," 76% 2060/2698 [56:19<17:15,  1.62s/it]\u001b[A\n"," 76% 2061/2698 [56:21<17:14,  1.62s/it]\u001b[A\n"," 76% 2062/2698 [56:23<17:14,  1.63s/it]\u001b[A\n"," 76% 2063/2698 [56:24<17:13,  1.63s/it]\u001b[A\n"," 77% 2064/2698 [56:26<17:13,  1.63s/it]\u001b[A\n"," 77% 2065/2698 [56:28<17:11,  1.63s/it]\u001b[A\n"," 77% 2066/2698 [56:29<17:07,  1.63s/it]\u001b[A\n"," 77% 2067/2698 [56:31<17:05,  1.63s/it]\u001b[A\n"," 77% 2068/2698 [56:32<17:01,  1.62s/it]\u001b[A\n"," 77% 2069/2698 [56:34<17:00,  1.62s/it]\u001b[A\n"," 77% 2070/2698 [56:36<16:58,  1.62s/it]\u001b[A\n"," 77% 2071/2698 [56:37<16:57,  1.62s/it]\u001b[A\n"," 77% 2072/2698 [56:39<16:58,  1.63s/it]\u001b[A\n"," 77% 2073/2698 [56:41<16:57,  1.63s/it]\u001b[A\n"," 77% 2074/2698 [56:42<16:54,  1.63s/it]\u001b[A\n"," 77% 2075/2698 [56:44<16:52,  1.63s/it]\u001b[A\n"," 77% 2076/2698 [56:45<16:51,  1.63s/it]\u001b[A\n"," 77% 2077/2698 [56:47<16:50,  1.63s/it]\u001b[A\n"," 77% 2078/2698 [56:49<16:50,  1.63s/it]\u001b[A\n"," 77% 2079/2698 [56:50<16:46,  1.63s/it]\u001b[A\n"," 77% 2080/2698 [56:52<16:47,  1.63s/it]\u001b[A\n"," 77% 2081/2698 [56:54<16:45,  1.63s/it]\u001b[A\n"," 77% 2082/2698 [56:55<16:45,  1.63s/it]\u001b[A\n"," 77% 2083/2698 [56:57<16:45,  1.63s/it]\u001b[A\n"," 77% 2084/2698 [56:58<16:44,  1.64s/it]\u001b[A\n"," 77% 2085/2698 [57:00<16:40,  1.63s/it]\u001b[A\n"," 77% 2086/2698 [57:02<16:39,  1.63s/it]\u001b[A\n"," 77% 2087/2698 [57:03<16:37,  1.63s/it]\u001b[A\n"," 77% 2088/2698 [57:05<16:33,  1.63s/it]\u001b[A\n"," 77% 2089/2698 [57:07<16:34,  1.63s/it]\u001b[A\n"," 77% 2090/2698 [57:08<16:33,  1.63s/it]\u001b[A\n"," 78% 2091/2698 [57:10<16:33,  1.64s/it]\u001b[A\n"," 78% 2092/2698 [57:12<16:31,  1.64s/it]\u001b[A\n"," 78% 2093/2698 [57:13<16:31,  1.64s/it]\u001b[A\n"," 78% 2094/2698 [57:15<16:28,  1.64s/it]\u001b[A\n"," 78% 2095/2698 [57:16<16:28,  1.64s/it]\u001b[A\n"," 78% 2096/2698 [57:18<16:27,  1.64s/it]\u001b[A\n"," 78% 2097/2698 [57:20<16:26,  1.64s/it]\u001b[A\n"," 78% 2098/2698 [57:21<16:23,  1.64s/it]\u001b[A\n"," 78% 2099/2698 [57:23<16:25,  1.64s/it]\u001b[A\n"," 78% 2100/2698 [57:25<16:25,  1.65s/it]\u001b[A\n"," 78% 2101/2698 [57:26<16:24,  1.65s/it]\u001b[A\n"," 78% 2102/2698 [57:28<16:22,  1.65s/it]\u001b[A\n"," 78% 2103/2698 [57:30<16:21,  1.65s/it]\u001b[A\n"," 78% 2104/2698 [57:31<16:18,  1.65s/it]\u001b[A\n"," 78% 2105/2698 [57:33<16:17,  1.65s/it]\u001b[A\n"," 78% 2106/2698 [57:35<16:16,  1.65s/it]\u001b[A\n"," 78% 2107/2698 [57:36<16:14,  1.65s/it]\u001b[A\n"," 78% 2108/2698 [57:38<16:12,  1.65s/it]\u001b[A\n"," 78% 2109/2698 [57:40<16:08,  1.65s/it]\u001b[A\n"," 78% 2110/2698 [57:41<16:08,  1.65s/it]\u001b[A\n"," 78% 2111/2698 [57:43<16:04,  1.64s/it]\u001b[A\n"," 78% 2112/2698 [57:44<16:03,  1.65s/it]\u001b[A\n"," 78% 2113/2698 [57:46<15:59,  1.64s/it]\u001b[A\n"," 78% 2114/2698 [57:48<15:59,  1.64s/it]\u001b[A\n"," 78% 2115/2698 [57:49<15:58,  1.64s/it]\u001b[A\n"," 78% 2116/2698 [57:51<15:56,  1.64s/it]\u001b[A\n"," 78% 2117/2698 [57:53<15:55,  1.64s/it]\u001b[A\n"," 79% 2118/2698 [57:54<15:51,  1.64s/it]\u001b[A\n"," 79% 2119/2698 [57:56<15:51,  1.64s/it]\u001b[A\n"," 79% 2120/2698 [57:58<15:48,  1.64s/it]\u001b[A\n"," 79% 2121/2698 [57:59<15:47,  1.64s/it]\u001b[A\n"," 79% 2122/2698 [58:01<15:46,  1.64s/it]\u001b[A\n"," 79% 2123/2698 [58:03<15:43,  1.64s/it]\u001b[A\n"," 79% 2124/2698 [58:04<15:43,  1.64s/it]\u001b[A\n"," 79% 2125/2698 [58:06<15:43,  1.65s/it]\u001b[A\n"," 79% 2126/2698 [58:07<15:43,  1.65s/it]\u001b[A\n"," 79% 2127/2698 [58:09<15:40,  1.65s/it]\u001b[A\n"," 79% 2128/2698 [58:11<15:40,  1.65s/it]\u001b[A\n"," 79% 2129/2698 [58:12<15:39,  1.65s/it]\u001b[A\n"," 79% 2130/2698 [58:14<15:39,  1.65s/it]\u001b[A\n"," 79% 2131/2698 [58:16<15:37,  1.65s/it]\u001b[A\n"," 79% 2132/2698 [58:17<15:35,  1.65s/it]\u001b[A\n"," 79% 2133/2698 [58:19<15:31,  1.65s/it]\u001b[A\n"," 79% 2134/2698 [58:21<15:29,  1.65s/it]\u001b[A\n"," 79% 2135/2698 [58:22<15:28,  1.65s/it]\u001b[A\n"," 79% 2136/2698 [58:24<15:26,  1.65s/it]\u001b[A\n"," 79% 2137/2698 [58:26<15:22,  1.65s/it]\u001b[A\n"," 79% 2138/2698 [58:27<15:22,  1.65s/it]\u001b[A\n"," 79% 2139/2698 [58:29<15:21,  1.65s/it]\u001b[A\n"," 79% 2140/2698 [58:31<15:18,  1.65s/it]\u001b[A\n"," 79% 2141/2698 [58:32<15:17,  1.65s/it]\u001b[A\n"," 79% 2142/2698 [58:34<15:16,  1.65s/it]\u001b[A\n"," 79% 2143/2698 [58:36<15:15,  1.65s/it]\u001b[A\n"," 79% 2144/2698 [58:37<15:14,  1.65s/it]\u001b[A\n"," 80% 2145/2698 [58:39<15:11,  1.65s/it]\u001b[A\n"," 80% 2146/2698 [58:40<15:11,  1.65s/it]\u001b[A\n"," 80% 2147/2698 [58:42<15:09,  1.65s/it]\u001b[A\n"," 80% 2148/2698 [58:44<15:07,  1.65s/it]\u001b[A\n"," 80% 2149/2698 [58:45<15:05,  1.65s/it]\u001b[A\n"," 80% 2150/2698 [58:47<15:03,  1.65s/it]\u001b[A\n"," 80% 2151/2698 [58:49<15:02,  1.65s/it]\u001b[A\n"," 80% 2152/2698 [58:50<15:02,  1.65s/it]\u001b[A\n"," 80% 2153/2698 [58:52<15:01,  1.65s/it]\u001b[A\n"," 80% 2154/2698 [58:54<15:01,  1.66s/it]\u001b[A\n"," 80% 2155/2698 [58:55<15:00,  1.66s/it]\u001b[A\n"," 80% 2156/2698 [58:57<14:56,  1.65s/it]\u001b[A\n"," 80% 2157/2698 [58:59<14:54,  1.65s/it]\u001b[A\n"," 80% 2158/2698 [59:00<14:53,  1.66s/it]\u001b[A\n"," 80% 2159/2698 [59:02<14:51,  1.65s/it]\u001b[A\n"," 80% 2160/2698 [59:04<14:49,  1.65s/it]\u001b[A\n"," 80% 2161/2698 [59:05<14:47,  1.65s/it]\u001b[A\n"," 80% 2162/2698 [59:07<14:45,  1.65s/it]\u001b[A\n"," 80% 2163/2698 [59:09<14:45,  1.65s/it]\u001b[A\n"," 80% 2164/2698 [59:10<14:42,  1.65s/it]\u001b[A\n"," 80% 2165/2698 [59:12<14:41,  1.65s/it]\u001b[A\n"," 80% 2166/2698 [59:14<14:39,  1.65s/it]\u001b[A\n"," 80% 2167/2698 [59:15<14:36,  1.65s/it]\u001b[A\n"," 80% 2168/2698 [59:17<14:35,  1.65s/it]\u001b[A\n"," 80% 2169/2698 [59:18<14:33,  1.65s/it]\u001b[A\n"," 80% 2170/2698 [59:20<14:32,  1.65s/it]\u001b[A\n"," 80% 2171/2698 [59:22<14:28,  1.65s/it]\u001b[A\n"," 81% 2172/2698 [59:23<14:26,  1.65s/it]\u001b[A\n"," 81% 2173/2698 [59:25<14:26,  1.65s/it]\u001b[A\n"," 81% 2174/2698 [59:27<14:25,  1.65s/it]\u001b[A\n"," 81% 2175/2698 [59:28<14:25,  1.65s/it]\u001b[A\n"," 81% 2176/2698 [59:30<14:22,  1.65s/it]\u001b[A\n"," 81% 2177/2698 [59:32<14:21,  1.65s/it]\u001b[A\n"," 81% 2178/2698 [59:33<14:19,  1.65s/it]\u001b[A\n"," 81% 2179/2698 [59:35<14:17,  1.65s/it]\u001b[A\n"," 81% 2180/2698 [59:37<14:14,  1.65s/it]\u001b[A\n"," 81% 2181/2698 [59:38<14:15,  1.65s/it]\u001b[A\n"," 81% 2182/2698 [59:40<14:12,  1.65s/it]\u001b[A\n"," 81% 2183/2698 [59:42<14:10,  1.65s/it]\u001b[A\n"," 81% 2184/2698 [59:43<14:09,  1.65s/it]\u001b[A\n"," 81% 2185/2698 [59:45<14:07,  1.65s/it]\u001b[A\n"," 81% 2186/2698 [59:47<14:03,  1.65s/it]\u001b[A\n"," 81% 2187/2698 [59:48<14:00,  1.64s/it]\u001b[A\n"," 81% 2188/2698 [59:50<13:58,  1.64s/it]\u001b[A\n"," 81% 2189/2698 [59:51<13:56,  1.64s/it]\u001b[A\n"," 81% 2190/2698 [59:53<13:56,  1.65s/it]\u001b[A\n"," 81% 2191/2698 [59:55<13:55,  1.65s/it]\u001b[A\n"," 81% 2192/2698 [59:56<13:53,  1.65s/it]\u001b[A\n"," 81% 2193/2698 [59:58<13:52,  1.65s/it]\u001b[A\n"," 81% 2194/2698 [1:00:00<13:47,  1.64s/it]\u001b[A\n"," 81% 2195/2698 [1:00:01<13:45,  1.64s/it]\u001b[A\n"," 81% 2196/2698 [1:00:03<13:44,  1.64s/it]\u001b[A\n"," 81% 2197/2698 [1:00:05<13:41,  1.64s/it]\u001b[A\n"," 81% 2198/2698 [1:00:06<13:39,  1.64s/it]\u001b[A\n"," 82% 2199/2698 [1:00:08<13:36,  1.64s/it]\u001b[A\n"," 82% 2200/2698 [1:00:10<13:34,  1.64s/it]\u001b[A\n"," 82% 2201/2698 [1:00:11<13:34,  1.64s/it]\u001b[A\n"," 82% 2202/2698 [1:00:13<13:34,  1.64s/it]\u001b[A\n"," 82% 2203/2698 [1:00:14<13:31,  1.64s/it]\u001b[A\n"," 82% 2204/2698 [1:00:16<13:28,  1.64s/it]\u001b[A\n"," 82% 2205/2698 [1:00:18<13:27,  1.64s/it]\u001b[A\n"," 82% 2206/2698 [1:00:19<13:26,  1.64s/it]\u001b[A\n"," 82% 2207/2698 [1:00:21<13:23,  1.64s/it]\u001b[A\n"," 82% 2208/2698 [1:00:23<13:22,  1.64s/it]\u001b[A\n"," 82% 2209/2698 [1:00:24<13:20,  1.64s/it]\u001b[A\n"," 82% 2210/2698 [1:00:26<13:19,  1.64s/it]\u001b[A\n"," 82% 2211/2698 [1:00:28<13:16,  1.64s/it]\u001b[A\n"," 82% 2212/2698 [1:00:29<13:15,  1.64s/it]\u001b[A\n"," 82% 2213/2698 [1:00:31<13:12,  1.63s/it]\u001b[A\n"," 82% 2214/2698 [1:00:32<13:11,  1.64s/it]\u001b[A\n"," 82% 2215/2698 [1:00:34<13:10,  1.64s/it]\u001b[A\n"," 82% 2216/2698 [1:00:36<13:09,  1.64s/it]\u001b[A\n"," 82% 2217/2698 [1:00:37<13:07,  1.64s/it]\u001b[A\n"," 82% 2218/2698 [1:00:39<13:04,  1.63s/it]\u001b[A\n"," 82% 2219/2698 [1:00:41<13:04,  1.64s/it]\u001b[A\n"," 82% 2220/2698 [1:00:42<13:02,  1.64s/it]\u001b[A\n"," 82% 2221/2698 [1:00:44<13:00,  1.64s/it]\u001b[A\n"," 82% 2222/2698 [1:00:46<12:59,  1.64s/it]\u001b[A\n"," 82% 2223/2698 [1:00:47<12:56,  1.64s/it]\u001b[A\n"," 82% 2224/2698 [1:00:49<12:57,  1.64s/it]\u001b[A\n"," 82% 2225/2698 [1:00:50<12:55,  1.64s/it]\u001b[A\n"," 83% 2226/2698 [1:00:52<12:52,  1.64s/it]\u001b[A\n"," 83% 2227/2698 [1:00:54<12:48,  1.63s/it]\u001b[A\n"," 83% 2228/2698 [1:00:55<12:46,  1.63s/it]\u001b[A\n"," 83% 2229/2698 [1:00:57<12:46,  1.63s/it]\u001b[A\n"," 83% 2230/2698 [1:00:59<12:46,  1.64s/it]\u001b[A\n"," 83% 2231/2698 [1:01:00<12:45,  1.64s/it]\u001b[A\n"," 83% 2232/2698 [1:01:02<12:43,  1.64s/it]\u001b[A\n"," 83% 2233/2698 [1:01:04<12:41,  1.64s/it]\u001b[A\n"," 83% 2234/2698 [1:01:05<12:38,  1.63s/it]\u001b[A\n"," 83% 2235/2698 [1:01:07<12:36,  1.63s/it]\u001b[A\n"," 83% 2236/2698 [1:01:08<12:34,  1.63s/it]\u001b[A\n"," 83% 2237/2698 [1:01:10<12:34,  1.64s/it]\u001b[A\n"," 83% 2238/2698 [1:01:12<12:33,  1.64s/it]\u001b[A\n"," 83% 2239/2698 [1:01:13<12:30,  1.64s/it]\u001b[A\n"," 83% 2240/2698 [1:01:15<12:28,  1.63s/it]\u001b[A\n"," 83% 2241/2698 [1:01:17<12:26,  1.63s/it]\u001b[A\n"," 83% 2242/2698 [1:01:18<12:24,  1.63s/it]\u001b[A\n"," 83% 2243/2698 [1:01:20<12:23,  1.63s/it]\u001b[A\n"," 83% 2244/2698 [1:01:22<12:22,  1.64s/it]\u001b[A\n"," 83% 2245/2698 [1:01:23<12:21,  1.64s/it]\u001b[A\n"," 83% 2246/2698 [1:01:25<12:21,  1.64s/it]\u001b[A\n"," 83% 2247/2698 [1:01:26<12:18,  1.64s/it]\u001b[A\n"," 83% 2248/2698 [1:01:28<12:18,  1.64s/it]\u001b[A\n"," 83% 2249/2698 [1:01:30<12:17,  1.64s/it]\u001b[A\n"," 83% 2250/2698 [1:01:31<12:13,  1.64s/it]\u001b[A\n"," 83% 2251/2698 [1:01:33<12:09,  1.63s/it]\u001b[A\n"," 83% 2252/2698 [1:01:35<12:07,  1.63s/it]\u001b[A\n"," 84% 2253/2698 [1:01:36<12:05,  1.63s/it]\u001b[A\n"," 84% 2254/2698 [1:01:38<12:05,  1.63s/it]\u001b[A\n"," 84% 2255/2698 [1:01:40<12:03,  1.63s/it]\u001b[A\n"," 84% 2256/2698 [1:01:41<12:01,  1.63s/it]\u001b[A\n"," 84% 2257/2698 [1:01:43<12:01,  1.64s/it]\u001b[A\n"," 84% 2258/2698 [1:01:44<12:03,  1.64s/it]\u001b[A\n"," 84% 2259/2698 [1:01:46<12:00,  1.64s/it]\u001b[A\n"," 84% 2260/2698 [1:01:48<12:00,  1.64s/it]\u001b[A\n"," 84% 2261/2698 [1:01:49<11:58,  1.64s/it]\u001b[A\n"," 84% 2262/2698 [1:01:51<11:56,  1.64s/it]\u001b[A\n"," 84% 2263/2698 [1:01:53<11:55,  1.65s/it]\u001b[A\n"," 84% 2264/2698 [1:01:54<11:54,  1.65s/it]\u001b[A\n"," 84% 2265/2698 [1:01:56<11:52,  1.64s/it]\u001b[A\n"," 84% 2266/2698 [1:01:58<11:50,  1.65s/it]\u001b[A\n"," 84% 2267/2698 [1:01:59<11:46,  1.64s/it]\u001b[A\n"," 84% 2268/2698 [1:02:01<11:45,  1.64s/it]\u001b[A\n"," 84% 2269/2698 [1:02:03<11:45,  1.64s/it]\u001b[A\n"," 84% 2270/2698 [1:02:04<11:43,  1.64s/it]\u001b[A\n"," 84% 2271/2698 [1:02:06<11:42,  1.64s/it]\u001b[A\n"," 84% 2272/2698 [1:02:07<11:40,  1.64s/it]\u001b[A\n"," 84% 2273/2698 [1:02:09<11:39,  1.65s/it]\u001b[A\n"," 84% 2274/2698 [1:02:11<11:37,  1.65s/it]\u001b[A\n"," 84% 2275/2698 [1:02:12<11:37,  1.65s/it]\u001b[A\n"," 84% 2276/2698 [1:02:14<11:35,  1.65s/it]\u001b[A\n"," 84% 2277/2698 [1:02:16<11:34,  1.65s/it]\u001b[A\n"," 84% 2278/2698 [1:02:17<11:32,  1.65s/it]\u001b[A\n"," 84% 2279/2698 [1:02:19<11:28,  1.64s/it]\u001b[A\n"," 85% 2280/2698 [1:02:21<11:29,  1.65s/it]\u001b[A\n"," 85% 2281/2698 [1:02:22<11:27,  1.65s/it]\u001b[A\n"," 85% 2282/2698 [1:02:24<11:26,  1.65s/it]\u001b[A\n"," 85% 2283/2698 [1:02:26<11:23,  1.65s/it]\u001b[A\n"," 85% 2284/2698 [1:02:27<11:21,  1.65s/it]\u001b[A\n"," 85% 2285/2698 [1:02:29<11:19,  1.64s/it]\u001b[A\n"," 85% 2286/2698 [1:02:31<11:17,  1.64s/it]\u001b[A\n"," 85% 2287/2698 [1:02:32<11:15,  1.64s/it]\u001b[A\n"," 85% 2288/2698 [1:02:34<11:14,  1.64s/it]\u001b[A\n"," 85% 2289/2698 [1:02:35<11:11,  1.64s/it]\u001b[A\n"," 85% 2290/2698 [1:02:37<11:10,  1.64s/it]\u001b[A\n"," 85% 2291/2698 [1:02:39<11:11,  1.65s/it]\u001b[A\n"," 85% 2292/2698 [1:02:40<11:07,  1.64s/it]\u001b[A\n"," 85% 2293/2698 [1:02:42<11:06,  1.65s/it]\u001b[A\n"," 85% 2294/2698 [1:02:44<11:05,  1.65s/it]\u001b[A\n"," 85% 2295/2698 [1:02:45<11:04,  1.65s/it]\u001b[A\n"," 85% 2296/2698 [1:02:47<11:03,  1.65s/it]\u001b[A\n"," 85% 2297/2698 [1:02:49<11:01,  1.65s/it]\u001b[A\n"," 85% 2298/2698 [1:02:50<11:01,  1.65s/it]\u001b[A\n"," 85% 2299/2698 [1:02:52<10:58,  1.65s/it]\u001b[A\n"," 85% 2300/2698 [1:02:54<10:56,  1.65s/it]\u001b[A\n"," 85% 2301/2698 [1:02:55<10:55,  1.65s/it]\u001b[A\n"," 85% 2302/2698 [1:02:57<10:52,  1.65s/it]\u001b[A\n"," 85% 2303/2698 [1:02:59<10:49,  1.64s/it]\u001b[A\n"," 85% 2304/2698 [1:03:00<10:47,  1.64s/it]\u001b[A\n"," 85% 2305/2698 [1:03:02<10:45,  1.64s/it]\u001b[A\n"," 85% 2306/2698 [1:03:03<10:43,  1.64s/it]\u001b[A\n"," 86% 2307/2698 [1:03:05<10:41,  1.64s/it]\u001b[A\n"," 86% 2308/2698 [1:03:07<10:40,  1.64s/it]\u001b[A\n"," 86% 2309/2698 [1:03:08<10:37,  1.64s/it]\u001b[A\n"," 86% 2310/2698 [1:03:10<10:35,  1.64s/it]\u001b[A\n"," 86% 2311/2698 [1:03:12<10:35,  1.64s/it]\u001b[A\n"," 86% 2312/2698 [1:03:13<10:33,  1.64s/it]\u001b[A\n"," 86% 2313/2698 [1:03:15<10:32,  1.64s/it]\u001b[A\n"," 86% 2314/2698 [1:03:17<10:31,  1.64s/it]\u001b[A\n"," 86% 2315/2698 [1:03:18<10:31,  1.65s/it]\u001b[A\n"," 86% 2316/2698 [1:03:20<10:28,  1.65s/it]\u001b[A\n"," 86% 2317/2698 [1:03:22<10:27,  1.65s/it]\u001b[A\n"," 86% 2318/2698 [1:03:23<10:23,  1.64s/it]\u001b[A\n"," 86% 2319/2698 [1:03:25<10:21,  1.64s/it]\u001b[A\n"," 86% 2320/2698 [1:03:26<10:19,  1.64s/it]\u001b[A\n"," 86% 2321/2698 [1:03:28<10:18,  1.64s/it]\u001b[A\n"," 86% 2322/2698 [1:03:30<10:16,  1.64s/it]\u001b[A\n"," 86% 2323/2698 [1:03:31<10:14,  1.64s/it]\u001b[A\n"," 86% 2324/2698 [1:03:33<10:14,  1.64s/it]\u001b[A\n"," 86% 2325/2698 [1:03:35<10:12,  1.64s/it]\u001b[A\n"," 86% 2326/2698 [1:03:36<10:11,  1.64s/it]\u001b[A\n"," 86% 2327/2698 [1:03:38<10:10,  1.65s/it]\u001b[A\n"," 86% 2328/2698 [1:03:40<10:08,  1.65s/it]\u001b[A\n"," 86% 2329/2698 [1:03:41<10:06,  1.64s/it]\u001b[A\n"," 86% 2330/2698 [1:03:43<10:05,  1.64s/it]\u001b[A\n"," 86% 2331/2698 [1:03:45<10:02,  1.64s/it]\u001b[A\n"," 86% 2332/2698 [1:03:46<10:00,  1.64s/it]\u001b[A\n"," 86% 2333/2698 [1:03:48<09:59,  1.64s/it]\u001b[A\n"," 87% 2334/2698 [1:03:49<09:57,  1.64s/it]\u001b[A\n"," 87% 2335/2698 [1:03:51<09:57,  1.65s/it]\u001b[A\n"," 87% 2336/2698 [1:03:53<09:55,  1.64s/it]\u001b[A\n"," 87% 2337/2698 [1:03:54<09:52,  1.64s/it]\u001b[A\n"," 87% 2338/2698 [1:03:56<09:52,  1.65s/it]\u001b[A\n"," 87% 2339/2698 [1:03:58<09:50,  1.64s/it]\u001b[A\n"," 87% 2340/2698 [1:03:59<09:48,  1.64s/it]\u001b[A\n"," 87% 2341/2698 [1:04:01<09:47,  1.65s/it]\u001b[A\n"," 87% 2342/2698 [1:04:03<09:45,  1.65s/it]\u001b[A\n"," 87% 2343/2698 [1:04:04<09:45,  1.65s/it]\u001b[A\n"," 87% 2344/2698 [1:04:06<09:43,  1.65s/it]\u001b[A\n"," 87% 2345/2698 [1:04:08<09:43,  1.65s/it]\u001b[A\n"," 87% 2346/2698 [1:04:09<09:40,  1.65s/it]\u001b[A\n"," 87% 2347/2698 [1:04:11<09:37,  1.65s/it]\u001b[A\n"," 87% 2348/2698 [1:04:12<09:36,  1.65s/it]\u001b[A\n"," 87% 2349/2698 [1:04:14<09:34,  1.65s/it]\u001b[A\n"," 87% 2350/2698 [1:04:16<09:32,  1.64s/it]\u001b[A\n"," 87% 2351/2698 [1:04:17<09:31,  1.65s/it]\u001b[A\n"," 87% 2352/2698 [1:04:19<09:29,  1.65s/it]\u001b[A\n"," 87% 2353/2698 [1:04:21<09:29,  1.65s/it]\u001b[A\n"," 87% 2354/2698 [1:04:22<09:27,  1.65s/it]\u001b[A\n"," 87% 2355/2698 [1:04:24<09:25,  1.65s/it]\u001b[A\n"," 87% 2356/2698 [1:04:26<09:23,  1.65s/it]\u001b[A\n"," 87% 2357/2698 [1:04:27<09:20,  1.64s/it]\u001b[A\n"," 87% 2358/2698 [1:04:29<09:19,  1.65s/it]\u001b[A\n"," 87% 2359/2698 [1:04:31<09:17,  1.64s/it]\u001b[A\n"," 87% 2360/2698 [1:04:32<09:14,  1.64s/it]\u001b[A\n"," 88% 2361/2698 [1:04:34<09:13,  1.64s/it]\u001b[A\n"," 88% 2362/2698 [1:04:36<09:10,  1.64s/it]\u001b[A\n"," 88% 2363/2698 [1:04:37<09:08,  1.64s/it]\u001b[A\n"," 88% 2364/2698 [1:04:39<09:06,  1.64s/it]\u001b[A\n"," 88% 2365/2698 [1:04:40<09:05,  1.64s/it]\u001b[A\n"," 88% 2366/2698 [1:04:42<09:02,  1.63s/it]\u001b[A\n"," 88% 2367/2698 [1:04:44<09:02,  1.64s/it]\u001b[A\n"," 88% 2368/2698 [1:04:45<09:00,  1.64s/it]\u001b[A\n"," 88% 2369/2698 [1:04:47<08:57,  1.64s/it]\u001b[A\n"," 88% 2370/2698 [1:04:49<08:56,  1.64s/it]\u001b[A\n"," 88% 2371/2698 [1:04:50<08:55,  1.64s/it]\u001b[A\n"," 88% 2372/2698 [1:04:52<08:53,  1.64s/it]\u001b[A\n"," 88% 2373/2698 [1:04:54<08:51,  1.64s/it]\u001b[A\n"," 88% 2374/2698 [1:04:55<08:49,  1.63s/it]\u001b[A\n"," 88% 2375/2698 [1:04:57<08:48,  1.64s/it]\u001b[A\n"," 88% 2376/2698 [1:04:58<08:45,  1.63s/it]\u001b[A\n"," 88% 2377/2698 [1:05:00<08:43,  1.63s/it]\u001b[A\n"," 88% 2378/2698 [1:05:02<08:42,  1.63s/it]\u001b[A\n"," 88% 2379/2698 [1:05:03<08:41,  1.63s/it]\u001b[A\n"," 88% 2380/2698 [1:05:05<08:38,  1.63s/it]\u001b[A\n"," 88% 2381/2698 [1:05:07<08:36,  1.63s/it]\u001b[A\n"," 88% 2382/2698 [1:05:08<08:36,  1.63s/it]\u001b[A\n"," 88% 2383/2698 [1:05:10<08:34,  1.63s/it]\u001b[A\n"," 88% 2384/2698 [1:05:11<08:32,  1.63s/it]\u001b[A\n"," 88% 2385/2698 [1:05:13<08:29,  1.63s/it]\u001b[A\n"," 88% 2386/2698 [1:05:15<08:27,  1.63s/it]\u001b[A\n"," 88% 2387/2698 [1:05:16<08:27,  1.63s/it]\u001b[A\n"," 89% 2388/2698 [1:05:18<08:25,  1.63s/it]\u001b[A\n"," 89% 2389/2698 [1:05:20<08:24,  1.63s/it]\u001b[A\n"," 89% 2390/2698 [1:05:21<08:22,  1.63s/it]\u001b[A\n"," 89% 2391/2698 [1:05:23<08:19,  1.63s/it]\u001b[A\n"," 89% 2392/2698 [1:05:24<08:18,  1.63s/it]\u001b[A\n"," 89% 2393/2698 [1:05:26<08:16,  1.63s/it]\u001b[A\n"," 89% 2394/2698 [1:05:28<08:14,  1.63s/it]\u001b[A\n"," 89% 2395/2698 [1:05:29<08:13,  1.63s/it]\u001b[A\n"," 89% 2396/2698 [1:05:31<08:12,  1.63s/it]\u001b[A\n"," 89% 2397/2698 [1:05:33<08:10,  1.63s/it]\u001b[A\n"," 89% 2398/2698 [1:05:34<08:09,  1.63s/it]\u001b[A\n"," 89% 2399/2698 [1:05:36<08:08,  1.63s/it]\u001b[A\n"," 89% 2400/2698 [1:05:38<08:07,  1.64s/it]\u001b[A\n"," 89% 2401/2698 [1:05:39<08:07,  1.64s/it]\u001b[A\n"," 89% 2402/2698 [1:05:41<08:06,  1.64s/it]\u001b[A\n"," 89% 2403/2698 [1:05:42<08:03,  1.64s/it]\u001b[A\n"," 89% 2404/2698 [1:05:44<08:00,  1.64s/it]\u001b[A\n"," 89% 2405/2698 [1:05:46<07:57,  1.63s/it]\u001b[A\n"," 89% 2406/2698 [1:05:47<07:56,  1.63s/it]\u001b[A\n"," 89% 2407/2698 [1:05:49<07:55,  1.63s/it]\u001b[A\n"," 89% 2408/2698 [1:05:51<07:53,  1.63s/it]\u001b[A\n"," 89% 2409/2698 [1:05:52<07:52,  1.63s/it]\u001b[A\n"," 89% 2410/2698 [1:05:54<07:51,  1.64s/it]\u001b[A\n"," 89% 2411/2698 [1:05:56<07:49,  1.64s/it]\u001b[A\n"," 89% 2412/2698 [1:05:57<07:48,  1.64s/it]\u001b[A\n"," 89% 2413/2698 [1:05:59<07:46,  1.64s/it]\u001b[A\n"," 89% 2414/2698 [1:06:00<07:46,  1.64s/it]\u001b[A\n"," 90% 2415/2698 [1:06:02<07:44,  1.64s/it]\u001b[A\n"," 90% 2416/2698 [1:06:04<07:42,  1.64s/it]\u001b[A\n"," 90% 2417/2698 [1:06:05<07:41,  1.64s/it]\u001b[A\n"," 90% 2418/2698 [1:06:07<07:38,  1.64s/it]\u001b[A\n"," 90% 2419/2698 [1:06:09<07:37,  1.64s/it]\u001b[A\n"," 90% 2420/2698 [1:06:10<07:35,  1.64s/it]\u001b[A\n"," 90% 2421/2698 [1:06:12<07:33,  1.64s/it]\u001b[A\n"," 90% 2422/2698 [1:06:14<07:31,  1.64s/it]\u001b[A\n"," 90% 2423/2698 [1:06:15<07:30,  1.64s/it]\u001b[A\n"," 90% 2424/2698 [1:06:17<07:27,  1.63s/it]\u001b[A\n"," 90% 2425/2698 [1:06:18<07:26,  1.64s/it]\u001b[A\n"," 90% 2426/2698 [1:06:20<07:25,  1.64s/it]\u001b[A\n"," 90% 2427/2698 [1:06:22<07:26,  1.65s/it]\u001b[A\n"," 90% 2428/2698 [1:06:23<07:24,  1.65s/it]\u001b[A\n"," 90% 2429/2698 [1:06:25<07:22,  1.65s/it]\u001b[A\n"," 90% 2430/2698 [1:06:27<07:22,  1.65s/it]\u001b[A\n"," 90% 2431/2698 [1:06:28<07:19,  1.65s/it]\u001b[A\n"," 90% 2432/2698 [1:06:30<07:16,  1.64s/it]\u001b[A\n"," 90% 2433/2698 [1:06:32<07:15,  1.64s/it]\u001b[A\n"," 90% 2434/2698 [1:06:33<07:13,  1.64s/it]\u001b[A\n"," 90% 2435/2698 [1:06:35<07:11,  1.64s/it]\u001b[A\n"," 90% 2436/2698 [1:06:37<07:10,  1.64s/it]\u001b[A\n"," 90% 2437/2698 [1:06:38<07:09,  1.65s/it]\u001b[A\n"," 90% 2438/2698 [1:06:40<07:09,  1.65s/it]\u001b[A\n"," 90% 2439/2698 [1:06:42<07:07,  1.65s/it]\u001b[A\n"," 90% 2440/2698 [1:06:43<07:06,  1.65s/it]\u001b[A\n"," 90% 2441/2698 [1:06:45<07:03,  1.65s/it]\u001b[A\n"," 91% 2442/2698 [1:06:47<07:03,  1.65s/it]\u001b[A\n"," 91% 2443/2698 [1:06:48<07:02,  1.66s/it]\u001b[A\n"," 91% 2444/2698 [1:06:50<07:01,  1.66s/it]\u001b[A\n"," 91% 2445/2698 [1:06:52<06:59,  1.66s/it]\u001b[A\n"," 91% 2446/2698 [1:06:53<06:56,  1.65s/it]\u001b[A\n"," 91% 2447/2698 [1:06:55<06:54,  1.65s/it]\u001b[A\n"," 91% 2448/2698 [1:06:56<06:52,  1.65s/it]\u001b[A\n"," 91% 2449/2698 [1:06:58<06:50,  1.65s/it]\u001b[A\n"," 91% 2450/2698 [1:07:00<06:47,  1.65s/it]\u001b[A\n"," 91% 2451/2698 [1:07:01<06:46,  1.65s/it]\u001b[A\n"," 91% 2452/2698 [1:07:03<06:45,  1.65s/it]\u001b[A\n"," 91% 2453/2698 [1:07:05<06:44,  1.65s/it]\u001b[A\n"," 91% 2454/2698 [1:07:06<06:41,  1.64s/it]\u001b[A\n"," 91% 2455/2698 [1:07:08<06:39,  1.65s/it]\u001b[A\n"," 91% 2456/2698 [1:07:10<06:38,  1.65s/it]\u001b[A\n"," 91% 2457/2698 [1:07:11<06:36,  1.64s/it]\u001b[A\n"," 91% 2458/2698 [1:07:13<06:34,  1.65s/it]\u001b[A\n"," 91% 2459/2698 [1:07:15<06:32,  1.64s/it]\u001b[A\n"," 91% 2460/2698 [1:07:16<06:31,  1.64s/it]\u001b[A\n"," 91% 2461/2698 [1:07:18<06:29,  1.64s/it]\u001b[A\n"," 91% 2462/2698 [1:07:19<06:28,  1.65s/it]\u001b[A\n"," 91% 2463/2698 [1:07:21<06:26,  1.64s/it]\u001b[A\n"," 91% 2464/2698 [1:07:23<06:24,  1.64s/it]\u001b[A\n"," 91% 2465/2698 [1:07:24<06:23,  1.65s/it]\u001b[A\n"," 91% 2466/2698 [1:07:26<06:21,  1.65s/it]\u001b[A\n"," 91% 2467/2698 [1:07:28<06:21,  1.65s/it]\u001b[A\n"," 91% 2468/2698 [1:07:29<06:19,  1.65s/it]\u001b[A\n"," 92% 2469/2698 [1:07:31<06:17,  1.65s/it]\u001b[A\n"," 92% 2470/2698 [1:07:33<06:15,  1.65s/it]\u001b[A\n"," 92% 2471/2698 [1:07:34<06:14,  1.65s/it]\u001b[A\n"," 92% 2472/2698 [1:07:36<06:11,  1.65s/it]\u001b[A\n"," 92% 2473/2698 [1:07:38<06:10,  1.65s/it]\u001b[A\n"," 92% 2474/2698 [1:07:39<06:08,  1.65s/it]\u001b[A\n"," 92% 2475/2698 [1:07:41<06:05,  1.64s/it]\u001b[A\n"," 92% 2476/2698 [1:07:43<06:04,  1.64s/it]\u001b[A\n"," 92% 2477/2698 [1:07:44<06:03,  1.64s/it]\u001b[A\n"," 92% 2478/2698 [1:07:46<06:01,  1.64s/it]\u001b[A\n"," 92% 2479/2698 [1:07:47<05:59,  1.64s/it]\u001b[A\n"," 92% 2480/2698 [1:07:49<05:58,  1.65s/it]\u001b[A\n"," 92% 2481/2698 [1:07:51<05:58,  1.65s/it]\u001b[A\n"," 92% 2482/2698 [1:07:52<05:56,  1.65s/it]\u001b[A\n"," 92% 2483/2698 [1:07:54<05:54,  1.65s/it]\u001b[A\n"," 92% 2484/2698 [1:07:56<05:53,  1.65s/it]\u001b[A\n"," 92% 2485/2698 [1:07:57<05:52,  1.65s/it]\u001b[A\n"," 92% 2486/2698 [1:07:59<05:50,  1.65s/it]\u001b[A\n"," 92% 2487/2698 [1:08:01<05:47,  1.65s/it]\u001b[A\n"," 92% 2488/2698 [1:08:02<05:45,  1.65s/it]\u001b[A\n"," 92% 2489/2698 [1:08:04<05:43,  1.64s/it]\u001b[A\n"," 92% 2490/2698 [1:08:06<05:41,  1.64s/it]\u001b[A\n"," 92% 2491/2698 [1:08:07<05:40,  1.64s/it]\u001b[A\n"," 92% 2492/2698 [1:08:09<05:39,  1.65s/it]\u001b[A\n"," 92% 2493/2698 [1:08:11<05:37,  1.65s/it]\u001b[A\n"," 92% 2494/2698 [1:08:12<05:35,  1.65s/it]\u001b[A\n"," 92% 2495/2698 [1:08:14<05:35,  1.65s/it]\u001b[A\n"," 93% 2496/2698 [1:08:15<05:34,  1.65s/it]\u001b[A\n"," 93% 2497/2698 [1:08:17<05:31,  1.65s/it]\u001b[A\n"," 93% 2498/2698 [1:08:19<05:30,  1.65s/it]\u001b[A\n"," 93% 2499/2698 [1:08:20<05:28,  1.65s/it]\u001b[A\n"," 93% 2500/2698 [1:08:22<05:26,  1.65s/it]\u001b[A\n"," 93% 2501/2698 [1:08:24<05:24,  1.65s/it]\u001b[A\n"," 93% 2502/2698 [1:08:25<05:22,  1.64s/it]\u001b[A\n"," 93% 2503/2698 [1:08:27<05:20,  1.64s/it]\u001b[A\n"," 93% 2504/2698 [1:08:29<05:19,  1.65s/it]\u001b[A\n"," 93% 2505/2698 [1:08:30<05:18,  1.65s/it]\u001b[A\n"," 93% 2506/2698 [1:08:32<05:17,  1.65s/it]\u001b[A\n"," 93% 2507/2698 [1:08:34<05:16,  1.65s/it]\u001b[A\n"," 93% 2508/2698 [1:08:35<05:14,  1.65s/it]\u001b[A\n"," 93% 2509/2698 [1:08:37<05:12,  1.65s/it]\u001b[A\n"," 93% 2510/2698 [1:08:39<05:10,  1.65s/it]\u001b[A\n"," 93% 2511/2698 [1:08:40<05:09,  1.65s/it]\u001b[A\n"," 93% 2512/2698 [1:08:42<05:07,  1.66s/it]\u001b[A\n"," 93% 2513/2698 [1:08:44<05:05,  1.65s/it]\u001b[A\n"," 93% 2514/2698 [1:08:45<05:04,  1.65s/it]\u001b[A\n"," 93% 2515/2698 [1:08:47<05:01,  1.65s/it]\u001b[A\n"," 93% 2516/2698 [1:08:49<05:01,  1.66s/it]\u001b[A\n"," 93% 2517/2698 [1:08:50<05:00,  1.66s/it]\u001b[A\n"," 93% 2518/2698 [1:08:52<04:58,  1.66s/it]\u001b[A\n"," 93% 2519/2698 [1:08:53<04:56,  1.65s/it]\u001b[A\n"," 93% 2520/2698 [1:08:55<04:53,  1.65s/it]\u001b[A\n"," 93% 2521/2698 [1:08:57<04:52,  1.65s/it]\u001b[A\n"," 93% 2522/2698 [1:08:58<04:51,  1.66s/it]\u001b[A\n"," 94% 2523/2698 [1:09:00<04:49,  1.65s/it]\u001b[A\n"," 94% 2524/2698 [1:09:02<04:46,  1.65s/it]\u001b[A\n"," 94% 2525/2698 [1:09:03<04:45,  1.65s/it]\u001b[A\n"," 94% 2526/2698 [1:09:05<04:43,  1.65s/it]\u001b[A\n"," 94% 2527/2698 [1:09:07<04:41,  1.65s/it]\u001b[A\n"," 94% 2528/2698 [1:09:08<04:39,  1.65s/it]\u001b[A\n"," 94% 2529/2698 [1:09:10<04:38,  1.65s/it]\u001b[A\n"," 94% 2530/2698 [1:09:12<04:36,  1.64s/it]\u001b[A\n"," 94% 2531/2698 [1:09:13<04:34,  1.64s/it]\u001b[A\n"," 94% 2532/2698 [1:09:15<04:32,  1.64s/it]\u001b[A\n"," 94% 2533/2698 [1:09:16<04:29,  1.63s/it]\u001b[A\n"," 94% 2534/2698 [1:09:18<04:28,  1.64s/it]\u001b[A\n"," 94% 2535/2698 [1:09:20<04:26,  1.63s/it]\u001b[A\n"," 94% 2536/2698 [1:09:21<04:25,  1.64s/it]\u001b[A\n"," 94% 2537/2698 [1:09:23<04:23,  1.64s/it]\u001b[A\n"," 94% 2538/2698 [1:09:25<04:21,  1.64s/it]\u001b[A\n"," 94% 2539/2698 [1:09:26<04:20,  1.64s/it]\u001b[A\n"," 94% 2540/2698 [1:09:28<04:18,  1.64s/it]\u001b[A\n"," 94% 2541/2698 [1:09:30<04:17,  1.64s/it]\u001b[A\n"," 94% 2542/2698 [1:09:31<04:14,  1.63s/it]\u001b[A\n"," 94% 2543/2698 [1:09:33<04:13,  1.63s/it]\u001b[A\n"," 94% 2544/2698 [1:09:34<04:10,  1.63s/it]\u001b[A\n"," 94% 2545/2698 [1:09:36<04:10,  1.63s/it]\u001b[A\n"," 94% 2546/2698 [1:09:38<04:08,  1.63s/it]\u001b[A\n"," 94% 2547/2698 [1:09:39<04:06,  1.64s/it]\u001b[A\n"," 94% 2548/2698 [1:09:41<04:05,  1.63s/it]\u001b[A\n"," 94% 2549/2698 [1:09:43<04:03,  1.63s/it]\u001b[A\n"," 95% 2550/2698 [1:09:44<04:01,  1.64s/it]\u001b[A\n"," 95% 2551/2698 [1:09:46<04:00,  1.63s/it]\u001b[A\n"," 95% 2552/2698 [1:09:48<03:58,  1.64s/it]\u001b[A\n"," 95% 2553/2698 [1:09:49<03:57,  1.64s/it]\u001b[A\n"," 95% 2554/2698 [1:09:51<03:55,  1.64s/it]\u001b[A\n"," 95% 2555/2698 [1:09:52<03:53,  1.63s/it]\u001b[A\n"," 95% 2556/2698 [1:09:54<03:52,  1.63s/it]\u001b[A\n"," 95% 2557/2698 [1:09:56<03:50,  1.64s/it]\u001b[A\n"," 95% 2558/2698 [1:09:57<03:48,  1.63s/it]\u001b[A\n"," 95% 2559/2698 [1:09:59<03:47,  1.64s/it]\u001b[A\n"," 95% 2560/2698 [1:10:01<03:46,  1.64s/it]\u001b[A\n"," 95% 2561/2698 [1:10:02<03:44,  1.64s/it]\u001b[A\n"," 95% 2562/2698 [1:10:04<03:43,  1.64s/it]\u001b[A\n"," 95% 2563/2698 [1:10:06<03:40,  1.64s/it]\u001b[A\n"," 95% 2564/2698 [1:10:07<03:39,  1.64s/it]\u001b[A\n"," 95% 2565/2698 [1:10:09<03:37,  1.64s/it]\u001b[A\n"," 95% 2566/2698 [1:10:10<03:35,  1.63s/it]\u001b[A\n"," 95% 2567/2698 [1:10:12<03:34,  1.63s/it]\u001b[A\n"," 95% 2568/2698 [1:10:14<03:32,  1.63s/it]\u001b[A\n"," 95% 2569/2698 [1:10:15<03:30,  1.63s/it]\u001b[A\n"," 95% 2570/2698 [1:10:17<03:29,  1.63s/it]\u001b[A\n"," 95% 2571/2698 [1:10:19<03:27,  1.63s/it]\u001b[A\n"," 95% 2572/2698 [1:10:20<03:25,  1.63s/it]\u001b[A\n"," 95% 2573/2698 [1:10:22<03:23,  1.63s/it]\u001b[A\n"," 95% 2574/2698 [1:10:24<03:21,  1.63s/it]\u001b[A\n"," 95% 2575/2698 [1:10:25<03:20,  1.63s/it]\u001b[A\n"," 95% 2576/2698 [1:10:27<03:19,  1.63s/it]\u001b[A\n"," 96% 2577/2698 [1:10:28<03:17,  1.63s/it]\u001b[A\n"," 96% 2578/2698 [1:10:30<03:16,  1.64s/it]\u001b[A\n"," 96% 2579/2698 [1:10:32<03:14,  1.64s/it]\u001b[A\n"," 96% 2580/2698 [1:10:33<03:13,  1.64s/it]\u001b[A\n"," 96% 2581/2698 [1:10:35<03:11,  1.64s/it]\u001b[A\n"," 96% 2582/2698 [1:10:37<03:09,  1.64s/it]\u001b[A\n"," 96% 2583/2698 [1:10:38<03:08,  1.64s/it]\u001b[A\n"," 96% 2584/2698 [1:10:40<03:06,  1.63s/it]\u001b[A\n"," 96% 2585/2698 [1:10:42<03:05,  1.64s/it]\u001b[A\n"," 96% 2586/2698 [1:10:43<03:03,  1.64s/it]\u001b[A\n"," 96% 2587/2698 [1:10:45<03:01,  1.64s/it]\u001b[A\n"," 96% 2588/2698 [1:10:46<02:59,  1.63s/it]\u001b[A\n"," 96% 2589/2698 [1:10:48<02:57,  1.63s/it]\u001b[A\n"," 96% 2590/2698 [1:10:50<02:55,  1.63s/it]\u001b[A\n"," 96% 2591/2698 [1:10:51<02:54,  1.63s/it]\u001b[A\n"," 96% 2592/2698 [1:10:53<02:53,  1.63s/it]\u001b[A\n"," 96% 2593/2698 [1:10:55<02:50,  1.63s/it]\u001b[A\n"," 96% 2594/2698 [1:10:56<02:49,  1.63s/it]\u001b[A\n"," 96% 2595/2698 [1:10:58<02:48,  1.63s/it]\u001b[A\n"," 96% 2596/2698 [1:10:59<02:46,  1.64s/it]\u001b[A\n"," 96% 2597/2698 [1:11:01<02:45,  1.64s/it]\u001b[A\n"," 96% 2598/2698 [1:11:03<02:43,  1.64s/it]\u001b[A\n"," 96% 2599/2698 [1:11:04<02:42,  1.64s/it]\u001b[A\n"," 96% 2600/2698 [1:11:06<02:40,  1.64s/it]\u001b[A\n"," 96% 2601/2698 [1:11:08<02:39,  1.64s/it]\u001b[A\n"," 96% 2602/2698 [1:11:09<02:37,  1.64s/it]\u001b[A\n"," 96% 2603/2698 [1:11:11<02:35,  1.64s/it]\u001b[A\n"," 97% 2604/2698 [1:11:13<02:34,  1.64s/it]\u001b[A\n"," 97% 2605/2698 [1:11:14<02:32,  1.64s/it]\u001b[A\n"," 97% 2606/2698 [1:11:16<02:31,  1.65s/it]\u001b[A\n"," 97% 2607/2698 [1:11:18<02:29,  1.65s/it]\u001b[A\n"," 97% 2608/2698 [1:11:19<02:27,  1.64s/it]\u001b[A\n"," 97% 2609/2698 [1:11:21<02:26,  1.65s/it]\u001b[A\n"," 97% 2610/2698 [1:11:22<02:24,  1.64s/it]\u001b[A\n"," 97% 2611/2698 [1:11:24<02:23,  1.65s/it]\u001b[A\n"," 97% 2612/2698 [1:11:26<02:21,  1.65s/it]\u001b[A\n"," 97% 2613/2698 [1:11:27<02:20,  1.65s/it]\u001b[A\n"," 97% 2614/2698 [1:11:29<02:18,  1.65s/it]\u001b[A\n"," 97% 2615/2698 [1:11:31<02:16,  1.65s/it]\u001b[A\n"," 97% 2616/2698 [1:11:32<02:14,  1.64s/it]\u001b[A\n"," 97% 2617/2698 [1:11:34<02:13,  1.64s/it]\u001b[A\n"," 97% 2618/2698 [1:11:36<02:11,  1.64s/it]\u001b[A\n"," 97% 2619/2698 [1:11:37<02:09,  1.63s/it]\u001b[A\n"," 97% 2620/2698 [1:11:39<02:08,  1.64s/it]\u001b[A\n"," 97% 2621/2698 [1:11:41<02:06,  1.64s/it]\u001b[A\n"," 97% 2622/2698 [1:11:42<02:04,  1.64s/it]\u001b[A\n"," 97% 2623/2698 [1:11:44<02:02,  1.64s/it]\u001b[A\n"," 97% 2624/2698 [1:11:45<02:01,  1.64s/it]\u001b[A\n"," 97% 2625/2698 [1:11:47<01:59,  1.64s/it]\u001b[A\n"," 97% 2626/2698 [1:11:49<01:58,  1.64s/it]\u001b[A\n"," 97% 2627/2698 [1:11:50<01:56,  1.64s/it]\u001b[A\n"," 97% 2628/2698 [1:11:52<01:54,  1.64s/it]\u001b[A\n"," 97% 2629/2698 [1:11:54<01:52,  1.64s/it]\u001b[A\n"," 97% 2630/2698 [1:11:55<01:51,  1.64s/it]\u001b[A\n"," 98% 2631/2698 [1:11:57<01:49,  1.64s/it]\u001b[A\n"," 98% 2632/2698 [1:11:59<01:48,  1.64s/it]\u001b[A\n"," 98% 2633/2698 [1:12:00<01:46,  1.64s/it]\u001b[A\n"," 98% 2634/2698 [1:12:02<01:45,  1.64s/it]\u001b[A\n"," 98% 2635/2698 [1:12:04<01:43,  1.65s/it]\u001b[A\n"," 98% 2636/2698 [1:12:05<01:42,  1.65s/it]\u001b[A\n"," 98% 2637/2698 [1:12:07<01:40,  1.65s/it]\u001b[A\n"," 98% 2638/2698 [1:12:08<01:38,  1.65s/it]\u001b[A\n"," 98% 2639/2698 [1:12:10<01:37,  1.65s/it]\u001b[A\n"," 98% 2640/2698 [1:12:12<01:35,  1.65s/it]\u001b[A\n"," 98% 2641/2698 [1:12:13<01:33,  1.65s/it]\u001b[A\n"," 98% 2642/2698 [1:12:15<01:32,  1.65s/it]\u001b[A\n"," 98% 2643/2698 [1:12:17<01:30,  1.64s/it]\u001b[A\n"," 98% 2644/2698 [1:12:18<01:28,  1.64s/it]\u001b[A\n"," 98% 2645/2698 [1:12:20<01:27,  1.65s/it]\u001b[A\n"," 98% 2646/2698 [1:12:22<01:25,  1.64s/it]\u001b[A\n"," 98% 2647/2698 [1:12:23<01:23,  1.64s/it]\u001b[A\n"," 98% 2648/2698 [1:12:25<01:22,  1.64s/it]\u001b[A\n"," 98% 2649/2698 [1:12:27<01:20,  1.64s/it]\u001b[A\n"," 98% 2650/2698 [1:12:28<01:19,  1.65s/it]\u001b[A\n"," 98% 2651/2698 [1:12:30<01:17,  1.65s/it]\u001b[A\n"," 98% 2652/2698 [1:12:31<01:15,  1.65s/it]\u001b[A\n"," 98% 2653/2698 [1:12:33<01:14,  1.64s/it]\u001b[A\n"," 98% 2654/2698 [1:12:35<01:12,  1.65s/it]\u001b[A\n"," 98% 2655/2698 [1:12:36<01:10,  1.64s/it]\u001b[A\n"," 98% 2656/2698 [1:12:38<01:09,  1.65s/it]\u001b[A\n"," 98% 2657/2698 [1:12:40<01:07,  1.64s/it]\u001b[A\n"," 99% 2658/2698 [1:12:41<01:05,  1.64s/it]\u001b[A\n"," 99% 2659/2698 [1:12:43<01:04,  1.65s/it]\u001b[A\n"," 99% 2660/2698 [1:12:45<01:02,  1.64s/it]\u001b[A\n"," 99% 2661/2698 [1:12:46<01:01,  1.65s/it]\u001b[A\n"," 99% 2662/2698 [1:12:48<00:59,  1.65s/it]\u001b[A\n"," 99% 2663/2698 [1:12:50<00:57,  1.65s/it]\u001b[A\n"," 99% 2664/2698 [1:12:51<00:56,  1.65s/it]\u001b[A\n"," 99% 2665/2698 [1:12:53<00:54,  1.65s/it]\u001b[A\n"," 99% 2666/2698 [1:12:55<00:52,  1.65s/it]\u001b[A\n"," 99% 2667/2698 [1:12:56<00:51,  1.65s/it]\u001b[A\n"," 99% 2668/2698 [1:12:58<00:49,  1.64s/it]\u001b[A\n"," 99% 2669/2698 [1:12:59<00:47,  1.64s/it]\u001b[A\n"," 99% 2670/2698 [1:13:01<00:46,  1.64s/it]\u001b[A\n"," 99% 2671/2698 [1:13:03<00:44,  1.65s/it]\u001b[A\n"," 99% 2672/2698 [1:13:04<00:42,  1.64s/it]\u001b[A\n"," 99% 2673/2698 [1:13:06<00:41,  1.64s/it]\u001b[A\n"," 99% 2674/2698 [1:13:08<00:39,  1.64s/it]\u001b[A\n"," 99% 2675/2698 [1:13:09<00:37,  1.64s/it]\u001b[A\n"," 99% 2676/2698 [1:13:11<00:36,  1.64s/it]\u001b[A\n"," 99% 2677/2698 [1:13:13<00:34,  1.64s/it]\u001b[A\n"," 99% 2678/2698 [1:13:14<00:32,  1.64s/it]\u001b[A\n"," 99% 2679/2698 [1:13:16<00:31,  1.64s/it]\u001b[A\n"," 99% 2680/2698 [1:13:18<00:29,  1.65s/it]\u001b[A\n"," 99% 2681/2698 [1:13:19<00:27,  1.65s/it]\u001b[A\n"," 99% 2682/2698 [1:13:21<00:26,  1.65s/it]\u001b[A\n"," 99% 2683/2698 [1:13:23<00:24,  1.65s/it]\u001b[A\n"," 99% 2684/2698 [1:13:24<00:23,  1.65s/it]\u001b[A\n","100% 2685/2698 [1:13:26<00:21,  1.65s/it]\u001b[A\n","100% 2686/2698 [1:13:27<00:19,  1.65s/it]\u001b[A\n","100% 2687/2698 [1:13:29<00:18,  1.65s/it]\u001b[A\n","100% 2688/2698 [1:13:31<00:16,  1.65s/it]\u001b[A\n","100% 2689/2698 [1:13:32<00:14,  1.65s/it]\u001b[A\n","100% 2690/2698 [1:13:34<00:13,  1.65s/it]\u001b[A\n","100% 2691/2698 [1:13:36<00:11,  1.65s/it]\u001b[A\n","100% 2692/2698 [1:13:37<00:09,  1.65s/it]\u001b[A\n","100% 2693/2698 [1:13:39<00:08,  1.64s/it]\u001b[A\n","100% 2694/2698 [1:13:41<00:06,  1.64s/it]\u001b[A\n","100% 2695/2698 [1:13:42<00:04,  1.64s/it]\u001b[A\n","100% 2696/2698 [1:13:44<00:03,  1.65s/it]\u001b[A\n","100% 2697/2698 [1:13:46<00:01,  1.64s/it]\u001b[A\n","100% 2698/2698 [1:13:47<00:00,  1.64s/it]\u001b[A11/06/2022 14:13:25 - INFO - src.trainer -   Best dev result: 0.9079633951187134\n","Epoch:   5% 1/20 [1:52:40<35:40:53, 6760.70s/it]11/06/2022 14:16:38 - INFO - src.trainer -   Train loss: 0.38943308049982245\n","11/06/2022 14:20:26 - INFO - src.trainer -   Train loss: 0.35148551247336646\n","11/06/2022 14:24:13 - INFO - src.trainer -   Train loss: 0.3519928672096946\n","11/06/2022 14:28:01 - INFO - src.trainer -   Train loss: 0.34829573197798297\n","11/06/2022 14:31:49 - INFO - src.trainer -   Train loss: 0.3541356867009943\n","11/06/2022 14:35:37 - INFO - src.trainer -   Train loss: 0.3701990300958807\n","11/06/2022 14:39:25 - INFO - src.trainer -   Train loss: 0.3372254805131392\n","11/06/2022 14:43:12 - INFO - src.trainer -   Train loss: 0.3378836891867898\n","11/06/2022 14:46:59 - INFO - src.trainer -   Train loss: 0.3635656183416193\n","11/06/2022 14:50:46 - INFO - src.trainer -   Train loss: 0.32463489879261365\n","\n","2699it [1:52:54, 705.05s/it]             \u001b[A\n","2700it [1:52:55, 494.02s/it]\u001b[A\n","2701it [1:52:57, 346.30s/it]\u001b[A\n","2702it [1:52:58, 242.90s/it]\u001b[A\n","2703it [1:53:00, 170.52s/it]\u001b[A\n","2704it [1:53:02, 119.85s/it]\u001b[A\n","2705it [1:53:03, 84.39s/it] \u001b[A\n","2706it [1:53:05, 59.56s/it]\u001b[A\n","2707it [1:53:07, 42.18s/it]\u001b[A\n","2708it [1:53:08, 30.02s/it]\u001b[A\n","2709it [1:53:10, 21.51s/it]\u001b[A\n","2710it [1:53:11, 15.54s/it]\u001b[A\n","2711it [1:53:13, 11.37s/it]\u001b[A\n","2712it [1:53:15,  8.45s/it]\u001b[A\n","2713it [1:53:16,  6.40s/it]\u001b[A\n","2714it [1:53:18,  4.97s/it]\u001b[A\n","2715it [1:53:20,  3.97s/it]\u001b[A\n","2716it [1:53:21,  3.27s/it]\u001b[A\n","2717it [1:53:23,  2.78s/it]\u001b[A\n","2718it [1:53:25,  2.43s/it]\u001b[A\n","2719it [1:53:26,  2.19s/it]\u001b[A\n","2720it [1:53:28,  2.02s/it]\u001b[A\n","2721it [1:53:29,  1.91s/it]\u001b[A\n","2722it [1:53:31,  1.82s/it]\u001b[A\n","2723it [1:53:33,  1.76s/it]\u001b[A\n","2724it [1:53:34,  1.73s/it]\u001b[A\n","2725it [1:53:36,  1.69s/it]\u001b[A\n","2726it [1:53:38,  1.67s/it]\u001b[A\n","2727it [1:53:39,  1.66s/it]\u001b[A\n","2728it [1:53:41,  1.65s/it]\u001b[A\n","2729it [1:53:42,  1.64s/it]\u001b[A\n","2730it [1:53:44,  1.63s/it]\u001b[A\n","2731it [1:53:46,  1.63s/it]\u001b[A\n","2732it [1:53:47,  1.64s/it]\u001b[A\n","2733it [1:53:49,  1.63s/it]\u001b[A\n","2734it [1:53:51,  1.63s/it]\u001b[A\n","2735it [1:53:52,  1.63s/it]\u001b[A\n","2736it [1:53:54,  1.63s/it]\u001b[A\n","2737it [1:53:55,  1.63s/it]\u001b[A\n","2738it [1:53:57,  1.63s/it]\u001b[A\n","2739it [1:53:59,  1.63s/it]\u001b[A\n","2740it [1:54:00,  1.63s/it]\u001b[A\n","2741it [1:54:02,  1.63s/it]\u001b[A\n","2742it [1:54:04,  1.62s/it]\u001b[A\n","2743it [1:54:05,  1.62s/it]\u001b[A\n","2744it [1:54:07,  1.62s/it]\u001b[A\n","2745it [1:54:08,  1.62s/it]\u001b[A\n","2746it [1:54:10,  1.62s/it]\u001b[A\n","2747it [1:54:12,  1.62s/it]\u001b[A\n","2748it [1:54:13,  1.63s/it]\u001b[A\n","2749it [1:54:15,  1.63s/it]\u001b[A\n","2750it [1:54:17,  1.63s/it]\u001b[A\n","2751it [1:54:18,  1.63s/it]\u001b[A\n","2752it [1:54:20,  1.63s/it]\u001b[A\n","2753it [1:54:21,  1.62s/it]\u001b[A\n","2754it [1:54:23,  1.62s/it]\u001b[A\n","2755it [1:54:25,  1.63s/it]\u001b[A\n","2756it [1:54:26,  1.63s/it]\u001b[A\n","2757it [1:54:28,  1.63s/it]\u001b[A\n","2758it [1:54:30,  1.63s/it]\u001b[A\n","2759it [1:54:31,  1.63s/it]\u001b[A\n","2760it [1:54:33,  1.63s/it]\u001b[A\n","2761it [1:54:34,  1.63s/it]\u001b[A\n","2762it [1:54:36,  1.63s/it]\u001b[A\n","2763it [1:54:38,  1.63s/it]\u001b[A\n","2764it [1:54:39,  1.64s/it]\u001b[A\n","2765it [1:54:41,  1.64s/it]\u001b[A\n","2766it [1:54:43,  1.64s/it]\u001b[A\n","2767it [1:54:44,  1.65s/it]\u001b[A\n","2768it [1:54:46,  1.64s/it]\u001b[A\n","2769it [1:54:48,  1.64s/it]\u001b[A\n","2770it [1:54:49,  1.64s/it]\u001b[A\n","2771it [1:54:51,  1.65s/it]\u001b[A\n","2772it [1:54:53,  1.64s/it]\u001b[A\n","2773it [1:54:54,  1.64s/it]\u001b[A\n","2774it [1:54:56,  1.65s/it]\u001b[A\n","2775it [1:54:58,  1.65s/it]\u001b[A\n","2776it [1:54:59,  1.65s/it]\u001b[A\n","2777it [1:55:01,  1.66s/it]\u001b[A\n","2778it [1:55:02,  1.65s/it]\u001b[A\n","2779it [1:55:04,  1.65s/it]\u001b[A\n","2780it [1:55:06,  1.65s/it]\u001b[A\n","2781it [1:55:07,  1.66s/it]\u001b[A\n","2782it [1:55:09,  1.65s/it]\u001b[A\n","2783it [1:55:11,  1.65s/it]\u001b[A\n","2784it [1:55:12,  1.66s/it]\u001b[A\n","2785it [1:55:14,  1.65s/it]\u001b[A\n","2786it [1:55:16,  1.66s/it]\u001b[A\n","2787it [1:55:17,  1.65s/it]\u001b[A\n","2788it [1:55:19,  1.65s/it]\u001b[A\n","2789it [1:55:21,  1.65s/it]\u001b[A\n","2790it [1:55:22,  1.65s/it]\u001b[A\n","2791it [1:55:24,  1.65s/it]\u001b[A\n","2792it [1:55:26,  1.65s/it]\u001b[A\n","2793it [1:55:27,  1.65s/it]\u001b[A\n","2794it [1:55:29,  1.64s/it]\u001b[A\n","2795it [1:55:31,  1.64s/it]\u001b[A\n","2796it [1:55:32,  1.64s/it]\u001b[A\n","2797it [1:55:34,  1.65s/it]\u001b[A\n","2798it [1:55:35,  1.65s/it]\u001b[A\n","2799it [1:55:37,  1.65s/it]\u001b[A\n","2800it [1:55:39,  1.65s/it]\u001b[A\n","2801it [1:55:40,  1.65s/it]\u001b[A\n","2802it [1:55:42,  1.65s/it]\u001b[A\n","2803it [1:55:44,  1.65s/it]\u001b[A\n","2804it [1:55:45,  1.65s/it]\u001b[A\n","2805it [1:55:47,  1.65s/it]\u001b[A\n","2806it [1:55:49,  1.65s/it]\u001b[A\n","2807it [1:55:50,  1.65s/it]\u001b[A\n","2808it [1:55:52,  1.65s/it]\u001b[A\n","2809it [1:55:54,  1.65s/it]\u001b[A\n","2810it [1:55:55,  1.65s/it]\u001b[A\n","2811it [1:55:57,  1.64s/it]\u001b[A\n","2812it [1:55:59,  1.64s/it]\u001b[A\n","2813it [1:56:00,  1.64s/it]\u001b[A\n","2814it [1:56:02,  1.64s/it]\u001b[A\n","2815it [1:56:03,  1.64s/it]\u001b[A\n","2816it [1:56:05,  1.64s/it]\u001b[A\n","2817it [1:56:07,  1.65s/it]\u001b[A\n","2818it [1:56:08,  1.65s/it]\u001b[A\n","2819it [1:56:10,  1.65s/it]\u001b[A\n","2820it [1:56:12,  1.65s/it]\u001b[A\n","2821it [1:56:13,  1.64s/it]\u001b[A\n","2822it [1:56:15,  1.64s/it]\u001b[A\n","2823it [1:56:17,  1.65s/it]\u001b[A\n","2824it [1:56:18,  1.64s/it]\u001b[A\n","2825it [1:56:20,  1.64s/it]\u001b[A\n","2826it [1:56:22,  1.64s/it]\u001b[A\n","2827it [1:56:23,  1.64s/it]\u001b[A\n","2828it [1:56:25,  1.64s/it]\u001b[A\n","2829it [1:56:26,  1.64s/it]\u001b[A\n","2830it [1:56:28,  1.64s/it]\u001b[A\n","2831it [1:56:30,  1.64s/it]\u001b[A\n","2832it [1:56:31,  1.64s/it]\u001b[A\n","2833it [1:56:33,  1.65s/it]\u001b[A\n","2834it [1:56:35,  1.64s/it]\u001b[A\n","2835it [1:56:36,  1.64s/it]\u001b[A\n","2836it [1:56:38,  1.64s/it]\u001b[A\n","2837it [1:56:40,  1.65s/it]\u001b[A\n","2838it [1:56:41,  1.64s/it]\u001b[A\n","2839it [1:56:43,  1.64s/it]\u001b[A\n","2840it [1:56:45,  1.64s/it]\u001b[A\n","2841it [1:56:46,  1.64s/it]\u001b[A\n","2842it [1:56:48,  1.64s/it]\u001b[A\n","2843it [1:56:49,  1.64s/it]\u001b[A\n","2844it [1:56:51,  1.64s/it]\u001b[A\n","2845it [1:56:53,  1.65s/it]\u001b[A\n","2846it [1:56:54,  1.65s/it]\u001b[A\n","2847it [1:56:56,  1.65s/it]\u001b[A\n","2848it [1:56:58,  1.64s/it]\u001b[A\n","2849it [1:56:59,  1.64s/it]\u001b[A\n","2850it [1:57:01,  1.65s/it]\u001b[A\n","2851it [1:57:03,  1.65s/it]\u001b[A\n","2852it [1:57:04,  1.64s/it]\u001b[A\n","2853it [1:57:06,  1.64s/it]\u001b[A\n","2854it [1:57:08,  1.64s/it]\u001b[A\n","2855it [1:57:09,  1.65s/it]\u001b[A\n","2856it [1:57:11,  1.65s/it]\u001b[A\n","2857it [1:57:12,  1.64s/it]\u001b[A\n","2858it [1:57:14,  1.65s/it]\u001b[A\n","2859it [1:57:16,  1.65s/it]\u001b[A\n","2860it [1:57:17,  1.64s/it]\u001b[A\n","2861it [1:57:19,  1.64s/it]\u001b[A\n","2862it [1:57:21,  1.64s/it]\u001b[A\n","2863it [1:57:22,  1.64s/it]\u001b[A\n","2864it [1:57:24,  1.64s/it]\u001b[A\n","2865it [1:57:26,  1.64s/it]\u001b[A\n","2866it [1:57:27,  1.64s/it]\u001b[A\n","2867it [1:57:29,  1.64s/it]\u001b[A\n","2868it [1:57:31,  1.64s/it]\u001b[A\n","2869it [1:57:32,  1.64s/it]\u001b[A\n","2870it [1:57:34,  1.64s/it]\u001b[A\n","2871it [1:57:35,  1.63s/it]\u001b[A\n","2872it [1:57:37,  1.63s/it]\u001b[A\n","2873it [1:57:39,  1.64s/it]\u001b[A\n","2874it [1:57:40,  1.63s/it]\u001b[A\n","2875it [1:57:42,  1.63s/it]\u001b[A\n","2876it [1:57:44,  1.64s/it]\u001b[A\n","2877it [1:57:45,  1.63s/it]\u001b[A\n","2878it [1:57:47,  1.63s/it]\u001b[A\n","2879it [1:57:49,  1.63s/it]\u001b[A\n","2880it [1:57:50,  1.63s/it]\u001b[A\n","2881it [1:57:52,  1.63s/it]\u001b[A\n","2882it [1:57:53,  1.63s/it]\u001b[A\n","2883it [1:57:55,  1.63s/it]\u001b[A\n","2884it [1:57:57,  1.63s/it]\u001b[A\n","2885it [1:57:58,  1.63s/it]\u001b[A\n","2886it [1:58:00,  1.63s/it]\u001b[A\n","2887it [1:58:02,  1.63s/it]\u001b[A\n","2888it [1:58:03,  1.63s/it]\u001b[A\n","2889it [1:58:05,  1.63s/it]\u001b[A\n","2890it [1:58:06,  1.64s/it]\u001b[A\n","2891it [1:58:08,  1.63s/it]\u001b[A\n","2892it [1:58:10,  1.63s/it]\u001b[A\n","2893it [1:58:11,  1.63s/it]\u001b[A\n","2894it [1:58:13,  1.63s/it]\u001b[A\n","2895it [1:58:15,  1.63s/it]\u001b[A\n","2896it [1:58:16,  1.63s/it]\u001b[A\n","2897it [1:58:18,  1.63s/it]\u001b[A\n","2898it [1:58:19,  1.63s/it]\u001b[A\n","2899it [1:58:21,  1.63s/it]\u001b[A\n","2900it [1:58:23,  1.63s/it]\u001b[A\n","2901it [1:58:24,  1.63s/it]\u001b[A\n","2902it [1:58:26,  1.63s/it]\u001b[A\n","2903it [1:58:28,  1.63s/it]\u001b[A\n","2904it [1:58:29,  1.63s/it]\u001b[A\n","2905it [1:58:31,  1.63s/it]\u001b[A\n","2906it [1:58:33,  1.63s/it]\u001b[A\n","2907it [1:58:34,  1.63s/it]\u001b[A\n","2908it [1:58:36,  1.63s/it]\u001b[A\n","2909it [1:58:37,  1.63s/it]\u001b[A\n","2910it [1:58:39,  1.63s/it]\u001b[A\n","2911it [1:58:41,  1.63s/it]\u001b[A\n","2912it [1:58:42,  1.63s/it]\u001b[A\n","2913it [1:58:44,  1.63s/it]\u001b[A\n","2914it [1:58:46,  1.63s/it]\u001b[A\n","2915it [1:58:47,  1.63s/it]\u001b[A\n","2916it [1:58:49,  1.62s/it]\u001b[A\n","2917it [1:58:50,  1.62s/it]\u001b[A\n","2918it [1:58:52,  1.62s/it]\u001b[A\n","2919it [1:58:54,  1.62s/it]\u001b[A\n","2920it [1:58:55,  1.63s/it]\u001b[A\n","2921it [1:58:57,  1.63s/it]\u001b[A\n","2922it [1:58:59,  1.63s/it]\u001b[A\n","2923it [1:59:00,  1.63s/it]\u001b[A\n","2924it [1:59:02,  1.63s/it]\u001b[A\n","2925it [1:59:03,  1.63s/it]\u001b[A\n","2926it [1:59:05,  1.63s/it]\u001b[A\n","2927it [1:59:07,  1.63s/it]\u001b[A\n","2928it [1:59:08,  1.63s/it]\u001b[A\n","2929it [1:59:10,  1.63s/it]\u001b[A\n","2930it [1:59:12,  1.63s/it]\u001b[A\n","2931it [1:59:13,  1.63s/it]\u001b[A\n","2932it [1:59:15,  1.63s/it]\u001b[A\n","2933it [1:59:16,  1.63s/it]\u001b[A\n","2934it [1:59:18,  1.63s/it]\u001b[A\n","2935it [1:59:20,  1.64s/it]\u001b[A\n","2936it [1:59:21,  1.64s/it]\u001b[A\n","2937it [1:59:23,  1.64s/it]\u001b[A\n","2938it [1:59:25,  1.64s/it]\u001b[A\n","2939it [1:59:26,  1.64s/it]\u001b[A\n","2940it [1:59:28,  1.64s/it]\u001b[A\n","2941it [1:59:30,  1.64s/it]\u001b[A\n","2942it [1:59:31,  1.64s/it]\u001b[A\n","2943it [1:59:33,  1.64s/it]\u001b[A\n","2944it [1:59:35,  1.65s/it]\u001b[A\n","2945it [1:59:36,  1.64s/it]\u001b[A\n","2946it [1:59:38,  1.64s/it]\u001b[A\n","2947it [1:59:39,  1.64s/it]\u001b[A\n","2948it [1:59:41,  1.64s/it]\u001b[A\n","2949it [1:59:43,  1.64s/it]\u001b[A\n","2950it [1:59:44,  1.64s/it]\u001b[A\n","2951it [1:59:46,  1.64s/it]\u001b[A\n","2952it [1:59:48,  1.64s/it]\u001b[A\n","2953it [1:59:49,  1.64s/it]\u001b[A\n","2954it [1:59:51,  1.65s/it]\u001b[A\n","2955it [1:59:53,  1.65s/it]\u001b[A\n","2956it [1:59:54,  1.64s/it]\u001b[A\n","2957it [1:59:56,  1.64s/it]\u001b[A\n","2958it [1:59:58,  1.64s/it]\u001b[A\n","2959it [1:59:59,  1.64s/it]\u001b[A\n","2960it [2:00:01,  1.64s/it]\u001b[A\n","2961it [2:00:02,  1.64s/it]\u001b[A\n","2962it [2:00:04,  1.64s/it]\u001b[A\n","2963it [2:00:06,  1.64s/it]\u001b[A\n","2964it [2:00:07,  1.64s/it]\u001b[A\n","2965it [2:00:09,  1.64s/it]\u001b[A\n","2966it [2:00:11,  1.64s/it]\u001b[A\n","2967it [2:00:12,  1.64s/it]\u001b[A\n","2968it [2:00:14,  1.65s/it]\u001b[A\n","2969it [2:00:16,  1.65s/it]\u001b[A\n","2970it [2:00:17,  1.64s/it]\u001b[A\n","2971it [2:00:19,  1.65s/it]\u001b[A\n","2972it [2:00:21,  1.65s/it]\u001b[A\n","2973it [2:00:22,  1.65s/it]\u001b[A\n","2974it [2:00:24,  1.65s/it]\u001b[A\n","2975it [2:00:26,  1.65s/it]\u001b[A\n","2976it [2:00:27,  1.65s/it]\u001b[A\n","2977it [2:00:29,  1.65s/it]\u001b[A\n","2978it [2:00:30,  1.65s/it]\u001b[A\n","2979it [2:00:32,  1.65s/it]\u001b[A\n","2980it [2:00:34,  1.65s/it]\u001b[A\n","2981it [2:00:35,  1.65s/it]\u001b[A\n","2982it [2:00:37,  1.65s/it]\u001b[A\n","2983it [2:00:39,  1.65s/it]\u001b[A\n","2984it [2:00:40,  1.65s/it]\u001b[A\n","2985it [2:00:42,  1.65s/it]\u001b[A\n","2986it [2:00:44,  1.65s/it]\u001b[A\n","2987it [2:00:45,  1.65s/it]\u001b[A\n","2988it [2:00:47,  1.65s/it]\u001b[A\n","2989it [2:00:49,  1.65s/it]\u001b[A\n","2990it [2:00:50,  1.65s/it]\u001b[A\n","2991it [2:00:52,  1.64s/it]\u001b[A\n","2992it [2:00:54,  1.64s/it]\u001b[A\n","2993it [2:00:55,  1.65s/it]\u001b[A\n","2994it [2:00:57,  1.65s/it]\u001b[A\n","2995it [2:00:58,  1.65s/it]\u001b[A\n","2996it [2:01:00,  1.65s/it]\u001b[A\n","2997it [2:01:02,  1.65s/it]\u001b[A\n","2998it [2:01:03,  1.65s/it]\u001b[A\n","2999it [2:01:05,  1.65s/it]\u001b[A\n","3000it [2:01:07,  1.65s/it]\u001b[A\n","3001it [2:01:08,  1.65s/it]\u001b[A\n","3002it [2:01:10,  1.65s/it]\u001b[A\n","3003it [2:01:12,  1.65s/it]\u001b[A\n","3004it [2:01:13,  1.65s/it]\u001b[A\n","3005it [2:01:15,  1.65s/it]\u001b[A\n","3006it [2:01:17,  1.65s/it]\u001b[A\n","3007it [2:01:18,  1.65s/it]\u001b[A\n","3008it [2:01:20,  1.65s/it]\u001b[A\n","3009it [2:01:22,  1.65s/it]\u001b[A\n","3010it [2:01:23,  1.65s/it]\u001b[A\n","3011it [2:01:25,  1.65s/it]\u001b[A\n","3012it [2:01:27,  1.66s/it]\u001b[A\n","3013it [2:01:28,  1.65s/it]\u001b[A\n","3014it [2:01:30,  1.65s/it]\u001b[A\n","3015it [2:01:31,  1.65s/it]\u001b[A\n","3016it [2:01:33,  1.65s/it]\u001b[A\n","3017it [2:01:35,  1.65s/it]\u001b[A\n","3018it [2:01:36,  1.65s/it]\u001b[A\n","3019it [2:01:38,  1.65s/it]\u001b[A\n","3020it [2:01:40,  1.66s/it]\u001b[A\n","3021it [2:01:41,  1.65s/it]\u001b[A\n","3022it [2:01:43,  1.65s/it]\u001b[A\n","3023it [2:01:45,  1.65s/it]\u001b[A\n","3024it [2:01:46,  1.65s/it]\u001b[A\n","3025it [2:01:48,  1.65s/it]\u001b[A\n","3026it [2:01:50,  1.65s/it]\u001b[A\n","3027it [2:01:51,  1.65s/it]\u001b[A\n","3028it [2:01:53,  1.65s/it]\u001b[A\n","3029it [2:01:55,  1.65s/it]\u001b[A\n","3030it [2:01:56,  1.65s/it]\u001b[A\n","3031it [2:01:58,  1.65s/it]\u001b[A\n","3032it [2:02:00,  1.65s/it]\u001b[A\n","3033it [2:02:01,  1.65s/it]\u001b[A\n","3034it [2:02:03,  1.65s/it]\u001b[A\n","3035it [2:02:04,  1.65s/it]\u001b[A\n","3036it [2:02:06,  1.64s/it]\u001b[A\n","3037it [2:02:08,  1.64s/it]\u001b[A\n","3038it [2:02:09,  1.64s/it]\u001b[A\n","3039it [2:02:11,  1.63s/it]\u001b[A\n","3040it [2:02:13,  1.63s/it]\u001b[A\n","3041it [2:02:14,  1.64s/it]\u001b[A\n","3042it [2:02:16,  1.64s/it]\u001b[A\n","3043it [2:02:18,  1.64s/it]\u001b[A\n","3044it [2:02:19,  1.63s/it]\u001b[A\n","3045it [2:02:21,  1.64s/it]\u001b[A\n","3046it [2:02:22,  1.64s/it]\u001b[A\n","3047it [2:02:24,  1.63s/it]\u001b[A\n","3048it [2:02:26,  1.63s/it]\u001b[A\n","3049it [2:02:27,  1.63s/it]\u001b[A\n","3050it [2:02:29,  1.63s/it]\u001b[A\n","3051it [2:02:31,  1.64s/it]\u001b[A\n","3052it [2:02:32,  1.64s/it]\u001b[A\n","3053it [2:02:34,  1.64s/it]\u001b[A\n","3054it [2:02:36,  1.63s/it]\u001b[A\n","3055it [2:02:37,  1.64s/it]\u001b[A\n","3056it [2:02:39,  1.64s/it]\u001b[A\n","3057it [2:02:40,  1.64s/it]\u001b[A\n","3058it [2:02:42,  1.64s/it]\u001b[A\n","3059it [2:02:44,  1.64s/it]\u001b[A\n","3060it [2:02:45,  1.64s/it]\u001b[A\n","3061it [2:02:47,  1.64s/it]\u001b[A\n","3062it [2:02:49,  1.64s/it]\u001b[A\n","3063it [2:02:50,  1.64s/it]\u001b[A\n","3064it [2:02:52,  1.63s/it]\u001b[A\n","3065it [2:02:54,  1.64s/it]\u001b[A\n","3066it [2:02:55,  1.64s/it]\u001b[A\n","3067it [2:02:57,  1.64s/it]\u001b[A\n","3068it [2:02:58,  1.64s/it]\u001b[A\n","3069it [2:03:00,  1.64s/it]\u001b[A\n","3070it [2:03:02,  1.64s/it]\u001b[A\n","3071it [2:03:03,  1.64s/it]\u001b[A\n","3072it [2:03:05,  1.64s/it]\u001b[A\n","3073it [2:03:07,  1.64s/it]\u001b[A\n","3074it [2:03:08,  1.64s/it]\u001b[A\n","3075it [2:03:10,  1.64s/it]\u001b[A\n","3076it [2:03:12,  1.65s/it]\u001b[A\n","3077it [2:03:13,  1.64s/it]\u001b[A\n","3078it [2:03:15,  1.64s/it]\u001b[A\n","3079it [2:03:17,  1.64s/it]\u001b[A\n","3080it [2:03:18,  1.64s/it]\u001b[A\n","3081it [2:03:20,  1.64s/it]\u001b[A\n","3082it [2:03:21,  1.64s/it]\u001b[A\n","3083it [2:03:23,  1.64s/it]\u001b[A\n","3084it [2:03:25,  1.64s/it]\u001b[A\n","3085it [2:03:26,  1.64s/it]\u001b[A\n","3086it [2:03:28,  1.64s/it]\u001b[A\n","3087it [2:03:30,  1.64s/it]\u001b[A\n","3088it [2:03:31,  1.63s/it]\u001b[A\n","3089it [2:03:33,  1.64s/it]\u001b[A\n","3090it [2:03:35,  1.64s/it]\u001b[A\n","3091it [2:03:36,  1.64s/it]\u001b[A\n","3092it [2:03:38,  1.64s/it]\u001b[A\n","3093it [2:03:39,  1.64s/it]\u001b[A\n","3094it [2:03:41,  1.64s/it]\u001b[A\n","3095it [2:03:43,  1.64s/it]\u001b[A\n","3096it [2:03:44,  1.64s/it]\u001b[A\n","3097it [2:03:46,  1.64s/it]\u001b[A\n","3098it [2:03:48,  1.64s/it]\u001b[A\n","3099it [2:03:49,  1.63s/it]\u001b[A\n","3100it [2:03:51,  1.64s/it]\u001b[A\n","3101it [2:03:53,  1.64s/it]\u001b[A\n","3102it [2:03:54,  1.64s/it]\u001b[A\n","3103it [2:03:56,  1.65s/it]\u001b[A\n","3104it [2:03:58,  1.65s/it]\u001b[A\n","3105it [2:03:59,  1.65s/it]\u001b[A\n","3106it [2:04:01,  1.65s/it]\u001b[A\n","3107it [2:04:02,  1.65s/it]\u001b[A\n","3108it [2:04:04,  1.65s/it]\u001b[A\n","3109it [2:04:06,  1.65s/it]\u001b[A\n","3110it [2:04:07,  1.65s/it]\u001b[A\n","3111it [2:04:09,  1.65s/it]\u001b[A\n","3112it [2:04:11,  1.65s/it]\u001b[A\n","3113it [2:04:12,  1.65s/it]\u001b[A\n","3114it [2:04:14,  1.65s/it]\u001b[A\n","3115it [2:04:16,  1.65s/it]\u001b[A\n","3116it [2:04:17,  1.65s/it]\u001b[A\n","3117it [2:04:19,  1.65s/it]\u001b[A\n","3118it [2:04:21,  1.65s/it]\u001b[A\n","3119it [2:04:22,  1.65s/it]\u001b[A\n","3120it [2:04:24,  1.65s/it]\u001b[A\n","3121it [2:04:26,  1.64s/it]\u001b[A\n","3122it [2:04:27,  1.65s/it]\u001b[A\n","3123it [2:04:29,  1.65s/it]\u001b[A\n","3124it [2:04:30,  1.65s/it]\u001b[A\n","3125it [2:04:32,  1.65s/it]\u001b[A\n","3126it [2:04:34,  1.65s/it]\u001b[A\n","3127it [2:04:35,  1.65s/it]\u001b[A\n","3128it [2:04:37,  1.65s/it]\u001b[A\n","3129it [2:04:39,  1.65s/it]\u001b[A\n","3130it [2:04:40,  1.65s/it]\u001b[A\n","3131it [2:04:42,  1.65s/it]\u001b[A\n","3132it [2:04:44,  1.65s/it]\u001b[A\n","3133it [2:04:45,  1.65s/it]\u001b[A\n","3134it [2:04:47,  1.64s/it]\u001b[A\n","3135it [2:04:49,  1.65s/it]\u001b[A\n","3136it [2:04:50,  1.64s/it]\u001b[A\n","3137it [2:04:52,  1.65s/it]\u001b[A\n","3138it [2:04:54,  1.65s/it]\u001b[A\n","3139it [2:04:55,  1.65s/it]\u001b[A\n","3140it [2:04:57,  1.65s/it]\u001b[A\n","3141it [2:04:59,  1.65s/it]\u001b[A\n","3142it [2:05:00,  1.65s/it]\u001b[A\n","3143it [2:05:02,  1.65s/it]\u001b[A\n","3144it [2:05:03,  1.65s/it]\u001b[A\n","3145it [2:05:05,  1.65s/it]\u001b[A\n","3146it [2:05:07,  1.65s/it]\u001b[A\n","3147it [2:05:08,  1.65s/it]\u001b[A\n","3148it [2:05:10,  1.65s/it]\u001b[A\n","3149it [2:05:12,  1.65s/it]\u001b[A\n","3150it [2:05:13,  1.65s/it]\u001b[A\n","3151it [2:05:15,  1.65s/it]\u001b[A\n","3152it [2:05:17,  1.64s/it]\u001b[A\n","3153it [2:05:18,  1.64s/it]\u001b[A\n","3154it [2:05:20,  1.64s/it]\u001b[A\n","3155it [2:05:22,  1.64s/it]\u001b[A\n","3156it [2:05:23,  1.65s/it]\u001b[A\n","3157it [2:05:25,  1.64s/it]\u001b[A\n","3158it [2:05:26,  1.64s/it]\u001b[A\n","3159it [2:05:28,  1.65s/it]\u001b[A\n","3160it [2:05:30,  1.65s/it]\u001b[A\n","3161it [2:05:31,  1.64s/it]\u001b[A\n","3162it [2:05:33,  1.65s/it]\u001b[A\n","3163it [2:05:35,  1.64s/it]\u001b[A\n","3164it [2:05:36,  1.65s/it]\u001b[A\n","3165it [2:05:38,  1.65s/it]\u001b[A\n","3166it [2:05:40,  1.64s/it]\u001b[A\n","3167it [2:05:41,  1.65s/it]\u001b[A\n","3168it [2:05:43,  1.65s/it]\u001b[A\n","3169it [2:05:45,  1.65s/it]\u001b[A\n","3170it [2:05:46,  1.65s/it]\u001b[A\n","3171it [2:05:48,  1.65s/it]\u001b[A\n","3172it [2:05:50,  1.65s/it]\u001b[A\n","3173it [2:05:51,  1.65s/it]\u001b[A\n","3174it [2:05:53,  1.65s/it]\u001b[A\n","3175it [2:05:55,  1.65s/it]\u001b[A\n","3176it [2:05:56,  1.65s/it]\u001b[A\n","3177it [2:05:58,  1.65s/it]\u001b[A\n","3178it [2:05:59,  1.64s/it]\u001b[A\n","3179it [2:06:01,  1.64s/it]\u001b[A\n","3180it [2:06:03,  1.65s/it]\u001b[A\n","3181it [2:06:04,  1.65s/it]\u001b[A\n","3182it [2:06:06,  1.65s/it]\u001b[A\n","3183it [2:06:08,  1.65s/it]\u001b[A\n","3184it [2:06:09,  1.65s/it]\u001b[A\n","3185it [2:06:11,  1.65s/it]\u001b[A\n","3186it [2:06:13,  1.65s/it]\u001b[A\n","3187it [2:06:14,  1.65s/it]\u001b[A\n","3188it [2:06:16,  1.65s/it]\u001b[A\n","3189it [2:06:18,  1.65s/it]\u001b[A\n","3190it [2:06:19,  1.65s/it]\u001b[A\n","3191it [2:06:21,  1.65s/it]\u001b[A\n","3192it [2:06:23,  1.65s/it]\u001b[A\n","3193it [2:06:24,  1.65s/it]\u001b[A\n","3194it [2:06:26,  1.65s/it]\u001b[A\n","3195it [2:06:27,  1.65s/it]\u001b[A\n","3196it [2:06:29,  1.65s/it]\u001b[A\n","3197it [2:06:31,  1.65s/it]\u001b[A\n","3198it [2:06:32,  1.64s/it]\u001b[A\n","3199it [2:06:34,  1.65s/it]\u001b[A\n","3200it [2:06:36,  1.65s/it]\u001b[A\n","3201it [2:06:37,  1.65s/it]\u001b[A\n","3202it [2:06:39,  1.64s/it]\u001b[A\n","3203it [2:06:41,  1.64s/it]\u001b[A\n","3204it [2:06:42,  1.64s/it]\u001b[A\n","3205it [2:06:44,  1.64s/it]\u001b[A\n","3206it [2:06:46,  1.64s/it]\u001b[A\n","3207it [2:06:47,  1.63s/it]\u001b[A\n","3208it [2:06:49,  1.64s/it]\u001b[A\n","3209it [2:06:50,  1.64s/it]\u001b[A\n","3210it [2:06:52,  1.63s/it]\u001b[A\n","3211it [2:06:54,  1.63s/it]\u001b[A\n","3212it [2:06:55,  1.63s/it]\u001b[A\n","3213it [2:06:57,  1.63s/it]\u001b[A\n","3214it [2:06:59,  1.63s/it]\u001b[A\n","3215it [2:07:00,  1.63s/it]\u001b[A\n","3216it [2:07:02,  1.63s/it]\u001b[A\n","3217it [2:07:03,  1.63s/it]\u001b[A\n","3218it [2:07:05,  1.63s/it]\u001b[A\n","3219it [2:07:07,  1.63s/it]\u001b[A\n","3220it [2:07:08,  1.62s/it]\u001b[A\n","3221it [2:07:10,  1.62s/it]\u001b[A\n","3222it [2:07:12,  1.62s/it]\u001b[A\n","3223it [2:07:13,  1.62s/it]\u001b[A\n","3224it [2:07:15,  1.63s/it]\u001b[A\n","3225it [2:07:16,  1.63s/it]\u001b[A\n","3226it [2:07:18,  1.63s/it]\u001b[A\n","3227it [2:07:20,  1.63s/it]\u001b[A\n","3228it [2:07:21,  1.63s/it]\u001b[A\n","3229it [2:07:23,  1.64s/it]\u001b[A\n","3230it [2:07:25,  1.64s/it]\u001b[A\n","3231it [2:07:26,  1.64s/it]\u001b[A\n","3232it [2:07:28,  1.64s/it]\u001b[A\n","3233it [2:07:30,  1.63s/it]\u001b[A\n","3234it [2:07:31,  1.63s/it]\u001b[A\n","3235it [2:07:33,  1.63s/it]\u001b[A\n","3236it [2:07:34,  1.64s/it]\u001b[A\n","3237it [2:07:36,  1.64s/it]\u001b[A\n","3238it [2:07:38,  1.64s/it]\u001b[A\n","3239it [2:07:39,  1.64s/it]\u001b[A\n","3240it [2:07:41,  1.64s/it]\u001b[A\n","3241it [2:07:43,  1.64s/it]\u001b[A\n","3242it [2:07:44,  1.64s/it]\u001b[A\n","3243it [2:07:46,  1.64s/it]\u001b[A\n","3244it [2:07:48,  1.64s/it]\u001b[A\n","3245it [2:07:49,  1.64s/it]\u001b[A\n","3246it [2:07:51,  1.64s/it]\u001b[A\n","3247it [2:07:53,  1.64s/it]\u001b[A\n","3248it [2:07:54,  1.64s/it]\u001b[A\n","3249it [2:07:56,  1.64s/it]\u001b[A\n","3250it [2:07:57,  1.64s/it]\u001b[A\n","3251it [2:07:59,  1.64s/it]\u001b[A\n","3252it [2:08:01,  1.64s/it]\u001b[A\n","3253it [2:08:02,  1.64s/it]\u001b[A\n","3254it [2:08:04,  1.64s/it]\u001b[A\n","3255it [2:08:06,  1.64s/it]\u001b[A\n","3256it [2:08:07,  1.63s/it]\u001b[A\n","3257it [2:08:09,  1.63s/it]\u001b[A\n","3258it [2:08:11,  1.63s/it]\u001b[A\n","3259it [2:08:12,  1.63s/it]\u001b[A\n","3260it [2:08:14,  1.63s/it]\u001b[A\n","3261it [2:08:15,  1.63s/it]\u001b[A\n","3262it [2:08:17,  1.63s/it]\u001b[A\n","3263it [2:08:19,  1.62s/it]\u001b[A\n","3264it [2:08:20,  1.62s/it]\u001b[A\n","3265it [2:08:22,  1.63s/it]\u001b[A\n","3266it [2:08:24,  1.63s/it]\u001b[A\n","3267it [2:08:25,  1.63s/it]\u001b[A\n","3268it [2:08:27,  1.63s/it]\u001b[A\n","3269it [2:08:28,  1.63s/it]\u001b[A\n","3270it [2:08:30,  1.63s/it]\u001b[A\n","3271it [2:08:32,  1.64s/it]\u001b[A\n","3272it [2:08:33,  1.64s/it]\u001b[A\n","3273it [2:08:35,  1.65s/it]\u001b[A\n","3274it [2:08:37,  1.65s/it]\u001b[A\n","3275it [2:08:38,  1.65s/it]\u001b[A\n","3276it [2:08:40,  1.65s/it]\u001b[A\n","3277it [2:08:42,  1.65s/it]\u001b[A\n","3278it [2:08:43,  1.64s/it]\u001b[A\n","3279it [2:08:45,  1.64s/it]\u001b[A\n","3280it [2:08:47,  1.65s/it]\u001b[A\n","3281it [2:08:48,  1.65s/it]\u001b[A\n","3282it [2:08:50,  1.65s/it]\u001b[A\n","3283it [2:08:52,  1.65s/it]\u001b[A\n","3284it [2:08:53,  1.64s/it]\u001b[A\n","3285it [2:08:55,  1.64s/it]\u001b[A\n","3286it [2:08:56,  1.65s/it]\u001b[A\n","3287it [2:08:58,  1.65s/it]\u001b[A\n","3288it [2:09:00,  1.65s/it]\u001b[A\n","3289it [2:09:01,  1.65s/it]\u001b[A\n","3290it [2:09:03,  1.65s/it]\u001b[A\n","3291it [2:09:05,  1.65s/it]\u001b[A\n","3292it [2:09:06,  1.65s/it]\u001b[A\n","3293it [2:09:08,  1.65s/it]\u001b[A\n","3294it [2:09:10,  1.65s/it]\u001b[A\n","3295it [2:09:11,  1.64s/it]\u001b[A\n","3296it [2:09:13,  1.64s/it]\u001b[A\n","3297it [2:09:15,  1.64s/it]\u001b[A\n","3298it [2:09:16,  1.64s/it]\u001b[A\n","3299it [2:09:18,  1.64s/it]\u001b[A\n","3300it [2:09:19,  1.64s/it]\u001b[A\n","3301it [2:09:21,  1.65s/it]\u001b[A\n","3302it [2:09:23,  1.65s/it]\u001b[A\n","3303it [2:09:24,  1.64s/it]\u001b[A\n","3304it [2:09:26,  1.64s/it]\u001b[A\n","3305it [2:09:28,  1.64s/it]\u001b[A\n","3306it [2:09:29,  1.64s/it]\u001b[A\n","3307it [2:09:31,  1.65s/it]\u001b[A\n","3308it [2:09:33,  1.65s/it]\u001b[A\n","3309it [2:09:34,  1.64s/it]\u001b[A\n","3310it [2:09:36,  1.65s/it]\u001b[A\n","3311it [2:09:38,  1.64s/it]\u001b[A\n","3312it [2:09:39,  1.65s/it]\u001b[A\n","3313it [2:09:41,  1.64s/it]\u001b[A\n","3314it [2:09:42,  1.65s/it]\u001b[A\n","3315it [2:09:44,  1.65s/it]\u001b[A\n","3316it [2:09:46,  1.65s/it]\u001b[A\n","3317it [2:09:47,  1.64s/it]\u001b[A\n","3318it [2:09:49,  1.65s/it]\u001b[A\n","3319it [2:09:51,  1.65s/it]\u001b[A\n","3320it [2:09:52,  1.64s/it]\u001b[A\n","3321it [2:09:54,  1.64s/it]\u001b[A\n","3322it [2:09:56,  1.65s/it]\u001b[A\n","3323it [2:09:57,  1.65s/it]\u001b[A\n","3324it [2:09:59,  1.65s/it]\u001b[A\n","3325it [2:10:01,  1.65s/it]\u001b[A\n","3326it [2:10:02,  1.65s/it]\u001b[A\n","3327it [2:10:04,  1.65s/it]\u001b[A\n","3328it [2:10:06,  1.65s/it]\u001b[A\n","3329it [2:10:07,  1.64s/it]\u001b[A\n","3330it [2:10:09,  1.64s/it]\u001b[A\n","3331it [2:10:10,  1.64s/it]\u001b[A\n","3332it [2:10:12,  1.65s/it]\u001b[A\n","3333it [2:10:14,  1.65s/it]\u001b[A\n","3334it [2:10:15,  1.65s/it]\u001b[A\n","3335it [2:10:17,  1.65s/it]\u001b[A\n","3336it [2:10:19,  1.65s/it]\u001b[A\n","3337it [2:10:20,  1.65s/it]\u001b[A\n","3338it [2:10:22,  1.65s/it]\u001b[A\n","3339it [2:10:24,  1.65s/it]\u001b[A\n","3340it [2:10:25,  1.65s/it]\u001b[A\n","3341it [2:10:27,  1.65s/it]\u001b[A\n","3342it [2:10:29,  1.65s/it]\u001b[A\n","3343it [2:10:30,  1.65s/it]\u001b[A\n","3344it [2:10:32,  1.65s/it]\u001b[A\n","3345it [2:10:34,  1.65s/it]\u001b[A\n","3346it [2:10:35,  1.65s/it]\u001b[A\n","3347it [2:10:37,  1.65s/it]\u001b[A\n","3348it [2:10:39,  1.65s/it]\u001b[A\n","3349it [2:10:40,  1.65s/it]\u001b[A\n","3350it [2:10:42,  1.65s/it]\u001b[A\n","3351it [2:10:43,  1.65s/it]\u001b[A\n","3352it [2:10:45,  1.65s/it]\u001b[A\n","3353it [2:10:47,  1.65s/it]\u001b[A\n","3354it [2:10:48,  1.65s/it]\u001b[A\n","3355it [2:10:50,  1.65s/it]\u001b[A\n","3356it [2:10:52,  1.66s/it]\u001b[A\n","3357it [2:10:53,  1.66s/it]\u001b[A\n","3358it [2:10:55,  1.65s/it]\u001b[A\n","3359it [2:10:57,  1.65s/it]\u001b[A\n","3360it [2:10:58,  1.65s/it]\u001b[A\n","3361it [2:11:00,  1.65s/it]\u001b[A\n","3362it [2:11:02,  1.65s/it]\u001b[A\n","3363it [2:11:03,  1.65s/it]\u001b[A\n","3364it [2:11:05,  1.65s/it]\u001b[A\n","3365it [2:11:07,  1.65s/it]\u001b[A\n","3366it [2:11:08,  1.64s/it]\u001b[A\n","3367it [2:11:10,  1.64s/it]\u001b[A\n","3368it [2:11:11,  1.64s/it]\u001b[A\n","3369it [2:11:13,  1.64s/it]\u001b[A\n","3370it [2:11:15,  1.64s/it]\u001b[A\n","3371it [2:11:16,  1.64s/it]\u001b[A\n","3372it [2:11:18,  1.64s/it]\u001b[A\n","3373it [2:11:20,  1.64s/it]\u001b[A\n","3374it [2:11:21,  1.64s/it]\u001b[A\n","3375it [2:11:23,  1.64s/it]\u001b[A\n","3376it [2:11:25,  1.64s/it]\u001b[A\n","3377it [2:11:26,  1.64s/it]\u001b[A\n","3378it [2:11:28,  1.64s/it]\u001b[A\n","3379it [2:11:30,  1.64s/it]\u001b[A\n","3380it [2:11:31,  1.64s/it]\u001b[A\n","3381it [2:11:33,  1.64s/it]\u001b[A\n","3382it [2:11:34,  1.64s/it]\u001b[A\n","3383it [2:11:36,  1.64s/it]\u001b[A\n","3384it [2:11:38,  1.64s/it]\u001b[A\n","3385it [2:11:39,  1.64s/it]\u001b[A\n","3386it [2:11:41,  1.63s/it]\u001b[A\n","3387it [2:11:43,  1.63s/it]\u001b[A\n","3388it [2:11:44,  1.63s/it]\u001b[A\n","3389it [2:11:46,  1.63s/it]\u001b[A\n","3390it [2:11:48,  1.64s/it]\u001b[A\n","3391it [2:11:49,  1.64s/it]\u001b[A\n","3392it [2:11:51,  1.64s/it]\u001b[A\n","3393it [2:11:52,  1.64s/it]\u001b[A\n","3394it [2:11:54,  1.64s/it]\u001b[A\n","3395it [2:11:56,  1.64s/it]\u001b[A\n","3396it [2:11:57,  1.64s/it]\u001b[A\n","3397it [2:11:59,  1.64s/it]\u001b[A\n","3398it [2:12:01,  1.64s/it]\u001b[A\n","3399it [2:12:02,  1.64s/it]\u001b[A\n","3400it [2:12:04,  1.64s/it]\u001b[A\n","3401it [2:12:06,  1.64s/it]\u001b[A\n","3402it [2:12:07,  1.64s/it]\u001b[A\n","3403it [2:12:09,  1.64s/it]\u001b[A\n","3404it [2:12:10,  1.64s/it]\u001b[A\n","3405it [2:12:12,  1.63s/it]\u001b[A\n","3406it [2:12:14,  1.63s/it]\u001b[A\n","3407it [2:12:15,  1.63s/it]\u001b[A\n","3408it [2:12:17,  1.63s/it]\u001b[A\n","3409it [2:12:19,  1.63s/it]\u001b[A\n","3410it [2:12:20,  1.63s/it]\u001b[A\n","3411it [2:12:22,  1.63s/it]\u001b[A\n","3412it [2:12:24,  1.64s/it]\u001b[A\n","3413it [2:12:25,  1.64s/it]\u001b[A\n","3414it [2:12:27,  1.64s/it]\u001b[A\n","3415it [2:12:28,  1.64s/it]\u001b[A\n","3416it [2:12:30,  1.63s/it]\u001b[A\n","3417it [2:12:32,  1.63s/it]\u001b[A\n","3418it [2:12:33,  1.63s/it]\u001b[A\n","3419it [2:12:35,  1.64s/it]\u001b[A\n","3420it [2:12:37,  1.64s/it]\u001b[A\n","3421it [2:12:38,  1.64s/it]\u001b[A\n","3422it [2:12:40,  1.64s/it]\u001b[A\n","3423it [2:12:42,  1.64s/it]\u001b[A\n","3424it [2:12:43,  1.64s/it]\u001b[A\n","3425it [2:12:45,  1.63s/it]\u001b[A\n","3426it [2:12:46,  1.63s/it]\u001b[A\n","3427it [2:12:48,  1.63s/it]\u001b[A\n","3428it [2:12:50,  1.64s/it]\u001b[A\n","3429it [2:12:51,  1.63s/it]\u001b[A\n","3430it [2:12:53,  1.63s/it]\u001b[A\n","3431it [2:12:55,  1.64s/it]\u001b[A\n","3432it [2:12:56,  1.64s/it]\u001b[A\n","3433it [2:12:58,  1.63s/it]\u001b[A\n","3434it [2:12:59,  1.63s/it]\u001b[A\n","3435it [2:13:01,  1.63s/it]\u001b[A\n","3436it [2:13:03,  1.63s/it]\u001b[A\n","3437it [2:13:04,  1.63s/it]\u001b[A\n","3438it [2:13:06,  1.64s/it]\u001b[A\n","3439it [2:13:08,  1.64s/it]\u001b[A\n","3440it [2:13:09,  1.64s/it]\u001b[A\n","3441it [2:13:11,  1.64s/it]\u001b[A\n","3442it [2:13:13,  1.65s/it]\u001b[A\n","3443it [2:13:14,  1.64s/it]\u001b[A\n","3444it [2:13:16,  1.64s/it]\u001b[A\n","3445it [2:13:18,  1.64s/it]\u001b[A\n","3446it [2:13:19,  1.64s/it]\u001b[A\n","3447it [2:13:21,  1.64s/it]\u001b[A\n","3448it [2:13:22,  1.65s/it]\u001b[A\n","3449it [2:13:24,  1.65s/it]\u001b[A\n","3450it [2:13:26,  1.65s/it]\u001b[A\n","3451it [2:13:27,  1.65s/it]\u001b[A\n","3452it [2:13:29,  1.65s/it]\u001b[A\n","3453it [2:13:31,  1.65s/it]\u001b[A\n","3454it [2:13:32,  1.65s/it]\u001b[A\n","3455it [2:13:34,  1.65s/it]\u001b[A\n","3456it [2:13:36,  1.65s/it]\u001b[A\n","3457it [2:13:37,  1.65s/it]\u001b[A\n","3458it [2:13:39,  1.65s/it]\u001b[A\n","3459it [2:13:41,  1.65s/it]\u001b[A\n","3460it [2:13:42,  1.65s/it]\u001b[A\n","3461it [2:13:44,  1.65s/it]\u001b[A\n","3462it [2:13:46,  1.64s/it]\u001b[A\n","3463it [2:13:47,  1.65s/it]\u001b[A\n","3464it [2:13:49,  1.64s/it]\u001b[A\n","3465it [2:13:51,  1.64s/it]\u001b[A\n","3466it [2:13:52,  1.64s/it]\u001b[A\n","3467it [2:13:54,  1.64s/it]\u001b[A\n","3468it [2:13:55,  1.64s/it]\u001b[A\n","3469it [2:13:57,  1.65s/it]\u001b[A\n","3470it [2:13:59,  1.65s/it]\u001b[A\n","3471it [2:14:00,  1.65s/it]\u001b[A\n","3472it [2:14:02,  1.65s/it]\u001b[A\n","3473it [2:14:04,  1.65s/it]\u001b[A\n","3474it [2:14:05,  1.65s/it]\u001b[A\n","3475it [2:14:07,  1.65s/it]\u001b[A\n","3476it [2:14:09,  1.65s/it]\u001b[A\n","3477it [2:14:10,  1.65s/it]\u001b[A\n","3478it [2:14:12,  1.66s/it]\u001b[A\n","3479it [2:14:14,  1.65s/it]\u001b[A\n","3480it [2:14:15,  1.66s/it]\u001b[A\n","3481it [2:14:17,  1.66s/it]\u001b[A\n","3482it [2:14:19,  1.65s/it]\u001b[A\n","3483it [2:14:20,  1.65s/it]\u001b[A\n","3484it [2:14:22,  1.65s/it]\u001b[A\n","3485it [2:14:24,  1.65s/it]\u001b[A\n","3486it [2:14:25,  1.65s/it]\u001b[A\n","3487it [2:14:27,  1.65s/it]\u001b[A\n","3488it [2:14:28,  1.65s/it]\u001b[A\n","3489it [2:14:30,  1.65s/it]\u001b[A\n","3490it [2:14:32,  1.64s/it]\u001b[A\n","3491it [2:14:33,  1.65s/it]\u001b[A\n","3492it [2:14:35,  1.65s/it]\u001b[A\n","3493it [2:14:37,  1.65s/it]\u001b[A\n","3494it [2:14:38,  1.65s/it]\u001b[A\n","3495it [2:14:40,  1.65s/it]\u001b[A\n","3496it [2:14:42,  1.65s/it]\u001b[A\n","3497it [2:14:43,  1.65s/it]\u001b[A\n","3498it [2:14:45,  1.65s/it]\u001b[A\n","3499it [2:14:47,  1.65s/it]\u001b[A\n","3500it [2:14:48,  1.65s/it]\u001b[A\n","3501it [2:14:50,  1.65s/it]\u001b[A\n","3502it [2:14:52,  1.65s/it]\u001b[A\n","3503it [2:14:53,  1.65s/it]\u001b[A\n","3504it [2:14:55,  1.66s/it]\u001b[A\n","3505it [2:14:57,  1.65s/it]\u001b[A\n","3506it [2:14:58,  1.65s/it]\u001b[A\n","3507it [2:15:00,  1.65s/it]\u001b[A\n","3508it [2:15:01,  1.66s/it]\u001b[A\n","3509it [2:15:03,  1.66s/it]\u001b[A\n","3510it [2:15:05,  1.65s/it]\u001b[A\n","3511it [2:15:06,  1.65s/it]\u001b[A\n","3512it [2:15:08,  1.66s/it]\u001b[A\n","3513it [2:15:10,  1.65s/it]\u001b[A\n","3514it [2:15:11,  1.65s/it]\u001b[A\n","3515it [2:15:13,  1.65s/it]\u001b[A\n","3516it [2:15:15,  1.65s/it]\u001b[A\n","3517it [2:15:16,  1.65s/it]\u001b[A\n","3518it [2:15:18,  1.65s/it]\u001b[A\n","3519it [2:15:20,  1.65s/it]\u001b[A\n","3520it [2:15:21,  1.65s/it]\u001b[A\n","3521it [2:15:23,  1.65s/it]\u001b[A\n","3522it [2:15:25,  1.65s/it]\u001b[A\n","3523it [2:15:26,  1.65s/it]\u001b[A\n","3524it [2:15:28,  1.65s/it]\u001b[A\n","3525it [2:15:30,  1.65s/it]\u001b[A\n","3526it [2:15:31,  1.65s/it]\u001b[A\n","3527it [2:15:33,  1.65s/it]\u001b[A\n","3528it [2:15:34,  1.65s/it]\u001b[A\n","3529it [2:15:36,  1.64s/it]\u001b[A\n","3530it [2:15:38,  1.65s/it]\u001b[A\n","3531it [2:15:39,  1.65s/it]\u001b[A\n","3532it [2:15:41,  1.65s/it]\u001b[A\n","3533it [2:15:43,  1.65s/it]\u001b[A\n","3534it [2:15:44,  1.65s/it]\u001b[A\n","3535it [2:15:46,  1.65s/it]\u001b[A\n","3536it [2:15:48,  1.64s/it]\u001b[A\n","3537it [2:15:49,  1.64s/it]\u001b[A\n","3538it [2:15:51,  1.64s/it]\u001b[A\n","3539it [2:15:53,  1.65s/it]\u001b[A\n","3540it [2:15:54,  1.65s/it]\u001b[A\n","3541it [2:15:56,  1.65s/it]\u001b[A\n","3542it [2:15:58,  1.65s/it]\u001b[A\n","3543it [2:15:59,  1.64s/it]\u001b[A\n","3544it [2:16:01,  1.64s/it]\u001b[A\n","3545it [2:16:02,  1.64s/it]\u001b[A\n","3546it [2:16:04,  1.64s/it]\u001b[A\n","3547it [2:16:06,  1.64s/it]\u001b[A\n","3548it [2:16:07,  1.64s/it]\u001b[A\n","3549it [2:16:09,  1.63s/it]\u001b[A\n","3550it [2:16:11,  1.64s/it]\u001b[A\n","3551it [2:16:12,  1.64s/it]\u001b[A\n","3552it [2:16:14,  1.64s/it]\u001b[A\n","3553it [2:16:16,  1.64s/it]\u001b[A\n","3554it [2:16:17,  1.64s/it]\u001b[A\n","3555it [2:16:19,  1.64s/it]\u001b[A\n","3556it [2:16:20,  1.64s/it]\u001b[A\n","3557it [2:16:22,  1.64s/it]\u001b[A\n","3558it [2:16:24,  1.64s/it]\u001b[A\n","3559it [2:16:25,  1.64s/it]\u001b[A\n","3560it [2:16:27,  1.63s/it]\u001b[A\n","3561it [2:16:29,  1.63s/it]\u001b[A\n","3562it [2:16:30,  1.63s/it]\u001b[A\n","3563it [2:16:32,  1.64s/it]\u001b[A\n","3564it [2:16:34,  1.64s/it]\u001b[A\n","3565it [2:16:35,  1.64s/it]\u001b[A\n","3566it [2:16:37,  1.64s/it]\u001b[A\n","3567it [2:16:38,  1.64s/it]\u001b[A\n","3568it [2:16:40,  1.64s/it]\u001b[A\n","3569it [2:16:42,  1.63s/it]\u001b[A\n","3570it [2:16:43,  1.64s/it]\u001b[A\n","3571it [2:16:45,  1.64s/it]\u001b[A\n","3572it [2:16:47,  1.63s/it]\u001b[A\n","3573it [2:16:48,  1.63s/it]\u001b[A\n","3574it [2:16:50,  1.64s/it]\u001b[A\n","3575it [2:16:52,  1.63s/it]\u001b[A\n","3576it [2:16:53,  1.64s/it]\u001b[A\n","3577it [2:16:55,  1.64s/it]\u001b[A\n","3578it [2:16:56,  1.64s/it]\u001b[A\n","3579it [2:16:58,  1.64s/it]\u001b[A\n","3580it [2:17:00,  1.64s/it]\u001b[A\n","3581it [2:17:01,  1.64s/it]\u001b[A\n","3582it [2:17:03,  1.64s/it]\u001b[A\n","3583it [2:17:05,  1.64s/it]\u001b[A\n","3584it [2:17:06,  1.64s/it]\u001b[A\n","3585it [2:17:08,  1.64s/it]\u001b[A\n","3586it [2:17:10,  1.64s/it]\u001b[A\n","3587it [2:17:11,  1.64s/it]\u001b[A\n","3588it [2:17:13,  1.64s/it]\u001b[A\n","3589it [2:17:14,  1.64s/it]\u001b[A\n","3590it [2:17:16,  1.64s/it]\u001b[A\n","3591it [2:17:18,  1.64s/it]\u001b[A\n","3592it [2:17:19,  1.64s/it]\u001b[A\n","3593it [2:17:21,  1.64s/it]\u001b[A\n","3594it [2:17:23,  1.64s/it]\u001b[A\n","3595it [2:17:24,  1.64s/it]\u001b[A\n","3596it [2:17:26,  1.64s/it]\u001b[A\n","3597it [2:17:28,  1.64s/it]\u001b[A\n","3598it [2:17:29,  1.64s/it]\u001b[A\n","3599it [2:17:31,  1.64s/it]\u001b[A\n","3600it [2:17:33,  1.64s/it]\u001b[A\n","3601it [2:17:34,  1.64s/it]\u001b[A\n","3602it [2:17:36,  1.64s/it]\u001b[A\n","3603it [2:17:37,  1.64s/it]\u001b[A\n","3604it [2:17:39,  1.64s/it]\u001b[A\n","3605it [2:17:41,  1.63s/it]\u001b[A\n","3606it [2:17:42,  1.63s/it]\u001b[A\n","3607it [2:17:44,  1.64s/it]\u001b[A\n","3608it [2:17:46,  1.64s/it]\u001b[A\n","3609it [2:17:47,  1.65s/it]\u001b[A\n","3610it [2:17:49,  1.64s/it]\u001b[A\n","3611it [2:17:51,  1.65s/it]\u001b[A\n","3612it [2:17:52,  1.65s/it]\u001b[A\n","3613it [2:17:54,  1.65s/it]\u001b[A\n","3614it [2:17:56,  1.65s/it]\u001b[A\n","3615it [2:17:57,  1.65s/it]\u001b[A\n","3616it [2:17:59,  1.65s/it]\u001b[A\n","3617it [2:18:00,  1.65s/it]\u001b[A\n","3618it [2:18:02,  1.65s/it]\u001b[A\n","3619it [2:18:04,  1.65s/it]\u001b[A\n","3620it [2:18:05,  1.65s/it]\u001b[A\n","3621it [2:18:07,  1.65s/it]\u001b[A\n","3622it [2:18:09,  1.65s/it]\u001b[A\n","3623it [2:18:10,  1.65s/it]\u001b[A\n","3624it [2:18:12,  1.65s/it]\u001b[A\n","3625it [2:18:14,  1.65s/it]\u001b[A\n","3626it [2:18:15,  1.65s/it]\u001b[A\n","3627it [2:18:17,  1.65s/it]\u001b[A\n","3628it [2:18:19,  1.65s/it]\u001b[A\n","3629it [2:18:20,  1.64s/it]\u001b[A\n","3630it [2:18:22,  1.64s/it]\u001b[A\n","3631it [2:18:24,  1.64s/it]\u001b[A\n","3632it [2:18:25,  1.65s/it]\u001b[A\n","3633it [2:18:27,  1.64s/it]\u001b[A\n","3634it [2:18:28,  1.64s/it]\u001b[A\n","3635it [2:18:30,  1.65s/it]\u001b[A\n","3636it [2:18:32,  1.64s/it]\u001b[A\n","3637it [2:18:33,  1.64s/it]\u001b[A\n","3638it [2:18:35,  1.64s/it]\u001b[A\n","3639it [2:18:37,  1.64s/it]\u001b[A\n","3640it [2:18:38,  1.65s/it]\u001b[A\n","3641it [2:18:40,  1.64s/it]\u001b[A\n","3642it [2:18:42,  1.64s/it]\u001b[A\n","3643it [2:18:43,  1.65s/it]\u001b[A\n","3644it [2:18:45,  1.65s/it]\u001b[A\n","3645it [2:18:47,  1.65s/it]\u001b[A\n","3646it [2:18:48,  1.65s/it]\u001b[A\n","3647it [2:18:50,  1.65s/it]\u001b[A\n","3648it [2:18:52,  1.65s/it]\u001b[A\n","3649it [2:18:53,  1.65s/it]\u001b[A\n","3650it [2:18:55,  1.65s/it]\u001b[A\n","3651it [2:18:56,  1.64s/it]\u001b[A\n","3652it [2:18:58,  1.65s/it]\u001b[A\n","3653it [2:19:00,  1.65s/it]\u001b[A\n","3654it [2:19:01,  1.64s/it]\u001b[A\n","3655it [2:19:03,  1.64s/it]\u001b[A\n","3656it [2:19:05,  1.64s/it]\u001b[A\n","3657it [2:19:06,  1.65s/it]\u001b[A\n","3658it [2:19:08,  1.64s/it]\u001b[A\n","3659it [2:19:10,  1.64s/it]\u001b[A\n","3660it [2:19:11,  1.65s/it]\u001b[A\n","3661it [2:19:13,  1.65s/it]\u001b[A\n","3662it [2:19:15,  1.64s/it]\u001b[A\n","3663it [2:19:16,  1.65s/it]\u001b[A\n","3664it [2:19:18,  1.64s/it]\u001b[A\n","3665it [2:19:19,  1.64s/it]\u001b[A\n","3666it [2:19:21,  1.64s/it]\u001b[A\n","3667it [2:19:23,  1.64s/it]\u001b[A\n","3668it [2:19:24,  1.64s/it]\u001b[A\n","3669it [2:19:26,  1.65s/it]\u001b[A\n","3670it [2:19:28,  1.65s/it]\u001b[A\n","3671it [2:19:29,  1.65s/it]\u001b[A\n","3672it [2:19:31,  1.65s/it]\u001b[A\n","3673it [2:19:33,  1.65s/it]\u001b[A\n","3674it [2:19:34,  1.65s/it]\u001b[A\n","3675it [2:19:36,  1.64s/it]\u001b[A\n","3676it [2:19:38,  1.65s/it]\u001b[A\n","3677it [2:19:39,  1.65s/it]\u001b[A\n","3678it [2:19:41,  1.65s/it]\u001b[A\n","3679it [2:19:43,  1.65s/it]\u001b[A\n","3680it [2:19:44,  1.65s/it]\u001b[A\n","3681it [2:19:46,  1.65s/it]\u001b[A\n","3682it [2:19:47,  1.64s/it]\u001b[A\n","3683it [2:19:49,  1.65s/it]\u001b[A\n","3684it [2:19:51,  1.65s/it]\u001b[A\n","3685it [2:19:52,  1.65s/it]\u001b[A\n","3686it [2:19:54,  1.65s/it]\u001b[A\n","3687it [2:19:56,  1.65s/it]\u001b[A\n","3688it [2:19:57,  1.65s/it]\u001b[A\n","3689it [2:19:59,  1.64s/it]\u001b[A\n","3690it [2:20:01,  1.64s/it]\u001b[A\n","3691it [2:20:02,  1.64s/it]\u001b[A\n","3692it [2:20:04,  1.64s/it]\u001b[A\n","3693it [2:20:06,  1.65s/it]\u001b[A\n","3694it [2:20:07,  1.65s/it]\u001b[A\n","3695it [2:20:09,  1.64s/it]\u001b[A\n","3696it [2:20:10,  1.64s/it]\u001b[A\n","3697it [2:20:12,  1.65s/it]\u001b[A\n","3698it [2:20:14,  1.65s/it]\u001b[A\n","3699it [2:20:15,  1.65s/it]\u001b[A\n","3700it [2:20:17,  1.65s/it]\u001b[A\n","3701it [2:20:19,  1.65s/it]\u001b[A\n","3702it [2:20:20,  1.65s/it]\u001b[A\n","3703it [2:20:22,  1.64s/it]\u001b[A\n","3704it [2:20:24,  1.65s/it]\u001b[A\n","3705it [2:20:25,  1.64s/it]\u001b[A\n","3706it [2:20:27,  1.64s/it]\u001b[A\n","3707it [2:20:29,  1.64s/it]\u001b[A\n","3708it [2:20:30,  1.64s/it]\u001b[A\n","3709it [2:20:32,  1.64s/it]\u001b[A\n","3710it [2:20:33,  1.64s/it]\u001b[A\n","3711it [2:20:35,  1.64s/it]\u001b[A\n","3712it [2:20:37,  1.64s/it]\u001b[A\n","3713it [2:20:38,  1.64s/it]\u001b[A\n","3714it [2:20:40,  1.63s/it]\u001b[A\n","3715it [2:20:42,  1.64s/it]\u001b[A\n","3716it [2:20:43,  1.64s/it]\u001b[A\n","3717it [2:20:45,  1.63s/it]\u001b[A\n","3718it [2:20:47,  1.63s/it]\u001b[A\n","3719it [2:20:48,  1.63s/it]\u001b[A\n","3720it [2:20:50,  1.63s/it]\u001b[A\n","3721it [2:20:51,  1.63s/it]\u001b[A\n","3722it [2:20:53,  1.63s/it]\u001b[A\n","3723it [2:20:55,  1.63s/it]\u001b[A\n","3724it [2:20:56,  1.63s/it]\u001b[A\n","3725it [2:20:58,  1.63s/it]\u001b[A\n","3726it [2:21:00,  1.63s/it]\u001b[A\n","3727it [2:21:01,  1.63s/it]\u001b[A\n","3728it [2:21:03,  1.63s/it]\u001b[A\n","3729it [2:21:04,  1.63s/it]\u001b[A\n","3730it [2:21:06,  1.63s/it]\u001b[A\n","3731it [2:21:08,  1.63s/it]\u001b[A\n","3732it [2:21:09,  1.63s/it]\u001b[A\n","3733it [2:21:11,  1.63s/it]\u001b[A\n","3734it [2:21:13,  1.63s/it]\u001b[A\n","3735it [2:21:14,  1.63s/it]\u001b[A\n","3736it [2:21:16,  1.63s/it]\u001b[A\n","3737it [2:21:18,  1.63s/it]\u001b[A\n","3738it [2:21:19,  1.63s/it]\u001b[A\n","3739it [2:21:21,  1.63s/it]\u001b[A\n","3740it [2:21:22,  1.63s/it]\u001b[A\n","3741it [2:21:24,  1.63s/it]\u001b[A\n","3742it [2:21:26,  1.63s/it]\u001b[A\n","3743it [2:21:27,  1.62s/it]\u001b[A\n","3744it [2:21:29,  1.62s/it]\u001b[A\n","3745it [2:21:31,  1.63s/it]\u001b[A\n","3746it [2:21:32,  1.62s/it]\u001b[A\n","3747it [2:21:34,  1.63s/it]\u001b[A\n","3748it [2:21:35,  1.63s/it]\u001b[A\n","3749it [2:21:37,  1.63s/it]\u001b[A\n","3750it [2:21:39,  1.63s/it]\u001b[A\n","3751it [2:21:40,  1.63s/it]\u001b[A\n","3752it [2:21:42,  1.63s/it]\u001b[A\n","3753it [2:21:44,  1.63s/it]\u001b[A\n","3754it [2:21:45,  1.63s/it]\u001b[A\n","3755it [2:21:47,  1.63s/it]\u001b[A\n","3756it [2:21:48,  1.63s/it]\u001b[A\n","3757it [2:21:50,  1.63s/it]\u001b[A\n","3758it [2:21:52,  1.63s/it]\u001b[A\n","3759it [2:21:53,  1.63s/it]\u001b[A\n","3760it [2:21:55,  1.63s/it]\u001b[A\n","3761it [2:21:57,  1.64s/it]\u001b[A\n","3762it [2:21:58,  1.63s/it]\u001b[A\n","3763it [2:22:00,  1.64s/it]\u001b[A\n","3764it [2:22:02,  1.64s/it]\u001b[A\n","3765it [2:22:03,  1.63s/it]\u001b[A\n","3766it [2:22:05,  1.64s/it]\u001b[A\n","3767it [2:22:06,  1.64s/it]\u001b[A\n","3768it [2:22:08,  1.64s/it]\u001b[A\n","3769it [2:22:10,  1.64s/it]\u001b[A\n","3770it [2:22:11,  1.63s/it]\u001b[A\n","3771it [2:22:13,  1.63s/it]\u001b[A\n","3772it [2:22:15,  1.63s/it]\u001b[A\n","3773it [2:22:16,  1.63s/it]\u001b[A\n","3774it [2:22:18,  1.63s/it]\u001b[A\n","3775it [2:22:20,  1.63s/it]\u001b[A\n","3776it [2:22:21,  1.64s/it]\u001b[A\n","3777it [2:22:23,  1.64s/it]\u001b[A\n","3778it [2:22:24,  1.64s/it]\u001b[A\n","3779it [2:22:26,  1.64s/it]\u001b[A\n","3780it [2:22:28,  1.64s/it]\u001b[A\n","3781it [2:22:29,  1.64s/it]\u001b[A\n","3782it [2:22:31,  1.64s/it]\u001b[A\n","3783it [2:22:33,  1.64s/it]\u001b[A\n","3784it [2:22:34,  1.64s/it]\u001b[A\n","3785it [2:22:36,  1.64s/it]\u001b[A\n","3786it [2:22:38,  1.65s/it]\u001b[A\n","3787it [2:22:39,  1.65s/it]\u001b[A\n","3788it [2:22:41,  1.65s/it]\u001b[A\n","3789it [2:22:43,  1.65s/it]\u001b[A\n","3790it [2:22:44,  1.65s/it]\u001b[A\n","3791it [2:22:46,  1.65s/it]\u001b[A\n","3792it [2:22:48,  1.65s/it]\u001b[A\n","3793it [2:22:49,  1.65s/it]\u001b[A\n","3794it [2:22:51,  1.66s/it]\u001b[A\n","3795it [2:22:52,  1.65s/it]\u001b[A\n","3796it [2:22:54,  1.65s/it]\u001b[A\n","3797it [2:22:56,  1.65s/it]\u001b[A\n","3798it [2:22:57,  1.65s/it]\u001b[A\n","3799it [2:22:59,  1.65s/it]\u001b[A\n","3800it [2:23:01,  1.65s/it]\u001b[A\n","3801it [2:23:02,  1.65s/it]\u001b[A\n","3802it [2:23:04,  1.65s/it]\u001b[A\n","3803it [2:23:06,  1.64s/it]\u001b[A\n","3804it [2:23:07,  1.64s/it]\u001b[A\n","3805it [2:23:09,  1.65s/it]\u001b[A\n","3806it [2:23:11,  1.64s/it]\u001b[A\n","3807it [2:23:12,  1.65s/it]\u001b[A\n","3808it [2:23:14,  1.65s/it]\u001b[A\n","3809it [2:23:16,  1.65s/it]\u001b[A\n","3810it [2:23:17,  1.64s/it]\u001b[A\n","3811it [2:23:19,  1.65s/it]\u001b[A\n","3812it [2:23:20,  1.65s/it]\u001b[A\n","3813it [2:23:22,  1.65s/it]\u001b[A\n","3814it [2:23:24,  1.65s/it]\u001b[A\n","3815it [2:23:25,  1.65s/it]\u001b[A\n","3816it [2:23:27,  1.65s/it]\u001b[A\n","3817it [2:23:29,  1.65s/it]\u001b[A\n","3818it [2:23:30,  1.65s/it]\u001b[A\n","3819it [2:23:32,  1.65s/it]\u001b[A\n","3820it [2:23:34,  1.65s/it]\u001b[A\n","3821it [2:23:35,  1.65s/it]\u001b[A\n","3822it [2:23:37,  1.65s/it]\u001b[A\n","3823it [2:23:39,  1.65s/it]\u001b[A\n","3824it [2:23:40,  1.65s/it]\u001b[A\n","3825it [2:23:42,  1.65s/it]\u001b[A\n","3826it [2:23:44,  1.65s/it]\u001b[A\n","3827it [2:23:45,  1.65s/it]\u001b[A\n","3828it [2:23:47,  1.65s/it]\u001b[A\n","3829it [2:23:49,  1.65s/it]\u001b[A\n","3830it [2:23:50,  1.65s/it]\u001b[A\n","3831it [2:23:52,  1.65s/it]\u001b[A\n","3832it [2:23:53,  1.65s/it]\u001b[A\n","3833it [2:23:55,  1.66s/it]\u001b[A\n","3834it [2:23:57,  1.66s/it]\u001b[A\n","3835it [2:23:58,  1.65s/it]\u001b[A\n","3836it [2:24:00,  1.65s/it]\u001b[A\n","3837it [2:24:02,  1.65s/it]\u001b[A\n","3838it [2:24:03,  1.65s/it]\u001b[A\n","3839it [2:24:05,  1.65s/it]\u001b[A\n","3840it [2:24:07,  1.65s/it]\u001b[A\n","3841it [2:24:08,  1.65s/it]\u001b[A\n","3842it [2:24:10,  1.65s/it]\u001b[A\n","3843it [2:24:12,  1.65s/it]\u001b[A\n","3844it [2:24:13,  1.65s/it]\u001b[A\n","3845it [2:24:15,  1.66s/it]\u001b[A\n","3846it [2:24:17,  1.66s/it]\u001b[A\n","3847it [2:24:18,  1.65s/it]\u001b[A\n","3848it [2:24:20,  1.65s/it]\u001b[A\n","3849it [2:24:22,  1.65s/it]\u001b[A\n","3850it [2:24:23,  1.65s/it]\u001b[A\n","3851it [2:24:25,  1.65s/it]\u001b[A\n","3852it [2:24:27,  1.65s/it]\u001b[A\n","3853it [2:24:28,  1.65s/it]\u001b[A\n","3854it [2:24:30,  1.65s/it]\u001b[A\n","3855it [2:24:31,  1.65s/it]\u001b[A\n","3856it [2:24:33,  1.65s/it]\u001b[A\n","3857it [2:24:35,  1.65s/it]\u001b[A\n","3858it [2:24:36,  1.66s/it]\u001b[A\n","3859it [2:24:38,  1.65s/it]\u001b[A\n","3860it [2:24:40,  1.65s/it]\u001b[A\n","3861it [2:24:41,  1.65s/it]\u001b[A\n","3862it [2:24:43,  1.65s/it]\u001b[A\n","3863it [2:24:45,  1.66s/it]\u001b[A\n","3864it [2:24:46,  1.65s/it]\u001b[A\n","3865it [2:24:48,  1.65s/it]\u001b[A\n","3866it [2:24:50,  1.65s/it]\u001b[A\n","3867it [2:24:51,  1.65s/it]\u001b[A\n","3868it [2:24:53,  1.65s/it]\u001b[A\n","3869it [2:24:55,  1.65s/it]\u001b[A\n","3870it [2:24:56,  1.66s/it]\u001b[A\n","3871it [2:24:58,  1.66s/it]\u001b[A\n","3872it [2:25:00,  1.65s/it]\u001b[A\n","3873it [2:25:01,  1.65s/it]\u001b[A\n","3874it [2:25:03,  1.65s/it]\u001b[A\n","3875it [2:25:05,  1.65s/it]\u001b[A\n","3876it [2:25:06,  1.65s/it]\u001b[A\n","3877it [2:25:08,  1.65s/it]\u001b[A\n","3878it [2:25:09,  1.65s/it]\u001b[A\n","3879it [2:25:11,  1.65s/it]\u001b[A\n","3880it [2:25:13,  1.64s/it]\u001b[A\n","3881it [2:25:14,  1.64s/it]\u001b[A\n","3882it [2:25:16,  1.64s/it]\u001b[A\n","3883it [2:25:18,  1.64s/it]\u001b[A\n","3884it [2:25:19,  1.64s/it]\u001b[A\n","3885it [2:25:21,  1.64s/it]\u001b[A\n","3886it [2:25:23,  1.64s/it]\u001b[A\n","3887it [2:25:24,  1.64s/it]\u001b[A\n","3888it [2:25:26,  1.64s/it]\u001b[A\n","3889it [2:25:28,  1.64s/it]\u001b[A\n","3890it [2:25:29,  1.64s/it]\u001b[A\n","3891it [2:25:31,  1.64s/it]\u001b[A\n","3892it [2:25:32,  1.63s/it]\u001b[A\n","3893it [2:25:34,  1.63s/it]\u001b[A\n","3894it [2:25:36,  1.64s/it]\u001b[A\n","3895it [2:25:37,  1.64s/it]\u001b[A\n","3896it [2:25:39,  1.64s/it]\u001b[A\n","3897it [2:25:41,  1.64s/it]\u001b[A\n","3898it [2:25:42,  1.64s/it]\u001b[A\n","3899it [2:25:44,  1.64s/it]\u001b[A\n","3900it [2:25:46,  1.64s/it]\u001b[A\n","3901it [2:25:47,  1.64s/it]\u001b[A\n","3902it [2:25:49,  1.64s/it]\u001b[A\n","3903it [2:25:50,  1.64s/it]\u001b[A\n","3904it [2:25:52,  1.64s/it]\u001b[A\n","3905it [2:25:54,  1.64s/it]\u001b[A\n","3906it [2:25:55,  1.64s/it]\u001b[A\n","3907it [2:25:57,  1.64s/it]\u001b[A\n","3908it [2:25:59,  1.63s/it]\u001b[A\n","3909it [2:26:00,  1.63s/it]\u001b[A\n","3910it [2:26:02,  1.63s/it]\u001b[A\n","3911it [2:26:04,  1.63s/it]\u001b[A\n","3912it [2:26:05,  1.63s/it]\u001b[A\n","3913it [2:26:07,  1.63s/it]\u001b[A\n","3914it [2:26:08,  1.64s/it]\u001b[A\n","3915it [2:26:10,  1.64s/it]\u001b[A\n","3916it [2:26:12,  1.64s/it]\u001b[A\n","3917it [2:26:13,  1.63s/it]\u001b[A\n","3918it [2:26:15,  1.63s/it]\u001b[A\n","3919it [2:26:17,  1.64s/it]\u001b[A\n","3920it [2:26:18,  1.64s/it]\u001b[A\n","3921it [2:26:20,  1.64s/it]\u001b[A\n","3922it [2:26:22,  1.64s/it]\u001b[A\n","3923it [2:26:23,  1.64s/it]\u001b[A\n","3924it [2:26:25,  1.64s/it]\u001b[A\n","3925it [2:26:26,  1.64s/it]\u001b[A\n","3926it [2:26:28,  1.64s/it]\u001b[A\n","3927it [2:26:30,  1.64s/it]\u001b[A\n","3928it [2:26:31,  1.64s/it]\u001b[A\n","3929it [2:26:33,  1.64s/it]\u001b[A\n","3930it [2:26:35,  1.63s/it]\u001b[A\n","3931it [2:26:36,  1.63s/it]\u001b[A\n","3932it [2:26:38,  1.63s/it]\u001b[A\n","3933it [2:26:40,  1.63s/it]\u001b[A\n","3934it [2:26:41,  1.63s/it]\u001b[A\n","3935it [2:26:43,  1.63s/it]\u001b[A\n","3936it [2:26:44,  1.63s/it]\u001b[A\n","3937it [2:26:46,  1.63s/it]\u001b[A\n","3938it [2:26:48,  1.64s/it]\u001b[A\n","3939it [2:26:49,  1.64s/it]\u001b[A\n","3940it [2:26:51,  1.64s/it]\u001b[A\n","3941it [2:26:53,  1.64s/it]\u001b[A\n","3942it [2:26:54,  1.64s/it]\u001b[A\n","3943it [2:26:56,  1.64s/it]\u001b[A\n","3944it [2:26:58,  1.64s/it]\u001b[A\n","3945it [2:26:59,  1.64s/it]\u001b[A\n","3946it [2:27:01,  1.64s/it]\u001b[A\n","3947it [2:27:02,  1.64s/it]\u001b[A\n","3948it [2:27:04,  1.64s/it]\u001b[A\n","3949it [2:27:06,  1.64s/it]\u001b[A\n","3950it [2:27:07,  1.65s/it]\u001b[A\n","3951it [2:27:09,  1.65s/it]\u001b[A\n","3952it [2:27:11,  1.65s/it]\u001b[A\n","3953it [2:27:12,  1.64s/it]\u001b[A\n","3954it [2:27:14,  1.65s/it]\u001b[A\n","3955it [2:27:16,  1.65s/it]\u001b[A\n","3956it [2:27:17,  1.65s/it]\u001b[A\n","3957it [2:27:19,  1.65s/it]\u001b[A\n","3958it [2:27:21,  1.65s/it]\u001b[A\n","3959it [2:27:22,  1.64s/it]\u001b[A\n","3960it [2:27:24,  1.65s/it]\u001b[A\n","3961it [2:27:26,  1.65s/it]\u001b[A\n","3962it [2:27:27,  1.65s/it]\u001b[A\n","3963it [2:27:29,  1.65s/it]\u001b[A\n","3964it [2:27:31,  1.65s/it]\u001b[A\n","3965it [2:27:32,  1.65s/it]\u001b[A\n","3966it [2:27:34,  1.65s/it]\u001b[A\n","3967it [2:27:35,  1.65s/it]\u001b[A\n","3968it [2:27:37,  1.65s/it]\u001b[A\n","3969it [2:27:39,  1.65s/it]\u001b[A\n","3970it [2:27:40,  1.65s/it]\u001b[A\n","3971it [2:27:42,  1.65s/it]\u001b[A\n","3972it [2:27:44,  1.65s/it]\u001b[A\n","3973it [2:27:45,  1.65s/it]\u001b[A\n","3974it [2:27:47,  1.65s/it]\u001b[A\n","3975it [2:27:49,  1.64s/it]\u001b[A\n","3976it [2:27:50,  1.64s/it]\u001b[A\n","3977it [2:27:52,  1.64s/it]\u001b[A\n","3978it [2:27:54,  1.64s/it]\u001b[A\n","3979it [2:27:55,  1.64s/it]\u001b[A\n","3980it [2:27:57,  1.65s/it]\u001b[A\n","3981it [2:27:59,  1.65s/it]\u001b[A\n","3982it [2:28:00,  1.65s/it]\u001b[A\n","3983it [2:28:02,  1.65s/it]\u001b[A\n","3984it [2:28:03,  1.65s/it]\u001b[A\n","3985it [2:28:05,  1.65s/it]\u001b[A\n","3986it [2:28:07,  1.65s/it]\u001b[A\n","3987it [2:28:08,  1.65s/it]\u001b[A\n","3988it [2:28:10,  1.65s/it]\u001b[A\n","3989it [2:28:12,  1.65s/it]\u001b[A\n","3990it [2:28:13,  1.65s/it]\u001b[A\n","3991it [2:28:15,  1.65s/it]\u001b[A\n","3992it [2:28:17,  1.65s/it]\u001b[A\n","3993it [2:28:18,  1.65s/it]\u001b[A\n","3994it [2:28:20,  1.65s/it]\u001b[A\n","3995it [2:28:22,  1.65s/it]\u001b[A\n","3996it [2:28:23,  1.65s/it]\u001b[A\n","3997it [2:28:25,  1.65s/it]\u001b[A\n","3998it [2:28:27,  1.65s/it]\u001b[A\n","3999it [2:28:28,  1.65s/it]\u001b[A\n","4000it [2:28:30,  1.65s/it]\u001b[A\n","4001it [2:28:32,  1.65s/it]\u001b[A\n","4002it [2:28:33,  1.65s/it]\u001b[A\n","4003it [2:28:35,  1.65s/it]\u001b[A\n","4004it [2:28:36,  1.64s/it]\u001b[A\n","4005it [2:28:38,  1.65s/it]\u001b[A\n","4006it [2:28:40,  1.64s/it]\u001b[A\n","4007it [2:28:41,  1.64s/it]\u001b[A\n","4008it [2:28:43,  1.64s/it]\u001b[A\n","4009it [2:28:45,  1.64s/it]\u001b[A\n","4010it [2:28:46,  1.65s/it]\u001b[A\n","4011it [2:28:48,  1.65s/it]\u001b[A\n","4012it [2:28:50,  1.65s/it]\u001b[A\n","4013it [2:28:51,  1.65s/it]\u001b[A\n","4014it [2:28:53,  1.65s/it]\u001b[A\n","4015it [2:28:55,  1.65s/it]\u001b[A\n","4016it [2:28:56,  1.65s/it]\u001b[A\n","4017it [2:28:58,  1.64s/it]\u001b[A\n","4018it [2:28:59,  1.64s/it]\u001b[A\n","4019it [2:29:01,  1.65s/it]\u001b[A\n","4020it [2:29:03,  1.65s/it]\u001b[A\n","4021it [2:29:04,  1.65s/it]\u001b[A\n","4022it [2:29:06,  1.65s/it]\u001b[A\n","4023it [2:29:08,  1.64s/it]\u001b[A\n","4024it [2:29:09,  1.65s/it]\u001b[A\n","4025it [2:29:11,  1.65s/it]\u001b[A\n","4026it [2:29:13,  1.65s/it]\u001b[A\n","4027it [2:29:14,  1.65s/it]\u001b[A\n","4028it [2:29:16,  1.64s/it]\u001b[A\n","4029it [2:29:18,  1.65s/it]\u001b[A\n","4030it [2:29:19,  1.65s/it]\u001b[A\n","4031it [2:29:21,  1.65s/it]\u001b[A\n","4032it [2:29:23,  1.65s/it]\u001b[A\n","4033it [2:29:24,  1.65s/it]\u001b[A\n","4034it [2:29:26,  1.65s/it]\u001b[A\n","4035it [2:29:28,  1.65s/it]\u001b[A\n","4036it [2:29:29,  1.65s/it]\u001b[A\n","4037it [2:29:31,  1.65s/it]\u001b[A\n","4038it [2:29:32,  1.65s/it]\u001b[A\n","4039it [2:29:34,  1.65s/it]\u001b[A\n","4040it [2:29:36,  1.65s/it]\u001b[A\n","4041it [2:29:37,  1.65s/it]\u001b[A\n","4042it [2:29:39,  1.65s/it]\u001b[A\n","4043it [2:29:41,  1.64s/it]\u001b[A\n","4044it [2:29:42,  1.64s/it]\u001b[A\n","4045it [2:29:44,  1.64s/it]\u001b[A\n","4046it [2:29:46,  1.65s/it]\u001b[A\n","4047it [2:29:47,  1.64s/it]\u001b[A\n","4048it [2:29:49,  1.64s/it]\u001b[A\n","4049it [2:29:51,  1.64s/it]\u001b[A\n","4050it [2:29:52,  1.64s/it]\u001b[A\n","4051it [2:29:54,  1.64s/it]\u001b[A\n","4052it [2:29:55,  1.64s/it]\u001b[A\n","4053it [2:29:57,  1.64s/it]\u001b[A\n","4054it [2:29:59,  1.64s/it]\u001b[A\n","4055it [2:30:00,  1.64s/it]\u001b[A\n","4056it [2:30:02,  1.64s/it]\u001b[A\n","4057it [2:30:04,  1.64s/it]\u001b[A\n","4058it [2:30:05,  1.64s/it]\u001b[A\n","4059it [2:30:07,  1.64s/it]\u001b[A\n","4060it [2:30:09,  1.64s/it]\u001b[A\n","4061it [2:30:10,  1.63s/it]\u001b[A\n","4062it [2:30:12,  1.64s/it]\u001b[A\n","4063it [2:30:13,  1.64s/it]\u001b[A\n","4064it [2:30:15,  1.64s/it]\u001b[A\n","4065it [2:30:17,  1.64s/it]\u001b[A\n","4066it [2:30:18,  1.64s/it]\u001b[A\n","4067it [2:30:20,  1.63s/it]\u001b[A\n","4068it [2:30:22,  1.63s/it]\u001b[A\n","4069it [2:30:23,  1.64s/it]\u001b[A\n","4070it [2:30:25,  1.64s/it]\u001b[A\n","4071it [2:30:27,  1.64s/it]\u001b[A\n","4072it [2:30:28,  1.64s/it]\u001b[A\n","4073it [2:30:30,  1.64s/it]\u001b[A\n","4074it [2:30:31,  1.64s/it]\u001b[A\n","4075it [2:30:33,  1.64s/it]\u001b[A\n","4076it [2:30:35,  1.64s/it]\u001b[A\n","4077it [2:30:36,  1.64s/it]\u001b[A\n","4078it [2:30:38,  1.64s/it]\u001b[A\n","4079it [2:30:40,  1.64s/it]\u001b[A\n","4080it [2:30:41,  1.64s/it]\u001b[A\n","4081it [2:30:43,  1.64s/it]\u001b[A\n","4082it [2:30:45,  1.63s/it]\u001b[A\n","4083it [2:30:46,  1.63s/it]\u001b[A\n","4084it [2:30:48,  1.63s/it]\u001b[A\n","4085it [2:30:49,  1.63s/it]\u001b[A\n","4086it [2:30:51,  1.63s/it]\u001b[A\n","4087it [2:30:53,  1.63s/it]\u001b[A\n","4088it [2:30:54,  1.64s/it]\u001b[A\n","4089it [2:30:56,  1.64s/it]\u001b[A\n","4090it [2:30:58,  1.63s/it]\u001b[A\n","4091it [2:30:59,  1.63s/it]\u001b[A\n","4092it [2:31:01,  1.63s/it]\u001b[A\n","4093it [2:31:03,  1.63s/it]\u001b[A\n","4094it [2:31:04,  1.63s/it]\u001b[A\n","4095it [2:31:06,  1.63s/it]\u001b[A\n","4096it [2:31:07,  1.64s/it]\u001b[A\n","4097it [2:31:09,  1.64s/it]\u001b[A\n","4098it [2:31:11,  1.63s/it]\u001b[A\n","4099it [2:31:12,  1.63s/it]\u001b[A\n","4100it [2:31:14,  1.64s/it]\u001b[A\n","4101it [2:31:16,  1.64s/it]\u001b[A\n","4102it [2:31:17,  1.63s/it]\u001b[A\n","4103it [2:31:19,  1.64s/it]\u001b[A\n","4104it [2:31:20,  1.63s/it]\u001b[A\n","4105it [2:31:22,  1.63s/it]\u001b[A\n","4106it [2:31:24,  1.63s/it]\u001b[A\n","4107it [2:31:25,  1.63s/it]\u001b[A\n","4108it [2:31:27,  1.63s/it]\u001b[A\n","4109it [2:31:29,  1.63s/it]\u001b[A\n","4110it [2:31:30,  1.63s/it]\u001b[A\n","4111it [2:31:32,  1.63s/it]\u001b[A\n","4112it [2:31:34,  1.63s/it]\u001b[A\n","4113it [2:31:35,  1.64s/it]\u001b[A\n","4114it [2:31:37,  1.64s/it]\u001b[A\n","4115it [2:31:38,  1.64s/it]\u001b[A\n","4116it [2:31:40,  1.64s/it]\u001b[A\n","4117it [2:31:42,  1.64s/it]\u001b[A\n","4118it [2:31:43,  1.64s/it]\u001b[A\n","4119it [2:31:45,  1.64s/it]\u001b[A\n","4120it [2:31:47,  1.64s/it]\u001b[A\n","4121it [2:31:48,  1.64s/it]\u001b[A\n","4122it [2:31:50,  1.64s/it]\u001b[A\n","4123it [2:31:52,  1.64s/it]\u001b[A\n","4124it [2:31:53,  1.65s/it]\u001b[A\n","4125it [2:31:55,  1.65s/it]\u001b[A\n","4126it [2:31:57,  1.65s/it]\u001b[A\n","4127it [2:31:58,  1.65s/it]\u001b[A\n","4128it [2:32:00,  1.65s/it]\u001b[A\n","4129it [2:32:02,  1.65s/it]\u001b[A\n","4130it [2:32:03,  1.65s/it]\u001b[A\n","4131it [2:32:05,  1.65s/it]\u001b[A\n","4132it [2:32:06,  1.65s/it]\u001b[A\n","4133it [2:32:08,  1.65s/it]\u001b[A\n","4134it [2:32:10,  1.65s/it]\u001b[A\n","4135it [2:32:11,  1.65s/it]\u001b[A\n","4136it [2:32:13,  1.65s/it]\u001b[A\n","4137it [2:32:15,  1.64s/it]\u001b[A\n","4138it [2:32:16,  1.64s/it]\u001b[A\n","4139it [2:32:18,  1.64s/it]\u001b[A\n","4140it [2:32:20,  1.64s/it]\u001b[A\n","4141it [2:32:21,  1.64s/it]\u001b[A\n","4142it [2:32:23,  1.65s/it]\u001b[A\n","4143it [2:32:25,  1.65s/it]\u001b[A\n","4144it [2:32:26,  1.65s/it]\u001b[A\n","4145it [2:32:28,  1.65s/it]\u001b[A\n","4146it [2:32:30,  1.65s/it]\u001b[A\n","4147it [2:32:31,  1.65s/it]\u001b[A\n","4148it [2:32:33,  1.65s/it]\u001b[A\n","4149it [2:32:34,  1.65s/it]\u001b[A\n","4150it [2:32:36,  1.65s/it]\u001b[A\n","4151it [2:32:38,  1.65s/it]\u001b[A\n","4152it [2:32:39,  1.65s/it]\u001b[A\n","4153it [2:32:41,  1.66s/it]\u001b[A\n","4154it [2:32:43,  1.65s/it]\u001b[A\n","4155it [2:32:44,  1.65s/it]\u001b[A\n","4156it [2:32:46,  1.65s/it]\u001b[A\n","4157it [2:32:48,  1.65s/it]\u001b[A\n","4158it [2:32:49,  1.65s/it]\u001b[A\n","4159it [2:32:51,  1.65s/it]\u001b[A\n","4160it [2:32:53,  1.65s/it]\u001b[A\n","4161it [2:32:54,  1.65s/it]\u001b[A\n","4162it [2:32:56,  1.65s/it]\u001b[A\n","4163it [2:32:58,  1.65s/it]\u001b[A\n","4164it [2:32:59,  1.64s/it]\u001b[A\n","4165it [2:33:01,  1.64s/it]\u001b[A\n","4166it [2:33:03,  1.65s/it]\u001b[A\n","4167it [2:33:04,  1.65s/it]\u001b[A\n","4168it [2:33:06,  1.65s/it]\u001b[A\n","4169it [2:33:08,  1.65s/it]\u001b[A\n","4170it [2:33:09,  1.65s/it]\u001b[A\n","4171it [2:33:11,  1.65s/it]\u001b[A\n","4172it [2:33:12,  1.65s/it]\u001b[A\n","4173it [2:33:14,  1.65s/it]\u001b[A\n","4174it [2:33:16,  1.65s/it]\u001b[A\n","4175it [2:33:17,  1.65s/it]\u001b[A\n","4176it [2:33:19,  1.64s/it]\u001b[A\n","4177it [2:33:21,  1.65s/it]\u001b[A\n","4178it [2:33:22,  1.65s/it]\u001b[A\n","4179it [2:33:24,  1.65s/it]\u001b[A\n","4180it [2:33:26,  1.65s/it]\u001b[A\n","4181it [2:33:27,  1.65s/it]\u001b[A\n","4182it [2:33:29,  1.65s/it]\u001b[A\n","4183it [2:33:31,  1.65s/it]\u001b[A\n","4184it [2:33:32,  1.65s/it]\u001b[A\n","4185it [2:33:34,  1.65s/it]\u001b[A\n","4186it [2:33:36,  1.65s/it]\u001b[A\n","4187it [2:33:37,  1.65s/it]\u001b[A\n","4188it [2:33:39,  1.65s/it]\u001b[A\n","4189it [2:33:40,  1.65s/it]\u001b[A\n","4190it [2:33:42,  1.65s/it]\u001b[A\n","4191it [2:33:44,  1.65s/it]\u001b[A\n","4192it [2:33:45,  1.65s/it]\u001b[A\n","4193it [2:33:47,  1.65s/it]\u001b[A\n","4194it [2:33:49,  1.65s/it]\u001b[A\n","4195it [2:33:50,  1.65s/it]\u001b[A\n","4196it [2:33:52,  1.65s/it]\u001b[A\n","4197it [2:33:54,  1.65s/it]\u001b[A\n","4198it [2:33:55,  1.65s/it]\u001b[A\n","4199it [2:33:57,  1.65s/it]\u001b[A\n","4200it [2:33:59,  1.65s/it]\u001b[A\n","4201it [2:34:00,  1.65s/it]\u001b[A\n","4202it [2:34:02,  1.65s/it]\u001b[A\n","4203it [2:34:04,  1.65s/it]\u001b[A\n","4204it [2:34:05,  1.65s/it]\u001b[A\n","4205it [2:34:07,  1.65s/it]\u001b[A\n","4206it [2:34:08,  1.65s/it]\u001b[A\n","4207it [2:34:10,  1.65s/it]\u001b[A\n","4208it [2:34:12,  1.65s/it]\u001b[A\n","4209it [2:34:13,  1.65s/it]\u001b[A\n","4210it [2:34:15,  1.65s/it]\u001b[A\n","4211it [2:34:17,  1.65s/it]\u001b[A\n","4212it [2:34:18,  1.64s/it]\u001b[A\n","4213it [2:34:20,  1.65s/it]\u001b[A\n","4214it [2:34:22,  1.65s/it]\u001b[A\n","4215it [2:34:23,  1.65s/it]\u001b[A\n","4216it [2:34:25,  1.64s/it]\u001b[A\n","4217it [2:34:27,  1.64s/it]\u001b[A\n","4218it [2:34:28,  1.64s/it]\u001b[A\n","4219it [2:34:30,  1.64s/it]\u001b[A\n","4220it [2:34:31,  1.64s/it]\u001b[A\n","4221it [2:34:33,  1.64s/it]\u001b[A\n","4222it [2:34:35,  1.64s/it]\u001b[A\n","4223it [2:34:36,  1.64s/it]\u001b[A\n","4224it [2:34:38,  1.63s/it]\u001b[A\n","4225it [2:34:40,  1.63s/it]\u001b[A\n","4226it [2:34:41,  1.63s/it]\u001b[A\n","4227it [2:34:43,  1.64s/it]\u001b[A\n","4228it [2:34:45,  1.64s/it]\u001b[A\n","4229it [2:34:46,  1.63s/it]\u001b[A\n","4230it [2:34:48,  1.63s/it]\u001b[A\n","4231it [2:34:49,  1.63s/it]\u001b[A\n","4232it [2:34:51,  1.64s/it]\u001b[A\n","4233it [2:34:53,  1.63s/it]\u001b[A\n","4234it [2:34:54,  1.63s/it]\u001b[A\n","4235it [2:34:56,  1.63s/it]\u001b[A\n","4236it [2:34:58,  1.64s/it]\u001b[A\n","4237it [2:34:59,  1.64s/it]\u001b[A\n","4238it [2:35:01,  1.64s/it]\u001b[A\n","4239it [2:35:03,  1.64s/it]\u001b[A\n","4240it [2:35:04,  1.64s/it]\u001b[A\n","4241it [2:35:06,  1.64s/it]\u001b[A\n","4242it [2:35:07,  1.64s/it]\u001b[A\n","4243it [2:35:09,  1.64s/it]\u001b[A\n","4244it [2:35:11,  1.64s/it]\u001b[A\n","4245it [2:35:12,  1.64s/it]\u001b[A\n","4246it [2:35:14,  1.64s/it]\u001b[A\n","4247it [2:35:16,  1.63s/it]\u001b[A\n","4248it [2:35:17,  1.63s/it]\u001b[A\n","4249it [2:35:19,  1.63s/it]\u001b[A\n","4250it [2:35:21,  1.63s/it]\u001b[A\n","4251it [2:35:22,  1.63s/it]\u001b[A\n","4252it [2:35:24,  1.63s/it]\u001b[A\n","4253it [2:35:25,  1.63s/it]\u001b[A\n","4254it [2:35:27,  1.63s/it]\u001b[A\n","4255it [2:35:29,  1.64s/it]\u001b[A\n","4256it [2:35:30,  1.64s/it]\u001b[A\n","4257it [2:35:32,  1.64s/it]\u001b[A\n","4258it [2:35:34,  1.64s/it]\u001b[A\n","4259it [2:35:35,  1.63s/it]\u001b[A\n","4260it [2:35:37,  1.63s/it]\u001b[A\n","4261it [2:35:39,  1.63s/it]\u001b[A\n","4262it [2:35:40,  1.63s/it]\u001b[A\n","4263it [2:35:42,  1.63s/it]\u001b[A\n","4264it [2:35:43,  1.63s/it]\u001b[A\n","4265it [2:35:45,  1.63s/it]\u001b[A\n","4266it [2:35:47,  1.63s/it]\u001b[A\n","4267it [2:35:48,  1.63s/it]\u001b[A\n","4268it [2:35:50,  1.63s/it]\u001b[A\n","4269it [2:35:52,  1.63s/it]\u001b[A\n","4270it [2:35:53,  1.63s/it]\u001b[A\n","4271it [2:35:55,  1.63s/it]\u001b[A\n","4272it [2:35:56,  1.63s/it]\u001b[A\n","4273it [2:35:58,  1.63s/it]\u001b[A\n","4274it [2:36:00,  1.63s/it]\u001b[A\n","4275it [2:36:01,  1.63s/it]\u001b[A\n","4276it [2:36:03,  1.63s/it]\u001b[A\n","4277it [2:36:05,  1.63s/it]\u001b[A\n","4278it [2:36:06,  1.63s/it]\u001b[A\n","4279it [2:36:08,  1.63s/it]\u001b[A\n","4280it [2:36:09,  1.63s/it]\u001b[A\n","4281it [2:36:11,  1.63s/it]\u001b[A\n","4282it [2:36:13,  1.63s/it]\u001b[A\n","4283it [2:36:14,  1.63s/it]\u001b[A\n","4284it [2:36:16,  1.64s/it]\u001b[A\n","4285it [2:36:18,  1.64s/it]\u001b[A\n","4286it [2:36:19,  1.64s/it]\u001b[A\n","4287it [2:36:21,  1.64s/it]\u001b[A\n","4288it [2:36:23,  1.64s/it]\u001b[A\n","4289it [2:36:24,  1.65s/it]\u001b[A\n","4290it [2:36:26,  1.64s/it]\u001b[A\n","4291it [2:36:28,  1.64s/it]\u001b[A\n","4292it [2:36:29,  1.65s/it]\u001b[A\n","4293it [2:36:31,  1.65s/it]\u001b[A\n","4294it [2:36:32,  1.65s/it]\u001b[A\n","4295it [2:36:34,  1.65s/it]\u001b[A\n","4296it [2:36:36,  1.65s/it]\u001b[A\n","4297it [2:36:37,  1.65s/it]\u001b[A\n","4298it [2:36:39,  1.65s/it]\u001b[A\n","4299it [2:36:41,  1.65s/it]\u001b[A\n","4300it [2:36:42,  1.65s/it]\u001b[A\n","4301it [2:36:44,  1.65s/it]\u001b[A\n","4302it [2:36:46,  1.65s/it]\u001b[A\n","4303it [2:36:47,  1.65s/it]\u001b[A\n","4304it [2:36:49,  1.65s/it]\u001b[A\n","4305it [2:36:51,  1.64s/it]\u001b[A\n","4306it [2:36:52,  1.65s/it]\u001b[A\n","4307it [2:36:54,  1.64s/it]\u001b[A\n","4308it [2:36:56,  1.64s/it]\u001b[A\n","4309it [2:36:57,  1.64s/it]\u001b[A\n","4310it [2:36:59,  1.64s/it]\u001b[A\n","4311it [2:37:00,  1.64s/it]\u001b[A\n","4312it [2:37:02,  1.65s/it]\u001b[A\n","4313it [2:37:04,  1.65s/it]\u001b[A\n","4314it [2:37:05,  1.64s/it]\u001b[A\n","4315it [2:37:07,  1.64s/it]\u001b[A\n","4316it [2:37:09,  1.64s/it]\u001b[A\n","4317it [2:37:10,  1.65s/it]\u001b[A\n","4318it [2:37:12,  1.65s/it]\u001b[A\n","4319it [2:37:14,  1.65s/it]\u001b[A\n","4320it [2:37:15,  1.65s/it]\u001b[A\n","4321it [2:37:17,  1.65s/it]\u001b[A\n","4322it [2:37:19,  1.65s/it]\u001b[A\n","4323it [2:37:20,  1.65s/it]\u001b[A\n","4324it [2:37:22,  1.65s/it]\u001b[A\n","4325it [2:37:24,  1.65s/it]\u001b[A\n","4326it [2:37:25,  1.65s/it]\u001b[A\n","4327it [2:37:27,  1.65s/it]\u001b[A\n","4328it [2:37:28,  1.65s/it]\u001b[A\n","4329it [2:37:30,  1.64s/it]\u001b[A\n","4330it [2:37:32,  1.64s/it]\u001b[A\n","4331it [2:37:33,  1.64s/it]\u001b[A\n","4332it [2:37:35,  1.64s/it]\u001b[A\n","4333it [2:37:37,  1.64s/it]\u001b[A\n","4334it [2:37:38,  1.64s/it]\u001b[A\n","4335it [2:37:40,  1.64s/it]\u001b[A\n","4336it [2:37:42,  1.65s/it]\u001b[A\n","4337it [2:37:43,  1.65s/it]\u001b[A\n","4338it [2:37:45,  1.65s/it]\u001b[A\n","4339it [2:37:47,  1.65s/it]\u001b[A\n","4340it [2:37:48,  1.65s/it]\u001b[A\n","4341it [2:37:50,  1.64s/it]\u001b[A\n","4342it [2:37:52,  1.64s/it]\u001b[A\n","4343it [2:37:53,  1.65s/it]\u001b[A\n","4344it [2:37:55,  1.64s/it]\u001b[A\n","4345it [2:37:56,  1.64s/it]\u001b[A\n","4346it [2:37:58,  1.64s/it]\u001b[A\n","4347it [2:38:00,  1.64s/it]\u001b[A\n","4348it [2:38:01,  1.64s/it]\u001b[A\n","4349it [2:38:03,  1.64s/it]\u001b[A\n","4350it [2:38:05,  1.65s/it]\u001b[A\n","4351it [2:38:06,  1.65s/it]\u001b[A\n","4352it [2:38:08,  1.65s/it]\u001b[A\n","4353it [2:38:10,  1.65s/it]\u001b[A\n","4354it [2:38:11,  1.65s/it]\u001b[A\n","4355it [2:38:13,  1.65s/it]\u001b[A\n","4356it [2:38:15,  1.65s/it]\u001b[A\n","4357it [2:38:16,  1.65s/it]\u001b[A\n","4358it [2:38:18,  1.64s/it]\u001b[A\n","4359it [2:38:19,  1.64s/it]\u001b[A\n","4360it [2:38:21,  1.64s/it]\u001b[A\n","4361it [2:38:23,  1.64s/it]\u001b[A\n","4362it [2:38:24,  1.64s/it]\u001b[A\n","4363it [2:38:26,  1.64s/it]\u001b[A\n","4364it [2:38:28,  1.64s/it]\u001b[A\n","4365it [2:38:29,  1.64s/it]\u001b[A\n","4366it [2:38:31,  1.64s/it]\u001b[A\n","4367it [2:38:33,  1.64s/it]\u001b[A\n","4368it [2:38:34,  1.64s/it]\u001b[A\n","4369it [2:38:36,  1.65s/it]\u001b[A\n","4370it [2:38:38,  1.65s/it]\u001b[A\n","4371it [2:38:39,  1.65s/it]\u001b[A\n","4372it [2:38:41,  1.65s/it]\u001b[A\n","4373it [2:38:43,  1.65s/it]\u001b[A\n","4374it [2:38:44,  1.65s/it]\u001b[A\n","4375it [2:38:46,  1.64s/it]\u001b[A\n","4376it [2:38:47,  1.65s/it]\u001b[A\n","4377it [2:38:49,  1.65s/it]\u001b[A\n","4378it [2:38:51,  1.64s/it]\u001b[A\n","4379it [2:38:52,  1.64s/it]\u001b[A\n","4380it [2:38:54,  1.64s/it]\u001b[A\n","4381it [2:38:56,  1.64s/it]\u001b[A\n","4382it [2:38:57,  1.64s/it]\u001b[A\n","4383it [2:38:59,  1.64s/it]\u001b[A\n","4384it [2:39:01,  1.64s/it]\u001b[A\n","4385it [2:39:02,  1.64s/it]\u001b[A\n","4386it [2:39:04,  1.64s/it]\u001b[A\n","4387it [2:39:05,  1.64s/it]\u001b[A\n","4388it [2:39:07,  1.64s/it]\u001b[A\n","4389it [2:39:09,  1.64s/it]\u001b[A\n","4390it [2:39:10,  1.64s/it]\u001b[A\n","4391it [2:39:12,  1.64s/it]\u001b[A\n","4392it [2:39:14,  1.64s/it]\u001b[A\n","4393it [2:39:15,  1.64s/it]\u001b[A\n","4394it [2:39:17,  1.64s/it]\u001b[A\n","4395it [2:39:19,  1.64s/it]\u001b[A\n","4396it [2:39:20,  1.64s/it]\u001b[A\n","4397it [2:39:22,  1.64s/it]\u001b[A\n","4398it [2:39:24,  1.64s/it]\u001b[A\n","4399it [2:39:25,  1.63s/it]\u001b[A\n","4400it [2:39:27,  1.63s/it]\u001b[A\n","4401it [2:39:28,  1.63s/it]\u001b[A\n","4402it [2:39:30,  1.63s/it]\u001b[A\n","4403it [2:39:32,  1.63s/it]\u001b[A\n","4404it [2:39:33,  1.63s/it]\u001b[A\n","4405it [2:39:35,  1.63s/it]\u001b[A\n","4406it [2:39:37,  1.64s/it]\u001b[A\n","4407it [2:39:38,  1.64s/it]\u001b[A\n","4408it [2:39:40,  1.63s/it]\u001b[A\n","4409it [2:39:41,  1.64s/it]\u001b[A\n","4410it [2:39:43,  1.64s/it]\u001b[A\n","4411it [2:39:45,  1.64s/it]\u001b[A\n","4412it [2:39:46,  1.64s/it]\u001b[A\n","4413it [2:39:48,  1.63s/it]\u001b[A\n","4414it [2:39:50,  1.64s/it]\u001b[A\n","4415it [2:39:51,  1.64s/it]\u001b[A\n","4416it [2:39:53,  1.64s/it]\u001b[A\n","4417it [2:39:55,  1.64s/it]\u001b[A\n","4418it [2:39:56,  1.63s/it]\u001b[A\n","4419it [2:39:58,  1.63s/it]\u001b[A\n","4420it [2:39:59,  1.63s/it]\u001b[A\n","4421it [2:40:01,  1.63s/it]\u001b[A\n","4422it [2:40:03,  1.63s/it]\u001b[A\n","4423it [2:40:04,  1.63s/it]\u001b[A\n","4424it [2:40:06,  1.63s/it]\u001b[A\n","4425it [2:40:08,  1.63s/it]\u001b[A\n","4426it [2:40:09,  1.63s/it]\u001b[A\n","4427it [2:40:11,  1.63s/it]\u001b[A\n","4428it [2:40:13,  1.63s/it]\u001b[A\n","4429it [2:40:14,  1.63s/it]\u001b[A\n","4430it [2:40:16,  1.63s/it]\u001b[A\n","4431it [2:40:17,  1.63s/it]\u001b[A\n","4432it [2:40:19,  1.63s/it]\u001b[A\n","4433it [2:40:21,  1.63s/it]\u001b[A\n","4434it [2:40:22,  1.63s/it]\u001b[A\n","4435it [2:40:24,  1.63s/it]\u001b[A\n","4436it [2:40:26,  1.63s/it]\u001b[A\n","4437it [2:40:27,  1.63s/it]\u001b[A\n","4438it [2:40:29,  1.63s/it]\u001b[A\n","4439it [2:40:30,  1.63s/it]\u001b[A\n","4440it [2:40:32,  1.63s/it]\u001b[A\n","4441it [2:40:34,  1.63s/it]\u001b[A\n","4442it [2:40:35,  1.63s/it]\u001b[A\n","4443it [2:40:37,  1.63s/it]\u001b[A\n","4444it [2:40:39,  1.63s/it]\u001b[A\n","4445it [2:40:40,  1.63s/it]\u001b[A\n","4446it [2:40:42,  1.64s/it]\u001b[A\n","4447it [2:40:44,  1.63s/it]\u001b[A\n","4448it [2:40:45,  1.63s/it]\u001b[A\n","4449it [2:40:47,  1.63s/it]\u001b[A\n","4450it [2:40:48,  1.64s/it]\u001b[A\n","4451it [2:40:50,  1.64s/it]\u001b[A\n","4452it [2:40:52,  1.64s/it]\u001b[A\n","4453it [2:40:53,  1.64s/it]\u001b[A\n","4454it [2:40:55,  1.64s/it]\u001b[A\n","4455it [2:40:57,  1.64s/it]\u001b[A\n","4456it [2:40:58,  1.64s/it]\u001b[A\n","4457it [2:41:00,  1.64s/it]\u001b[A\n","4458it [2:41:02,  1.64s/it]\u001b[A\n","4459it [2:41:03,  1.64s/it]\u001b[A\n","4460it [2:41:05,  1.65s/it]\u001b[A\n","4461it [2:41:07,  1.65s/it]\u001b[A\n","4462it [2:41:08,  1.65s/it]\u001b[A\n","4463it [2:41:10,  1.65s/it]\u001b[A\n","4464it [2:41:11,  1.65s/it]\u001b[A\n","4465it [2:41:13,  1.65s/it]\u001b[A\n","4466it [2:41:15,  1.65s/it]\u001b[A\n","4467it [2:41:16,  1.66s/it]\u001b[A\n","4468it [2:41:18,  1.65s/it]\u001b[A\n","4469it [2:41:20,  1.65s/it]\u001b[A\n","4470it [2:41:21,  1.65s/it]\u001b[A\n","4471it [2:41:23,  1.65s/it]\u001b[A\n","4472it [2:41:25,  1.65s/it]\u001b[A\n","4473it [2:41:26,  1.65s/it]\u001b[A\n","4474it [2:41:28,  1.65s/it]\u001b[A\n","4475it [2:41:30,  1.65s/it]\u001b[A\n","4476it [2:41:31,  1.65s/it]\u001b[A\n","4477it [2:41:33,  1.64s/it]\u001b[A\n","4478it [2:41:35,  1.65s/it]\u001b[A\n","4479it [2:41:36,  1.65s/it]\u001b[A\n","4480it [2:41:38,  1.65s/it]\u001b[A\n","4481it [2:41:39,  1.65s/it]\u001b[A\n","4482it [2:41:41,  1.65s/it]\u001b[A\n","4483it [2:41:43,  1.64s/it]\u001b[A\n","4484it [2:41:44,  1.65s/it]\u001b[A\n","4485it [2:41:46,  1.65s/it]\u001b[A\n","4486it [2:41:48,  1.65s/it]\u001b[A\n","4487it [2:41:49,  1.65s/it]\u001b[A\n","4488it [2:41:51,  1.65s/it]\u001b[A\n","4489it [2:41:53,  1.65s/it]\u001b[A\n","4490it [2:41:54,  1.65s/it]\u001b[A\n","4491it [2:41:56,  1.65s/it]\u001b[A\n","4492it [2:41:58,  1.65s/it]\u001b[A\n","4493it [2:41:59,  1.65s/it]\u001b[A\n","4494it [2:42:01,  1.65s/it]\u001b[A\n","4495it [2:42:03,  1.65s/it]\u001b[A\n","4496it [2:42:04,  1.65s/it]\u001b[A\n","4497it [2:42:06,  1.64s/it]\u001b[A\n","4498it [2:42:07,  1.64s/it]\u001b[A\n","4499it [2:42:09,  1.65s/it]\u001b[A\n","4500it [2:42:11,  1.64s/it]\u001b[A\n","4501it [2:42:12,  1.64s/it]\u001b[A\n","4502it [2:42:14,  1.65s/it]\u001b[A\n","4503it [2:42:16,  1.65s/it]\u001b[A\n","4504it [2:42:17,  1.65s/it]\u001b[A\n","4505it [2:42:19,  1.65s/it]\u001b[A\n","4506it [2:42:21,  1.64s/it]\u001b[A\n","4507it [2:42:22,  1.65s/it]\u001b[A\n","4508it [2:42:24,  1.65s/it]\u001b[A\n","4509it [2:42:26,  1.65s/it]\u001b[A\n","4510it [2:42:27,  1.64s/it]\u001b[A\n","4511it [2:42:29,  1.64s/it]\u001b[A\n","4512it [2:42:31,  1.64s/it]\u001b[A\n","4513it [2:42:32,  1.64s/it]\u001b[A\n","4514it [2:42:34,  1.64s/it]\u001b[A\n","4515it [2:42:35,  1.65s/it]\u001b[A\n","4516it [2:42:37,  1.65s/it]\u001b[A\n","4517it [2:42:39,  1.65s/it]\u001b[A\n","4518it [2:42:40,  1.65s/it]\u001b[A\n","4519it [2:42:42,  1.65s/it]\u001b[A\n","4520it [2:42:44,  1.65s/it]\u001b[A\n","4521it [2:42:45,  1.65s/it]\u001b[A\n","4522it [2:42:47,  1.65s/it]\u001b[A\n","4523it [2:42:49,  1.65s/it]\u001b[A\n","4524it [2:42:50,  1.65s/it]\u001b[A\n","4525it [2:42:52,  1.65s/it]\u001b[A\n","4526it [2:42:54,  1.65s/it]\u001b[A\n","4527it [2:42:55,  1.65s/it]\u001b[A\n","4528it [2:42:57,  1.65s/it]\u001b[A\n","4529it [2:42:59,  1.65s/it]\u001b[A\n","4530it [2:43:00,  1.65s/it]\u001b[A\n","4531it [2:43:02,  1.65s/it]\u001b[A\n","4532it [2:43:04,  1.65s/it]\u001b[A\n","4533it [2:43:05,  1.65s/it]\u001b[A\n","4534it [2:43:07,  1.65s/it]\u001b[A\n","4535it [2:43:08,  1.65s/it]\u001b[A\n","4536it [2:43:10,  1.65s/it]\u001b[A\n","4537it [2:43:12,  1.65s/it]\u001b[A\n","4538it [2:43:13,  1.65s/it]\u001b[A\n","4539it [2:43:15,  1.65s/it]\u001b[A\n","4540it [2:43:17,  1.65s/it]\u001b[A\n","4541it [2:43:18,  1.65s/it]\u001b[A\n","4542it [2:43:20,  1.65s/it]\u001b[A\n","4543it [2:43:22,  1.65s/it]\u001b[A\n","4544it [2:43:23,  1.65s/it]\u001b[A\n","4545it [2:43:25,  1.65s/it]\u001b[A\n","4546it [2:43:27,  1.65s/it]\u001b[A\n","4547it [2:43:28,  1.65s/it]\u001b[A\n","4548it [2:43:30,  1.64s/it]\u001b[A\n","4549it [2:43:32,  1.65s/it]\u001b[A\n","4550it [2:43:33,  1.64s/it]\u001b[A\n","4551it [2:43:35,  1.64s/it]\u001b[A\n","4552it [2:43:36,  1.64s/it]\u001b[A\n","4553it [2:43:38,  1.64s/it]\u001b[A\n","4554it [2:43:40,  1.64s/it]\u001b[A\n","4555it [2:43:41,  1.64s/it]\u001b[A\n","4556it [2:43:43,  1.64s/it]\u001b[A\n","4557it [2:43:45,  1.63s/it]\u001b[A\n","4558it [2:43:46,  1.64s/it]\u001b[A\n","4559it [2:43:48,  1.64s/it]\u001b[A\n","4560it [2:43:50,  1.64s/it]\u001b[A\n","4561it [2:43:51,  1.64s/it]\u001b[A\n","4562it [2:43:53,  1.64s/it]\u001b[A\n","4563it [2:43:54,  1.64s/it]\u001b[A\n","4564it [2:43:56,  1.64s/it]\u001b[A\n","4565it [2:43:58,  1.64s/it]\u001b[A\n","4566it [2:43:59,  1.64s/it]\u001b[A\n","4567it [2:44:01,  1.64s/it]\u001b[A\n","4568it [2:44:03,  1.64s/it]\u001b[A\n","4569it [2:44:04,  1.64s/it]\u001b[A\n","4570it [2:44:06,  1.64s/it]\u001b[A\n","4571it [2:44:08,  1.64s/it]\u001b[A\n","4572it [2:44:09,  1.64s/it]\u001b[A\n","4573it [2:44:11,  1.64s/it]\u001b[A\n","4574it [2:44:13,  1.64s/it]\u001b[A\n","4575it [2:44:14,  1.64s/it]\u001b[A\n","4576it [2:44:16,  1.65s/it]\u001b[A\n","4577it [2:44:17,  1.64s/it]\u001b[A\n","4578it [2:44:19,  1.64s/it]\u001b[A\n","4579it [2:44:21,  1.64s/it]\u001b[A\n","4580it [2:44:22,  1.64s/it]\u001b[A\n","4581it [2:44:24,  1.64s/it]\u001b[A\n","4582it [2:44:26,  1.64s/it]\u001b[A\n","4583it [2:44:27,  1.64s/it]\u001b[A\n","4584it [2:44:29,  1.64s/it]\u001b[A\n","4585it [2:44:31,  1.64s/it]\u001b[A\n","4586it [2:44:32,  1.64s/it]\u001b[A\n","4587it [2:44:34,  1.64s/it]\u001b[A\n","4588it [2:44:35,  1.63s/it]\u001b[A\n","4589it [2:44:37,  1.63s/it]\u001b[A\n","4590it [2:44:39,  1.63s/it]\u001b[A\n","4591it [2:44:40,  1.63s/it]\u001b[A\n","4592it [2:44:42,  1.63s/it]\u001b[A\n","4593it [2:44:44,  1.64s/it]\u001b[A\n","4594it [2:44:45,  1.64s/it]\u001b[A\n","4595it [2:44:47,  1.64s/it]\u001b[A\n","4596it [2:44:49,  1.64s/it]\u001b[A\n","4597it [2:44:50,  1.64s/it]\u001b[A\n","4598it [2:44:52,  1.64s/it]\u001b[A\n","4599it [2:44:53,  1.64s/it]\u001b[A\n","4600it [2:44:55,  1.64s/it]\u001b[A\n","4601it [2:44:57,  1.64s/it]\u001b[A\n","4602it [2:44:58,  1.64s/it]\u001b[A\n","4603it [2:45:00,  1.64s/it]\u001b[A\n","4604it [2:45:02,  1.64s/it]\u001b[A\n","4605it [2:45:03,  1.64s/it]\u001b[A\n","4606it [2:45:05,  1.64s/it]\u001b[A\n","4607it [2:45:07,  1.64s/it]\u001b[A\n","4608it [2:45:08,  1.64s/it]\u001b[A\n","4609it [2:45:10,  1.64s/it]\u001b[A\n","4610it [2:45:11,  1.64s/it]\u001b[A\n","4611it [2:45:13,  1.63s/it]\u001b[A\n","4612it [2:45:15,  1.64s/it]\u001b[A\n","4613it [2:45:16,  1.64s/it]\u001b[A\n","4614it [2:45:18,  1.64s/it]\u001b[A\n","4615it [2:45:20,  1.64s/it]\u001b[A\n","4616it [2:45:21,  1.64s/it]\u001b[A\n","4617it [2:45:23,  1.63s/it]\u001b[A\n","4618it [2:45:25,  1.64s/it]\u001b[A\n","4619it [2:45:26,  1.64s/it]\u001b[A\n","4620it [2:45:28,  1.64s/it]\u001b[A\n","4621it [2:45:29,  1.65s/it]\u001b[A\n","4622it [2:45:31,  1.65s/it]\u001b[A\n","4623it [2:45:33,  1.65s/it]\u001b[A\n","4624it [2:45:34,  1.65s/it]\u001b[A\n","4625it [2:45:36,  1.65s/it]\u001b[A\n","4626it [2:45:38,  1.65s/it]\u001b[A\n","4627it [2:45:39,  1.65s/it]\u001b[A\n","4628it [2:45:41,  1.65s/it]\u001b[A\n","4629it [2:45:43,  1.65s/it]\u001b[A\n","4630it [2:45:44,  1.65s/it]\u001b[A\n","4631it [2:45:46,  1.65s/it]\u001b[A\n","4632it [2:45:48,  1.65s/it]\u001b[A\n","4633it [2:45:49,  1.65s/it]\u001b[A\n","4634it [2:45:51,  1.65s/it]\u001b[A\n","4635it [2:45:53,  1.65s/it]\u001b[A\n","4636it [2:45:54,  1.65s/it]\u001b[A\n","4637it [2:45:56,  1.65s/it]\u001b[A\n","4638it [2:45:58,  1.65s/it]\u001b[A\n","4639it [2:45:59,  1.65s/it]\u001b[A\n","4640it [2:46:01,  1.65s/it]\u001b[A\n","4641it [2:46:03,  1.65s/it]\u001b[A\n","4642it [2:46:04,  1.65s/it]\u001b[A\n","4643it [2:46:06,  1.65s/it]\u001b[A\n","4644it [2:46:07,  1.65s/it]\u001b[A\n","4645it [2:46:09,  1.64s/it]\u001b[A\n","4646it [2:46:11,  1.64s/it]\u001b[A\n","4647it [2:46:12,  1.64s/it]\u001b[A\n","4648it [2:46:14,  1.65s/it]\u001b[A\n","4649it [2:46:16,  1.64s/it]\u001b[A\n","4650it [2:46:17,  1.64s/it]\u001b[A\n","4651it [2:46:19,  1.64s/it]\u001b[A\n","4652it [2:46:21,  1.64s/it]\u001b[A\n","4653it [2:46:22,  1.65s/it]\u001b[A\n","4654it [2:46:24,  1.65s/it]\u001b[A\n","4655it [2:46:26,  1.65s/it]\u001b[A\n","4656it [2:46:27,  1.65s/it]\u001b[A\n","4657it [2:46:29,  1.65s/it]\u001b[A\n","4658it [2:46:30,  1.65s/it]\u001b[A\n","4659it [2:46:32,  1.65s/it]\u001b[A\n","4660it [2:46:34,  1.65s/it]\u001b[A\n","4661it [2:46:35,  1.65s/it]\u001b[A\n","4662it [2:46:37,  1.65s/it]\u001b[A\n","4663it [2:46:39,  1.65s/it]\u001b[A\n","4664it [2:46:40,  1.65s/it]\u001b[A\n","4665it [2:46:42,  1.65s/it]\u001b[A\n","4666it [2:46:44,  1.65s/it]\u001b[A\n","4667it [2:46:45,  1.65s/it]\u001b[A\n","4668it [2:46:47,  1.65s/it]\u001b[A\n","4669it [2:46:49,  1.65s/it]\u001b[A\n","4670it [2:46:50,  1.65s/it]\u001b[A\n","4671it [2:46:52,  1.65s/it]\u001b[A\n","4672it [2:46:54,  1.65s/it]\u001b[A\n","4673it [2:46:55,  1.65s/it]\u001b[A\n","4674it [2:46:57,  1.65s/it]\u001b[A\n","4675it [2:46:59,  1.65s/it]\u001b[A\n","4676it [2:47:00,  1.65s/it]\u001b[A\n","4677it [2:47:02,  1.65s/it]\u001b[A\n","4678it [2:47:04,  1.65s/it]\u001b[A\n","4679it [2:47:05,  1.65s/it]\u001b[A\n","4680it [2:47:07,  1.65s/it]\u001b[A\n","4681it [2:47:08,  1.65s/it]\u001b[A\n","4682it [2:47:10,  1.65s/it]\u001b[A\n","4683it [2:47:12,  1.65s/it]\u001b[A\n","4684it [2:47:13,  1.65s/it]\u001b[A\n","4685it [2:47:15,  1.65s/it]\u001b[A\n","4686it [2:47:17,  1.65s/it]\u001b[A\n","4687it [2:47:18,  1.65s/it]\u001b[A\n","4688it [2:47:20,  1.65s/it]\u001b[A\n","4689it [2:47:22,  1.65s/it]\u001b[A\n","4690it [2:47:23,  1.65s/it]\u001b[A\n","4691it [2:47:25,  1.65s/it]\u001b[A\n","4692it [2:47:27,  1.65s/it]\u001b[A\n","4693it [2:47:28,  1.65s/it]\u001b[A\n","4694it [2:47:30,  1.64s/it]\u001b[A\n","4695it [2:47:32,  1.64s/it]\u001b[A\n","4696it [2:47:33,  1.65s/it]\u001b[A\n","4697it [2:47:35,  1.64s/it]\u001b[A\n","4698it [2:47:36,  1.64s/it]\u001b[A\n","4699it [2:47:38,  1.65s/it]\u001b[A\n","4700it [2:47:40,  1.65s/it]\u001b[A\n","4701it [2:47:41,  1.65s/it]\u001b[A\n","4702it [2:47:43,  1.64s/it]\u001b[A\n","4703it [2:47:45,  1.65s/it]\u001b[A\n","4704it [2:47:46,  1.65s/it]\u001b[A\n","4705it [2:47:48,  1.65s/it]\u001b[A\n","4706it [2:47:50,  1.65s/it]\u001b[A\n","4707it [2:47:51,  1.65s/it]\u001b[A\n","4708it [2:47:53,  1.65s/it]\u001b[A\n","4709it [2:47:55,  1.65s/it]\u001b[A\n","4710it [2:47:56,  1.65s/it]\u001b[A\n","4711it [2:47:58,  1.65s/it]\u001b[A\n","4712it [2:48:00,  1.65s/it]\u001b[A\n","4713it [2:48:01,  1.65s/it]\u001b[A\n","4714it [2:48:03,  1.65s/it]\u001b[A\n","4715it [2:48:04,  1.65s/it]\u001b[A\n","4716it [2:48:06,  1.65s/it]\u001b[A\n","4717it [2:48:08,  1.65s/it]\u001b[A\n","4718it [2:48:09,  1.65s/it]\u001b[A\n","4719it [2:48:11,  1.65s/it]\u001b[A\n","4720it [2:48:13,  1.65s/it]\u001b[A\n","4721it [2:48:14,  1.65s/it]\u001b[A\n","4722it [2:48:16,  1.65s/it]\u001b[A\n","4723it [2:48:18,  1.64s/it]\u001b[A\n","4724it [2:48:19,  1.64s/it]\u001b[A\n","4725it [2:48:21,  1.64s/it]\u001b[A\n","4726it [2:48:23,  1.64s/it]\u001b[A\n","4727it [2:48:24,  1.64s/it]\u001b[A\n","4728it [2:48:26,  1.64s/it]\u001b[A\n","4729it [2:48:27,  1.64s/it]\u001b[A\n","4730it [2:48:29,  1.63s/it]\u001b[A\n","4731it [2:48:31,  1.64s/it]\u001b[A\n","4732it [2:48:32,  1.64s/it]\u001b[A\n","4733it [2:48:34,  1.64s/it]\u001b[A\n","4734it [2:48:36,  1.64s/it]\u001b[A\n","4735it [2:48:37,  1.64s/it]\u001b[A\n","4736it [2:48:39,  1.64s/it]\u001b[A\n","4737it [2:48:41,  1.64s/it]\u001b[A\n","4738it [2:48:42,  1.64s/it]\u001b[A\n","4739it [2:48:44,  1.63s/it]\u001b[A\n","4740it [2:48:45,  1.63s/it]\u001b[A\n","4741it [2:48:47,  1.63s/it]\u001b[A\n","4742it [2:48:49,  1.63s/it]\u001b[A\n","4743it [2:48:50,  1.63s/it]\u001b[A\n","4744it [2:48:52,  1.64s/it]\u001b[A\n","4745it [2:48:54,  1.64s/it]\u001b[A\n","4746it [2:48:55,  1.64s/it]\u001b[A\n","4747it [2:48:57,  1.64s/it]\u001b[A\n","4748it [2:48:59,  1.64s/it]\u001b[A\n","4749it [2:49:00,  1.64s/it]\u001b[A\n","4750it [2:49:02,  1.63s/it]\u001b[A\n","4751it [2:49:03,  1.64s/it]\u001b[A\n","4752it [2:49:05,  1.63s/it]\u001b[A\n","4753it [2:49:07,  1.63s/it]\u001b[A\n","4754it [2:49:08,  1.63s/it]\u001b[A\n","4755it [2:49:10,  1.63s/it]\u001b[A\n","4756it [2:49:12,  1.63s/it]\u001b[A\n","4757it [2:49:13,  1.63s/it]\u001b[A\n","4758it [2:49:15,  1.63s/it]\u001b[A\n","4759it [2:49:16,  1.63s/it]\u001b[A\n","4760it [2:49:18,  1.63s/it]\u001b[A\n","4761it [2:49:20,  1.63s/it]\u001b[A\n","4762it [2:49:21,  1.64s/it]\u001b[A\n","4763it [2:49:23,  1.63s/it]\u001b[A\n","4764it [2:49:25,  1.63s/it]\u001b[A\n","4765it [2:49:26,  1.63s/it]\u001b[A\n","4766it [2:49:28,  1.63s/it]\u001b[A\n","4767it [2:49:30,  1.63s/it]\u001b[A\n","4768it [2:49:31,  1.63s/it]\u001b[A\n","4769it [2:49:33,  1.63s/it]\u001b[A\n","4770it [2:49:34,  1.64s/it]\u001b[A\n","4771it [2:49:36,  1.64s/it]\u001b[A\n","4772it [2:49:38,  1.64s/it]\u001b[A\n","4773it [2:49:39,  1.63s/it]\u001b[A\n","4774it [2:49:41,  1.63s/it]\u001b[A\n","4775it [2:49:43,  1.63s/it]\u001b[A\n","4776it [2:49:44,  1.63s/it]\u001b[A\n","4777it [2:49:46,  1.63s/it]\u001b[A\n","4778it [2:49:48,  1.63s/it]\u001b[A\n","4779it [2:49:49,  1.64s/it]\u001b[A\n","4780it [2:49:51,  1.64s/it]\u001b[A\n","4781it [2:49:52,  1.64s/it]\u001b[A\n","4782it [2:49:54,  1.64s/it]\u001b[A\n","4783it [2:49:56,  1.64s/it]\u001b[A\n","4784it [2:49:57,  1.64s/it]\u001b[A\n","4785it [2:49:59,  1.64s/it]\u001b[A\n","4786it [2:50:01,  1.63s/it]\u001b[A\n","4787it [2:50:02,  1.64s/it]\u001b[A\n","4788it [2:50:04,  1.64s/it]\u001b[A\n","4789it [2:50:06,  1.64s/it]\u001b[A\n","4790it [2:50:07,  1.64s/it]\u001b[A\n","4791it [2:50:09,  1.64s/it]\u001b[A\n","4792it [2:50:10,  1.64s/it]\u001b[A\n","4793it [2:50:12,  1.64s/it]\u001b[A\n","4794it [2:50:14,  1.64s/it]\u001b[A\n","4795it [2:50:15,  1.65s/it]\u001b[A\n","4796it [2:50:17,  1.64s/it]\u001b[A\n","4797it [2:50:19,  1.65s/it]\u001b[A\n","4798it [2:50:20,  1.65s/it]\u001b[A\n","4799it [2:50:22,  1.65s/it]\u001b[A\n","4800it [2:50:24,  1.65s/it]\u001b[A\n","4801it [2:50:25,  1.65s/it]\u001b[A\n","4802it [2:50:27,  1.65s/it]\u001b[A\n","4803it [2:50:29,  1.65s/it]\u001b[A\n","4804it [2:50:30,  1.65s/it]\u001b[A\n","4805it [2:50:32,  1.65s/it]\u001b[A\n","4806it [2:50:34,  1.65s/it]\u001b[A\n","4807it [2:50:35,  1.65s/it]\u001b[A\n","4808it [2:50:37,  1.65s/it]\u001b[A\n","4809it [2:50:39,  1.65s/it]\u001b[A\n","4810it [2:50:40,  1.65s/it]\u001b[A\n","4811it [2:50:42,  1.65s/it]\u001b[A\n","4812it [2:50:43,  1.65s/it]\u001b[A\n","4813it [2:50:45,  1.65s/it]\u001b[A\n","4814it [2:50:47,  1.65s/it]\u001b[A\n","4815it [2:50:48,  1.65s/it]\u001b[A\n","4816it [2:50:50,  1.64s/it]\u001b[A\n","4817it [2:50:52,  1.65s/it]\u001b[A\n","4818it [2:50:53,  1.64s/it]\u001b[A\n","4819it [2:50:55,  1.64s/it]\u001b[A\n","4820it [2:50:57,  1.65s/it]\u001b[A\n","4821it [2:50:58,  1.64s/it]\u001b[A\n","4822it [2:51:00,  1.65s/it]\u001b[A\n","4823it [2:51:02,  1.65s/it]\u001b[A\n","4824it [2:51:03,  1.65s/it]\u001b[A\n","4825it [2:51:05,  1.65s/it]\u001b[A\n","4826it [2:51:07,  1.65s/it]\u001b[A\n","4827it [2:51:08,  1.65s/it]\u001b[A\n","4828it [2:51:10,  1.66s/it]\u001b[A\n","4829it [2:51:12,  1.66s/it]\u001b[A\n","4830it [2:51:13,  1.65s/it]\u001b[A\n","4831it [2:51:15,  1.65s/it]\u001b[A\n","4832it [2:51:16,  1.65s/it]\u001b[A\n","4833it [2:51:18,  1.65s/it]\u001b[A\n","4834it [2:51:20,  1.65s/it]\u001b[A\n","4835it [2:51:21,  1.64s/it]\u001b[A\n","4836it [2:51:23,  1.65s/it]\u001b[A\n","4837it [2:51:25,  1.65s/it]\u001b[A\n","4838it [2:51:26,  1.64s/it]\u001b[A\n","4839it [2:51:28,  1.65s/it]\u001b[A\n","4840it [2:51:30,  1.65s/it]\u001b[A\n","4841it [2:51:31,  1.65s/it]\u001b[A\n","4842it [2:51:33,  1.65s/it]\u001b[A\n","4843it [2:51:35,  1.65s/it]\u001b[A\n","4844it [2:51:36,  1.65s/it]\u001b[A\n","4845it [2:51:38,  1.65s/it]\u001b[A\n","4846it [2:51:40,  1.65s/it]\u001b[A\n","4847it [2:51:41,  1.65s/it]\u001b[A\n","4848it [2:51:43,  1.65s/it]\u001b[A\n","4849it [2:51:44,  1.65s/it]\u001b[A\n","4850it [2:51:46,  1.65s/it]\u001b[A\n","4851it [2:51:48,  1.65s/it]\u001b[A\n","4852it [2:51:49,  1.66s/it]\u001b[A\n","4853it [2:51:51,  1.66s/it]\u001b[A\n","4854it [2:51:53,  1.65s/it]\u001b[A\n","4855it [2:51:54,  1.65s/it]\u001b[A\n","4856it [2:51:56,  1.65s/it]\u001b[A\n","4857it [2:51:58,  1.65s/it]\u001b[A\n","4858it [2:51:59,  1.65s/it]\u001b[A\n","4859it [2:52:01,  1.65s/it]\u001b[A\n","4860it [2:52:03,  1.65s/it]\u001b[A\n","4861it [2:52:04,  1.65s/it]\u001b[A\n","4862it [2:52:06,  1.65s/it]\u001b[A\n","4863it [2:52:08,  1.65s/it]\u001b[A\n","4864it [2:52:09,  1.65s/it]\u001b[A\n","4865it [2:52:11,  1.65s/it]\u001b[A\n","4866it [2:52:13,  1.65s/it]\u001b[A\n","4867it [2:52:14,  1.65s/it]\u001b[A\n","4868it [2:52:16,  1.65s/it]\u001b[A\n","4869it [2:52:17,  1.64s/it]\u001b[A\n","4870it [2:52:19,  1.64s/it]\u001b[A\n","4871it [2:52:21,  1.64s/it]\u001b[A\n","4872it [2:52:22,  1.65s/it]\u001b[A\n","4873it [2:52:24,  1.65s/it]\u001b[A\n","4874it [2:52:26,  1.65s/it]\u001b[A\n","4875it [2:52:27,  1.65s/it]\u001b[A\n","4876it [2:52:29,  1.65s/it]\u001b[A\n","4877it [2:52:31,  1.65s/it]\u001b[A\n","4878it [2:52:32,  1.64s/it]\u001b[A\n","4879it [2:52:34,  1.65s/it]\u001b[A\n","4880it [2:52:36,  1.65s/it]\u001b[A\n","4881it [2:52:37,  1.65s/it]\u001b[A\n","4882it [2:52:39,  1.65s/it]\u001b[A\n","4883it [2:52:41,  1.65s/it]\u001b[A\n","4884it [2:52:42,  1.65s/it]\u001b[A\n","4885it [2:52:44,  1.64s/it]\u001b[A\n","4886it [2:52:45,  1.64s/it]\u001b[A\n","4887it [2:52:47,  1.64s/it]\u001b[A\n","4888it [2:52:49,  1.65s/it]\u001b[A\n","4889it [2:52:50,  1.64s/it]\u001b[A\n","4890it [2:52:52,  1.65s/it]\u001b[A\n","4891it [2:52:54,  1.64s/it]\u001b[A\n","4892it [2:52:55,  1.64s/it]\u001b[A\n","4893it [2:52:57,  1.64s/it]\u001b[A\n","4894it [2:52:59,  1.64s/it]\u001b[A\n","4895it [2:53:00,  1.64s/it]\u001b[A\n","4896it [2:53:02,  1.64s/it]\u001b[A\n","4897it [2:53:04,  1.64s/it]\u001b[A\n","4898it [2:53:05,  1.64s/it]\u001b[A\n","4899it [2:53:07,  1.64s/it]\u001b[A\n","4900it [2:53:08,  1.64s/it]\u001b[A\n","4901it [2:53:10,  1.64s/it]\u001b[A\n","4902it [2:53:12,  1.63s/it]\u001b[A\n","4903it [2:53:13,  1.63s/it]\u001b[A\n","4904it [2:53:15,  1.64s/it]\u001b[A\n","4905it [2:53:17,  1.63s/it]\u001b[A\n","4906it [2:53:18,  1.63s/it]\u001b[A\n","4907it [2:53:20,  1.63s/it]\u001b[A\n","4908it [2:53:21,  1.64s/it]\u001b[A\n","4909it [2:53:23,  1.64s/it]\u001b[A\n","4910it [2:53:25,  1.64s/it]\u001b[A\n","4911it [2:53:26,  1.63s/it]\u001b[A\n","4912it [2:53:28,  1.63s/it]\u001b[A\n","4913it [2:53:30,  1.64s/it]\u001b[A\n","4914it [2:53:31,  1.64s/it]\u001b[A\n","4915it [2:53:33,  1.64s/it]\u001b[A\n","4916it [2:53:35,  1.63s/it]\u001b[A\n","4917it [2:53:36,  1.64s/it]\u001b[A\n","4918it [2:53:38,  1.63s/it]\u001b[A\n","4919it [2:53:39,  1.63s/it]\u001b[A\n","4920it [2:53:41,  1.63s/it]\u001b[A\n","4921it [2:53:43,  1.63s/it]\u001b[A\n","4922it [2:53:44,  1.64s/it]\u001b[A\n","4923it [2:53:46,  1.64s/it]\u001b[A\n","4924it [2:53:48,  1.64s/it]\u001b[A\n","4925it [2:53:49,  1.63s/it]\u001b[A\n","4926it [2:53:51,  1.63s/it]\u001b[A\n","4927it [2:53:53,  1.63s/it]\u001b[A\n","4928it [2:53:54,  1.64s/it]\u001b[A\n","4929it [2:53:56,  1.64s/it]\u001b[A\n","4930it [2:53:57,  1.64s/it]\u001b[A\n","4931it [2:53:59,  1.64s/it]\u001b[A\n","4932it [2:54:01,  1.63s/it]\u001b[A\n","4933it [2:54:02,  1.63s/it]\u001b[A\n","4934it [2:54:04,  1.63s/it]\u001b[A\n","4935it [2:54:06,  1.64s/it]\u001b[A\n","4936it [2:54:07,  1.63s/it]\u001b[A\n","4937it [2:54:09,  1.63s/it]\u001b[A\n","4938it [2:54:11,  1.63s/it]\u001b[A\n","4939it [2:54:12,  1.63s/it]\u001b[A\n","4940it [2:54:14,  1.63s/it]\u001b[A\n","4941it [2:54:15,  1.63s/it]\u001b[A\n","4942it [2:54:17,  1.64s/it]\u001b[A\n","4943it [2:54:19,  1.64s/it]\u001b[A\n","4944it [2:54:20,  1.64s/it]\u001b[A\n","4945it [2:54:22,  1.64s/it]\u001b[A\n","4946it [2:54:24,  1.64s/it]\u001b[A\n","4947it [2:54:25,  1.64s/it]\u001b[A\n","4948it [2:54:27,  1.64s/it]\u001b[A\n","4949it [2:54:29,  1.63s/it]\u001b[A\n","4950it [2:54:30,  1.63s/it]\u001b[A\n","4951it [2:54:32,  1.63s/it]\u001b[A\n","4952it [2:54:33,  1.63s/it]\u001b[A\n","4953it [2:54:35,  1.63s/it]\u001b[A\n","4954it [2:54:37,  1.63s/it]\u001b[A\n","4955it [2:54:38,  1.64s/it]\u001b[A\n","4956it [2:54:40,  1.64s/it]\u001b[A\n","4957it [2:54:42,  1.64s/it]\u001b[A\n","4958it [2:54:43,  1.64s/it]\u001b[A\n","4959it [2:54:45,  1.64s/it]\u001b[A\n","4960it [2:54:47,  1.64s/it]\u001b[A\n","4961it [2:54:48,  1.64s/it]\u001b[A\n","4962it [2:54:50,  1.65s/it]\u001b[A\n","4963it [2:54:52,  1.64s/it]\u001b[A\n","4964it [2:54:53,  1.65s/it]\u001b[A\n","4965it [2:54:55,  1.64s/it]\u001b[A\n","4966it [2:54:56,  1.64s/it]\u001b[A\n","4967it [2:54:58,  1.65s/it]\u001b[A\n","4968it [2:55:00,  1.65s/it]\u001b[A\n","4969it [2:55:01,  1.64s/it]\u001b[A\n","4970it [2:55:03,  1.64s/it]\u001b[A\n","4971it [2:55:05,  1.65s/it]\u001b[A\n","4972it [2:55:06,  1.65s/it]\u001b[A\n","4973it [2:55:08,  1.65s/it]\u001b[A\n","4974it [2:55:10,  1.65s/it]\u001b[A\n","4975it [2:55:11,  1.65s/it]\u001b[A\n","4976it [2:55:13,  1.65s/it]\u001b[A\n","4977it [2:55:15,  1.64s/it]\u001b[A\n","4978it [2:55:16,  1.65s/it]\u001b[A\n","4979it [2:55:18,  1.65s/it]\u001b[A\n","4980it [2:55:20,  1.65s/it]\u001b[A\n","4981it [2:55:21,  1.65s/it]\u001b[A\n","4982it [2:55:23,  1.65s/it]\u001b[A\n","4983it [2:55:24,  1.65s/it]\u001b[A\n","4984it [2:55:26,  1.65s/it]\u001b[A\n","4985it [2:55:28,  1.65s/it]\u001b[A\n","4986it [2:55:29,  1.65s/it]\u001b[A\n","4987it [2:55:31,  1.64s/it]\u001b[A\n","4988it [2:55:33,  1.65s/it]\u001b[A\n","4989it [2:55:34,  1.65s/it]\u001b[A\n","4990it [2:55:36,  1.65s/it]\u001b[A\n","4991it [2:55:38,  1.65s/it]\u001b[A\n","4992it [2:55:39,  1.65s/it]\u001b[A\n","4993it [2:55:41,  1.65s/it]\u001b[A\n","4994it [2:55:43,  1.65s/it]\u001b[A\n","4995it [2:55:44,  1.65s/it]\u001b[A\n","4996it [2:55:46,  1.65s/it]\u001b[A\n","4997it [2:55:48,  1.65s/it]\u001b[A\n","4998it [2:55:49,  1.65s/it]\u001b[A\n","4999it [2:55:51,  1.65s/it]\u001b[A\n","5000it [2:55:53,  1.65s/it]\u001b[A\n","5001it [2:55:54,  1.65s/it]\u001b[A\n","5002it [2:55:56,  1.65s/it]\u001b[A\n","5003it [2:55:57,  1.64s/it]\u001b[A\n","5004it [2:55:59,  1.64s/it]\u001b[A\n","5005it [2:56:01,  1.64s/it]\u001b[A\n","5006it [2:56:02,  1.65s/it]\u001b[A\n","5007it [2:56:04,  1.64s/it]\u001b[A\n","5008it [2:56:06,  1.64s/it]\u001b[A\n","5009it [2:56:07,  1.65s/it]\u001b[A\n","5010it [2:56:09,  1.65s/it]\u001b[A\n","5011it [2:56:11,  1.65s/it]\u001b[A\n","5012it [2:56:12,  1.65s/it]\u001b[A\n","5013it [2:56:14,  1.65s/it]\u001b[A\n","5014it [2:56:16,  1.65s/it]\u001b[A\n","5015it [2:56:17,  1.65s/it]\u001b[A\n","5016it [2:56:19,  1.64s/it]\u001b[A\n","5017it [2:56:20,  1.64s/it]\u001b[A\n","5018it [2:56:22,  1.64s/it]\u001b[A\n","5019it [2:56:24,  1.64s/it]\u001b[A\n","5020it [2:56:25,  1.64s/it]\u001b[A\n","5021it [2:56:27,  1.64s/it]\u001b[A\n","5022it [2:56:29,  1.65s/it]\u001b[A\n","5023it [2:56:30,  1.65s/it]\u001b[A\n","5024it [2:56:32,  1.65s/it]\u001b[A\n","5025it [2:56:34,  1.65s/it]\u001b[A\n","5026it [2:56:35,  1.65s/it]\u001b[A\n","5027it [2:56:37,  1.65s/it]\u001b[A\n","5028it [2:56:39,  1.65s/it]\u001b[A\n","5029it [2:56:40,  1.65s/it]\u001b[A\n","5030it [2:56:42,  1.65s/it]\u001b[A\n","5031it [2:56:44,  1.65s/it]\u001b[A\n","5032it [2:56:45,  1.65s/it]\u001b[A\n","5033it [2:56:47,  1.65s/it]\u001b[A\n","5034it [2:56:49,  1.65s/it]\u001b[A\n","5035it [2:56:50,  1.65s/it]\u001b[A\n","5036it [2:56:52,  1.65s/it]\u001b[A\n","5037it [2:56:53,  1.65s/it]\u001b[A\n","5038it [2:56:55,  1.65s/it]\u001b[A\n","5039it [2:56:57,  1.65s/it]\u001b[A\n","5040it [2:56:58,  1.65s/it]\u001b[A\n","5041it [2:57:00,  1.65s/it]\u001b[A\n","5042it [2:57:02,  1.65s/it]\u001b[A\n","5043it [2:57:03,  1.65s/it]\u001b[A\n","5044it [2:57:05,  1.65s/it]\u001b[A\n","5045it [2:57:07,  1.65s/it]\u001b[A\n","5046it [2:57:08,  1.65s/it]\u001b[A\n","5047it [2:57:10,  1.65s/it]\u001b[A\n","5048it [2:57:12,  1.65s/it]\u001b[A\n","5049it [2:57:13,  1.65s/it]\u001b[A\n","5050it [2:57:15,  1.65s/it]\u001b[A\n","5051it [2:57:17,  1.65s/it]\u001b[A\n","5052it [2:57:18,  1.65s/it]\u001b[A\n","5053it [2:57:20,  1.65s/it]\u001b[A\n","5054it [2:57:22,  1.65s/it]\u001b[A\n","5055it [2:57:23,  1.65s/it]\u001b[A\n","5056it [2:57:25,  1.65s/it]\u001b[A\n","5057it [2:57:26,  1.65s/it]\u001b[A\n","5058it [2:57:28,  1.64s/it]\u001b[A\n","5059it [2:57:30,  1.64s/it]\u001b[A\n","5060it [2:57:31,  1.64s/it]\u001b[A\n","5061it [2:57:33,  1.64s/it]\u001b[A\n","5062it [2:57:35,  1.64s/it]\u001b[A\n","5063it [2:57:36,  1.64s/it]\u001b[A\n","5064it [2:57:38,  1.64s/it]\u001b[A\n","5065it [2:57:40,  1.64s/it]\u001b[A\n","5066it [2:57:41,  1.64s/it]\u001b[A\n","5067it [2:57:43,  1.64s/it]\u001b[A\n","5068it [2:57:44,  1.64s/it]\u001b[A\n","5069it [2:57:46,  1.64s/it]\u001b[A\n","5070it [2:57:48,  1.64s/it]\u001b[A\n","5071it [2:57:49,  1.64s/it]\u001b[A\n","5072it [2:57:51,  1.63s/it]\u001b[A\n","5073it [2:57:53,  1.63s/it]\u001b[A\n","5074it [2:57:54,  1.63s/it]\u001b[A\n","5075it [2:57:56,  1.63s/it]\u001b[A\n","5076it [2:57:58,  1.63s/it]\u001b[A\n","5077it [2:57:59,  1.63s/it]\u001b[A\n","5078it [2:58:01,  1.63s/it]\u001b[A\n","5079it [2:58:02,  1.63s/it]\u001b[A\n","5080it [2:58:04,  1.63s/it]\u001b[A\n","5081it [2:58:06,  1.63s/it]\u001b[A\n","5082it [2:58:07,  1.64s/it]\u001b[A\n","5083it [2:58:09,  1.63s/it]\u001b[A\n","5084it [2:58:11,  1.63s/it]\u001b[A\n","5085it [2:58:12,  1.63s/it]\u001b[A\n","5086it [2:58:14,  1.64s/it]\u001b[A\n","5087it [2:58:16,  1.63s/it]\u001b[A\n","5088it [2:58:17,  1.63s/it]\u001b[A\n","5089it [2:58:19,  1.63s/it]\u001b[A\n","5090it [2:58:20,  1.63s/it]\u001b[A\n","5091it [2:58:22,  1.63s/it]\u001b[A\n","5092it [2:58:24,  1.63s/it]\u001b[A\n","5093it [2:58:25,  1.63s/it]\u001b[A\n","5094it [2:58:27,  1.63s/it]\u001b[A\n","5095it [2:58:29,  1.63s/it]\u001b[A\n","5096it [2:58:30,  1.63s/it]\u001b[A\n","5097it [2:58:32,  1.63s/it]\u001b[A\n","5098it [2:58:33,  1.64s/it]\u001b[A\n","5099it [2:58:35,  1.64s/it]\u001b[A\n","5100it [2:58:37,  1.64s/it]\u001b[A\n","5101it [2:58:38,  1.64s/it]\u001b[A\n","5102it [2:58:40,  1.63s/it]\u001b[A\n","5103it [2:58:42,  1.63s/it]\u001b[A\n","5104it [2:58:43,  1.63s/it]\u001b[A\n","5105it [2:58:45,  1.63s/it]\u001b[A\n","5106it [2:58:47,  1.63s/it]\u001b[A\n","5107it [2:58:48,  1.63s/it]\u001b[A\n","5108it [2:58:50,  1.63s/it]\u001b[A\n","5109it [2:58:51,  1.63s/it]\u001b[A\n","5110it [2:58:53,  1.63s/it]\u001b[A\n","5111it [2:58:55,  1.63s/it]\u001b[A\n","5112it [2:58:56,  1.64s/it]\u001b[A\n","5113it [2:58:58,  1.64s/it]\u001b[A\n","5114it [2:59:00,  1.63s/it]\u001b[A\n","5115it [2:59:01,  1.64s/it]\u001b[A\n","5116it [2:59:03,  1.63s/it]\u001b[A\n","5117it [2:59:04,  1.63s/it]\u001b[A\n","5118it [2:59:06,  1.63s/it]\u001b[A\n","5119it [2:59:08,  1.63s/it]\u001b[A\n","5120it [2:59:09,  1.63s/it]\u001b[A\n","5121it [2:59:11,  1.63s/it]\u001b[A\n","5122it [2:59:13,  1.63s/it]\u001b[A\n","5123it [2:59:14,  1.63s/it]\u001b[A\n","5124it [2:59:16,  1.63s/it]\u001b[A\n","5125it [2:59:18,  1.64s/it]\u001b[A\n","5126it [2:59:19,  1.64s/it]\u001b[A\n","5127it [2:59:21,  1.64s/it]\u001b[A\n","5128it [2:59:23,  1.65s/it]\u001b[A\n","5129it [2:59:24,  1.64s/it]\u001b[A\n","5130it [2:59:26,  1.64s/it]\u001b[A\n","5131it [2:59:27,  1.64s/it]\u001b[A\n","5132it [2:59:29,  1.64s/it]\u001b[A\n","5133it [2:59:31,  1.64s/it]\u001b[A\n","5134it [2:59:32,  1.64s/it]\u001b[A\n","5135it [2:59:34,  1.64s/it]\u001b[A\n","5136it [2:59:36,  1.65s/it]\u001b[A\n","5137it [2:59:37,  1.65s/it]\u001b[A\n","5138it [2:59:39,  1.65s/it]\u001b[A\n","5139it [2:59:41,  1.65s/it]\u001b[A\n","5140it [2:59:42,  1.65s/it]\u001b[A\n","5141it [2:59:44,  1.65s/it]\u001b[A\n","5142it [2:59:46,  1.66s/it]\u001b[A\n","5143it [2:59:47,  1.65s/it]\u001b[A\n","5144it [2:59:49,  1.65s/it]\u001b[A\n","5145it [2:59:51,  1.65s/it]\u001b[A\n","5146it [2:59:52,  1.65s/it]\u001b[A\n","5147it [2:59:54,  1.65s/it]\u001b[A\n","5148it [2:59:55,  1.65s/it]\u001b[A\n","5149it [2:59:57,  1.65s/it]\u001b[A\n","5150it [2:59:59,  1.65s/it]\u001b[A\n","5151it [3:00:00,  1.65s/it]\u001b[A\n","5152it [3:00:02,  1.65s/it]\u001b[A\n","5153it [3:00:04,  1.65s/it]\u001b[A\n","5154it [3:00:05,  1.65s/it]\u001b[A\n","5155it [3:00:07,  1.65s/it]\u001b[A\n","5156it [3:00:09,  1.65s/it]\u001b[A\n","5157it [3:00:10,  1.65s/it]\u001b[A\n","5158it [3:00:12,  1.65s/it]\u001b[A\n","5159it [3:00:14,  1.65s/it]\u001b[A\n","5160it [3:00:15,  1.65s/it]\u001b[A\n","5161it [3:00:17,  1.65s/it]\u001b[A\n","5162it [3:00:19,  1.65s/it]\u001b[A\n","5163it [3:00:20,  1.65s/it]\u001b[A\n","5164it [3:00:22,  1.65s/it]\u001b[A\n","5165it [3:00:23,  1.65s/it]\u001b[A\n","5166it [3:00:25,  1.65s/it]\u001b[A\n","5167it [3:00:27,  1.65s/it]\u001b[A\n","5168it [3:00:28,  1.65s/it]\u001b[A\n","5169it [3:00:30,  1.65s/it]\u001b[A\n","5170it [3:00:32,  1.65s/it]\u001b[A\n","5171it [3:00:33,  1.65s/it]\u001b[A\n","5172it [3:00:35,  1.65s/it]\u001b[A\n","5173it [3:00:37,  1.64s/it]\u001b[A\n","5174it [3:00:38,  1.64s/it]\u001b[A\n","5175it [3:00:40,  1.64s/it]\u001b[A\n","5176it [3:00:42,  1.64s/it]\u001b[A\n","5177it [3:00:43,  1.64s/it]\u001b[A\n","5178it [3:00:45,  1.65s/it]\u001b[A\n","5179it [3:00:47,  1.65s/it]\u001b[A\n","5180it [3:00:48,  1.65s/it]\u001b[A\n","5181it [3:00:50,  1.65s/it]\u001b[A\n","5182it [3:00:51,  1.65s/it]\u001b[A\n","5183it [3:00:53,  1.65s/it]\u001b[A\n","5184it [3:00:55,  1.65s/it]\u001b[A\n","5185it [3:00:56,  1.65s/it]\u001b[A\n","5186it [3:00:58,  1.65s/it]\u001b[A\n","5187it [3:01:00,  1.64s/it]\u001b[A\n","5188it [3:01:01,  1.64s/it]\u001b[A\n","5189it [3:01:03,  1.65s/it]\u001b[A\n","5190it [3:01:05,  1.65s/it]\u001b[A\n","5191it [3:01:06,  1.65s/it]\u001b[A\n","5192it [3:01:08,  1.64s/it]\u001b[A\n","5193it [3:01:10,  1.65s/it]\u001b[A\n","5194it [3:01:11,  1.65s/it]\u001b[A\n","5195it [3:01:13,  1.65s/it]\u001b[A\n","5196it [3:01:15,  1.65s/it]\u001b[A\n","5197it [3:01:16,  1.65s/it]\u001b[A\n","5198it [3:01:18,  1.65s/it]\u001b[A\n","5199it [3:01:19,  1.64s/it]\u001b[A\n","5200it [3:01:21,  1.64s/it]\u001b[A\n","5201it [3:01:23,  1.64s/it]\u001b[A\n","5202it [3:01:24,  1.64s/it]\u001b[A\n","5203it [3:01:26,  1.65s/it]\u001b[A\n","5204it [3:01:28,  1.65s/it]\u001b[A\n","5205it [3:01:29,  1.65s/it]\u001b[A\n","5206it [3:01:31,  1.65s/it]\u001b[A\n","5207it [3:01:33,  1.65s/it]\u001b[A\n","5208it [3:01:34,  1.65s/it]\u001b[A\n","5209it [3:01:36,  1.65s/it]\u001b[A\n","5210it [3:01:38,  1.65s/it]\u001b[A\n","5211it [3:01:39,  1.65s/it]\u001b[A\n","5212it [3:01:41,  1.65s/it]\u001b[A\n","5213it [3:01:43,  1.64s/it]\u001b[A\n","5214it [3:01:44,  1.65s/it]\u001b[A\n","5215it [3:01:46,  1.65s/it]\u001b[A\n","5216it [3:01:48,  1.65s/it]\u001b[A\n","5217it [3:01:49,  1.65s/it]\u001b[A\n","5218it [3:01:51,  1.65s/it]\u001b[A\n","5219it [3:01:52,  1.65s/it]\u001b[A\n","5220it [3:01:54,  1.65s/it]\u001b[A\n","5221it [3:01:56,  1.65s/it]\u001b[A\n","5222it [3:01:57,  1.65s/it]\u001b[A\n","5223it [3:01:59,  1.65s/it]\u001b[A\n","5224it [3:02:01,  1.64s/it]\u001b[A\n","5225it [3:02:02,  1.64s/it]\u001b[A\n","5226it [3:02:04,  1.64s/it]\u001b[A\n","5227it [3:02:06,  1.64s/it]\u001b[A\n","5228it [3:02:07,  1.64s/it]\u001b[A\n","5229it [3:02:09,  1.64s/it]\u001b[A\n","5230it [3:02:11,  1.64s/it]\u001b[A\n","5231it [3:02:12,  1.63s/it]\u001b[A\n","5232it [3:02:14,  1.64s/it]\u001b[A\n","5233it [3:02:15,  1.64s/it]\u001b[A\n","5234it [3:02:17,  1.64s/it]\u001b[A\n","5235it [3:02:19,  1.64s/it]\u001b[A\n","5236it [3:02:20,  1.64s/it]\u001b[A\n","5237it [3:02:22,  1.64s/it]\u001b[A\n","5238it [3:02:24,  1.64s/it]\u001b[A\n","5239it [3:02:25,  1.64s/it]\u001b[A\n","5240it [3:02:27,  1.63s/it]\u001b[A\n","5241it [3:02:28,  1.63s/it]\u001b[A\n","5242it [3:02:30,  1.63s/it]\u001b[A\n","5243it [3:02:32,  1.63s/it]\u001b[A\n","5244it [3:02:33,  1.63s/it]\u001b[A\n","5245it [3:02:35,  1.64s/it]\u001b[A\n","5246it [3:02:37,  1.63s/it]\u001b[A\n","5247it [3:02:38,  1.63s/it]\u001b[A\n","5248it [3:02:40,  1.63s/it]\u001b[A\n","5249it [3:02:42,  1.63s/it]\u001b[A\n","5250it [3:02:43,  1.63s/it]\u001b[A\n","5251it [3:02:45,  1.63s/it]\u001b[A\n","5252it [3:02:46,  1.64s/it]\u001b[A\n","5253it [3:02:48,  1.63s/it]\u001b[A\n","5254it [3:02:50,  1.63s/it]\u001b[A\n","5255it [3:02:51,  1.64s/it]\u001b[A\n","5256it [3:02:53,  1.63s/it]\u001b[A\n","5257it [3:02:55,  1.64s/it]\u001b[A\n","5258it [3:02:56,  1.64s/it]\u001b[A\n","5259it [3:02:58,  1.64s/it]\u001b[A\n","5260it [3:03:00,  1.64s/it]\u001b[A\n","5261it [3:03:01,  1.63s/it]\u001b[A\n","5262it [3:03:03,  1.63s/it]\u001b[A\n","5263it [3:03:04,  1.63s/it]\u001b[A\n","5264it [3:03:06,  1.63s/it]\u001b[A\n","5265it [3:03:08,  1.63s/it]\u001b[A\n","5266it [3:03:09,  1.63s/it]\u001b[A\n","5267it [3:03:11,  1.63s/it]\u001b[A\n","5268it [3:03:13,  1.63s/it]\u001b[A\n","5269it [3:03:14,  1.63s/it]\u001b[A\n","5270it [3:03:16,  1.63s/it]\u001b[A\n","5271it [3:03:17,  1.63s/it]\u001b[A\n","5272it [3:03:19,  1.63s/it]\u001b[A\n","5273it [3:03:21,  1.63s/it]\u001b[A\n","5274it [3:03:22,  1.63s/it]\u001b[A\n","5275it [3:03:24,  1.63s/it]\u001b[A\n","5276it [3:03:26,  1.63s/it]\u001b[A\n","5277it [3:03:27,  1.63s/it]\u001b[A\n","5278it [3:03:29,  1.64s/it]\u001b[A\n","5279it [3:03:31,  1.64s/it]\u001b[A\n","5280it [3:03:32,  1.64s/it]\u001b[A\n","5281it [3:03:34,  1.64s/it]\u001b[A\n","5282it [3:03:35,  1.64s/it]\u001b[A\n","5283it [3:03:37,  1.64s/it]\u001b[A\n","5284it [3:03:39,  1.64s/it]\u001b[A\n","5285it [3:03:40,  1.64s/it]\u001b[A\n","5286it [3:03:42,  1.63s/it]\u001b[A\n","5287it [3:03:44,  1.63s/it]\u001b[A\n","5288it [3:03:45,  1.63s/it]\u001b[A\n","5289it [3:03:47,  1.63s/it]\u001b[A\n","5290it [3:03:49,  1.63s/it]\u001b[A\n","5291it [3:03:50,  1.63s/it]\u001b[A\n","5292it [3:03:52,  1.62s/it]\u001b[A\n","5293it [3:03:53,  1.63s/it]\u001b[A\n","5294it [3:03:55,  1.64s/it]\u001b[A\n","5295it [3:03:57,  1.64s/it]\u001b[A\n","5296it [3:03:58,  1.64s/it]\u001b[A\n","5297it [3:04:00,  1.64s/it]\u001b[A\n","5298it [3:04:02,  1.64s/it]\u001b[A\n","5299it [3:04:03,  1.64s/it]\u001b[A\n","5300it [3:04:05,  1.64s/it]\u001b[A\n","5301it [3:04:07,  1.64s/it]\u001b[A\n","5302it [3:04:08,  1.64s/it]\u001b[A\n","5303it [3:04:10,  1.64s/it]\u001b[A\n","5304it [3:04:11,  1.65s/it]\u001b[A\n","5305it [3:04:13,  1.65s/it]\u001b[A\n","5306it [3:04:15,  1.64s/it]\u001b[A\n","5307it [3:04:16,  1.65s/it]\u001b[A\n","5308it [3:04:18,  1.64s/it]\u001b[A\n","5309it [3:04:20,  1.65s/it]\u001b[A\n","5310it [3:04:21,  1.65s/it]\u001b[A\n","5311it [3:04:23,  1.65s/it]\u001b[A\n","5312it [3:04:25,  1.65s/it]\u001b[A\n","5313it [3:04:26,  1.65s/it]\u001b[A\n","5314it [3:04:28,  1.65s/it]\u001b[A\n","5315it [3:04:30,  1.64s/it]\u001b[A\n","5316it [3:04:31,  1.64s/it]\u001b[A\n","5317it [3:04:33,  1.64s/it]\u001b[A\n","5318it [3:04:35,  1.65s/it]\u001b[A\n","5319it [3:04:36,  1.64s/it]\u001b[A\n","5320it [3:04:38,  1.64s/it]\u001b[A\n","5321it [3:04:39,  1.64s/it]\u001b[A\n","5322it [3:04:41,  1.64s/it]\u001b[A\n","5323it [3:04:43,  1.64s/it]\u001b[A\n","5324it [3:04:44,  1.64s/it]\u001b[A\n","5325it [3:04:46,  1.64s/it]\u001b[A\n","5326it [3:04:48,  1.64s/it]\u001b[A\n","5327it [3:04:49,  1.64s/it]\u001b[A\n","5328it [3:04:51,  1.64s/it]\u001b[A\n","5329it [3:04:53,  1.64s/it]\u001b[A\n","5330it [3:04:54,  1.64s/it]\u001b[A\n","5331it [3:04:56,  1.64s/it]\u001b[A\n","5332it [3:04:57,  1.64s/it]\u001b[A\n","5333it [3:04:59,  1.64s/it]\u001b[A\n","5334it [3:05:01,  1.65s/it]\u001b[A\n","5335it [3:05:02,  1.65s/it]\u001b[A\n","5336it [3:05:04,  1.65s/it]\u001b[A\n","5337it [3:05:06,  1.65s/it]\u001b[A\n","5338it [3:05:07,  1.65s/it]\u001b[A\n","5339it [3:05:09,  1.65s/it]\u001b[A\n","5340it [3:05:11,  1.65s/it]\u001b[A\n","5341it [3:05:12,  1.64s/it]\u001b[A\n","5342it [3:05:14,  1.64s/it]\u001b[A\n","5343it [3:05:16,  1.64s/it]\u001b[A\n","5344it [3:05:17,  1.64s/it]\u001b[A\n","5345it [3:05:19,  1.64s/it]\u001b[A\n","5346it [3:05:20,  1.64s/it]\u001b[A\n","5347it [3:05:22,  1.64s/it]\u001b[A\n","5348it [3:05:24,  1.65s/it]\u001b[A\n","5349it [3:05:25,  1.64s/it]\u001b[A\n","5350it [3:05:27,  1.65s/it]\u001b[A\n","5351it [3:05:29,  1.64s/it]\u001b[A\n","5352it [3:05:30,  1.65s/it]\u001b[A\n","5353it [3:05:32,  1.64s/it]\u001b[A\n","5354it [3:05:34,  1.65s/it]\u001b[A\n","5355it [3:05:35,  1.64s/it]\u001b[A\n","5356it [3:05:37,  1.64s/it]\u001b[A\n","5357it [3:05:39,  1.65s/it]\u001b[A\n","5358it [3:05:40,  1.64s/it]\u001b[A\n","5359it [3:05:42,  1.65s/it]\u001b[A\n","5360it [3:05:44,  1.64s/it]\u001b[A\n","5361it [3:05:45,  1.65s/it]\u001b[A\n","5362it [3:05:47,  1.65s/it]\u001b[A\n","5363it [3:05:48,  1.65s/it]\u001b[A\n","5364it [3:05:50,  1.65s/it]\u001b[A\n","5365it [3:05:52,  1.65s/it]\u001b[A\n","5366it [3:05:53,  1.65s/it]\u001b[A\n","5367it [3:05:55,  1.65s/it]\u001b[A\n","5368it [3:05:57,  1.65s/it]\u001b[A\n","5369it [3:05:58,  1.65s/it]\u001b[A\n","5370it [3:06:00,  1.65s/it]\u001b[A\n","5371it [3:06:02,  1.64s/it]\u001b[A\n","5372it [3:06:03,  1.64s/it]\u001b[A\n","5373it [3:06:05,  1.65s/it]\u001b[A\n","5374it [3:06:07,  1.65s/it]\u001b[A\n","5375it [3:06:08,  1.64s/it]\u001b[A\n","5376it [3:06:10,  1.64s/it]\u001b[A\n","5377it [3:06:12,  1.64s/it]\u001b[A\n","5378it [3:06:13,  1.65s/it]\u001b[A\n","5379it [3:06:15,  1.65s/it]\u001b[A\n","5380it [3:06:16,  1.65s/it]\u001b[A\n","5381it [3:06:18,  1.65s/it]\u001b[A\n","5382it [3:06:20,  1.65s/it]\u001b[A\n","5383it [3:06:21,  1.65s/it]\u001b[A\n","5384it [3:06:23,  1.65s/it]\u001b[A\n","5385it [3:06:25,  1.65s/it]\u001b[A\n","5386it [3:06:26,  1.65s/it]\u001b[A\n","5387it [3:06:28,  1.65s/it]\u001b[A\n","5388it [3:06:30,  1.65s/it]\u001b[A\n","5389it [3:06:31,  1.65s/it]\u001b[A\n","5390it [3:06:33,  1.65s/it]\u001b[A\n","5391it [3:06:35,  1.65s/it]\u001b[A\n","5392it [3:06:36,  1.65s/it]\u001b[A\n","5393it [3:06:38,  1.65s/it]\u001b[A\n","5394it [3:06:40,  1.65s/it]\u001b[A\n","5395it [3:06:41,  1.65s/it]\u001b[A\n","5396it [3:06:43,  1.65s/it]\u001b[A11/06/2022 16:06:21 - INFO - src.trainer -   Best dev result: 0.9124733209609985\n","Epoch:  10% 2/20 [3:45:36<33:49:32, 6765.16s/it]"]}],"source":["!source env/bin/activate; TAG=\"v1.0_demo\" TYPE=prompt-demo TASK=spoilers BS=32 LR=1e-5 SEED=21 MODEL=roberta-large bash run_experiment.sh \\\n"," \"--template *cls**sent_0*._It_was*mask*.*sep+* --mapping {0:'relevant',1:'irrelevant'} --first_sent_limit 160 --other_sent_limit 160\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42203024,"status":"ok","timestamp":1667865824379,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":480},"id":"V2A2QUl0Joq-","outputId":"4951b956-33f8-4f96-de21-634c078a1021"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","11/07/2022 12:20:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","11/07/2022 12:20:26 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/v1.0_FT_batch32', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=4500, warmup_steps=0, logging_dir='runs/Nov07_12-20-26_3b9de96eeead', logging_first_step=False, logging_steps=22, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=225, dataloader_num_workers=0, past_index=-1, run_name='result/v1.0_FT_batch32', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=True, save_logit_dir='result/v1.0_FT_batch32', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","11/07/2022 12:20:26 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","11/07/2022 12:20:30 - INFO - src.dataset -   Total num_sample for mode train: 1\n","11/07/2022 12:20:30 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/binary_v1.0/3615-21\n","11/07/2022 12:20:30 - INFO - filelock -   Lock 140151506591568 acquired on data/k-shot/spoilers/binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","11/07/2022 12:20:30 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers [took 0.042 s]\n","11/07/2022 12:20:30 - INFO - filelock -   Lock 140151506591568 released on data/k-shot/spoilers/binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","11/07/2022 12:20:33 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","11/07/2022 12:20:33 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/binary_v1.0/3615-21\n","11/07/2022 12:20:33 - INFO - filelock -   Lock 140151221065488 acquired on data/k-shot/spoilers/binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","11/07/2022 12:20:33 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers [took 0.437 s]\n","11/07/2022 12:20:33 - INFO - filelock -   Lock 140151221065488 released on data/k-shot/spoilers/binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","11/07/2022 12:20:34 - INFO - src.dataset -   *** Example ***\n","11/07/2022 12:20:34 - INFO - src.dataset -   guid: dev-0\n","11/07/2022 12:20:34 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 771, 523, 642, 38, 122, 33, 7036, 14, 51, 351, 17, 27, 90, 28, 37698, 24, 7, 18220, 42, 191, 4, 8133, 44412, 268, 4, 3421, 7, 8402, 9, 5, 2664, 636, 12266, 8, 471, 13, 127, 3627, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=None, label_word_list=None)\n","11/07/2022 12:20:34 - INFO - src.dataset -   text: <s>Welp I now have confirmation that they won‚Äôt be uploading it to Hulu this season. Motherfuckers. Time to dust of the tricorne and head for my ship.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n","11/07/2022 12:20:39 - INFO - src.dataset -   Total num_sample for mode test: 1\n","11/07/2022 12:20:39 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/binary_v1.0/3615-21\n","11/07/2022 12:20:39 - INFO - filelock -   Lock 140151221066128 acquired on data/k-shot/spoilers/binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers.lock\n","11/07/2022 12:20:39 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers [took 0.035 s]\n","11/07/2022 12:20:39 - INFO - filelock -   Lock 140151221066128 released on data/k-shot/spoilers/binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers.lock\n","11/07/2022 12:20:40 - INFO - src.dataset -   *** Example ***\n","11/07/2022 12:20:40 - INFO - src.dataset -   guid: test-0\n","11/07/2022 12:20:40 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 35703, 3996, 24, 4, 152, 3238, 21, 169, 357, 4, 38, 2638, 9925, 1253, 8725, 3860, 7, 2807, 227, 5, 232, 8, 1003, 73, 21290, 284, 4, 1534, 14, 12355, 8810, 37, 21, 35510, 145, 2997, 7, 116, 38, 794, 10, 14061, 15, 69, 15401, 4, 289, 10987, 21, 24282, 7735, 4, 264, 1449, 79, 965, 75, 49, 10295, 8, 172, 161, 79, 29, 202, 5, 8501, 9, 5, 2658, 19006, 4, 264, 332, 14, 70, 5, 1462, 3878, 74, 3999, 2854, 19, 18038, 53, 99, 59, 5, 32440, 6988, 661, 4, 1876, 9, 106, 58, 683, 12260, 2580, 50, 11, 5, 92, 2658, 19006, 8, 51, 1507, 19, 5, 910, 16250, 596, 74, 70, 5, 1462, 3878, 54, 129, 1467, 14, 9187, 3033, 624, 5, 6347, 1032, 13, 5, 1079, 9, 9187, 61, 1654, 106, 11, 42, 1068, 6, 17161, 102, 23737, 642, 14963, 53, 99, 38, 5324, 4, 497, 155, 35, 844, 1236, 12001, 161, 51, 1381, 13, 107, 546, 13, 5274, 4, 10426, 35, 5, 204, 12, 180, 498, 330, 1588, 4, 497, 262, 35, 4390, 289, 10987, 161, 51, 3033, 11, 5, 751, 232, 4, 152, 5072, 3137, 506, 21871, 14, 51, 1381, 15166, 4, 20, 129, 1219, 1368, 10987, 74, 33, 10, 1528, 1219, 7, 213, 74, 28, 7, 2097, 5, 910, 16250, 4, 3771, 31360, 4795, 16, 98, 3953, 24, 95, 14236, 162, 9, 821, 28261, 4795, 4, 38, 21, 2818, 821, 10810, 74, 9, 26, 4, 6553, 47, 192, 14, 1369, 116, 1586, 4691, 16, 25672, 7, 206, 381, 2558, 16, 164, 7, 912, 23, 42, 477, 4, 381, 2558, 16, 533, 7, 8439, 5, 1445, 232, 4, 21198, 2420, 11, 39996, 1026, 16, 10, 1856, 7, 70, 9, 106, 4356, 686, 4, 5363, 10720, 1223, 5101, 13, 99, 37, 222, 21, 3127, 17758, 4, 1223, 5101, 16, 35304, 744, 9287, 23, 70, 498, 38, 5170, 77, 37, 581, 1597, 4, 3791, 5, 3980, 58, 2343, 59, 501, 14200, 498, 1640, 118, 11590, 43, 20, 169, 26968, 4242, 17200, 4102, 5, 6529, 21, 5, 275, 169, 939, 115, 9, 5207, 7, 28, 2781, 4, 290, 73, 698, 13, 162, 44660, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=None, label_word_list=None)\n","11/07/2022 12:20:40 - INFO - src.dataset -   text: <s>Finally watched it. This episode was way better. I loved Jeans inner struggle to choose between the world and himself/future family. Is that Mikasa he was imagining being married to? I saw a scar on her cheek. Hange was kinda weird. She claims she isn't their superior and then says shes still the commander of the survey corps. She points that all the dead soldiers wouldnt agree with genocide but what about the yeagerist. Many of them were once cadets or in the new survey corps and they agreed with the rumbling why would all the dead soldiers who only knew that humanity lived within the walls fight for the rest of humanity which forced them in this situation, Kinda nitpicky but what I noticed. At 3:20 jean says they tried for years looking for answers. Years: the 4-year timeskip. At 7:57 Hange says they lived in the outside world. This basically comfirms that they tried diplomacy. The only reason hange would have a true reason to go would be to prevent the rumbling. Magaths argument is so weak it just reminds me of gabis argument. I was hoping gabi would of said. Did you see that happen? Armin is naive to think Eren is going to stop at this point. Eren is likely to destroy the entire world. Pieck in titan form is a threat to all of them im sure. Jean kicking Reiner for what he did was truly satisfying. Reiner is spitting death flags at all times I wonder when he'll die. Though the trees were shown about 140000 times(i counted) The way Yelena tore apart the alliance was the best way i could of hoped to be delivered. 8/10 for me :)</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","11/07/2022 12:20:57 - INFO - src.trainer -   ***** Running training *****\n","11/07/2022 12:20:57 - INFO - src.trainer -     Num examples = 7230\n","11/07/2022 12:20:57 - INFO - src.trainer -     Num Epochs = 20\n","11/07/2022 12:20:57 - INFO - src.trainer -     Instantaneous batch size per device = 2\n","11/07/2022 12:20:57 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 32\n","11/07/2022 12:20:57 - INFO - src.trainer -     Gradient Accumulation steps = 16\n","11/07/2022 12:20:57 - INFO - src.trainer -     Total optimization steps = 4500\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:   0% 0/20 [00:00<?, ?it/s]11/07/2022 12:24:35 - INFO - src.trainer -   Train loss: 0.6821103962984952\n","11/07/2022 12:28:15 - INFO - src.trainer -   Train loss: 0.6371039910749956\n","11/07/2022 12:31:55 - INFO - src.trainer -   Train loss: 0.5961326252330433\n","11/07/2022 12:35:35 - INFO - src.trainer -   Train loss: 0.5360856489701704\n","11/07/2022 12:39:15 - INFO - src.trainer -   Train loss: 0.5327457081187855\n","11/07/2022 12:42:56 - INFO - src.trainer -   Train loss: 0.5022302107377485\n","11/07/2022 12:46:36 - INFO - src.trainer -   Train loss: 0.46337821266867896\n","11/07/2022 12:50:16 - INFO - src.trainer -   Train loss: 0.47938364202325995\n","11/07/2022 12:53:56 - INFO - src.trainer -   Train loss: 0.4006018205122514\n","11/07/2022 12:57:36 - INFO - src.trainer -   Train loss: 0.41681393710049713\n","\n","  0% 0/169 [00:00<?, ?it/s]\u001b[A\n","  1% 2/169 [00:01<02:13,  1.25it/s]\u001b[A\n","  2% 3/169 [00:03<02:52,  1.04s/it]\u001b[A\n","  2% 4/169 [00:04<03:19,  1.21s/it]\u001b[A\n","  3% 5/169 [00:06<03:37,  1.33s/it]\u001b[A\n","  4% 6/169 [00:08<03:50,  1.41s/it]\u001b[A\n","  4% 7/169 [00:09<03:58,  1.47s/it]\u001b[A\n","  5% 8/169 [00:11<04:03,  1.51s/it]\u001b[A\n","  5% 9/169 [00:12<04:06,  1.54s/it]\u001b[A\n","  6% 10/169 [00:14<04:08,  1.56s/it]\u001b[A\n","  7% 11/169 [00:16<04:09,  1.58s/it]\u001b[A\n","  7% 12/169 [00:17<04:09,  1.59s/it]\u001b[A\n","  8% 13/169 [00:19<04:08,  1.59s/it]\u001b[A\n","  8% 14/169 [00:20<04:07,  1.60s/it]\u001b[A\n","  9% 15/169 [00:22<04:06,  1.60s/it]\u001b[A\n","  9% 16/169 [00:24<04:04,  1.60s/it]\u001b[A\n"," 10% 17/169 [00:25<04:03,  1.60s/it]\u001b[A\n"," 11% 18/169 [00:27<04:01,  1.60s/it]\u001b[A\n"," 11% 19/169 [00:28<04:00,  1.60s/it]\u001b[A\n"," 12% 20/169 [00:30<03:58,  1.60s/it]\u001b[A\n"," 12% 21/169 [00:32<03:57,  1.60s/it]\u001b[A\n"," 13% 22/169 [00:33<03:55,  1.60s/it]\u001b[A\n"," 14% 23/169 [00:35<03:54,  1.61s/it]\u001b[A\n"," 14% 24/169 [00:36<03:53,  1.61s/it]\u001b[A\n"," 15% 25/169 [00:38<03:51,  1.61s/it]\u001b[A\n"," 15% 26/169 [00:40<03:49,  1.61s/it]\u001b[A\n"," 16% 27/169 [00:41<03:48,  1.61s/it]\u001b[A\n"," 17% 28/169 [00:43<03:46,  1.61s/it]\u001b[A\n"," 17% 29/169 [00:44<03:44,  1.60s/it]\u001b[A\n"," 18% 30/169 [00:46<03:43,  1.60s/it]\u001b[A\n"," 18% 31/169 [00:48<03:41,  1.60s/it]\u001b[A\n"," 19% 32/169 [00:49<03:39,  1.61s/it]\u001b[A\n"," 20% 33/169 [00:51<03:38,  1.61s/it]\u001b[A\n"," 20% 34/169 [00:52<03:36,  1.60s/it]\u001b[A\n"," 21% 35/169 [00:54<03:34,  1.60s/it]\u001b[A\n"," 21% 36/169 [00:56<03:33,  1.60s/it]\u001b[A\n"," 22% 37/169 [00:57<03:31,  1.60s/it]\u001b[A\n"," 22% 38/169 [00:59<03:30,  1.60s/it]\u001b[A\n"," 23% 39/169 [01:01<03:28,  1.60s/it]\u001b[A\n"," 24% 40/169 [01:02<03:26,  1.60s/it]\u001b[A\n"," 24% 41/169 [01:04<03:25,  1.61s/it]\u001b[A\n"," 25% 42/169 [01:05<03:24,  1.61s/it]\u001b[A\n"," 25% 43/169 [01:07<03:22,  1.61s/it]\u001b[A\n"," 26% 44/169 [01:09<03:20,  1.61s/it]\u001b[A\n"," 27% 45/169 [01:10<03:19,  1.60s/it]\u001b[A\n"," 27% 46/169 [01:12<03:17,  1.60s/it]\u001b[A\n"," 28% 47/169 [01:13<03:15,  1.60s/it]\u001b[A\n"," 28% 48/169 [01:15<03:14,  1.60s/it]\u001b[A\n"," 29% 49/169 [01:17<03:12,  1.60s/it]\u001b[A\n"," 30% 50/169 [01:18<03:11,  1.61s/it]\u001b[A\n"," 30% 51/169 [01:20<03:09,  1.61s/it]\u001b[A\n"," 31% 52/169 [01:21<03:07,  1.61s/it]\u001b[A\n"," 31% 53/169 [01:23<03:06,  1.61s/it]\u001b[A\n"," 32% 54/169 [01:25<03:04,  1.61s/it]\u001b[A\n"," 33% 55/169 [01:26<03:02,  1.61s/it]\u001b[A\n"," 33% 56/169 [01:28<03:01,  1.60s/it]\u001b[A\n"," 34% 57/169 [01:29<02:59,  1.61s/it]\u001b[A\n"," 34% 58/169 [01:31<02:58,  1.60s/it]\u001b[A\n"," 35% 59/169 [01:33<02:56,  1.60s/it]\u001b[A\n"," 36% 60/169 [01:34<02:54,  1.60s/it]\u001b[A\n"," 36% 61/169 [01:36<02:53,  1.60s/it]\u001b[A\n"," 37% 62/169 [01:37<02:51,  1.60s/it]\u001b[A\n"," 37% 63/169 [01:39<02:49,  1.60s/it]\u001b[A\n"," 38% 64/169 [01:41<02:48,  1.60s/it]\u001b[A\n"," 38% 65/169 [01:42<02:46,  1.60s/it]\u001b[A\n"," 39% 66/169 [01:44<02:45,  1.60s/it]\u001b[A\n"," 40% 67/169 [01:45<02:43,  1.61s/it]\u001b[A\n"," 40% 68/169 [01:47<02:42,  1.61s/it]\u001b[A\n"," 41% 69/169 [01:49<02:41,  1.61s/it]\u001b[A\n"," 41% 70/169 [01:50<02:39,  1.62s/it]\u001b[A\n"," 42% 71/169 [01:52<02:38,  1.62s/it]\u001b[A\n"," 43% 72/169 [01:54<02:36,  1.62s/it]\u001b[A\n"," 43% 73/169 [01:55<02:35,  1.61s/it]\u001b[A\n"," 44% 74/169 [01:57<02:33,  1.62s/it]\u001b[A\n"," 44% 75/169 [01:58<02:32,  1.62s/it]\u001b[A\n"," 45% 76/169 [02:00<02:30,  1.61s/it]\u001b[A\n"," 46% 77/169 [02:02<02:28,  1.62s/it]\u001b[A\n"," 46% 78/169 [02:03<02:27,  1.62s/it]\u001b[A\n"," 47% 79/169 [02:05<02:25,  1.62s/it]\u001b[A\n"," 47% 80/169 [02:06<02:23,  1.62s/it]\u001b[A\n"," 48% 81/169 [02:08<02:22,  1.62s/it]\u001b[A\n"," 49% 82/169 [02:10<02:20,  1.62s/it]\u001b[A\n"," 49% 83/169 [02:11<02:19,  1.62s/it]\u001b[A\n"," 50% 84/169 [02:13<02:17,  1.62s/it]\u001b[A\n"," 50% 85/169 [02:15<02:16,  1.62s/it]\u001b[A\n"," 51% 86/169 [02:16<02:14,  1.62s/it]\u001b[A\n"," 51% 87/169 [02:18<02:12,  1.62s/it]\u001b[A\n"," 52% 88/169 [02:19<02:10,  1.62s/it]\u001b[A\n"," 53% 89/169 [02:21<02:09,  1.62s/it]\u001b[A\n"," 53% 90/169 [02:23<02:07,  1.61s/it]\u001b[A\n"," 54% 91/169 [02:24<02:05,  1.61s/it]\u001b[A\n"," 54% 92/169 [02:26<02:04,  1.61s/it]\u001b[A\n"," 55% 93/169 [02:27<02:02,  1.61s/it]\u001b[A\n"," 56% 94/169 [02:29<02:00,  1.61s/it]\u001b[A\n"," 56% 95/169 [02:31<01:59,  1.61s/it]\u001b[A\n"," 57% 96/169 [02:32<01:57,  1.61s/it]\u001b[A\n"," 57% 97/169 [02:34<01:56,  1.61s/it]\u001b[A\n"," 58% 98/169 [02:36<01:54,  1.61s/it]\u001b[A\n"," 59% 99/169 [02:37<01:52,  1.61s/it]\u001b[A\n"," 59% 100/169 [02:39<01:51,  1.61s/it]\u001b[A\n"," 60% 101/169 [02:40<01:49,  1.61s/it]\u001b[A\n"," 60% 102/169 [02:42<01:48,  1.62s/it]\u001b[A\n"," 61% 103/169 [02:44<01:46,  1.62s/it]\u001b[A\n"," 62% 104/169 [02:45<01:44,  1.61s/it]\u001b[A\n"," 62% 105/169 [02:47<01:43,  1.62s/it]\u001b[A\n"," 63% 106/169 [02:48<01:41,  1.62s/it]\u001b[A\n"," 63% 107/169 [02:50<01:40,  1.62s/it]\u001b[A\n"," 64% 108/169 [02:52<01:38,  1.62s/it]\u001b[A\n"," 64% 109/169 [02:53<01:37,  1.62s/it]\u001b[A\n"," 65% 110/169 [02:55<01:35,  1.62s/it]\u001b[A\n"," 66% 111/169 [02:57<01:33,  1.62s/it]\u001b[A\n"," 66% 112/169 [02:58<01:32,  1.62s/it]\u001b[A\n"," 67% 113/169 [03:00<01:30,  1.62s/it]\u001b[A\n"," 67% 114/169 [03:01<01:28,  1.62s/it]\u001b[A\n"," 68% 115/169 [03:03<01:27,  1.62s/it]\u001b[A\n"," 69% 116/169 [03:05<01:25,  1.62s/it]\u001b[A\n"," 69% 117/169 [03:06<01:24,  1.62s/it]\u001b[A\n"," 70% 118/169 [03:08<01:22,  1.61s/it]\u001b[A\n"," 70% 119/169 [03:10<01:20,  1.62s/it]\u001b[A\n"," 71% 120/169 [03:11<01:19,  1.62s/it]\u001b[A\n"," 72% 121/169 [03:13<01:17,  1.62s/it]\u001b[A\n"," 72% 122/169 [03:14<01:16,  1.62s/it]\u001b[A\n"," 73% 123/169 [03:16<01:14,  1.62s/it]\u001b[A\n"," 73% 124/169 [03:18<01:12,  1.62s/it]\u001b[A\n"," 74% 125/169 [03:19<01:11,  1.62s/it]\u001b[A\n"," 75% 126/169 [03:21<01:09,  1.62s/it]\u001b[A\n"," 75% 127/169 [03:22<01:07,  1.62s/it]\u001b[A\n"," 76% 128/169 [03:24<01:06,  1.62s/it]\u001b[A\n"," 76% 129/169 [03:26<01:04,  1.62s/it]\u001b[A\n"," 77% 130/169 [03:27<01:03,  1.62s/it]\u001b[A\n"," 78% 131/169 [03:29<01:01,  1.62s/it]\u001b[A\n"," 78% 132/169 [03:31<00:59,  1.62s/it]\u001b[A\n"," 79% 133/169 [03:32<00:58,  1.62s/it]\u001b[A\n"," 79% 134/169 [03:34<00:56,  1.62s/it]\u001b[A\n"," 80% 135/169 [03:35<00:55,  1.62s/it]\u001b[A\n"," 80% 136/169 [03:37<00:53,  1.62s/it]\u001b[A\n"," 81% 137/169 [03:39<00:51,  1.62s/it]\u001b[A\n"," 82% 138/169 [03:40<00:50,  1.62s/it]\u001b[A\n"," 82% 139/169 [03:42<00:48,  1.62s/it]\u001b[A\n"," 83% 140/169 [03:44<00:47,  1.62s/it]\u001b[A\n"," 83% 141/169 [03:45<00:45,  1.62s/it]\u001b[A\n"," 84% 142/169 [03:47<00:43,  1.62s/it]\u001b[A\n"," 85% 143/169 [03:48<00:42,  1.62s/it]\u001b[A\n"," 85% 144/169 [03:50<00:40,  1.62s/it]\u001b[A\n"," 86% 145/169 [03:52<00:38,  1.62s/it]\u001b[A\n"," 86% 146/169 [03:53<00:37,  1.62s/it]\u001b[A\n"," 87% 147/169 [03:55<00:35,  1.62s/it]\u001b[A\n"," 88% 148/169 [03:56<00:33,  1.62s/it]\u001b[A\n"," 88% 149/169 [03:58<00:32,  1.62s/it]\u001b[A\n"," 89% 150/169 [04:00<00:30,  1.62s/it]\u001b[A\n"," 89% 151/169 [04:01<00:29,  1.62s/it]\u001b[A\n"," 90% 152/169 [04:03<00:27,  1.62s/it]\u001b[A\n"," 91% 153/169 [04:05<00:25,  1.62s/it]\u001b[A\n"," 91% 154/169 [04:06<00:24,  1.62s/it]\u001b[A\n"," 92% 155/169 [04:08<00:22,  1.62s/it]\u001b[A\n"," 92% 156/169 [04:09<00:21,  1.62s/it]\u001b[A\n"," 93% 157/169 [04:11<00:19,  1.62s/it]\u001b[A\n"," 93% 158/169 [04:13<00:17,  1.62s/it]\u001b[A\n"," 94% 159/169 [04:14<00:16,  1.62s/it]\u001b[A\n"," 95% 160/169 [04:16<00:14,  1.62s/it]\u001b[A\n"," 95% 161/169 [04:18<00:12,  1.62s/it]\u001b[A\n"," 96% 162/169 [04:19<00:11,  1.62s/it]\u001b[A\n"," 96% 163/169 [04:21<00:09,  1.62s/it]\u001b[A\n"," 97% 164/169 [04:22<00:08,  1.62s/it]\u001b[A\n"," 98% 165/169 [04:24<00:06,  1.62s/it]\u001b[A\n"," 98% 166/169 [04:26<00:04,  1.61s/it]\u001b[A\n"," 99% 167/169 [04:27<00:03,  1.62s/it]\u001b[A\n"," 99% 168/169 [04:29<00:01,  1.62s/it]\u001b[A\n","100% 169/169 [04:30<00:00,  1.62s/it]\u001b[A11/07/2022 13:02:58 - INFO - src.trainer -   Best dev result: 0.8959147334098816\n","Epoch:   5% 1/20 [42:16<13:23:18, 2536.76s/it]11/07/2022 13:06:04 - INFO - src.trainer -   Train loss: 0.42280752008611505\n","11/07/2022 13:09:44 - INFO - src.trainer -   Train loss: 0.37874915383078833\n","11/07/2022 13:13:24 - INFO - src.trainer -   Train loss: 0.3669821999289773\n","11/07/2022 13:17:04 - INFO - src.trainer -   Train loss: 0.3950278542258523\n","11/07/2022 13:20:44 - INFO - src.trainer -   Train loss: 0.40900767933238635\n","11/07/2022 13:24:24 - INFO - src.trainer -   Train loss: 0.38058055530894885\n","11/07/2022 13:28:04 - INFO - src.trainer -   Train loss: 0.3557281494140625\n","11/07/2022 13:31:44 - INFO - src.trainer -   Train loss: 0.36163191361860797\n","11/07/2022 13:35:24 - INFO - src.trainer -   Train loss: 0.3726626309481534\n","11/07/2022 13:39:05 - INFO - src.trainer -   Train loss: 0.3598757657137784\n","\n","170it [42:18, 681.34s/it]            \u001b[A\n","171it [42:19, 477.42s/it]\u001b[A\n","172it [42:21, 334.67s/it]\u001b[A\n","173it [42:23, 234.75s/it]\u001b[A\n","174it [42:24, 164.81s/it]\u001b[A\n","175it [42:26, 115.85s/it]\u001b[A\n","176it [42:27, 81.57s/it] \u001b[A\n","177it [42:29, 57.58s/it]\u001b[A\n","178it [42:31, 40.79s/it]\u001b[A\n","179it [42:32, 29.04s/it]\u001b[A\n","180it [42:34, 20.81s/it]\u001b[A\n","181it [42:35, 15.05s/it]\u001b[A\n","182it [42:37, 11.02s/it]\u001b[A\n","183it [42:39,  8.19s/it]\u001b[A\n","184it [42:40,  6.22s/it]\u001b[A\n","185it [42:42,  4.83s/it]\u001b[A\n","186it [42:43,  3.86s/it]\u001b[A\n","187it [42:45,  3.19s/it]\u001b[A\n","188it [42:47,  2.71s/it]\u001b[A\n","189it [42:48,  2.38s/it]\u001b[A\n","190it [42:50,  2.15s/it]\u001b[A\n","191it [42:52,  1.98s/it]\u001b[A\n","192it [42:53,  1.87s/it]\u001b[A\n","193it [42:55,  1.80s/it]\u001b[A\n","194it [42:56,  1.74s/it]\u001b[A\n","195it [42:58,  1.70s/it]\u001b[A\n","196it [43:00,  1.67s/it]\u001b[A\n","197it [43:01,  1.65s/it]\u001b[A\n","198it [43:03,  1.64s/it]\u001b[A\n","199it [43:04,  1.63s/it]\u001b[A\n","200it [43:06,  1.62s/it]\u001b[A\n","201it [43:08,  1.62s/it]\u001b[A\n","202it [43:09,  1.61s/it]\u001b[A\n","203it [43:11,  1.61s/it]\u001b[A\n","204it [43:12,  1.61s/it]\u001b[A\n","205it [43:14,  1.60s/it]\u001b[A\n","206it [43:16,  1.61s/it]\u001b[A\n","207it [43:17,  1.61s/it]\u001b[A\n","208it [43:19,  1.60s/it]\u001b[A\n","209it [43:20,  1.60s/it]\u001b[A\n","210it [43:22,  1.61s/it]\u001b[A\n","211it [43:24,  1.61s/it]\u001b[A\n","212it [43:25,  1.61s/it]\u001b[A\n","213it [43:27,  1.61s/it]\u001b[A\n","214it [43:28,  1.61s/it]\u001b[A\n","215it [43:30,  1.60s/it]\u001b[A\n","216it [43:32,  1.60s/it]\u001b[A\n","217it [43:33,  1.60s/it]\u001b[A\n","218it [43:35,  1.60s/it]\u001b[A\n","219it [43:36,  1.61s/it]\u001b[A\n","220it [43:38,  1.61s/it]\u001b[A\n","221it [43:40,  1.61s/it]\u001b[A\n","222it [43:41,  1.61s/it]\u001b[A\n","223it [43:43,  1.61s/it]\u001b[A\n","224it [43:45,  1.61s/it]\u001b[A\n","225it [43:46,  1.61s/it]\u001b[A\n","226it [43:48,  1.61s/it]\u001b[A\n","227it [43:49,  1.61s/it]\u001b[A\n","228it [43:51,  1.61s/it]\u001b[A\n","229it [43:53,  1.61s/it]\u001b[A\n","230it [43:54,  1.61s/it]\u001b[A\n","231it [43:56,  1.61s/it]\u001b[A\n","232it [43:57,  1.61s/it]\u001b[A\n","233it [43:59,  1.61s/it]\u001b[A\n","234it [44:01,  1.61s/it]\u001b[A\n","235it [44:02,  1.61s/it]\u001b[A\n","236it [44:04,  1.61s/it]\u001b[A\n","237it [44:05,  1.61s/it]\u001b[A\n","238it [44:07,  1.62s/it]\u001b[A\n","239it [44:09,  1.62s/it]\u001b[A\n","240it [44:10,  1.62s/it]\u001b[A\n","241it [44:12,  1.62s/it]\u001b[A\n","242it [44:14,  1.62s/it]\u001b[A\n","243it [44:15,  1.62s/it]\u001b[A\n","244it [44:17,  1.62s/it]\u001b[A\n","245it [44:18,  1.62s/it]\u001b[A\n","246it [44:20,  1.62s/it]\u001b[A\n","247it [44:22,  1.62s/it]\u001b[A\n","248it [44:23,  1.62s/it]\u001b[A\n","249it [44:25,  1.62s/it]\u001b[A\n","250it [44:26,  1.62s/it]\u001b[A\n","251it [44:28,  1.62s/it]\u001b[A\n","252it [44:30,  1.62s/it]\u001b[A\n","253it [44:31,  1.62s/it]\u001b[A\n","254it [44:33,  1.62s/it]\u001b[A\n","255it [44:35,  1.62s/it]\u001b[A\n","256it [44:36,  1.62s/it]\u001b[A\n","257it [44:38,  1.62s/it]\u001b[A\n","258it [44:39,  1.62s/it]\u001b[A\n","259it [44:41,  1.62s/it]\u001b[A\n","260it [44:43,  1.62s/it]\u001b[A\n","261it [44:44,  1.62s/it]\u001b[A\n","262it [44:46,  1.61s/it]\u001b[A\n","263it [44:48,  1.61s/it]\u001b[A\n","264it [44:49,  1.62s/it]\u001b[A\n","265it [44:51,  1.62s/it]\u001b[A\n","266it [44:52,  1.62s/it]\u001b[A\n","267it [44:54,  1.62s/it]\u001b[A\n","268it [44:56,  1.62s/it]\u001b[A\n","269it [44:57,  1.62s/it]\u001b[A\n","270it [44:59,  1.62s/it]\u001b[A\n","271it [45:00,  1.62s/it]\u001b[A\n","272it [45:02,  1.62s/it]\u001b[A\n","273it [45:04,  1.62s/it]\u001b[A\n","274it [45:05,  1.62s/it]\u001b[A\n","275it [45:07,  1.62s/it]\u001b[A\n","276it [45:09,  1.63s/it]\u001b[A\n","277it [45:10,  1.62s/it]\u001b[A\n","278it [45:12,  1.62s/it]\u001b[A\n","279it [45:13,  1.62s/it]\u001b[A\n","280it [45:15,  1.63s/it]\u001b[A\n","281it [45:17,  1.62s/it]\u001b[A\n","282it [45:18,  1.62s/it]\u001b[A\n","283it [45:20,  1.62s/it]\u001b[A\n","284it [45:22,  1.62s/it]\u001b[A\n","285it [45:23,  1.62s/it]\u001b[A\n","286it [45:25,  1.62s/it]\u001b[A\n","287it [45:26,  1.62s/it]\u001b[A\n","288it [45:28,  1.62s/it]\u001b[A\n","289it [45:30,  1.62s/it]\u001b[A\n","290it [45:31,  1.62s/it]\u001b[A\n","291it [45:33,  1.62s/it]\u001b[A\n","292it [45:35,  1.62s/it]\u001b[A\n","293it [45:36,  1.62s/it]\u001b[A\n","294it [45:38,  1.62s/it]\u001b[A\n","295it [45:39,  1.62s/it]\u001b[A\n","296it [45:41,  1.62s/it]\u001b[A\n","297it [45:43,  1.62s/it]\u001b[A\n","298it [45:44,  1.62s/it]\u001b[A\n","299it [45:46,  1.62s/it]\u001b[A\n","300it [45:47,  1.62s/it]\u001b[A\n","301it [45:49,  1.62s/it]\u001b[A\n","302it [45:51,  1.62s/it]\u001b[A\n","303it [45:52,  1.62s/it]\u001b[A\n","304it [45:54,  1.62s/it]\u001b[A\n","305it [45:56,  1.62s/it]\u001b[A\n","306it [45:57,  1.62s/it]\u001b[A\n","307it [45:59,  1.62s/it]\u001b[A\n","308it [46:00,  1.62s/it]\u001b[A\n","309it [46:02,  1.62s/it]\u001b[A\n","310it [46:04,  1.62s/it]\u001b[A\n","311it [46:05,  1.62s/it]\u001b[A\n","312it [46:07,  1.62s/it]\u001b[A\n","313it [46:09,  1.62s/it]\u001b[A\n","314it [46:10,  1.62s/it]\u001b[A\n","315it [46:12,  1.62s/it]\u001b[A\n","316it [46:13,  1.62s/it]\u001b[A\n","317it [46:15,  1.62s/it]\u001b[A\n","318it [46:17,  1.62s/it]\u001b[A\n","319it [46:18,  1.63s/it]\u001b[A\n","320it [46:20,  1.62s/it]\u001b[A\n","321it [46:22,  1.63s/it]\u001b[A\n","322it [46:23,  1.62s/it]\u001b[A\n","323it [46:25,  1.63s/it]\u001b[A\n","324it [46:26,  1.62s/it]\u001b[A\n","325it [46:28,  1.62s/it]\u001b[A\n","326it [46:30,  1.62s/it]\u001b[A\n","327it [46:31,  1.62s/it]\u001b[A\n","328it [46:33,  1.62s/it]\u001b[A\n","329it [46:35,  1.62s/it]\u001b[A\n","330it [46:36,  1.62s/it]\u001b[A\n","331it [46:38,  1.62s/it]\u001b[A\n","332it [46:39,  1.62s/it]\u001b[A\n","333it [46:41,  1.62s/it]\u001b[A\n","334it [46:43,  1.62s/it]\u001b[A\n","335it [46:44,  1.62s/it]\u001b[A\n","336it [46:46,  1.62s/it]\u001b[A\n","337it [46:47,  1.62s/it]\u001b[A\n","338it [46:49,  1.62s/it]\u001b[A11/07/2022 13:45:17 - INFO - src.trainer -   Best dev result: 0.9070557951927185\n","Epoch:  10% 2/20 [1:24:35<12:41:12, 2537.36s/it]11/07/2022 13:47:33 - INFO - src.trainer -   Train loss: 0.35817024924538354\n","11/07/2022 13:51:13 - INFO - src.trainer -   Train loss: 0.29732513427734375\n","11/07/2022 13:54:53 - INFO - src.trainer -   Train loss: 0.28985456986860797\n","11/07/2022 13:58:33 - INFO - src.trainer -   Train loss: 0.31877621737393463\n","11/07/2022 14:02:13 - INFO - src.trainer -   Train loss: 0.3168917569247159\n","11/07/2022 14:05:53 - INFO - src.trainer -   Train loss: 0.2965261285955256\n","11/07/2022 14:09:33 - INFO - src.trainer -   Train loss: 0.2786074551669034\n","11/07/2022 14:13:13 - INFO - src.trainer -   Train loss: 0.27650521018288354\n","11/07/2022 14:16:54 - INFO - src.trainer -   Train loss: 0.32933807373046875\n","11/07/2022 14:20:33 - INFO - src.trainer -   Train loss: 0.3210990212180398\n","\n","339it [1:24:37, 681.41s/it]\u001b[A\n","340it [1:24:38, 477.47s/it]\u001b[A\n","341it [1:24:40, 334.71s/it]\u001b[A\n","342it [1:24:41, 234.78s/it]\u001b[A\n","343it [1:24:43, 164.82s/it]\u001b[A\n","344it [1:24:45, 115.86s/it]\u001b[A\n","345it [1:24:46, 81.58s/it] \u001b[A\n","346it [1:24:48, 57.59s/it]\u001b[A\n","347it [1:24:50, 40.79s/it]\u001b[A\n","348it [1:24:51, 29.04s/it]\u001b[A\n","349it [1:24:53, 20.81s/it]\u001b[A\n","350it [1:24:54, 15.05s/it]\u001b[A\n","351it [1:24:56, 11.02s/it]\u001b[A\n","352it [1:24:58,  8.19s/it]\u001b[A\n","353it [1:24:59,  6.21s/it]\u001b[A\n","354it [1:25:01,  4.83s/it]\u001b[A\n","355it [1:25:02,  3.86s/it]\u001b[A\n","356it [1:25:04,  3.18s/it]\u001b[A\n","357it [1:25:06,  2.71s/it]\u001b[A\n","358it [1:25:07,  2.38s/it]\u001b[A\n","359it [1:25:09,  2.14s/it]\u001b[A\n","360it [1:25:10,  1.98s/it]\u001b[A\n","361it [1:25:12,  1.87s/it]\u001b[A\n","362it [1:25:14,  1.79s/it]\u001b[A\n","363it [1:25:15,  1.73s/it]\u001b[A\n","364it [1:25:17,  1.69s/it]\u001b[A\n","365it [1:25:18,  1.67s/it]\u001b[A\n","366it [1:25:20,  1.65s/it]\u001b[A\n","367it [1:25:22,  1.63s/it]\u001b[A\n","368it [1:25:23,  1.63s/it]\u001b[A\n","369it [1:25:25,  1.62s/it]\u001b[A\n","370it [1:25:26,  1.61s/it]\u001b[A\n","371it [1:25:28,  1.61s/it]\u001b[A\n","372it [1:25:30,  1.61s/it]\u001b[A\n","373it [1:25:31,  1.61s/it]\u001b[A\n","374it [1:25:33,  1.61s/it]\u001b[A\n","375it [1:25:34,  1.61s/it]\u001b[A\n","376it [1:25:36,  1.61s/it]\u001b[A\n","377it [1:25:38,  1.60s/it]\u001b[A\n","378it [1:25:39,  1.60s/it]\u001b[A\n","379it [1:25:41,  1.61s/it]\u001b[A\n","380it [1:25:42,  1.61s/it]\u001b[A\n","381it [1:25:44,  1.61s/it]\u001b[A\n","382it [1:25:46,  1.61s/it]\u001b[A\n","383it [1:25:47,  1.61s/it]\u001b[A\n","384it [1:25:49,  1.61s/it]\u001b[A\n","385it [1:25:50,  1.61s/it]\u001b[A\n","386it [1:25:52,  1.61s/it]\u001b[A\n","387it [1:25:54,  1.61s/it]\u001b[A\n","388it [1:25:55,  1.61s/it]\u001b[A\n","389it [1:25:57,  1.61s/it]\u001b[A\n","390it [1:25:59,  1.61s/it]\u001b[A\n","391it [1:26:00,  1.61s/it]\u001b[A\n","392it [1:26:02,  1.61s/it]\u001b[A\n","393it [1:26:03,  1.61s/it]\u001b[A\n","394it [1:26:05,  1.61s/it]\u001b[A\n","395it [1:26:07,  1.61s/it]\u001b[A\n","396it [1:26:08,  1.61s/it]\u001b[A\n","397it [1:26:10,  1.61s/it]\u001b[A\n","398it [1:26:11,  1.61s/it]\u001b[A\n","399it [1:26:13,  1.61s/it]\u001b[A\n","400it [1:26:15,  1.61s/it]\u001b[A\n","401it [1:26:16,  1.61s/it]\u001b[A\n","402it [1:26:18,  1.61s/it]\u001b[A\n","403it [1:26:19,  1.60s/it]\u001b[A\n","404it [1:26:21,  1.61s/it]\u001b[A\n","405it [1:26:23,  1.61s/it]\u001b[A\n","406it [1:26:24,  1.61s/it]\u001b[A\n","407it [1:26:26,  1.62s/it]\u001b[A\n","408it [1:26:27,  1.62s/it]\u001b[A\n","409it [1:26:29,  1.62s/it]\u001b[A\n","410it [1:26:31,  1.62s/it]\u001b[A\n","411it [1:26:32,  1.62s/it]\u001b[A\n","412it [1:26:34,  1.62s/it]\u001b[A\n","413it [1:26:36,  1.62s/it]\u001b[A\n","414it [1:26:37,  1.62s/it]\u001b[A\n","415it [1:26:39,  1.62s/it]\u001b[A\n","416it [1:26:40,  1.62s/it]\u001b[A\n","417it [1:26:42,  1.62s/it]\u001b[A\n","418it [1:26:44,  1.62s/it]\u001b[A\n","419it [1:26:45,  1.62s/it]\u001b[A\n","420it [1:26:47,  1.62s/it]\u001b[A\n","421it [1:26:49,  1.62s/it]\u001b[A\n","422it [1:26:50,  1.62s/it]\u001b[A\n","423it [1:26:52,  1.62s/it]\u001b[A\n","424it [1:26:53,  1.62s/it]\u001b[A\n","425it [1:26:55,  1.62s/it]\u001b[A\n","426it [1:26:57,  1.62s/it]\u001b[A\n","427it [1:26:58,  1.62s/it]\u001b[A\n","428it [1:27:00,  1.62s/it]\u001b[A\n","429it [1:27:02,  1.62s/it]\u001b[A\n","430it [1:27:03,  1.62s/it]\u001b[A\n","431it [1:27:05,  1.62s/it]\u001b[A\n","432it [1:27:06,  1.62s/it]\u001b[A\n","433it [1:27:08,  1.62s/it]\u001b[A\n","434it [1:27:10,  1.62s/it]\u001b[A\n","435it [1:27:11,  1.62s/it]\u001b[A\n","436it [1:27:13,  1.62s/it]\u001b[A\n","437it [1:27:14,  1.62s/it]\u001b[A\n","438it [1:27:16,  1.62s/it]\u001b[A\n","439it [1:27:18,  1.62s/it]\u001b[A\n","440it [1:27:19,  1.62s/it]\u001b[A\n","441it [1:27:21,  1.62s/it]\u001b[A\n","442it [1:27:23,  1.62s/it]\u001b[A\n","443it [1:27:24,  1.62s/it]\u001b[A\n","444it [1:27:26,  1.62s/it]\u001b[A\n","445it [1:27:27,  1.63s/it]\u001b[A\n","446it [1:27:29,  1.62s/it]\u001b[A\n","447it [1:27:31,  1.62s/it]\u001b[A\n","448it [1:27:32,  1.62s/it]\u001b[A\n","449it [1:27:34,  1.62s/it]\u001b[A\n","450it [1:27:36,  1.62s/it]\u001b[A\n","451it [1:27:37,  1.62s/it]\u001b[A\n","452it [1:27:39,  1.62s/it]\u001b[A\n","453it [1:27:40,  1.62s/it]\u001b[A\n","454it [1:27:42,  1.62s/it]\u001b[A\n","455it [1:27:44,  1.62s/it]\u001b[A\n","456it [1:27:45,  1.62s/it]\u001b[A\n","457it [1:27:47,  1.62s/it]\u001b[A\n","458it [1:27:49,  1.62s/it]\u001b[A\n","459it [1:27:50,  1.62s/it]\u001b[A\n","460it [1:27:52,  1.62s/it]\u001b[A\n","461it [1:27:53,  1.62s/it]\u001b[A\n","462it [1:27:55,  1.62s/it]\u001b[A\n","463it [1:27:57,  1.62s/it]\u001b[A\n","464it [1:27:58,  1.62s/it]\u001b[A\n","465it [1:28:00,  1.62s/it]\u001b[A\n","466it [1:28:01,  1.62s/it]\u001b[A\n","467it [1:28:03,  1.62s/it]\u001b[A\n","468it [1:28:05,  1.62s/it]\u001b[A\n","469it [1:28:06,  1.62s/it]\u001b[A\n","470it [1:28:08,  1.62s/it]\u001b[A\n","471it [1:28:10,  1.62s/it]\u001b[A\n","472it [1:28:11,  1.62s/it]\u001b[A\n","473it [1:28:13,  1.62s/it]\u001b[A\n","474it [1:28:14,  1.62s/it]\u001b[A\n","475it [1:28:16,  1.62s/it]\u001b[A\n","476it [1:28:18,  1.62s/it]\u001b[A\n","477it [1:28:19,  1.62s/it]\u001b[A\n","478it [1:28:21,  1.62s/it]\u001b[A\n","479it [1:28:23,  1.62s/it]\u001b[A\n","480it [1:28:24,  1.62s/it]\u001b[A\n","481it [1:28:26,  1.62s/it]\u001b[A\n","482it [1:28:27,  1.62s/it]\u001b[A\n","483it [1:28:29,  1.62s/it]\u001b[A\n","484it [1:28:31,  1.62s/it]\u001b[A\n","485it [1:28:32,  1.62s/it]\u001b[A\n","486it [1:28:34,  1.62s/it]\u001b[A\n","487it [1:28:36,  1.62s/it]\u001b[A\n","488it [1:28:37,  1.62s/it]\u001b[A\n","489it [1:28:39,  1.62s/it]\u001b[A\n","490it [1:28:40,  1.62s/it]\u001b[A\n","491it [1:28:42,  1.62s/it]\u001b[A\n","492it [1:28:44,  1.62s/it]\u001b[A\n","493it [1:28:45,  1.62s/it]\u001b[A\n","494it [1:28:47,  1.62s/it]\u001b[A\n","495it [1:28:48,  1.62s/it]\u001b[A\n","496it [1:28:50,  1.62s/it]\u001b[A\n","497it [1:28:52,  1.62s/it]\u001b[A\n","498it [1:28:53,  1.62s/it]\u001b[A\n","499it [1:28:55,  1.62s/it]\u001b[A\n","500it [1:28:57,  1.62s/it]\u001b[A\n","501it [1:28:58,  1.62s/it]\u001b[A\n","502it [1:29:00,  1.62s/it]\u001b[A\n","503it [1:29:01,  1.62s/it]\u001b[A\n","504it [1:29:03,  1.62s/it]\u001b[A\n","505it [1:29:05,  1.62s/it]\u001b[A\n","506it [1:29:06,  1.62s/it]\u001b[A\n","Epoch:  15% 3/20 [2:06:48<11:58:31, 2535.98s/it]11/07/2022 14:28:55 - INFO - src.trainer -   Train loss: 0.2619712136008523\n","11/07/2022 14:32:36 - INFO - src.trainer -   Train loss: 0.1998929110440341\n","11/07/2022 14:36:16 - INFO - src.trainer -   Train loss: 0.21615461869673294\n","11/07/2022 14:39:56 - INFO - src.trainer -   Train loss: 0.18711991743607956\n","11/07/2022 14:43:36 - INFO - src.trainer -   Train loss: 0.2523859197443182\n","11/07/2022 14:47:16 - INFO - src.trainer -   Train loss: 0.22937844016335227\n","11/07/2022 14:50:57 - INFO - src.trainer -   Train loss: 0.22415993430397727\n","11/07/2022 14:54:37 - INFO - src.trainer -   Train loss: 0.1971310702237216\n","11/07/2022 14:58:17 - INFO - src.trainer -   Train loss: 0.21614213423295456\n","11/07/2022 15:01:57 - INFO - src.trainer -   Train loss: 0.22076416015625\n","\n","508it [2:06:50, 679.75s/it]\u001b[A\n","509it [2:06:52, 476.30s/it]\u001b[A\n","510it [2:06:53, 333.89s/it]\u001b[A\n","511it [2:06:55, 234.21s/it]\u001b[A\n","512it [2:06:56, 164.43s/it]\u001b[A\n","513it [2:06:58, 115.58s/it]\u001b[A\n","514it [2:07:00, 81.39s/it] \u001b[A\n","515it [2:07:01, 57.45s/it]\u001b[A\n","516it [2:07:03, 40.70s/it]\u001b[A\n","517it [2:07:04, 28.97s/it]\u001b[A\n","518it [2:07:06, 20.76s/it]\u001b[A\n","519it [2:07:08, 15.02s/it]\u001b[A\n","520it [2:07:09, 10.99s/it]\u001b[A\n","521it [2:07:11,  8.18s/it]\u001b[A\n","522it [2:07:12,  6.20s/it]\u001b[A\n","523it [2:07:14,  4.83s/it]\u001b[A\n","524it [2:07:16,  3.86s/it]\u001b[A\n","525it [2:07:17,  3.18s/it]\u001b[A\n","526it [2:07:19,  2.71s/it]\u001b[A\n","527it [2:07:20,  2.38s/it]\u001b[A\n","528it [2:07:22,  2.14s/it]\u001b[A\n","529it [2:07:24,  1.98s/it]\u001b[A\n","530it [2:07:25,  1.87s/it]\u001b[A\n","531it [2:07:27,  1.79s/it]\u001b[A\n","532it [2:07:29,  1.74s/it]\u001b[A\n","533it [2:07:30,  1.70s/it]\u001b[A\n","534it [2:07:32,  1.67s/it]\u001b[A\n","535it [2:07:33,  1.65s/it]\u001b[A\n","536it [2:07:35,  1.64s/it]\u001b[A\n","537it [2:07:37,  1.63s/it]\u001b[A\n","538it [2:07:38,  1.62s/it]\u001b[A\n","539it [2:07:40,  1.61s/it]\u001b[A\n","540it [2:07:41,  1.61s/it]\u001b[A\n","541it [2:07:43,  1.61s/it]\u001b[A\n","542it [2:07:45,  1.61s/it]\u001b[A\n","543it [2:07:46,  1.61s/it]\u001b[A\n","544it [2:07:48,  1.60s/it]\u001b[A\n","545it [2:07:49,  1.61s/it]\u001b[A\n","546it [2:07:51,  1.61s/it]\u001b[A\n","547it [2:07:53,  1.61s/it]\u001b[A\n","548it [2:07:54,  1.61s/it]\u001b[A\n","549it [2:07:56,  1.61s/it]\u001b[A\n","550it [2:07:57,  1.61s/it]\u001b[A\n","551it [2:07:59,  1.61s/it]\u001b[A\n","552it [2:08:01,  1.61s/it]\u001b[A\n","553it [2:08:02,  1.61s/it]\u001b[A\n","554it [2:08:04,  1.61s/it]\u001b[A\n","555it [2:08:05,  1.61s/it]\u001b[A\n","556it [2:08:07,  1.60s/it]\u001b[A\n","557it [2:08:09,  1.61s/it]\u001b[A\n","558it [2:08:10,  1.61s/it]\u001b[A\n","559it [2:08:12,  1.61s/it]\u001b[A\n","560it [2:08:13,  1.61s/it]\u001b[A\n","561it [2:08:15,  1.61s/it]\u001b[A\n","562it [2:08:17,  1.61s/it]\u001b[A\n","563it [2:08:18,  1.61s/it]\u001b[A\n","564it [2:08:20,  1.61s/it]\u001b[A\n","565it [2:08:21,  1.61s/it]\u001b[A\n","566it [2:08:23,  1.61s/it]\u001b[A\n","567it [2:08:25,  1.61s/it]\u001b[A\n","568it [2:08:26,  1.61s/it]\u001b[A\n","569it [2:08:28,  1.60s/it]\u001b[A\n","570it [2:08:30,  1.60s/it]\u001b[A\n","571it [2:08:31,  1.60s/it]\u001b[A\n","572it [2:08:33,  1.61s/it]\u001b[A\n","573it [2:08:34,  1.61s/it]\u001b[A\n","574it [2:08:36,  1.61s/it]\u001b[A\n","575it [2:08:38,  1.61s/it]\u001b[A\n","576it [2:08:39,  1.61s/it]\u001b[A\n","577it [2:08:41,  1.62s/it]\u001b[A\n","578it [2:08:42,  1.62s/it]\u001b[A\n","579it [2:08:44,  1.62s/it]\u001b[A\n","580it [2:08:46,  1.62s/it]\u001b[A\n","581it [2:08:47,  1.62s/it]\u001b[A\n","582it [2:08:49,  1.62s/it]\u001b[A\n","583it [2:08:51,  1.62s/it]\u001b[A\n","584it [2:08:52,  1.62s/it]\u001b[A\n","585it [2:08:54,  1.62s/it]\u001b[A\n","586it [2:08:55,  1.62s/it]\u001b[A\n","587it [2:08:57,  1.62s/it]\u001b[A\n","588it [2:08:59,  1.62s/it]\u001b[A\n","589it [2:09:00,  1.62s/it]\u001b[A\n","590it [2:09:02,  1.62s/it]\u001b[A\n","591it [2:09:04,  1.62s/it]\u001b[A\n","592it [2:09:05,  1.62s/it]\u001b[A\n","593it [2:09:07,  1.62s/it]\u001b[A\n","594it [2:09:08,  1.62s/it]\u001b[A\n","595it [2:09:10,  1.62s/it]\u001b[A\n","596it [2:09:12,  1.62s/it]\u001b[A\n","597it [2:09:13,  1.62s/it]\u001b[A\n","598it [2:09:15,  1.62s/it]\u001b[A\n","599it [2:09:16,  1.62s/it]\u001b[A\n","600it [2:09:18,  1.62s/it]\u001b[A\n","601it [2:09:20,  1.62s/it]\u001b[A\n","602it [2:09:21,  1.62s/it]\u001b[A\n","603it [2:09:23,  1.62s/it]\u001b[A\n","604it [2:09:25,  1.62s/it]\u001b[A\n","605it [2:09:26,  1.62s/it]\u001b[A\n","606it [2:09:28,  1.62s/it]\u001b[A\n","607it [2:09:29,  1.62s/it]\u001b[A\n","608it [2:09:31,  1.62s/it]\u001b[A\n","609it [2:09:33,  1.62s/it]\u001b[A\n","610it [2:09:34,  1.62s/it]\u001b[A\n","611it [2:09:36,  1.62s/it]\u001b[A\n","612it [2:09:37,  1.62s/it]\u001b[A\n","613it [2:09:39,  1.62s/it]\u001b[A\n","614it [2:09:41,  1.62s/it]\u001b[A\n","615it [2:09:42,  1.62s/it]\u001b[A\n","616it [2:09:44,  1.62s/it]\u001b[A\n","617it [2:09:46,  1.62s/it]\u001b[A\n","618it [2:09:47,  1.62s/it]\u001b[A\n","619it [2:09:49,  1.62s/it]\u001b[A\n","620it [2:09:50,  1.62s/it]\u001b[A\n","621it [2:09:52,  1.62s/it]\u001b[A\n","622it [2:09:54,  1.62s/it]\u001b[A\n","623it [2:09:55,  1.62s/it]\u001b[A\n","624it [2:09:57,  1.62s/it]\u001b[A\n","625it [2:09:59,  1.62s/it]\u001b[A\n","626it [2:10:00,  1.62s/it]\u001b[A\n","627it [2:10:02,  1.62s/it]\u001b[A\n","628it [2:10:03,  1.62s/it]\u001b[A\n","629it [2:10:05,  1.62s/it]\u001b[A\n","630it [2:10:07,  1.62s/it]\u001b[A\n","631it [2:10:08,  1.62s/it]\u001b[A\n","632it [2:10:10,  1.62s/it]\u001b[A\n","633it [2:10:12,  1.62s/it]\u001b[A\n","634it [2:10:13,  1.62s/it]\u001b[A\n","635it [2:10:15,  1.62s/it]\u001b[A\n","636it [2:10:16,  1.62s/it]\u001b[A\n","637it [2:10:18,  1.62s/it]\u001b[A\n","638it [2:10:20,  1.62s/it]\u001b[A\n","639it [2:10:21,  1.62s/it]\u001b[A\n","640it [2:10:23,  1.62s/it]\u001b[A\n","641it [2:10:24,  1.62s/it]\u001b[A\n","642it [2:10:26,  1.62s/it]\u001b[A\n","643it [2:10:28,  1.62s/it]\u001b[A\n","644it [2:10:29,  1.62s/it]\u001b[A\n","645it [2:10:31,  1.62s/it]\u001b[A\n","646it [2:10:33,  1.62s/it]\u001b[A\n","647it [2:10:34,  1.62s/it]\u001b[A\n","648it [2:10:36,  1.62s/it]\u001b[A\n","649it [2:10:37,  1.62s/it]\u001b[A\n","650it [2:10:39,  1.62s/it]\u001b[A\n","651it [2:10:41,  1.62s/it]\u001b[A\n","652it [2:10:42,  1.62s/it]\u001b[A\n","653it [2:10:44,  1.62s/it]\u001b[A\n","654it [2:10:46,  1.62s/it]\u001b[A\n","655it [2:10:47,  1.62s/it]\u001b[A\n","656it [2:10:49,  1.62s/it]\u001b[A\n","657it [2:10:50,  1.62s/it]\u001b[A\n","658it [2:10:52,  1.62s/it]\u001b[A\n","659it [2:10:54,  1.62s/it]\u001b[A\n","660it [2:10:55,  1.62s/it]\u001b[A\n","661it [2:10:57,  1.62s/it]\u001b[A\n","662it [2:10:58,  1.62s/it]\u001b[A\n","663it [2:11:00,  1.62s/it]\u001b[A\n","664it [2:11:02,  1.62s/it]\u001b[A\n","665it [2:11:03,  1.62s/it]\u001b[A\n","666it [2:11:05,  1.62s/it]\u001b[A\n","667it [2:11:07,  1.62s/it]\u001b[A\n","668it [2:11:08,  1.62s/it]\u001b[A\n","669it [2:11:10,  1.62s/it]\u001b[A\n","670it [2:11:11,  1.62s/it]\u001b[A\n","671it [2:11:13,  1.62s/it]\u001b[A\n","672it [2:11:15,  1.62s/it]\u001b[A\n","673it [2:11:16,  1.62s/it]\u001b[A\n","674it [2:11:18,  1.62s/it]\u001b[A\n","675it [2:11:20,  1.62s/it]\u001b[A\n","Epoch:  20% 4/20 [2:49:01<11:16:02, 2535.14s/it]11/07/2022 15:10:18 - INFO - src.trainer -   Train loss: 0.22661521218039773\n","11/07/2022 15:13:58 - INFO - src.trainer -   Train loss: 0.12685186212713068\n","11/07/2022 15:17:38 - INFO - src.trainer -   Train loss: 0.16457020152698865\n","11/07/2022 15:21:19 - INFO - src.trainer -   Train loss: 0.12851507013494318\n","11/07/2022 15:24:59 - INFO - src.trainer -   Train loss: 0.13800464976917615\n","11/07/2022 15:28:39 - INFO - src.trainer -   Train loss: 0.1762209805575284\n","11/07/2022 15:32:19 - INFO - src.trainer -   Train loss: 0.1573056307705966\n","11/07/2022 15:35:59 - INFO - src.trainer -   Train loss: 0.17128823020241477\n","11/07/2022 15:39:39 - INFO - src.trainer -   Train loss: 0.14044189453125\n","11/07/2022 15:43:19 - INFO - src.trainer -   Train loss: 0.11548822576349432\n","11/07/2022 15:46:59 - INFO - src.trainer -   Train loss: 0.15161271528764206\n","\n","677it [2:49:02, 679.48s/it]\u001b[A\n","678it [2:49:04, 476.12s/it]\u001b[A\n","679it [2:49:06, 333.77s/it]\u001b[A\n","680it [2:49:07, 234.12s/it]\u001b[A\n","681it [2:49:09, 164.36s/it]\u001b[A\n","682it [2:49:10, 115.53s/it]\u001b[A\n","683it [2:49:12, 81.35s/it] \u001b[A\n","684it [2:49:14, 57.43s/it]\u001b[A\n","685it [2:49:15, 40.68s/it]\u001b[A\n","686it [2:49:17, 28.96s/it]\u001b[A\n","687it [2:49:18, 20.75s/it]\u001b[A\n","688it [2:49:20, 15.01s/it]\u001b[A\n","689it [2:49:22, 10.99s/it]\u001b[A\n","690it [2:49:23,  8.17s/it]\u001b[A\n","691it [2:49:25,  6.20s/it]\u001b[A\n","692it [2:49:26,  4.82s/it]\u001b[A\n","693it [2:49:28,  3.86s/it]\u001b[A\n","694it [2:49:30,  3.18s/it]\u001b[A\n","695it [2:49:31,  2.71s/it]\u001b[A\n","696it [2:49:33,  2.37s/it]\u001b[A\n","697it [2:49:34,  2.14s/it]\u001b[A\n","698it [2:49:36,  1.98s/it]\u001b[A\n","699it [2:49:38,  1.87s/it]\u001b[A\n","700it [2:49:39,  1.79s/it]\u001b[A\n","701it [2:49:41,  1.73s/it]\u001b[A\n","702it [2:49:42,  1.70s/it]\u001b[A\n","703it [2:49:44,  1.67s/it]\u001b[A\n","704it [2:49:46,  1.65s/it]\u001b[A\n","705it [2:49:47,  1.63s/it]\u001b[A\n","706it [2:49:49,  1.62s/it]\u001b[A\n","707it [2:49:50,  1.62s/it]\u001b[A\n","708it [2:49:52,  1.61s/it]\u001b[A\n","709it [2:49:54,  1.61s/it]\u001b[A\n","710it [2:49:55,  1.61s/it]\u001b[A\n","711it [2:49:57,  1.61s/it]\u001b[A\n","712it [2:49:58,  1.61s/it]\u001b[A\n","713it [2:50:00,  1.60s/it]\u001b[A\n","714it [2:50:02,  1.60s/it]\u001b[A\n","715it [2:50:03,  1.60s/it]\u001b[A\n","716it [2:50:05,  1.60s/it]\u001b[A\n","717it [2:50:06,  1.61s/it]\u001b[A\n","718it [2:50:08,  1.61s/it]\u001b[A\n","719it [2:50:10,  1.61s/it]\u001b[A\n","720it [2:50:11,  1.61s/it]\u001b[A\n","721it [2:50:13,  1.61s/it]\u001b[A\n","722it [2:50:14,  1.61s/it]\u001b[A\n","723it [2:50:16,  1.61s/it]\u001b[A\n","724it [2:50:18,  1.61s/it]\u001b[A\n","725it [2:50:19,  1.61s/it]\u001b[A\n","726it [2:50:21,  1.61s/it]\u001b[A\n","727it [2:50:23,  1.61s/it]\u001b[A\n","728it [2:50:24,  1.61s/it]\u001b[A\n","729it [2:50:26,  1.61s/it]\u001b[A\n","730it [2:50:27,  1.61s/it]\u001b[A\n","731it [2:50:29,  1.60s/it]\u001b[A\n","732it [2:50:31,  1.60s/it]\u001b[A\n","733it [2:50:32,  1.61s/it]\u001b[A\n","734it [2:50:34,  1.61s/it]\u001b[A\n","735it [2:50:35,  1.60s/it]\u001b[A\n","736it [2:50:37,  1.60s/it]\u001b[A\n","737it [2:50:39,  1.61s/it]\u001b[A\n","738it [2:50:40,  1.61s/it]\u001b[A\n","739it [2:50:42,  1.60s/it]\u001b[A\n","740it [2:50:43,  1.61s/it]\u001b[A\n","741it [2:50:45,  1.60s/it]\u001b[A\n","742it [2:50:47,  1.60s/it]\u001b[A\n","743it [2:50:48,  1.61s/it]\u001b[A\n","744it [2:50:50,  1.61s/it]\u001b[A\n","745it [2:50:51,  1.61s/it]\u001b[A\n","746it [2:50:53,  1.62s/it]\u001b[A\n","747it [2:50:55,  1.62s/it]\u001b[A\n","748it [2:50:56,  1.62s/it]\u001b[A\n","749it [2:50:58,  1.62s/it]\u001b[A\n","750it [2:51:00,  1.62s/it]\u001b[A\n","751it [2:51:01,  1.62s/it]\u001b[A\n","752it [2:51:03,  1.62s/it]\u001b[A\n","753it [2:51:04,  1.62s/it]\u001b[A\n","754it [2:51:06,  1.62s/it]\u001b[A\n","755it [2:51:08,  1.62s/it]\u001b[A\n","756it [2:51:09,  1.62s/it]\u001b[A\n","757it [2:51:11,  1.62s/it]\u001b[A\n","758it [2:51:13,  1.62s/it]\u001b[A\n","759it [2:51:14,  1.62s/it]\u001b[A\n","760it [2:51:16,  1.62s/it]\u001b[A\n","761it [2:51:17,  1.62s/it]\u001b[A\n","762it [2:51:19,  1.62s/it]\u001b[A\n","763it [2:51:21,  1.62s/it]\u001b[A\n","764it [2:51:22,  1.62s/it]\u001b[A\n","765it [2:51:24,  1.62s/it]\u001b[A\n","766it [2:51:25,  1.62s/it]\u001b[A\n","767it [2:51:27,  1.62s/it]\u001b[A\n","768it [2:51:29,  1.62s/it]\u001b[A\n","769it [2:51:30,  1.62s/it]\u001b[A\n","770it [2:51:32,  1.62s/it]\u001b[A\n","771it [2:51:34,  1.62s/it]\u001b[A\n","772it [2:51:35,  1.62s/it]\u001b[A\n","773it [2:51:37,  1.62s/it]\u001b[A\n","774it [2:51:38,  1.62s/it]\u001b[A\n","775it [2:51:40,  1.62s/it]\u001b[A\n","776it [2:51:42,  1.62s/it]\u001b[A\n","777it [2:51:43,  1.62s/it]\u001b[A\n","778it [2:51:45,  1.62s/it]\u001b[A\n","779it [2:51:47,  1.62s/it]\u001b[A\n","780it [2:51:48,  1.62s/it]\u001b[A\n","781it [2:51:50,  1.62s/it]\u001b[A\n","782it [2:51:51,  1.62s/it]\u001b[A\n","783it [2:51:53,  1.63s/it]\u001b[A\n","784it [2:51:55,  1.62s/it]\u001b[A\n","785it [2:51:56,  1.62s/it]\u001b[A\n","786it [2:51:58,  1.62s/it]\u001b[A\n","787it [2:52:00,  1.62s/it]\u001b[A\n","788it [2:52:01,  1.62s/it]\u001b[A\n","789it [2:52:03,  1.62s/it]\u001b[A\n","790it [2:52:04,  1.62s/it]\u001b[A\n","791it [2:52:06,  1.62s/it]\u001b[A\n","792it [2:52:08,  1.62s/it]\u001b[A\n","793it [2:52:09,  1.62s/it]\u001b[A\n","794it [2:52:11,  1.62s/it]\u001b[A\n","795it [2:52:12,  1.62s/it]\u001b[A\n","796it [2:52:14,  1.62s/it]\u001b[A\n","797it [2:52:16,  1.62s/it]\u001b[A\n","798it [2:52:17,  1.62s/it]\u001b[A\n","799it [2:52:19,  1.62s/it]\u001b[A\n","800it [2:52:21,  1.62s/it]\u001b[A\n","801it [2:52:22,  1.62s/it]\u001b[A\n","802it [2:52:24,  1.62s/it]\u001b[A\n","803it [2:52:25,  1.62s/it]\u001b[A\n","804it [2:52:27,  1.62s/it]\u001b[A\n","805it [2:52:29,  1.62s/it]\u001b[A\n","806it [2:52:30,  1.62s/it]\u001b[A\n","807it [2:52:32,  1.62s/it]\u001b[A\n","808it [2:52:34,  1.62s/it]\u001b[A\n","809it [2:52:35,  1.62s/it]\u001b[A\n","810it [2:52:37,  1.62s/it]\u001b[A\n","811it [2:52:38,  1.62s/it]\u001b[A\n","812it [2:52:40,  1.62s/it]\u001b[A\n","813it [2:52:42,  1.62s/it]\u001b[A\n","814it [2:52:43,  1.62s/it]\u001b[A\n","815it [2:52:45,  1.62s/it]\u001b[A\n","816it [2:52:47,  1.62s/it]\u001b[A\n","817it [2:52:48,  1.62s/it]\u001b[A\n","818it [2:52:50,  1.62s/it]\u001b[A\n","819it [2:52:51,  1.62s/it]\u001b[A\n","820it [2:52:53,  1.62s/it]\u001b[A\n","821it [2:52:55,  1.62s/it]\u001b[A\n","822it [2:52:56,  1.62s/it]\u001b[A\n","823it [2:52:58,  1.62s/it]\u001b[A\n","824it [2:52:59,  1.62s/it]\u001b[A\n","825it [2:53:01,  1.62s/it]\u001b[A\n","826it [2:53:03,  1.62s/it]\u001b[A\n","827it [2:53:04,  1.62s/it]\u001b[A\n","828it [2:53:06,  1.62s/it]\u001b[A\n","829it [2:53:08,  1.62s/it]\u001b[A\n","830it [2:53:09,  1.62s/it]\u001b[A\n","831it [2:53:11,  1.62s/it]\u001b[A\n","832it [2:53:12,  1.62s/it]\u001b[A\n","833it [2:53:14,  1.62s/it]\u001b[A\n","834it [2:53:16,  1.62s/it]\u001b[A\n","835it [2:53:17,  1.62s/it]\u001b[A\n","836it [2:53:19,  1.62s/it]\u001b[A\n","837it [2:53:21,  1.62s/it]\u001b[A\n","838it [2:53:22,  1.62s/it]\u001b[A\n","839it [2:53:24,  1.62s/it]\u001b[A\n","840it [2:53:25,  1.62s/it]\u001b[A\n","841it [2:53:27,  1.62s/it]\u001b[A\n","842it [2:53:29,  1.62s/it]\u001b[A\n","843it [2:53:30,  1.62s/it]\u001b[A\n","844it [2:53:32,  1.62s/it]\u001b[A\n","Epoch:  25% 5/20 [3:31:13<10:33:34, 2534.30s/it]11/07/2022 15:55:21 - INFO - src.trainer -   Train loss: 0.11701271750710228\n","11/07/2022 15:59:01 - INFO - src.trainer -   Train loss: 0.08505665172230113\n","11/07/2022 16:02:41 - INFO - src.trainer -   Train loss: 0.12943614612926135\n","11/07/2022 16:06:21 - INFO - src.trainer -   Train loss: 0.11070945046164772\n","11/07/2022 16:10:01 - INFO - src.trainer -   Train loss: 0.10514692826704546\n","11/07/2022 16:13:42 - INFO - src.trainer -   Train loss: 0.14148642800071023\n","11/07/2022 16:17:22 - INFO - src.trainer -   Train loss: 0.10686284845525568\n","11/07/2022 16:21:01 - INFO - src.trainer -   Train loss: 0.12754405628551135\n","11/07/2022 16:24:42 - INFO - src.trainer -   Train loss: 0.0885162353515625\n","11/07/2022 16:28:22 - INFO - src.trainer -   Train loss: 0.09556857022372159\n","\n","846it [3:31:15, 679.60s/it]\u001b[A\n","847it [3:31:17, 476.20s/it]\u001b[A\n","848it [3:31:18, 333.81s/it]\u001b[A\n","849it [3:31:20, 234.15s/it]\u001b[A\n","850it [3:31:21, 164.39s/it]\u001b[A\n","851it [3:31:23, 115.55s/it]\u001b[A\n","852it [3:31:25, 81.37s/it] \u001b[A\n","853it [3:31:26, 57.44s/it]\u001b[A\n","854it [3:31:28, 40.69s/it]\u001b[A\n","855it [3:31:29, 28.96s/it]\u001b[A\n","856it [3:31:31, 20.75s/it]\u001b[A\n","857it [3:31:33, 15.01s/it]\u001b[A\n","858it [3:31:34, 10.99s/it]\u001b[A\n","859it [3:31:36,  8.17s/it]\u001b[A\n","860it [3:31:37,  6.20s/it]\u001b[A\n","861it [3:31:39,  4.82s/it]\u001b[A\n","862it [3:31:41,  3.85s/it]\u001b[A\n","863it [3:31:42,  3.18s/it]\u001b[A\n","864it [3:31:44,  2.70s/it]\u001b[A\n","865it [3:31:45,  2.37s/it]\u001b[A\n","866it [3:31:47,  2.14s/it]\u001b[A\n","867it [3:31:49,  1.98s/it]\u001b[A\n","868it [3:31:50,  1.87s/it]\u001b[A\n","869it [3:31:52,  1.79s/it]\u001b[A\n","870it [3:31:53,  1.73s/it]\u001b[A\n","871it [3:31:55,  1.69s/it]\u001b[A\n","872it [3:31:57,  1.67s/it]\u001b[A\n","873it [3:31:58,  1.65s/it]\u001b[A\n","874it [3:32:00,  1.63s/it]\u001b[A\n","875it [3:32:01,  1.62s/it]\u001b[A\n","876it [3:32:03,  1.62s/it]\u001b[A\n","877it [3:32:05,  1.61s/it]\u001b[A\n","878it [3:32:06,  1.61s/it]\u001b[A\n","879it [3:32:08,  1.61s/it]\u001b[A\n","880it [3:32:09,  1.60s/it]\u001b[A\n","881it [3:32:11,  1.60s/it]\u001b[A\n","882it [3:32:13,  1.60s/it]\u001b[A\n","883it [3:32:14,  1.60s/it]\u001b[A\n","884it [3:32:16,  1.60s/it]\u001b[A\n","885it [3:32:17,  1.60s/it]\u001b[A\n","886it [3:32:19,  1.60s/it]\u001b[A\n","887it [3:32:21,  1.61s/it]\u001b[A\n","888it [3:32:22,  1.60s/it]\u001b[A\n","889it [3:32:24,  1.60s/it]\u001b[A\n","890it [3:32:25,  1.60s/it]\u001b[A\n","891it [3:32:27,  1.60s/it]\u001b[A\n","892it [3:32:29,  1.60s/it]\u001b[A\n","893it [3:32:30,  1.60s/it]\u001b[A\n","894it [3:32:32,  1.60s/it]\u001b[A\n","895it [3:32:33,  1.60s/it]\u001b[A\n","896it [3:32:35,  1.60s/it]\u001b[A\n","897it [3:32:37,  1.60s/it]\u001b[A\n","898it [3:32:38,  1.60s/it]\u001b[A\n","899it [3:32:40,  1.60s/it]\u001b[A\n","900it [3:32:41,  1.60s/it]\u001b[A\n","901it [3:32:43,  1.60s/it]\u001b[A\n","902it [3:32:45,  1.60s/it]\u001b[A\n","903it [3:32:46,  1.60s/it]\u001b[A\n","904it [3:32:48,  1.60s/it]\u001b[A\n","905it [3:32:50,  1.60s/it]\u001b[A\n","906it [3:32:51,  1.60s/it]\u001b[A\n","907it [3:32:53,  1.60s/it]\u001b[A\n","908it [3:32:54,  1.60s/it]\u001b[A\n","909it [3:32:56,  1.60s/it]\u001b[A\n","910it [3:32:58,  1.60s/it]\u001b[A\n","911it [3:32:59,  1.60s/it]\u001b[A\n","912it [3:33:01,  1.61s/it]\u001b[A\n","913it [3:33:02,  1.61s/it]\u001b[A\n","914it [3:33:04,  1.61s/it]\u001b[A\n","915it [3:33:06,  1.62s/it]\u001b[A\n","916it [3:33:07,  1.62s/it]\u001b[A\n","917it [3:33:09,  1.62s/it]\u001b[A\n","918it [3:33:10,  1.62s/it]\u001b[A\n","919it [3:33:12,  1.62s/it]\u001b[A\n","920it [3:33:14,  1.62s/it]\u001b[A\n","921it [3:33:15,  1.61s/it]\u001b[A\n","922it [3:33:17,  1.62s/it]\u001b[A\n","923it [3:33:19,  1.62s/it]\u001b[A\n","924it [3:33:20,  1.62s/it]\u001b[A\n","925it [3:33:22,  1.62s/it]\u001b[A\n","926it [3:33:23,  1.62s/it]\u001b[A\n","927it [3:33:25,  1.62s/it]\u001b[A\n","928it [3:33:27,  1.62s/it]\u001b[A\n","929it [3:33:28,  1.62s/it]\u001b[A\n","930it [3:33:30,  1.62s/it]\u001b[A\n","931it [3:33:32,  1.62s/it]\u001b[A\n","932it [3:33:33,  1.62s/it]\u001b[A\n","933it [3:33:35,  1.62s/it]\u001b[A\n","934it [3:33:36,  1.62s/it]\u001b[A\n","935it [3:33:38,  1.62s/it]\u001b[A\n","936it [3:33:40,  1.61s/it]\u001b[A\n","937it [3:33:41,  1.62s/it]\u001b[A\n","938it [3:33:43,  1.61s/it]\u001b[A\n","939it [3:33:44,  1.61s/it]\u001b[A\n","940it [3:33:46,  1.62s/it]\u001b[A\n","941it [3:33:48,  1.62s/it]\u001b[A\n","942it [3:33:49,  1.61s/it]\u001b[A\n","943it [3:33:51,  1.61s/it]\u001b[A\n","944it [3:33:52,  1.62s/it]\u001b[A\n","945it [3:33:54,  1.61s/it]\u001b[A\n","946it [3:33:56,  1.62s/it]\u001b[A\n","947it [3:33:57,  1.62s/it]\u001b[A\n","948it [3:33:59,  1.62s/it]\u001b[A\n","949it [3:34:01,  1.62s/it]\u001b[A\n","950it [3:34:02,  1.62s/it]\u001b[A\n","951it [3:34:04,  1.62s/it]\u001b[A\n","952it [3:34:05,  1.62s/it]\u001b[A\n","953it [3:34:07,  1.62s/it]\u001b[A\n","954it [3:34:09,  1.62s/it]\u001b[A\n","955it [3:34:10,  1.62s/it]\u001b[A\n","956it [3:34:12,  1.62s/it]\u001b[A\n","957it [3:34:14,  1.62s/it]\u001b[A\n","958it [3:34:15,  1.62s/it]\u001b[A\n","959it [3:34:17,  1.62s/it]\u001b[A\n","960it [3:34:18,  1.62s/it]\u001b[A\n","961it [3:34:20,  1.62s/it]\u001b[A\n","962it [3:34:22,  1.62s/it]\u001b[A\n","963it [3:34:23,  1.62s/it]\u001b[A\n","964it [3:34:25,  1.62s/it]\u001b[A\n","965it [3:34:27,  1.62s/it]\u001b[A\n","966it [3:34:28,  1.62s/it]\u001b[A\n","967it [3:34:30,  1.62s/it]\u001b[A\n","968it [3:34:31,  1.62s/it]\u001b[A\n","969it [3:34:33,  1.62s/it]\u001b[A\n","970it [3:34:35,  1.62s/it]\u001b[A\n","971it [3:34:36,  1.62s/it]\u001b[A\n","972it [3:34:38,  1.62s/it]\u001b[A\n","973it [3:34:39,  1.62s/it]\u001b[A\n","974it [3:34:41,  1.62s/it]\u001b[A\n","975it [3:34:43,  1.62s/it]\u001b[A\n","976it [3:34:44,  1.62s/it]\u001b[A\n","977it [3:34:46,  1.62s/it]\u001b[A\n","978it [3:34:48,  1.62s/it]\u001b[A\n","979it [3:34:49,  1.62s/it]\u001b[A\n","980it [3:34:51,  1.62s/it]\u001b[A\n","981it [3:34:52,  1.62s/it]\u001b[A\n","982it [3:34:54,  1.62s/it]\u001b[A\n","983it [3:34:56,  1.62s/it]\u001b[A\n","984it [3:34:57,  1.62s/it]\u001b[A\n","985it [3:34:59,  1.62s/it]\u001b[A\n","986it [3:35:01,  1.62s/it]\u001b[A\n","987it [3:35:02,  1.62s/it]\u001b[A\n","988it [3:35:04,  1.62s/it]\u001b[A\n","989it [3:35:05,  1.62s/it]\u001b[A\n","990it [3:35:07,  1.62s/it]\u001b[A\n","991it [3:35:09,  1.61s/it]\u001b[A\n","992it [3:35:10,  1.62s/it]\u001b[A\n","993it [3:35:12,  1.62s/it]\u001b[A\n","994it [3:35:13,  1.62s/it]\u001b[A\n","995it [3:35:15,  1.62s/it]\u001b[A\n","996it [3:35:17,  1.62s/it]\u001b[A\n","997it [3:35:18,  1.62s/it]\u001b[A\n","998it [3:35:20,  1.62s/it]\u001b[A\n","999it [3:35:22,  1.62s/it]\u001b[A\n","1000it [3:35:23,  1.62s/it]\u001b[A\n","1001it [3:35:25,  1.62s/it]\u001b[A\n","1002it [3:35:26,  1.62s/it]\u001b[A\n","1003it [3:35:28,  1.62s/it]\u001b[A\n","1004it [3:35:30,  1.62s/it]\u001b[A\n","1005it [3:35:31,  1.61s/it]\u001b[A\n","1006it [3:35:33,  1.62s/it]\u001b[A\n","1007it [3:35:34,  1.62s/it]\u001b[A\n","1008it [3:35:36,  1.62s/it]\u001b[A\n","1009it [3:35:38,  1.62s/it]\u001b[A\n","1010it [3:35:39,  1.62s/it]\u001b[A\n","1011it [3:35:41,  1.61s/it]\u001b[A\n","1012it [3:35:43,  1.61s/it]\u001b[A\n","1013it [3:35:44,  1.61s/it]\u001b[A\n","Epoch:  30% 6/20 [4:13:26<9:51:11, 2533.70s/it] 11/07/2022 16:36:43 - INFO - src.trainer -   Train loss: 0.1006011962890625\n","11/07/2022 16:40:23 - INFO - src.trainer -   Train loss: 0.07097556374289772\n","11/07/2022 16:44:03 - INFO - src.trainer -   Train loss: 0.07151517001065341\n","11/07/2022 16:47:43 - INFO - src.trainer -   Train loss: 0.08525362881747159\n","11/07/2022 16:51:24 - INFO - src.trainer -   Train loss: 0.0706024169921875\n","11/07/2022 16:55:04 - INFO - src.trainer -   Train loss: 0.11709178577769887\n","11/07/2022 16:58:44 - INFO - src.trainer -   Train loss: 0.08390253240411932\n","11/07/2022 17:02:24 - INFO - src.trainer -   Train loss: 0.05517994273792614\n","11/07/2022 17:06:05 - INFO - src.trainer -   Train loss: 0.10242808948863637\n","11/07/2022 17:09:45 - INFO - src.trainer -   Train loss: 0.05947043678977273\n","\n","1015it [4:13:28, 679.72s/it]\u001b[A\n","1016it [4:13:29, 476.28s/it]\u001b[A\n","1017it [4:13:31, 333.88s/it]\u001b[A\n","1018it [4:13:33, 234.19s/it]\u001b[A\n","1019it [4:13:34, 164.42s/it]\u001b[A\n","1020it [4:13:36, 115.57s/it]\u001b[A\n","1021it [4:13:37, 81.38s/it] \u001b[A\n","1022it [4:13:39, 57.45s/it]\u001b[A\n","1023it [4:13:41, 40.69s/it]\u001b[A\n","1024it [4:13:42, 28.97s/it]\u001b[A\n","1025it [4:13:44, 20.76s/it]\u001b[A\n","1026it [4:13:45, 15.01s/it]\u001b[A\n","1027it [4:13:47, 10.99s/it]\u001b[A\n","1028it [4:13:49,  8.17s/it]\u001b[A\n","1029it [4:13:50,  6.20s/it]\u001b[A\n","1030it [4:13:52,  4.82s/it]\u001b[A\n","1031it [4:13:53,  3.86s/it]\u001b[A\n","1032it [4:13:55,  3.18s/it]\u001b[A\n","1033it [4:13:57,  2.70s/it]\u001b[A\n","1034it [4:13:58,  2.37s/it]\u001b[A\n","1035it [4:14:00,  2.14s/it]\u001b[A\n","1036it [4:14:01,  1.98s/it]\u001b[A\n","1037it [4:14:03,  1.87s/it]\u001b[A\n","1038it [4:14:05,  1.79s/it]\u001b[A\n","1039it [4:14:06,  1.73s/it]\u001b[A\n","1040it [4:14:08,  1.69s/it]\u001b[A\n","1041it [4:14:09,  1.66s/it]\u001b[A\n","1042it [4:14:11,  1.65s/it]\u001b[A\n","1043it [4:14:13,  1.63s/it]\u001b[A\n","1044it [4:14:14,  1.62s/it]\u001b[A\n","1045it [4:14:16,  1.62s/it]\u001b[A\n","1046it [4:14:17,  1.61s/it]\u001b[A\n","1047it [4:14:19,  1.61s/it]\u001b[A\n","1048it [4:14:21,  1.61s/it]\u001b[A\n","1049it [4:14:22,  1.60s/it]\u001b[A\n","1050it [4:14:24,  1.60s/it]\u001b[A\n","1051it [4:14:25,  1.60s/it]\u001b[A\n","1052it [4:14:27,  1.60s/it]\u001b[A\n","1053it [4:14:29,  1.60s/it]\u001b[A\n","1054it [4:14:30,  1.60s/it]\u001b[A\n","1055it [4:14:32,  1.60s/it]\u001b[A\n","1056it [4:14:33,  1.60s/it]\u001b[A\n","1057it [4:14:35,  1.60s/it]\u001b[A\n","1058it [4:14:37,  1.60s/it]\u001b[A\n","1059it [4:14:38,  1.60s/it]\u001b[A\n","1060it [4:14:40,  1.60s/it]\u001b[A\n","1061it [4:14:41,  1.60s/it]\u001b[A\n","1062it [4:14:43,  1.60s/it]\u001b[A\n","1063it [4:14:45,  1.60s/it]\u001b[A\n","1064it [4:14:46,  1.60s/it]\u001b[A\n","1065it [4:14:48,  1.60s/it]\u001b[A\n","1066it [4:14:49,  1.60s/it]\u001b[A\n","1067it [4:14:51,  1.60s/it]\u001b[A\n","1068it [4:14:53,  1.60s/it]\u001b[A\n","1069it [4:14:54,  1.60s/it]\u001b[A\n","1070it [4:14:56,  1.60s/it]\u001b[A\n","1071it [4:14:57,  1.60s/it]\u001b[A\n","1072it [4:14:59,  1.60s/it]\u001b[A\n","1073it [4:15:01,  1.60s/it]\u001b[A\n","1074it [4:15:02,  1.60s/it]\u001b[A\n","1075it [4:15:04,  1.60s/it]\u001b[A\n","1076it [4:15:05,  1.60s/it]\u001b[A\n","1077it [4:15:07,  1.60s/it]\u001b[A\n","1078it [4:15:09,  1.60s/it]\u001b[A\n","1079it [4:15:10,  1.60s/it]\u001b[A\n","1080it [4:15:12,  1.60s/it]\u001b[A\n","1081it [4:15:13,  1.61s/it]\u001b[A\n","1082it [4:15:15,  1.61s/it]\u001b[A\n","1083it [4:15:17,  1.61s/it]\u001b[A\n","1084it [4:15:18,  1.62s/it]\u001b[A\n","1085it [4:15:20,  1.62s/it]\u001b[A\n","1086it [4:15:22,  1.62s/it]\u001b[A\n","1087it [4:15:23,  1.62s/it]\u001b[A\n","1088it [4:15:25,  1.62s/it]\u001b[A\n","1089it [4:15:26,  1.61s/it]\u001b[A\n","1090it [4:15:28,  1.61s/it]\u001b[A\n","1091it [4:15:30,  1.62s/it]\u001b[A\n","1092it [4:15:31,  1.62s/it]\u001b[A\n","1093it [4:15:33,  1.62s/it]\u001b[A\n","1094it [4:15:34,  1.62s/it]\u001b[A\n","1095it [4:15:36,  1.62s/it]\u001b[A\n","1096it [4:15:38,  1.62s/it]\u001b[A\n","1097it [4:15:39,  1.62s/it]\u001b[A\n","1098it [4:15:41,  1.62s/it]\u001b[A\n","1099it [4:15:43,  1.62s/it]\u001b[A\n","1100it [4:15:44,  1.62s/it]\u001b[A\n","1101it [4:15:46,  1.62s/it]\u001b[A\n","1102it [4:15:47,  1.62s/it]\u001b[A\n","1103it [4:15:49,  1.62s/it]\u001b[A\n","1104it [4:15:51,  1.61s/it]\u001b[A\n","1105it [4:15:52,  1.61s/it]\u001b[A\n","1106it [4:15:54,  1.62s/it]\u001b[A\n","1107it [4:15:55,  1.61s/it]\u001b[A\n","1108it [4:15:57,  1.61s/it]\u001b[A\n","1109it [4:15:59,  1.62s/it]\u001b[A\n","1110it [4:16:00,  1.62s/it]\u001b[A\n","1111it [4:16:02,  1.61s/it]\u001b[A\n","1112it [4:16:04,  1.61s/it]\u001b[A\n","1113it [4:16:05,  1.61s/it]\u001b[A\n","1114it [4:16:07,  1.61s/it]\u001b[A\n","1115it [4:16:08,  1.61s/it]\u001b[A\n","1116it [4:16:10,  1.61s/it]\u001b[A\n","1117it [4:16:12,  1.61s/it]\u001b[A\n","1118it [4:16:13,  1.61s/it]\u001b[A\n","1119it [4:16:15,  1.62s/it]\u001b[A\n","1120it [4:16:16,  1.62s/it]\u001b[A\n","1121it [4:16:18,  1.62s/it]\u001b[A\n","1122it [4:16:20,  1.62s/it]\u001b[A\n","1123it [4:16:21,  1.62s/it]\u001b[A\n","1124it [4:16:23,  1.62s/it]\u001b[A\n","1125it [4:16:25,  1.62s/it]\u001b[A\n","1126it [4:16:26,  1.62s/it]\u001b[A\n","1127it [4:16:28,  1.62s/it]\u001b[A\n","1128it [4:16:29,  1.62s/it]\u001b[A\n","1129it [4:16:31,  1.62s/it]\u001b[A\n","1130it [4:16:33,  1.62s/it]\u001b[A\n","1131it [4:16:34,  1.62s/it]\u001b[A\n","1132it [4:16:36,  1.61s/it]\u001b[A\n","1133it [4:16:38,  1.61s/it]\u001b[A\n","1134it [4:16:39,  1.62s/it]\u001b[A\n","1135it [4:16:41,  1.62s/it]\u001b[A\n","1136it [4:16:42,  1.62s/it]\u001b[A\n","1137it [4:16:44,  1.62s/it]\u001b[A\n","1138it [4:16:46,  1.62s/it]\u001b[A\n","1139it [4:16:47,  1.62s/it]\u001b[A\n","1140it [4:16:49,  1.62s/it]\u001b[A\n","1141it [4:16:50,  1.62s/it]\u001b[A\n","1142it [4:16:52,  1.62s/it]\u001b[A\n","1143it [4:16:54,  1.62s/it]\u001b[A\n","1144it [4:16:55,  1.62s/it]\u001b[A\n","1145it [4:16:57,  1.62s/it]\u001b[A\n","1146it [4:16:59,  1.62s/it]\u001b[A\n","1147it [4:17:00,  1.62s/it]\u001b[A\n","1148it [4:17:02,  1.62s/it]\u001b[A\n","1149it [4:17:03,  1.62s/it]\u001b[A\n","1150it [4:17:05,  1.62s/it]\u001b[A\n","1151it [4:17:07,  1.62s/it]\u001b[A\n","1152it [4:17:08,  1.62s/it]\u001b[A\n","1153it [4:17:10,  1.62s/it]\u001b[A\n","1154it [4:17:12,  1.62s/it]\u001b[A\n","1155it [4:17:13,  1.62s/it]\u001b[A\n","1156it [4:17:15,  1.62s/it]\u001b[A\n","1157it [4:17:16,  1.62s/it]\u001b[A\n","1158it [4:17:18,  1.62s/it]\u001b[A\n","1159it [4:17:20,  1.61s/it]\u001b[A\n","1160it [4:17:21,  1.61s/it]\u001b[A\n","1161it [4:17:23,  1.62s/it]\u001b[A\n","1162it [4:17:24,  1.62s/it]\u001b[A\n","1163it [4:17:26,  1.62s/it]\u001b[A\n","1164it [4:17:28,  1.62s/it]\u001b[A\n","1165it [4:17:29,  1.62s/it]\u001b[A\n","1166it [4:17:31,  1.62s/it]\u001b[A\n","1167it [4:17:33,  1.62s/it]\u001b[A\n","1168it [4:17:34,  1.62s/it]\u001b[A\n","1169it [4:17:36,  1.62s/it]\u001b[A\n","1170it [4:17:37,  1.62s/it]\u001b[A\n","1171it [4:17:39,  1.62s/it]\u001b[A\n","1172it [4:17:41,  1.62s/it]\u001b[A\n","1173it [4:17:42,  1.62s/it]\u001b[A\n","1174it [4:17:44,  1.62s/it]\u001b[A\n","1175it [4:17:45,  1.62s/it]\u001b[A\n","1176it [4:17:47,  1.62s/it]\u001b[A\n","1177it [4:17:49,  1.62s/it]\u001b[A\n","1178it [4:17:50,  1.62s/it]\u001b[A\n","1179it [4:17:52,  1.62s/it]\u001b[A\n","1180it [4:17:54,  1.62s/it]\u001b[A\n","1181it [4:17:55,  1.62s/it]\u001b[A\n","1182it [4:17:57,  1.62s/it]\u001b[A\n","Epoch:  35% 7/20 [4:55:38<9:08:54, 2533.40s/it]11/07/2022 17:18:06 - INFO - src.trainer -   Train loss: 0.08316456187855113\n","11/07/2022 17:21:46 - INFO - src.trainer -   Train loss: 0.048524336381392044\n","11/07/2022 17:25:26 - INFO - src.trainer -   Train loss: 0.06445035067471591\n","11/07/2022 17:29:06 - INFO - src.trainer -   Train loss: 0.05644503506747159\n","11/07/2022 17:32:46 - INFO - src.trainer -   Train loss: 0.0600738525390625\n","11/07/2022 17:36:26 - INFO - src.trainer -   Train loss: 0.09128223765980113\n","11/07/2022 17:40:06 - INFO - src.trainer -   Train loss: 0.048445268110795456\n","11/07/2022 17:43:46 - INFO - src.trainer -   Train loss: 0.043440385298295456\n","11/07/2022 17:47:27 - INFO - src.trainer -   Train loss: 0.0506439208984375\n","11/07/2022 17:51:07 - INFO - src.trainer -   Train loss: 0.06569186123934659\n","\n","1184it [4:55:40, 679.64s/it]\u001b[A\n","1185it [4:55:42, 476.22s/it]\u001b[A\n","1186it [4:55:43, 333.84s/it]\u001b[A\n","1187it [4:55:45, 234.17s/it]\u001b[A\n","1188it [4:55:47, 164.40s/it]\u001b[A\n","1189it [4:55:48, 115.56s/it]\u001b[A\n","1190it [4:55:50, 81.37s/it] \u001b[A\n","1191it [4:55:51, 57.44s/it]\u001b[A\n","1192it [4:55:53, 40.69s/it]\u001b[A\n","1193it [4:55:55, 28.96s/it]\u001b[A\n","1194it [4:55:56, 20.76s/it]\u001b[A\n","1195it [4:55:58, 15.01s/it]\u001b[A\n","1196it [4:55:59, 10.99s/it]\u001b[A\n","1197it [4:56:01,  8.17s/it]\u001b[A\n","1198it [4:56:03,  6.20s/it]\u001b[A\n","1199it [4:56:04,  4.82s/it]\u001b[A\n","1200it [4:56:06,  3.85s/it]\u001b[A\n","1201it [4:56:07,  3.18s/it]\u001b[A\n","1202it [4:56:09,  2.71s/it]\u001b[A\n","1203it [4:56:11,  2.37s/it]\u001b[A\n","1204it [4:56:12,  2.14s/it]\u001b[A\n","1205it [4:56:14,  1.98s/it]\u001b[A\n","1206it [4:56:15,  1.87s/it]\u001b[A\n","1207it [4:56:17,  1.79s/it]\u001b[A\n","1208it [4:56:19,  1.73s/it]\u001b[A\n","1209it [4:56:20,  1.69s/it]\u001b[A\n","1210it [4:56:22,  1.66s/it]\u001b[A\n","1211it [4:56:23,  1.65s/it]\u001b[A\n","1212it [4:56:25,  1.63s/it]\u001b[A\n","1213it [4:56:27,  1.62s/it]\u001b[A\n","1214it [4:56:28,  1.62s/it]\u001b[A\n","1215it [4:56:30,  1.61s/it]\u001b[A\n","1216it [4:56:31,  1.61s/it]\u001b[A\n","1217it [4:56:33,  1.61s/it]\u001b[A\n","1218it [4:56:35,  1.61s/it]\u001b[A\n","1219it [4:56:36,  1.60s/it]\u001b[A\n","1220it [4:56:38,  1.60s/it]\u001b[A\n","1221it [4:56:39,  1.60s/it]\u001b[A\n","1222it [4:56:41,  1.60s/it]\u001b[A\n","1223it [4:56:43,  1.60s/it]\u001b[A\n","1224it [4:56:44,  1.60s/it]\u001b[A\n","1225it [4:56:46,  1.61s/it]\u001b[A\n","1226it [4:56:47,  1.61s/it]\u001b[A\n","1227it [4:56:49,  1.60s/it]\u001b[A\n","1228it [4:56:51,  1.60s/it]\u001b[A\n","1229it [4:56:52,  1.60s/it]\u001b[A\n","1230it [4:56:54,  1.60s/it]\u001b[A\n","1231it [4:56:55,  1.60s/it]\u001b[A\n","1232it [4:56:57,  1.60s/it]\u001b[A\n","1233it [4:56:59,  1.61s/it]\u001b[A\n","1234it [4:57:00,  1.60s/it]\u001b[A\n","1235it [4:57:02,  1.60s/it]\u001b[A\n","1236it [4:57:03,  1.60s/it]\u001b[A\n","1237it [4:57:05,  1.60s/it]\u001b[A\n","1238it [4:57:07,  1.60s/it]\u001b[A\n","1239it [4:57:08,  1.60s/it]\u001b[A\n","1240it [4:57:10,  1.61s/it]\u001b[A\n","1241it [4:57:11,  1.60s/it]\u001b[A\n","1242it [4:57:13,  1.60s/it]\u001b[A\n","1243it [4:57:15,  1.60s/it]\u001b[A\n","1244it [4:57:16,  1.61s/it]\u001b[A\n","1245it [4:57:18,  1.61s/it]\u001b[A\n","1246it [4:57:19,  1.60s/it]\u001b[A\n","1247it [4:57:21,  1.60s/it]\u001b[A\n","1248it [4:57:23,  1.60s/it]\u001b[A\n","1249it [4:57:24,  1.60s/it]\u001b[A\n","1250it [4:57:26,  1.61s/it]\u001b[A\n","1251it [4:57:28,  1.61s/it]\u001b[A\n","1252it [4:57:29,  1.61s/it]\u001b[A\n","1253it [4:57:31,  1.62s/it]\u001b[A\n","1254it [4:57:32,  1.62s/it]\u001b[A\n","1255it [4:57:34,  1.62s/it]\u001b[A\n","1256it [4:57:36,  1.62s/it]\u001b[A\n","1257it [4:57:37,  1.62s/it]\u001b[A\n","1258it [4:57:39,  1.62s/it]\u001b[A\n","1259it [4:57:40,  1.62s/it]\u001b[A\n","1260it [4:57:42,  1.62s/it]\u001b[A\n","1261it [4:57:44,  1.62s/it]\u001b[A\n","1262it [4:57:45,  1.62s/it]\u001b[A\n","1263it [4:57:47,  1.62s/it]\u001b[A\n","1264it [4:57:49,  1.62s/it]\u001b[A\n","1265it [4:57:50,  1.62s/it]\u001b[A\n","1266it [4:57:52,  1.62s/it]\u001b[A\n","1267it [4:57:53,  1.62s/it]\u001b[A\n","1268it [4:57:55,  1.62s/it]\u001b[A\n","1269it [4:57:57,  1.62s/it]\u001b[A\n","1270it [4:57:58,  1.62s/it]\u001b[A\n","1271it [4:58:00,  1.62s/it]\u001b[A\n","1272it [4:58:02,  1.62s/it]\u001b[A\n","1273it [4:58:03,  1.61s/it]\u001b[A\n","1274it [4:58:05,  1.62s/it]\u001b[A\n","1275it [4:58:06,  1.62s/it]\u001b[A\n","1276it [4:58:08,  1.62s/it]\u001b[A\n","1277it [4:58:10,  1.61s/it]\u001b[A\n","1278it [4:58:11,  1.61s/it]\u001b[A\n","1279it [4:58:13,  1.61s/it]\u001b[A\n","1280it [4:58:14,  1.61s/it]\u001b[A\n","1281it [4:58:16,  1.61s/it]\u001b[A\n","1282it [4:58:18,  1.61s/it]\u001b[A\n","1283it [4:58:19,  1.62s/it]\u001b[A\n","1284it [4:58:21,  1.62s/it]\u001b[A\n","1285it [4:58:22,  1.62s/it]\u001b[A\n","1286it [4:58:24,  1.62s/it]\u001b[A\n","1287it [4:58:26,  1.62s/it]\u001b[A\n","1288it [4:58:27,  1.62s/it]\u001b[A\n","1289it [4:58:29,  1.62s/it]\u001b[A\n","1290it [4:58:31,  1.62s/it]\u001b[A\n","1291it [4:58:32,  1.62s/it]\u001b[A\n","1292it [4:58:34,  1.62s/it]\u001b[A\n","1293it [4:58:35,  1.62s/it]\u001b[A\n","1294it [4:58:37,  1.62s/it]\u001b[A\n","1295it [4:58:39,  1.62s/it]\u001b[A\n","1296it [4:58:40,  1.62s/it]\u001b[A\n","1297it [4:58:42,  1.62s/it]\u001b[A\n","1298it [4:58:44,  1.62s/it]\u001b[A\n","1299it [4:58:45,  1.62s/it]\u001b[A\n","1300it [4:58:47,  1.62s/it]\u001b[A\n","1301it [4:58:48,  1.61s/it]\u001b[A\n","1302it [4:58:50,  1.61s/it]\u001b[A\n","1303it [4:58:52,  1.62s/it]\u001b[A\n","1304it [4:58:53,  1.62s/it]\u001b[A\n","1305it [4:58:55,  1.62s/it]\u001b[A\n","1306it [4:58:57,  1.62s/it]\u001b[A\n","1307it [4:58:58,  1.62s/it]\u001b[A\n","1308it [4:59:00,  1.62s/it]\u001b[A\n","1309it [4:59:01,  1.62s/it]\u001b[A\n","1310it [4:59:03,  1.62s/it]\u001b[A\n","1311it [4:59:05,  1.62s/it]\u001b[A\n","1312it [4:59:06,  1.62s/it]\u001b[A\n","1313it [4:59:08,  1.62s/it]\u001b[A\n","1314it [4:59:09,  1.62s/it]\u001b[A\n","1315it [4:59:11,  1.62s/it]\u001b[A\n","1316it [4:59:13,  1.62s/it]\u001b[A\n","1317it [4:59:14,  1.62s/it]\u001b[A\n","1318it [4:59:16,  1.62s/it]\u001b[A\n","1319it [4:59:18,  1.62s/it]\u001b[A\n","1320it [4:59:19,  1.62s/it]\u001b[A\n","1321it [4:59:21,  1.62s/it]\u001b[A\n","1322it [4:59:22,  1.62s/it]\u001b[A\n","1323it [4:59:24,  1.63s/it]\u001b[A\n","1324it [4:59:26,  1.62s/it]\u001b[A\n","1325it [4:59:27,  1.62s/it]\u001b[A\n","1326it [4:59:29,  1.62s/it]\u001b[A\n","1327it [4:59:31,  1.62s/it]\u001b[A\n","1328it [4:59:32,  1.62s/it]\u001b[A\n","1329it [4:59:34,  1.61s/it]\u001b[A\n","1330it [4:59:35,  1.62s/it]\u001b[A\n","1331it [4:59:37,  1.62s/it]\u001b[A\n","1332it [4:59:39,  1.62s/it]\u001b[A\n","1333it [4:59:40,  1.62s/it]\u001b[A\n","1334it [4:59:42,  1.62s/it]\u001b[A\n","1335it [4:59:43,  1.62s/it]\u001b[A\n","1336it [4:59:45,  1.62s/it]\u001b[A\n","1337it [4:59:47,  1.62s/it]\u001b[A\n","1338it [4:59:48,  1.62s/it]\u001b[A\n","1339it [4:59:50,  1.62s/it]\u001b[A\n","1340it [4:59:52,  1.62s/it]\u001b[A\n","1341it [4:59:53,  1.62s/it]\u001b[A\n","1342it [4:59:55,  1.62s/it]\u001b[A\n","1343it [4:59:56,  1.62s/it]\u001b[A\n","1344it [4:59:58,  1.62s/it]\u001b[A\n","1345it [5:00:00,  1.62s/it]\u001b[A\n","1346it [5:00:01,  1.62s/it]\u001b[A\n","1347it [5:00:03,  1.62s/it]\u001b[A\n","1348it [5:00:05,  1.62s/it]\u001b[A\n","1349it [5:00:06,  1.62s/it]\u001b[A\n","1350it [5:00:08,  1.62s/it]\u001b[A\n","1351it [5:00:09,  1.62s/it]\u001b[A\n","Epoch:  40% 8/20 [5:37:51<8:26:37, 2533.14s/it]11/07/2022 17:59:28 - INFO - src.trainer -   Train loss: 0.08378184925426137\n","11/07/2022 18:03:08 - INFO - src.trainer -   Train loss: 0.046682184392755684\n","11/07/2022 18:06:48 - INFO - src.trainer -   Train loss: 0.02694147283380682\n","11/07/2022 18:10:28 - INFO - src.trainer -   Train loss: 0.061614990234375\n","11/07/2022 18:14:08 - INFO - src.trainer -   Train loss: 0.05638538707386364\n","11/07/2022 18:17:48 - INFO - src.trainer -   Train loss: 0.056325739080255684\n","11/07/2022 18:21:29 - INFO - src.trainer -   Train loss: 0.05793068625710227\n","11/07/2022 18:25:09 - INFO - src.trainer -   Train loss: 0.06387467817826704\n","11/07/2022 18:28:49 - INFO - src.trainer -   Train loss: 0.07401067560369318\n","11/07/2022 18:32:29 - INFO - src.trainer -   Train loss: 0.01545992764559659\n","11/07/2022 18:36:09 - INFO - src.trainer -   Train loss: 0.04280783913352273\n","\n","1353it [5:37:52, 679.38s/it]\u001b[A\n","1354it [5:37:53, 476.05s/it]\u001b[A\n","1355it [5:37:55, 333.71s/it]\u001b[A\n","1356it [5:37:57, 234.08s/it]\u001b[A\n","1357it [5:37:58, 164.34s/it]\u001b[A\n","1358it [5:38:00, 115.52s/it]\u001b[A\n","1359it [5:38:01, 81.34s/it] \u001b[A\n","1360it [5:38:03, 57.42s/it]\u001b[A\n","1361it [5:38:05, 40.68s/it]\u001b[A\n","1362it [5:38:06, 28.96s/it]\u001b[A\n","1363it [5:38:08, 20.75s/it]\u001b[A\n","1364it [5:38:09, 15.01s/it]\u001b[A\n","1365it [5:38:11, 10.99s/it]\u001b[A\n","1366it [5:38:13,  8.17s/it]\u001b[A\n","1367it [5:38:14,  6.20s/it]\u001b[A\n","1368it [5:38:16,  4.82s/it]\u001b[A\n","1369it [5:38:17,  3.86s/it]\u001b[A\n","1370it [5:38:19,  3.18s/it]\u001b[A\n","1371it [5:38:21,  2.71s/it]\u001b[A\n","1372it [5:38:22,  2.38s/it]\u001b[A\n","1373it [5:38:24,  2.14s/it]\u001b[A\n","1374it [5:38:25,  1.98s/it]\u001b[A\n","1375it [5:38:27,  1.87s/it]\u001b[A\n","1376it [5:38:29,  1.79s/it]\u001b[A\n","1377it [5:38:30,  1.73s/it]\u001b[A\n","1378it [5:38:32,  1.70s/it]\u001b[A\n","1379it [5:38:34,  1.67s/it]\u001b[A\n","1380it [5:38:35,  1.65s/it]\u001b[A\n","1381it [5:38:37,  1.63s/it]\u001b[A\n","1382it [5:38:38,  1.62s/it]\u001b[A\n","1383it [5:38:40,  1.62s/it]\u001b[A\n","1384it [5:38:42,  1.61s/it]\u001b[A\n","1385it [5:38:43,  1.61s/it]\u001b[A\n","1386it [5:38:45,  1.61s/it]\u001b[A\n","1387it [5:38:46,  1.61s/it]\u001b[A\n","1388it [5:38:48,  1.60s/it]\u001b[A\n","1389it [5:38:50,  1.60s/it]\u001b[A\n","1390it [5:38:51,  1.60s/it]\u001b[A\n","1391it [5:38:53,  1.60s/it]\u001b[A\n","1392it [5:38:54,  1.60s/it]\u001b[A\n","1393it [5:38:56,  1.61s/it]\u001b[A\n","1394it [5:38:58,  1.61s/it]\u001b[A\n","1395it [5:38:59,  1.61s/it]\u001b[A\n","1396it [5:39:01,  1.61s/it]\u001b[A\n","1397it [5:39:02,  1.61s/it]\u001b[A\n","1398it [5:39:04,  1.60s/it]\u001b[A\n","1399it [5:39:06,  1.61s/it]\u001b[A\n","1400it [5:39:07,  1.60s/it]\u001b[A\n","1401it [5:39:09,  1.60s/it]\u001b[A\n","1402it [5:39:10,  1.61s/it]\u001b[A\n","1403it [5:39:12,  1.61s/it]\u001b[A\n","1404it [5:39:14,  1.61s/it]\u001b[A\n","1405it [5:39:15,  1.61s/it]\u001b[A\n","1406it [5:39:17,  1.61s/it]\u001b[A\n","1407it [5:39:18,  1.61s/it]\u001b[A\n","1408it [5:39:20,  1.61s/it]\u001b[A\n","1409it [5:39:22,  1.61s/it]\u001b[A\n","1410it [5:39:23,  1.61s/it]\u001b[A\n","1411it [5:39:25,  1.61s/it]\u001b[A\n","1412it [5:39:26,  1.61s/it]\u001b[A\n","1413it [5:39:28,  1.61s/it]\u001b[A\n","1414it [5:39:30,  1.61s/it]\u001b[A\n","1415it [5:39:31,  1.61s/it]\u001b[A\n","1416it [5:39:33,  1.61s/it]\u001b[A\n","1417it [5:39:35,  1.61s/it]\u001b[A\n","1418it [5:39:36,  1.61s/it]\u001b[A\n","1419it [5:39:38,  1.61s/it]\u001b[A\n","1420it [5:39:39,  1.61s/it]\u001b[A\n","1421it [5:39:41,  1.62s/it]\u001b[A\n","1422it [5:39:43,  1.62s/it]\u001b[A\n","1423it [5:39:44,  1.62s/it]\u001b[A\n","1424it [5:39:46,  1.62s/it]\u001b[A\n","1425it [5:39:47,  1.62s/it]\u001b[A\n","1426it [5:39:49,  1.62s/it]\u001b[A\n","1427it [5:39:51,  1.62s/it]\u001b[A\n","1428it [5:39:52,  1.61s/it]\u001b[A\n","1429it [5:39:54,  1.62s/it]\u001b[A\n","1430it [5:39:56,  1.62s/it]\u001b[A\n","1431it [5:39:57,  1.62s/it]\u001b[A\n","1432it [5:39:59,  1.62s/it]\u001b[A\n","1433it [5:40:00,  1.62s/it]\u001b[A\n","1434it [5:40:02,  1.62s/it]\u001b[A\n","1435it [5:40:04,  1.62s/it]\u001b[A\n","1436it [5:40:05,  1.62s/it]\u001b[A\n","1437it [5:40:07,  1.62s/it]\u001b[A\n","1438it [5:40:08,  1.62s/it]\u001b[A\n","1439it [5:40:10,  1.62s/it]\u001b[A\n","1440it [5:40:12,  1.62s/it]\u001b[A\n","1441it [5:40:13,  1.62s/it]\u001b[A\n","1442it [5:40:15,  1.61s/it]\u001b[A\n","1443it [5:40:17,  1.61s/it]\u001b[A\n","1444it [5:40:18,  1.61s/it]\u001b[A\n","1445it [5:40:20,  1.61s/it]\u001b[A\n","1446it [5:40:21,  1.61s/it]\u001b[A\n","1447it [5:40:23,  1.61s/it]\u001b[A\n","1448it [5:40:25,  1.61s/it]\u001b[A\n","1449it [5:40:26,  1.61s/it]\u001b[A\n","1450it [5:40:28,  1.61s/it]\u001b[A\n","1451it [5:40:29,  1.61s/it]\u001b[A\n","1452it [5:40:31,  1.61s/it]\u001b[A\n","1453it [5:40:33,  1.61s/it]\u001b[A\n","1454it [5:40:34,  1.62s/it]\u001b[A\n","1455it [5:40:36,  1.62s/it]\u001b[A\n","1456it [5:40:38,  1.62s/it]\u001b[A\n","1457it [5:40:39,  1.62s/it]\u001b[A\n","1458it [5:40:41,  1.62s/it]\u001b[A\n","1459it [5:40:42,  1.63s/it]\u001b[A\n","1460it [5:40:44,  1.62s/it]\u001b[A\n","1461it [5:40:46,  1.62s/it]\u001b[A\n","1462it [5:40:47,  1.62s/it]\u001b[A\n","1463it [5:40:49,  1.62s/it]\u001b[A\n","1464it [5:40:51,  1.62s/it]\u001b[A\n","1465it [5:40:52,  1.62s/it]\u001b[A\n","1466it [5:40:54,  1.62s/it]\u001b[A\n","1467it [5:40:55,  1.62s/it]\u001b[A\n","1468it [5:40:57,  1.62s/it]\u001b[A\n","1469it [5:40:59,  1.62s/it]\u001b[A\n","1470it [5:41:00,  1.62s/it]\u001b[A\n","1471it [5:41:02,  1.62s/it]\u001b[A\n","1472it [5:41:04,  1.62s/it]\u001b[A\n","1473it [5:41:05,  1.62s/it]\u001b[A\n","1474it [5:41:07,  1.62s/it]\u001b[A\n","1475it [5:41:08,  1.62s/it]\u001b[A\n","1476it [5:41:10,  1.62s/it]\u001b[A\n","1477it [5:41:12,  1.62s/it]\u001b[A\n","1478it [5:41:13,  1.62s/it]\u001b[A\n","1479it [5:41:15,  1.62s/it]\u001b[A\n","1480it [5:41:16,  1.62s/it]\u001b[A\n","1481it [5:41:18,  1.62s/it]\u001b[A\n","1482it [5:41:20,  1.62s/it]\u001b[A\n","1483it [5:41:21,  1.62s/it]\u001b[A\n","1484it [5:41:23,  1.62s/it]\u001b[A\n","1485it [5:41:25,  1.62s/it]\u001b[A\n","1486it [5:41:26,  1.62s/it]\u001b[A\n","1487it [5:41:28,  1.62s/it]\u001b[A\n","1488it [5:41:29,  1.62s/it]\u001b[A\n","1489it [5:41:31,  1.62s/it]\u001b[A\n","1490it [5:41:33,  1.62s/it]\u001b[A\n","1491it [5:41:34,  1.62s/it]\u001b[A\n","1492it [5:41:36,  1.62s/it]\u001b[A\n","1493it [5:41:38,  1.62s/it]\u001b[A\n","1494it [5:41:39,  1.62s/it]\u001b[A\n","1495it [5:41:41,  1.62s/it]\u001b[A\n","1496it [5:41:42,  1.62s/it]\u001b[A\n","1497it [5:41:44,  1.62s/it]\u001b[A\n","1498it [5:41:46,  1.62s/it]\u001b[A\n","1499it [5:41:47,  1.62s/it]\u001b[A\n","1500it [5:41:49,  1.62s/it]\u001b[A\n","1501it [5:41:51,  1.62s/it]\u001b[A\n","1502it [5:41:52,  1.62s/it]\u001b[A\n","1503it [5:41:54,  1.62s/it]\u001b[A\n","1504it [5:41:55,  1.62s/it]\u001b[A\n","1505it [5:41:57,  1.62s/it]\u001b[A\n","1506it [5:41:59,  1.63s/it]\u001b[A\n","1507it [5:42:00,  1.62s/it]\u001b[A\n","1508it [5:42:02,  1.62s/it]\u001b[A\n","1509it [5:42:03,  1.62s/it]\u001b[A\n","1510it [5:42:05,  1.62s/it]\u001b[A\n","1511it [5:42:07,  1.62s/it]\u001b[A\n","1512it [5:42:08,  1.62s/it]\u001b[A\n","1513it [5:42:10,  1.62s/it]\u001b[A\n","1514it [5:42:12,  1.62s/it]\u001b[A\n","1515it [5:42:13,  1.62s/it]\u001b[A\n","1516it [5:42:15,  1.62s/it]\u001b[A\n","1517it [5:42:16,  1.62s/it]\u001b[A\n","1518it [5:42:18,  1.62s/it]\u001b[A\n","1519it [5:42:20,  1.62s/it]\u001b[A\n","1520it [5:42:21,  1.62s/it]\u001b[A\n","Epoch:  45% 9/20 [6:20:03<7:44:20, 2532.78s/it]11/07/2022 18:44:30 - INFO - src.trainer -   Train loss: 0.023369529030539772\n","11/07/2022 18:48:11 - INFO - src.trainer -   Train loss: 0.011964277787642046\n","11/07/2022 18:51:51 - INFO - src.trainer -   Train loss: 0.04066051136363636\n","11/07/2022 18:55:31 - INFO - src.trainer -   Train loss: 0.059774225408380684\n","11/07/2022 18:59:11 - INFO - src.trainer -   Train loss: 0.048608953302556816\n","11/07/2022 19:02:52 - INFO - src.trainer -   Train loss: 0.03627430308948864\n","11/07/2022 19:06:32 - INFO - src.trainer -   Train loss: 0.04918740012428977\n","11/07/2022 19:10:12 - INFO - src.trainer -   Train loss: 0.057718450372869316\n","11/07/2022 19:13:51 - INFO - src.trainer -   Train loss: 0.06043590198863636\n","11/07/2022 19:17:31 - INFO - src.trainer -   Train loss: 0.038423018022017044\n","\n","1522it [6:20:04, 679.52s/it]\u001b[A\n","1523it [6:20:06, 476.15s/it]\u001b[A\n","1524it [6:20:07, 333.78s/it]\u001b[A\n","1525it [6:20:09, 234.13s/it]\u001b[A\n","1526it [6:20:11, 164.37s/it]\u001b[A\n","1527it [6:20:12, 115.54s/it]\u001b[A\n","1528it [6:20:14, 81.36s/it] \u001b[A\n","1529it [6:20:15, 57.43s/it]\u001b[A\n","1530it [6:20:17, 40.68s/it]\u001b[A\n","1531it [6:20:19, 28.96s/it]\u001b[A\n","1532it [6:20:20, 20.75s/it]\u001b[A\n","1533it [6:20:22, 15.01s/it]\u001b[A\n","1534it [6:20:23, 10.99s/it]\u001b[A\n","1535it [6:20:25,  8.17s/it]\u001b[A\n","1536it [6:20:27,  6.20s/it]\u001b[A\n","1537it [6:20:28,  4.82s/it]\u001b[A\n","1538it [6:20:30,  3.85s/it]\u001b[A\n","1539it [6:20:31,  3.18s/it]\u001b[A\n","1540it [6:20:33,  2.71s/it]\u001b[A\n","1541it [6:20:35,  2.37s/it]\u001b[A\n","1542it [6:20:36,  2.14s/it]\u001b[A\n","1543it [6:20:38,  1.98s/it]\u001b[A\n","1544it [6:20:39,  1.87s/it]\u001b[A\n","1545it [6:20:41,  1.79s/it]\u001b[A\n","1546it [6:20:43,  1.73s/it]\u001b[A\n","1547it [6:20:44,  1.69s/it]\u001b[A\n","1548it [6:20:46,  1.67s/it]\u001b[A\n","1549it [6:20:47,  1.65s/it]\u001b[A\n","1550it [6:20:49,  1.63s/it]\u001b[A\n","1551it [6:20:51,  1.63s/it]\u001b[A\n","1552it [6:20:52,  1.62s/it]\u001b[A\n","1553it [6:20:54,  1.62s/it]\u001b[A\n","1554it [6:20:56,  1.61s/it]\u001b[A\n","1555it [6:20:57,  1.61s/it]\u001b[A\n","1556it [6:20:59,  1.61s/it]\u001b[A\n","1557it [6:21:00,  1.61s/it]\u001b[A\n","1558it [6:21:02,  1.61s/it]\u001b[A\n","1559it [6:21:04,  1.61s/it]\u001b[A\n","1560it [6:21:05,  1.61s/it]\u001b[A\n","1561it [6:21:07,  1.61s/it]\u001b[A\n","1562it [6:21:08,  1.61s/it]\u001b[A\n","1563it [6:21:10,  1.61s/it]\u001b[A\n","1564it [6:21:12,  1.61s/it]\u001b[A\n","1565it [6:21:13,  1.61s/it]\u001b[A\n","1566it [6:21:15,  1.61s/it]\u001b[A\n","1567it [6:21:16,  1.61s/it]\u001b[A\n","1568it [6:21:18,  1.61s/it]\u001b[A\n","1569it [6:21:20,  1.61s/it]\u001b[A\n","1570it [6:21:21,  1.61s/it]\u001b[A\n","1571it [6:21:23,  1.61s/it]\u001b[A\n","1572it [6:21:24,  1.61s/it]\u001b[A\n","1573it [6:21:26,  1.61s/it]\u001b[A\n","1574it [6:21:28,  1.60s/it]\u001b[A\n","1575it [6:21:29,  1.60s/it]\u001b[A\n","1576it [6:21:31,  1.60s/it]\u001b[A\n","1577it [6:21:32,  1.60s/it]\u001b[A\n","1578it [6:21:34,  1.61s/it]\u001b[A\n","1579it [6:21:36,  1.61s/it]\u001b[A\n","1580it [6:21:37,  1.60s/it]\u001b[A\n","1581it [6:21:39,  1.60s/it]\u001b[A\n","1582it [6:21:40,  1.60s/it]\u001b[A\n","1583it [6:21:42,  1.60s/it]\u001b[A\n","1584it [6:21:44,  1.60s/it]\u001b[A\n","1585it [6:21:45,  1.60s/it]\u001b[A\n","1586it [6:21:47,  1.60s/it]\u001b[A\n","1587it [6:21:48,  1.60s/it]\u001b[A\n","1588it [6:21:50,  1.61s/it]\u001b[A\n","1589it [6:21:52,  1.61s/it]\u001b[A\n","1590it [6:21:53,  1.61s/it]\u001b[A\n","1591it [6:21:55,  1.61s/it]\u001b[A\n","1592it [6:21:57,  1.62s/it]\u001b[A\n","1593it [6:21:58,  1.62s/it]\u001b[A\n","1594it [6:22:00,  1.62s/it]\u001b[A\n","1595it [6:22:01,  1.62s/it]\u001b[A\n","1596it [6:22:03,  1.61s/it]\u001b[A\n","1597it [6:22:05,  1.61s/it]\u001b[A\n","1598it [6:22:06,  1.62s/it]\u001b[A\n","1599it [6:22:08,  1.62s/it]\u001b[A\n","1600it [6:22:09,  1.62s/it]\u001b[A\n","1601it [6:22:11,  1.62s/it]\u001b[A\n","1602it [6:22:13,  1.62s/it]\u001b[A\n","1603it [6:22:14,  1.62s/it]\u001b[A\n","1604it [6:22:16,  1.62s/it]\u001b[A\n","1605it [6:22:18,  1.62s/it]\u001b[A\n","1606it [6:22:19,  1.62s/it]\u001b[A\n","1607it [6:22:21,  1.62s/it]\u001b[A\n","1608it [6:22:22,  1.62s/it]\u001b[A\n","1609it [6:22:24,  1.62s/it]\u001b[A\n","1610it [6:22:26,  1.62s/it]\u001b[A\n","1611it [6:22:27,  1.61s/it]\u001b[A\n","1612it [6:22:29,  1.61s/it]\u001b[A\n","1613it [6:22:31,  1.61s/it]\u001b[A\n","1614it [6:22:32,  1.61s/it]\u001b[A\n","1615it [6:22:34,  1.61s/it]\u001b[A\n","1616it [6:22:35,  1.61s/it]\u001b[A\n","1617it [6:22:37,  1.61s/it]\u001b[A\n","1618it [6:22:39,  1.61s/it]\u001b[A\n","1619it [6:22:40,  1.61s/it]\u001b[A\n","1620it [6:22:42,  1.61s/it]\u001b[A\n","1621it [6:22:43,  1.61s/it]\u001b[A\n","1622it [6:22:45,  1.61s/it]\u001b[A\n","1623it [6:22:47,  1.61s/it]\u001b[A\n","1624it [6:22:48,  1.62s/it]\u001b[A\n","1625it [6:22:50,  1.61s/it]\u001b[A\n","1626it [6:22:51,  1.62s/it]\u001b[A\n","1627it [6:22:53,  1.62s/it]\u001b[A\n","1628it [6:22:55,  1.62s/it]\u001b[A\n","1629it [6:22:56,  1.62s/it]\u001b[A\n","1630it [6:22:58,  1.62s/it]\u001b[A\n","1631it [6:23:00,  1.62s/it]\u001b[A\n","1632it [6:23:01,  1.62s/it]\u001b[A\n","1633it [6:23:03,  1.62s/it]\u001b[A\n","1634it [6:23:04,  1.62s/it]\u001b[A\n","1635it [6:23:06,  1.61s/it]\u001b[A\n","1636it [6:23:08,  1.61s/it]\u001b[A\n","1637it [6:23:09,  1.61s/it]\u001b[A\n","1638it [6:23:11,  1.62s/it]\u001b[A\n","1639it [6:23:12,  1.61s/it]\u001b[A\n","1640it [6:23:14,  1.61s/it]\u001b[A\n","1641it [6:23:16,  1.62s/it]\u001b[A\n","1642it [6:23:17,  1.62s/it]\u001b[A\n","1643it [6:23:19,  1.62s/it]\u001b[A\n","1644it [6:23:21,  1.62s/it]\u001b[A\n","1645it [6:23:22,  1.62s/it]\u001b[A\n","1646it [6:23:24,  1.61s/it]\u001b[A\n","1647it [6:23:25,  1.62s/it]\u001b[A\n","1648it [6:23:27,  1.62s/it]\u001b[A\n","1649it [6:23:29,  1.62s/it]\u001b[A\n","1650it [6:23:30,  1.62s/it]\u001b[A\n","1651it [6:23:32,  1.62s/it]\u001b[A\n","1652it [6:23:34,  1.62s/it]\u001b[A\n","1653it [6:23:35,  1.62s/it]\u001b[A\n","1654it [6:23:37,  1.62s/it]\u001b[A\n","1655it [6:23:38,  1.62s/it]\u001b[A\n","1656it [6:23:40,  1.62s/it]\u001b[A\n","1657it [6:23:42,  1.62s/it]\u001b[A\n","1658it [6:23:43,  1.62s/it]\u001b[A\n","1659it [6:23:45,  1.62s/it]\u001b[A\n","1660it [6:23:47,  1.62s/it]\u001b[A\n","1661it [6:23:48,  1.62s/it]\u001b[A\n","1662it [6:23:50,  1.62s/it]\u001b[A\n","1663it [6:23:51,  1.62s/it]\u001b[A\n","1664it [6:23:53,  1.62s/it]\u001b[A\n","1665it [6:23:55,  1.62s/it]\u001b[A\n","1666it [6:23:56,  1.62s/it]\u001b[A\n","1667it [6:23:58,  1.62s/it]\u001b[A\n","1668it [6:23:59,  1.62s/it]\u001b[A\n","1669it [6:24:01,  1.62s/it]\u001b[A\n","1670it [6:24:03,  1.62s/it]\u001b[A\n","1671it [6:24:04,  1.62s/it]\u001b[A\n","1672it [6:24:06,  1.62s/it]\u001b[A\n","1673it [6:24:08,  1.62s/it]\u001b[A\n","1674it [6:24:09,  1.62s/it]\u001b[A\n","1675it [6:24:11,  1.62s/it]\u001b[A\n","1676it [6:24:12,  1.62s/it]\u001b[A\n","1677it [6:24:14,  1.62s/it]\u001b[A\n","1678it [6:24:16,  1.62s/it]\u001b[A\n","1679it [6:24:17,  1.62s/it]\u001b[A\n","1680it [6:24:19,  1.62s/it]\u001b[A\n","1681it [6:24:20,  1.61s/it]\u001b[A\n","1682it [6:24:22,  1.61s/it]\u001b[A\n","1683it [6:24:24,  1.62s/it]\u001b[A\n","1684it [6:24:25,  1.61s/it]\u001b[A\n","1685it [6:24:27,  1.61s/it]\u001b[A\n","1686it [6:24:29,  1.61s/it]\u001b[A\n","1687it [6:24:30,  1.61s/it]\u001b[A\n","1688it [6:24:32,  1.61s/it]\u001b[A\n","1689it [6:24:33,  1.61s/it]\u001b[A\n","Epoch:  50% 10/20 [7:02:15<7:02:05, 2532.56s/it]11/07/2022 19:25:52 - INFO - src.trainer -   Train loss: 0.025204745205965908\n","11/07/2022 19:29:32 - INFO - src.trainer -   Train loss: 0.021394209428267044\n","11/07/2022 19:33:12 - INFO - src.trainer -   Train loss: 0.036436601118607956\n","11/07/2022 19:36:52 - INFO - src.trainer -   Train loss: 0.017580899325284092\n","11/07/2022 19:40:32 - INFO - src.trainer -   Train loss: 0.03701504794034091\n","11/07/2022 19:44:12 - INFO - src.trainer -   Train loss: 0.014057506214488636\n","11/07/2022 19:47:52 - INFO - src.trainer -   Train loss: 0.0126800537109375\n","11/07/2022 19:51:32 - INFO - src.trainer -   Train loss: 0.0051727294921875\n","11/07/2022 19:55:12 - INFO - src.trainer -   Train loss: 0.014860673384232954\n","11/07/2022 19:58:52 - INFO - src.trainer -   Train loss: 0.06865761496803978\n","\n","1691it [7:02:15, 679.17s/it]\u001b[A\n","1692it [7:02:17, 475.90s/it]\u001b[A\n","1693it [7:02:18, 333.61s/it]\u001b[A\n","1694it [7:02:20, 234.01s/it]\u001b[A\n","1695it [7:02:22, 164.29s/it]\u001b[A\n","1696it [7:02:23, 115.48s/it]\u001b[A\n","1697it [7:02:25, 81.32s/it] \u001b[A\n","1698it [7:02:26, 57.41s/it]\u001b[A\n","1699it [7:02:28, 40.67s/it]\u001b[A\n","1700it [7:02:30, 28.95s/it]\u001b[A\n","1701it [7:02:31, 20.75s/it]\u001b[A\n","1702it [7:02:33, 15.00s/it]\u001b[A\n","1703it [7:02:34, 10.99s/it]\u001b[A\n","1704it [7:02:36,  8.17s/it]\u001b[A\n","1705it [7:02:38,  6.20s/it]\u001b[A\n","1706it [7:02:39,  4.82s/it]\u001b[A\n","1707it [7:02:41,  3.85s/it]\u001b[A\n","1708it [7:02:42,  3.18s/it]\u001b[A\n","1709it [7:02:44,  2.71s/it]\u001b[A\n","1710it [7:02:46,  2.37s/it]\u001b[A\n","1711it [7:02:47,  2.14s/it]\u001b[A\n","1712it [7:02:49,  1.98s/it]\u001b[A\n","1713it [7:02:50,  1.87s/it]\u001b[A\n","1714it [7:02:52,  1.79s/it]\u001b[A\n","1715it [7:02:54,  1.73s/it]\u001b[A\n","1716it [7:02:55,  1.69s/it]\u001b[A\n","1717it [7:02:57,  1.67s/it]\u001b[A\n","1718it [7:02:58,  1.65s/it]\u001b[A\n","1719it [7:03:00,  1.63s/it]\u001b[A\n","1720it [7:03:02,  1.62s/it]\u001b[A\n","1721it [7:03:03,  1.62s/it]\u001b[A\n","1722it [7:03:05,  1.61s/it]\u001b[A\n","1723it [7:03:06,  1.61s/it]\u001b[A\n","1724it [7:03:08,  1.61s/it]\u001b[A\n","1725it [7:03:10,  1.60s/it]\u001b[A\n","1726it [7:03:11,  1.60s/it]\u001b[A\n","1727it [7:03:13,  1.60s/it]\u001b[A\n","1728it [7:03:14,  1.60s/it]\u001b[A\n","1729it [7:03:16,  1.60s/it]\u001b[A\n","1730it [7:03:18,  1.60s/it]\u001b[A\n","1731it [7:03:19,  1.61s/it]\u001b[A\n","1732it [7:03:21,  1.61s/it]\u001b[A\n","1733it [7:03:22,  1.60s/it]\u001b[A\n","1734it [7:03:24,  1.60s/it]\u001b[A\n","1735it [7:03:26,  1.60s/it]\u001b[A\n","1736it [7:03:27,  1.60s/it]\u001b[A\n","1737it [7:03:29,  1.60s/it]\u001b[A\n","1738it [7:03:30,  1.60s/it]\u001b[A\n","1739it [7:03:32,  1.60s/it]\u001b[A\n","1740it [7:03:34,  1.60s/it]\u001b[A\n","1741it [7:03:35,  1.60s/it]\u001b[A\n","1742it [7:03:37,  1.60s/it]\u001b[A\n","1743it [7:03:38,  1.60s/it]\u001b[A\n","1744it [7:03:40,  1.60s/it]\u001b[A\n","1745it [7:03:42,  1.60s/it]\u001b[A\n","1746it [7:03:43,  1.60s/it]\u001b[A\n","1747it [7:03:45,  1.60s/it]\u001b[A\n","1748it [7:03:46,  1.60s/it]\u001b[A\n","1749it [7:03:48,  1.60s/it]\u001b[A\n","1750it [7:03:50,  1.60s/it]\u001b[A\n","1751it [7:03:51,  1.60s/it]\u001b[A\n","1752it [7:03:53,  1.60s/it]\u001b[A\n","1753it [7:03:54,  1.60s/it]\u001b[A\n","1754it [7:03:56,  1.60s/it]\u001b[A\n","1755it [7:03:58,  1.60s/it]\u001b[A\n","1756it [7:03:59,  1.60s/it]\u001b[A\n","1757it [7:04:01,  1.60s/it]\u001b[A\n","1758it [7:04:03,  1.61s/it]\u001b[A\n","1759it [7:04:04,  1.61s/it]\u001b[A\n","1760it [7:04:06,  1.61s/it]\u001b[A\n","1761it [7:04:07,  1.61s/it]\u001b[A\n","1762it [7:04:09,  1.62s/it]\u001b[A\n","1763it [7:04:11,  1.61s/it]\u001b[A\n","1764it [7:04:12,  1.61s/it]\u001b[A\n","1765it [7:04:14,  1.61s/it]\u001b[A\n","1766it [7:04:15,  1.61s/it]\u001b[A\n","1767it [7:04:17,  1.61s/it]\u001b[A\n","1768it [7:04:19,  1.61s/it]\u001b[A\n","1769it [7:04:20,  1.61s/it]\u001b[A\n","1770it [7:04:22,  1.61s/it]\u001b[A\n","1771it [7:04:23,  1.61s/it]\u001b[A\n","1772it [7:04:25,  1.61s/it]\u001b[A\n","1773it [7:04:27,  1.62s/it]\u001b[A\n","1774it [7:04:28,  1.62s/it]\u001b[A\n","1775it [7:04:30,  1.62s/it]\u001b[A\n","1776it [7:04:32,  1.62s/it]\u001b[A\n","1777it [7:04:33,  1.62s/it]\u001b[A\n","1778it [7:04:35,  1.62s/it]\u001b[A\n","1779it [7:04:36,  1.62s/it]\u001b[A\n","1780it [7:04:38,  1.61s/it]\u001b[A\n","1781it [7:04:40,  1.61s/it]\u001b[A\n","1782it [7:04:41,  1.61s/it]\u001b[A\n","1783it [7:04:43,  1.61s/it]\u001b[A\n","1784it [7:04:44,  1.61s/it]\u001b[A\n","1785it [7:04:46,  1.61s/it]\u001b[A\n","1786it [7:04:48,  1.61s/it]\u001b[A\n","1787it [7:04:49,  1.61s/it]\u001b[A\n","1788it [7:04:51,  1.61s/it]\u001b[A\n","1789it [7:04:53,  1.61s/it]\u001b[A\n","1790it [7:04:54,  1.61s/it]\u001b[A\n","1791it [7:04:56,  1.61s/it]\u001b[A\n","1792it [7:04:57,  1.61s/it]\u001b[A\n","1793it [7:04:59,  1.61s/it]\u001b[A\n","1794it [7:05:01,  1.61s/it]\u001b[A\n","1795it [7:05:02,  1.62s/it]\u001b[A\n","1796it [7:05:04,  1.62s/it]\u001b[A\n","1797it [7:05:05,  1.62s/it]\u001b[A\n","1798it [7:05:07,  1.62s/it]\u001b[A\n","1799it [7:05:09,  1.62s/it]\u001b[A\n","1800it [7:05:10,  1.62s/it]\u001b[A\n","1801it [7:05:12,  1.62s/it]\u001b[A\n","1802it [7:05:14,  1.62s/it]\u001b[A\n","1803it [7:05:15,  1.62s/it]\u001b[A\n","1804it [7:05:17,  1.61s/it]\u001b[A\n","1805it [7:05:18,  1.61s/it]\u001b[A\n","1806it [7:05:20,  1.62s/it]\u001b[A\n","1807it [7:05:22,  1.62s/it]\u001b[A\n","1808it [7:05:23,  1.61s/it]\u001b[A\n","1809it [7:05:25,  1.61s/it]\u001b[A\n","1810it [7:05:26,  1.62s/it]\u001b[A\n","1811it [7:05:28,  1.62s/it]\u001b[A\n","1812it [7:05:30,  1.62s/it]\u001b[A\n","1813it [7:05:31,  1.62s/it]\u001b[A\n","1814it [7:05:33,  1.62s/it]\u001b[A\n","1815it [7:05:35,  1.62s/it]\u001b[A\n","1816it [7:05:36,  1.62s/it]\u001b[A\n","1817it [7:05:38,  1.61s/it]\u001b[A\n","1818it [7:05:39,  1.61s/it]\u001b[A\n","1819it [7:05:41,  1.61s/it]\u001b[A\n","1820it [7:05:43,  1.61s/it]\u001b[A\n","1821it [7:05:44,  1.61s/it]\u001b[A\n","1822it [7:05:46,  1.61s/it]\u001b[A\n","1823it [7:05:47,  1.62s/it]\u001b[A\n","1824it [7:05:49,  1.61s/it]\u001b[A\n","1825it [7:05:51,  1.62s/it]\u001b[A\n","1826it [7:05:52,  1.62s/it]\u001b[A\n","1827it [7:05:54,  1.62s/it]\u001b[A\n","1828it [7:05:56,  1.61s/it]\u001b[A\n","1829it [7:05:57,  1.62s/it]\u001b[A\n","1830it [7:05:59,  1.62s/it]\u001b[A\n","1831it [7:06:00,  1.61s/it]\u001b[A\n","1832it [7:06:02,  1.61s/it]\u001b[A\n","1833it [7:06:04,  1.61s/it]\u001b[A\n","1834it [7:06:05,  1.61s/it]\u001b[A\n","1835it [7:06:07,  1.61s/it]\u001b[A\n","1836it [7:06:08,  1.61s/it]\u001b[A\n","1837it [7:06:10,  1.61s/it]\u001b[A\n","1838it [7:06:12,  1.62s/it]\u001b[A\n","1839it [7:06:13,  1.62s/it]\u001b[A\n","1840it [7:06:15,  1.62s/it]\u001b[A\n","1841it [7:06:17,  1.62s/it]\u001b[A\n","1842it [7:06:18,  1.62s/it]\u001b[A\n","1843it [7:06:20,  1.62s/it]\u001b[A\n","1844it [7:06:21,  1.62s/it]\u001b[A\n","1845it [7:06:23,  1.62s/it]\u001b[A\n","1846it [7:06:25,  1.62s/it]\u001b[A\n","1847it [7:06:26,  1.62s/it]\u001b[A\n","1848it [7:06:28,  1.62s/it]\u001b[A\n","1849it [7:06:30,  1.62s/it]\u001b[A\n","1850it [7:06:31,  1.62s/it]\u001b[A\n","1851it [7:06:33,  1.62s/it]\u001b[A\n","1852it [7:06:34,  1.62s/it]\u001b[A\n","1853it [7:06:36,  1.62s/it]\u001b[A\n","1854it [7:06:38,  1.62s/it]\u001b[A\n","1855it [7:06:39,  1.62s/it]\u001b[A\n","1856it [7:06:41,  1.61s/it]\u001b[A\n","1857it [7:06:42,  1.61s/it]\u001b[A\n","1858it [7:06:44,  1.61s/it]\u001b[A\n","Epoch:  55% 11/20 [7:44:26<6:19:48, 2532.01s/it]11/07/2022 20:07:13 - INFO - src.trainer -   Train loss: 0.042921586470170456\n","11/07/2022 20:10:53 - INFO - src.trainer -   Train loss: 0.0211181640625\n","11/07/2022 20:14:33 - INFO - src.trainer -   Train loss: 0.01374539462002841\n","11/07/2022 20:18:13 - INFO - src.trainer -   Train loss: 0.012269453568892046\n","11/07/2022 20:21:53 - INFO - src.trainer -   Train loss: 0.017518477006392044\n","11/07/2022 20:25:33 - INFO - src.trainer -   Train loss: 0.05192149769176136\n","11/07/2022 20:29:13 - INFO - src.trainer -   Train loss: 0.018311934037642044\n","11/07/2022 20:32:53 - INFO - src.trainer -   Train loss: 0.04603160511363636\n","11/07/2022 20:36:33 - INFO - src.trainer -   Train loss: 0.04308804598721591\n","11/07/2022 20:40:13 - INFO - src.trainer -   Train loss: 0.008597634055397728\n","\n","1860it [7:44:27, 679.41s/it]\u001b[A\n","1861it [7:44:28, 476.07s/it]\u001b[A\n","1862it [7:44:30, 333.73s/it]\u001b[A\n","1863it [7:44:31, 234.09s/it]\u001b[A\n","1864it [7:44:33, 164.34s/it]\u001b[A\n","1865it [7:44:35, 115.52s/it]\u001b[A\n","1866it [7:44:36, 81.35s/it] \u001b[A\n","1867it [7:44:38, 57.42s/it]\u001b[A\n","1868it [7:44:39, 40.68s/it]\u001b[A\n","1869it [7:44:41, 28.96s/it]\u001b[A\n","1870it [7:44:43, 20.75s/it]\u001b[A\n","1871it [7:44:44, 15.01s/it]\u001b[A\n","1872it [7:44:46, 10.99s/it]\u001b[A\n","1873it [7:44:47,  8.17s/it]\u001b[A\n","1874it [7:44:49,  6.20s/it]\u001b[A\n","1875it [7:44:51,  4.82s/it]\u001b[A\n","1876it [7:44:52,  3.86s/it]\u001b[A\n","1877it [7:44:54,  3.18s/it]\u001b[A\n","1878it [7:44:55,  2.71s/it]\u001b[A\n","1879it [7:44:57,  2.38s/it]\u001b[A\n","1880it [7:44:59,  2.14s/it]\u001b[A\n","1881it [7:45:00,  1.98s/it]\u001b[A\n","1882it [7:45:02,  1.87s/it]\u001b[A\n","1883it [7:45:04,  1.80s/it]\u001b[A\n","1884it [7:45:05,  1.74s/it]\u001b[A\n","1885it [7:45:07,  1.70s/it]\u001b[A\n","1886it [7:45:08,  1.67s/it]\u001b[A\n","1887it [7:45:10,  1.65s/it]\u001b[A\n","1888it [7:45:12,  1.63s/it]\u001b[A\n","1889it [7:45:13,  1.62s/it]\u001b[A\n","1890it [7:45:15,  1.62s/it]\u001b[A\n","1891it [7:45:16,  1.62s/it]\u001b[A\n","1892it [7:45:18,  1.61s/it]\u001b[A\n","1893it [7:45:20,  1.61s/it]\u001b[A\n","1894it [7:45:21,  1.61s/it]\u001b[A\n","1895it [7:45:23,  1.61s/it]\u001b[A\n","1896it [7:45:24,  1.61s/it]\u001b[A\n","1897it [7:45:26,  1.61s/it]\u001b[A\n","1898it [7:45:28,  1.61s/it]\u001b[A\n","1899it [7:45:29,  1.61s/it]\u001b[A\n","1900it [7:45:31,  1.61s/it]\u001b[A\n","1901it [7:45:32,  1.61s/it]\u001b[A\n","1902it [7:45:34,  1.61s/it]\u001b[A\n","1903it [7:45:36,  1.61s/it]\u001b[A\n","1904it [7:45:37,  1.61s/it]\u001b[A\n","1905it [7:45:39,  1.61s/it]\u001b[A\n","1906it [7:45:40,  1.61s/it]\u001b[A\n","1907it [7:45:42,  1.60s/it]\u001b[A\n","1908it [7:45:44,  1.60s/it]\u001b[A\n","1909it [7:45:45,  1.61s/it]\u001b[A\n","1910it [7:45:47,  1.61s/it]\u001b[A\n","1911it [7:45:48,  1.60s/it]\u001b[A\n","1912it [7:45:50,  1.60s/it]\u001b[A\n","1913it [7:45:52,  1.60s/it]\u001b[A\n","1914it [7:45:53,  1.61s/it]\u001b[A\n","1915it [7:45:55,  1.60s/it]\u001b[A\n","1916it [7:45:56,  1.61s/it]\u001b[A\n","1917it [7:45:58,  1.61s/it]\u001b[A\n","1918it [7:46:00,  1.60s/it]\u001b[A\n","1919it [7:46:01,  1.60s/it]\u001b[A\n","1920it [7:46:03,  1.60s/it]\u001b[A\n","1921it [7:46:05,  1.61s/it]\u001b[A\n","1922it [7:46:06,  1.61s/it]\u001b[A\n","1923it [7:46:08,  1.61s/it]\u001b[A\n","1924it [7:46:09,  1.61s/it]\u001b[A\n","1925it [7:46:11,  1.61s/it]\u001b[A\n","1926it [7:46:13,  1.61s/it]\u001b[A\n","1927it [7:46:14,  1.61s/it]\u001b[A\n","1928it [7:46:16,  1.61s/it]\u001b[A\n","1929it [7:46:17,  1.62s/it]\u001b[A\n","1930it [7:46:19,  1.62s/it]\u001b[A\n","1931it [7:46:21,  1.62s/it]\u001b[A\n","1932it [7:46:22,  1.62s/it]\u001b[A\n","1933it [7:46:24,  1.62s/it]\u001b[A\n","1934it [7:46:25,  1.62s/it]\u001b[A\n","1935it [7:46:27,  1.61s/it]\u001b[A\n","1936it [7:46:29,  1.62s/it]\u001b[A\n","1937it [7:46:30,  1.62s/it]\u001b[A\n","1938it [7:46:32,  1.62s/it]\u001b[A\n","1939it [7:46:34,  1.62s/it]\u001b[A\n","1940it [7:46:35,  1.62s/it]\u001b[A\n","1941it [7:46:37,  1.62s/it]\u001b[A\n","1942it [7:46:38,  1.62s/it]\u001b[A\n","1943it [7:46:40,  1.62s/it]\u001b[A\n","1944it [7:46:42,  1.62s/it]\u001b[A\n","1945it [7:46:43,  1.62s/it]\u001b[A\n","1946it [7:46:45,  1.62s/it]\u001b[A\n","1947it [7:46:47,  1.62s/it]\u001b[A\n","1948it [7:46:48,  1.62s/it]\u001b[A\n","1949it [7:46:50,  1.61s/it]\u001b[A\n","1950it [7:46:51,  1.61s/it]\u001b[A\n","1951it [7:46:53,  1.62s/it]\u001b[A\n","1952it [7:46:55,  1.61s/it]\u001b[A\n","1953it [7:46:56,  1.62s/it]\u001b[A\n","1954it [7:46:58,  1.62s/it]\u001b[A\n","1955it [7:46:59,  1.61s/it]\u001b[A\n","1956it [7:47:01,  1.61s/it]\u001b[A\n","1957it [7:47:03,  1.61s/it]\u001b[A\n","1958it [7:47:04,  1.61s/it]\u001b[A\n","1959it [7:47:06,  1.61s/it]\u001b[A\n","1960it [7:47:08,  1.62s/it]\u001b[A\n","1961it [7:47:09,  1.62s/it]\u001b[A\n","1962it [7:47:11,  1.62s/it]\u001b[A\n","1963it [7:47:12,  1.62s/it]\u001b[A\n","1964it [7:47:14,  1.62s/it]\u001b[A\n","1965it [7:47:16,  1.62s/it]\u001b[A\n","1966it [7:47:17,  1.62s/it]\u001b[A\n","1967it [7:47:19,  1.62s/it]\u001b[A\n","1968it [7:47:20,  1.62s/it]\u001b[A\n","1969it [7:47:22,  1.62s/it]\u001b[A\n","1970it [7:47:24,  1.62s/it]\u001b[A\n","1971it [7:47:25,  1.62s/it]\u001b[A\n","1972it [7:47:27,  1.62s/it]\u001b[A\n","1973it [7:47:29,  1.62s/it]\u001b[A\n","1974it [7:47:30,  1.62s/it]\u001b[A\n","1975it [7:47:32,  1.62s/it]\u001b[A\n","1976it [7:47:33,  1.62s/it]\u001b[A\n","1977it [7:47:35,  1.61s/it]\u001b[A\n","1978it [7:47:37,  1.62s/it]\u001b[A\n","1979it [7:47:38,  1.62s/it]\u001b[A\n","1980it [7:47:40,  1.62s/it]\u001b[A\n","1981it [7:47:42,  1.62s/it]\u001b[A\n","1982it [7:47:43,  1.62s/it]\u001b[A\n","1983it [7:47:45,  1.62s/it]\u001b[A\n","1984it [7:47:46,  1.62s/it]\u001b[A\n","1985it [7:47:48,  1.62s/it]\u001b[A\n","1986it [7:47:50,  1.62s/it]\u001b[A\n","1987it [7:47:51,  1.62s/it]\u001b[A\n","1988it [7:47:53,  1.62s/it]\u001b[A\n","1989it [7:47:54,  1.62s/it]\u001b[A\n","1990it [7:47:56,  1.62s/it]\u001b[A\n","1991it [7:47:58,  1.62s/it]\u001b[A\n","1992it [7:47:59,  1.62s/it]\u001b[A\n","1993it [7:48:01,  1.62s/it]\u001b[A\n","1994it [7:48:03,  1.62s/it]\u001b[A\n","1995it [7:48:04,  1.62s/it]\u001b[A\n","1996it [7:48:06,  1.62s/it]\u001b[A\n","1997it [7:48:07,  1.62s/it]\u001b[A\n","1998it [7:48:09,  1.62s/it]\u001b[A\n","1999it [7:48:11,  1.62s/it]\u001b[A\n","2000it [7:48:12,  1.62s/it]\u001b[A\n","2001it [7:48:14,  1.62s/it]\u001b[A\n","2002it [7:48:16,  1.62s/it]\u001b[A\n","2003it [7:48:17,  1.61s/it]\u001b[A\n","2004it [7:48:19,  1.61s/it]\u001b[A\n","2005it [7:48:20,  1.61s/it]\u001b[A\n","2006it [7:48:22,  1.62s/it]\u001b[A\n","2007it [7:48:24,  1.62s/it]\u001b[A\n","2008it [7:48:25,  1.62s/it]\u001b[A\n","2009it [7:48:27,  1.62s/it]\u001b[A\n","2010it [7:48:28,  1.62s/it]\u001b[A\n","2011it [7:48:30,  1.62s/it]\u001b[A\n","2012it [7:48:32,  1.62s/it]\u001b[A\n","2013it [7:48:33,  1.62s/it]\u001b[A\n","2014it [7:48:35,  1.62s/it]\u001b[A\n","2015it [7:48:37,  1.62s/it]\u001b[A\n","2016it [7:48:38,  1.62s/it]\u001b[A\n","2017it [7:48:40,  1.62s/it]\u001b[A\n","2018it [7:48:41,  1.62s/it]\u001b[A\n","2019it [7:48:43,  1.62s/it]\u001b[A\n","2020it [7:48:45,  1.62s/it]\u001b[A\n","2021it [7:48:46,  1.62s/it]\u001b[A\n","2022it [7:48:48,  1.62s/it]\u001b[A\n","2023it [7:48:49,  1.62s/it]\u001b[A\n","2024it [7:48:51,  1.62s/it]\u001b[A\n","2025it [7:48:53,  1.62s/it]\u001b[A\n","2026it [7:48:54,  1.62s/it]\u001b[A\n","2027it [7:48:56,  1.62s/it]\u001b[A\n","Epoch:  60% 12/20 [8:26:37<5:37:35, 2531.96s/it]11/07/2022 20:48:35 - INFO - src.trainer -   Train loss: 0.061155839399857956\n","11/07/2022 20:52:15 - INFO - src.trainer -   Train loss: 0.004666415127840909\n","11/07/2022 20:55:55 - INFO - src.trainer -   Train loss: 0.027418656782670456\n","11/07/2022 20:59:35 - INFO - src.trainer -   Train loss: 0.015604192560369318\n","11/07/2022 21:03:15 - INFO - src.trainer -   Train loss: 0.010629827325994318\n","11/07/2022 21:06:55 - INFO - src.trainer -   Train loss: 0.02673894708806818\n","11/07/2022 21:10:34 - INFO - src.trainer -   Train loss: 0.0017450506036931818\n","11/07/2022 21:14:14 - INFO - src.trainer -   Train loss: 0.013956243341619318\n","11/07/2022 21:17:54 - INFO - src.trainer -   Train loss: 0.009529807350852272\n","11/07/2022 21:21:34 - INFO - src.trainer -   Train loss: 0.01410189541903409\n","\n","2029it [8:26:37, 678.97s/it]\u001b[A\n","2030it [8:26:39, 475.76s/it]\u001b[A\n","2031it [8:26:40, 333.51s/it]\u001b[A\n","2032it [8:26:42, 233.94s/it]\u001b[A\n","2033it [8:26:43, 164.24s/it]\u001b[A\n","2034it [8:26:45, 115.45s/it]\u001b[A\n","2035it [8:26:47, 81.29s/it] \u001b[A\n","2036it [8:26:48, 57.39s/it]\u001b[A\n","2037it [8:26:50, 40.65s/it]\u001b[A\n","2038it [8:26:51, 28.94s/it]\u001b[A\n","2039it [8:26:53, 20.74s/it]\u001b[A\n","2040it [8:26:55, 15.00s/it]\u001b[A\n","2041it [8:26:56, 10.98s/it]\u001b[A\n","2042it [8:26:58,  8.17s/it]\u001b[A\n","2043it [8:26:59,  6.20s/it]\u001b[A\n","2044it [8:27:01,  4.82s/it]\u001b[A\n","2045it [8:27:03,  3.85s/it]\u001b[A\n","2046it [8:27:04,  3.18s/it]\u001b[A\n","2047it [8:27:06,  2.71s/it]\u001b[A\n","2048it [8:27:07,  2.37s/it]\u001b[A\n","2049it [8:27:09,  2.14s/it]\u001b[A\n","2050it [8:27:11,  1.98s/it]\u001b[A\n","2051it [8:27:12,  1.87s/it]\u001b[A\n","2052it [8:27:14,  1.79s/it]\u001b[A\n","2053it [8:27:15,  1.73s/it]\u001b[A\n","2054it [8:27:17,  1.69s/it]\u001b[A\n","2055it [8:27:19,  1.67s/it]\u001b[A\n","2056it [8:27:20,  1.65s/it]\u001b[A\n","2057it [8:27:22,  1.63s/it]\u001b[A\n","2058it [8:27:23,  1.62s/it]\u001b[A\n","2059it [8:27:25,  1.62s/it]\u001b[A\n","2060it [8:27:27,  1.61s/it]\u001b[A\n","2061it [8:27:28,  1.61s/it]\u001b[A\n","2062it [8:27:30,  1.61s/it]\u001b[A\n","2063it [8:27:32,  1.61s/it]\u001b[A\n","2064it [8:27:33,  1.60s/it]\u001b[A\n","2065it [8:27:35,  1.60s/it]\u001b[A\n","2066it [8:27:36,  1.60s/it]\u001b[A\n","2067it [8:27:38,  1.60s/it]\u001b[A\n","2068it [8:27:40,  1.60s/it]\u001b[A\n","2069it [8:27:41,  1.61s/it]\u001b[A\n","2070it [8:27:43,  1.60s/it]\u001b[A\n","2071it [8:27:44,  1.60s/it]\u001b[A\n","2072it [8:27:46,  1.60s/it]\u001b[A\n","2073it [8:27:48,  1.60s/it]\u001b[A\n","2074it [8:27:49,  1.60s/it]\u001b[A\n","2075it [8:27:51,  1.60s/it]\u001b[A\n","2076it [8:27:52,  1.60s/it]\u001b[A\n","2077it [8:27:54,  1.60s/it]\u001b[A\n","2078it [8:27:56,  1.60s/it]\u001b[A\n","2079it [8:27:57,  1.60s/it]\u001b[A\n","2080it [8:27:59,  1.60s/it]\u001b[A\n","2081it [8:28:00,  1.60s/it]\u001b[A\n","2082it [8:28:02,  1.60s/it]\u001b[A\n","2083it [8:28:04,  1.60s/it]\u001b[A\n","2084it [8:28:05,  1.60s/it]\u001b[A\n","2085it [8:28:07,  1.60s/it]\u001b[A\n","2086it [8:28:08,  1.60s/it]\u001b[A\n","2087it [8:28:10,  1.60s/it]\u001b[A\n","2088it [8:28:12,  1.60s/it]\u001b[A\n","2089it [8:28:13,  1.61s/it]\u001b[A\n","2090it [8:28:15,  1.60s/it]\u001b[A\n","2091it [8:28:16,  1.60s/it]\u001b[A\n","2092it [8:28:18,  1.60s/it]\u001b[A\n","2093it [8:28:20,  1.60s/it]\u001b[A\n","2094it [8:28:21,  1.60s/it]\u001b[A\n","2095it [8:28:23,  1.61s/it]\u001b[A\n","2096it [8:28:24,  1.61s/it]\u001b[A\n","2097it [8:28:26,  1.61s/it]\u001b[A\n","2098it [8:28:28,  1.61s/it]\u001b[A\n","2099it [8:28:29,  1.62s/it]\u001b[A\n","2100it [8:28:31,  1.62s/it]\u001b[A\n","2101it [8:28:32,  1.61s/it]\u001b[A\n","2102it [8:28:34,  1.62s/it]\u001b[A\n","2103it [8:28:36,  1.61s/it]\u001b[A\n","2104it [8:28:37,  1.61s/it]\u001b[A\n","2105it [8:28:39,  1.62s/it]\u001b[A\n","2106it [8:28:41,  1.62s/it]\u001b[A\n","2107it [8:28:42,  1.62s/it]\u001b[A\n","2108it [8:28:44,  1.62s/it]\u001b[A\n","2109it [8:28:45,  1.62s/it]\u001b[A\n","2110it [8:28:47,  1.62s/it]\u001b[A\n","2111it [8:28:49,  1.62s/it]\u001b[A\n","2112it [8:28:50,  1.62s/it]\u001b[A\n","2113it [8:28:52,  1.62s/it]\u001b[A\n","2114it [8:28:54,  1.62s/it]\u001b[A\n","2115it [8:28:55,  1.62s/it]\u001b[A\n","2116it [8:28:57,  1.62s/it]\u001b[A\n","2117it [8:28:58,  1.62s/it]\u001b[A\n","2118it [8:29:00,  1.61s/it]\u001b[A\n","2119it [8:29:02,  1.61s/it]\u001b[A\n","2120it [8:29:03,  1.62s/it]\u001b[A\n","2121it [8:29:05,  1.61s/it]\u001b[A\n","2122it [8:29:06,  1.61s/it]\u001b[A\n","2123it [8:29:08,  1.61s/it]\u001b[A\n","2124it [8:29:10,  1.61s/it]\u001b[A\n","2125it [8:29:11,  1.61s/it]\u001b[A\n","2126it [8:29:13,  1.61s/it]\u001b[A\n","2127it [8:29:15,  1.61s/it]\u001b[A\n","2128it [8:29:16,  1.61s/it]\u001b[A\n","2129it [8:29:18,  1.61s/it]\u001b[A\n","2130it [8:29:19,  1.61s/it]\u001b[A\n","2131it [8:29:21,  1.62s/it]\u001b[A\n","2132it [8:29:23,  1.62s/it]\u001b[A\n","2133it [8:29:24,  1.62s/it]\u001b[A\n","2134it [8:29:26,  1.61s/it]\u001b[A\n","2135it [8:29:27,  1.62s/it]\u001b[A\n","2136it [8:29:29,  1.62s/it]\u001b[A\n","2137it [8:29:31,  1.62s/it]\u001b[A\n","2138it [8:29:32,  1.62s/it]\u001b[A\n","2139it [8:29:34,  1.62s/it]\u001b[A\n","2140it [8:29:36,  1.62s/it]\u001b[A\n","2141it [8:29:37,  1.62s/it]\u001b[A\n","2142it [8:29:39,  1.61s/it]\u001b[A\n","2143it [8:29:40,  1.61s/it]\u001b[A\n","2144it [8:29:42,  1.61s/it]\u001b[A\n","2145it [8:29:44,  1.62s/it]\u001b[A\n","2146it [8:29:45,  1.61s/it]\u001b[A\n","2147it [8:29:47,  1.62s/it]\u001b[A\n","2148it [8:29:48,  1.62s/it]\u001b[A\n","2149it [8:29:50,  1.62s/it]\u001b[A\n","2150it [8:29:52,  1.62s/it]\u001b[A\n","2151it [8:29:53,  1.62s/it]\u001b[A\n","2152it [8:29:55,  1.62s/it]\u001b[A\n","2153it [8:29:57,  1.62s/it]\u001b[A\n","2154it [8:29:58,  1.62s/it]\u001b[A\n","2155it [8:30:00,  1.62s/it]\u001b[A\n","2156it [8:30:01,  1.62s/it]\u001b[A\n","2157it [8:30:03,  1.62s/it]\u001b[A\n","2158it [8:30:05,  1.62s/it]\u001b[A\n","2159it [8:30:06,  1.62s/it]\u001b[A\n","2160it [8:30:08,  1.62s/it]\u001b[A\n","2161it [8:30:10,  1.62s/it]\u001b[A\n","2162it [8:30:11,  1.62s/it]\u001b[A\n","2163it [8:30:13,  1.62s/it]\u001b[A\n","2164it [8:30:14,  1.62s/it]\u001b[A\n","2165it [8:30:16,  1.62s/it]\u001b[A\n","2166it [8:30:18,  1.62s/it]\u001b[A\n","2167it [8:30:19,  1.62s/it]\u001b[A\n","2168it [8:30:21,  1.62s/it]\u001b[A\n","2169it [8:30:22,  1.62s/it]\u001b[A\n","2170it [8:30:24,  1.61s/it]\u001b[A\n","2171it [8:30:26,  1.61s/it]\u001b[A\n","2172it [8:30:27,  1.61s/it]\u001b[A\n","2173it [8:30:29,  1.61s/it]\u001b[A\n","2174it [8:30:31,  1.61s/it]\u001b[A\n","2175it [8:30:32,  1.61s/it]\u001b[A\n","2176it [8:30:34,  1.62s/it]\u001b[A\n","2177it [8:30:35,  1.62s/it]\u001b[A\n","2178it [8:30:37,  1.62s/it]\u001b[A\n","2179it [8:30:39,  1.62s/it]\u001b[A\n","2180it [8:30:40,  1.62s/it]\u001b[A\n","2181it [8:30:42,  1.62s/it]\u001b[A\n","2182it [8:30:43,  1.62s/it]\u001b[A\n","2183it [8:30:45,  1.62s/it]\u001b[A\n","2184it [8:30:47,  1.62s/it]\u001b[A\n","2185it [8:30:48,  1.62s/it]\u001b[A\n","2186it [8:30:50,  1.62s/it]\u001b[A\n","2187it [8:30:52,  1.62s/it]\u001b[A\n","2188it [8:30:53,  1.62s/it]\u001b[A\n","2189it [8:30:55,  1.62s/it]\u001b[A\n","2190it [8:30:56,  1.62s/it]\u001b[A\n","2191it [8:30:58,  1.62s/it]\u001b[A\n","2192it [8:31:00,  1.62s/it]\u001b[A\n","2193it [8:31:01,  1.62s/it]\u001b[A\n","2194it [8:31:03,  1.61s/it]\u001b[A\n","2195it [8:31:04,  1.62s/it]\u001b[A\n","2196it [8:31:06,  1.61s/it]\u001b[A\n","Epoch:  65% 13/20 [9:08:48<4:55:19, 2531.41s/it]11/07/2022 21:29:55 - INFO - src.trainer -   Train loss: 0.0140228271484375\n","11/07/2022 21:33:35 - INFO - src.trainer -   Train loss: 0.004387595436789773\n","11/07/2022 21:37:15 - INFO - src.trainer -   Train loss: 0.01688593084161932\n","11/07/2022 21:40:55 - INFO - src.trainer -   Train loss: 0.011356700550426136\n","11/07/2022 21:44:35 - INFO - src.trainer -   Train loss: 0.006938587535511364\n","11/07/2022 21:48:15 - INFO - src.trainer -   Train loss: 0.010597922585227272\n","11/07/2022 21:51:55 - INFO - src.trainer -   Train loss: 0.02834528142755682\n","11/07/2022 21:55:34 - INFO - src.trainer -   Train loss: 0.006874778053977273\n","11/07/2022 21:59:14 - INFO - src.trainer -   Train loss: 0.008962457830255682\n","11/07/2022 22:02:54 - INFO - src.trainer -   Train loss: 0.00863370028409091\n","11/07/2022 22:06:34 - INFO - src.trainer -   Train loss: 0.015400279651988636\n","\n","2198it [9:08:47, 678.83s/it]\u001b[A\n","2199it [9:08:48, 475.66s/it]\u001b[A\n","2200it [9:08:50, 333.44s/it]\u001b[A\n","2201it [9:08:51, 233.89s/it]\u001b[A\n","2202it [9:08:53, 164.20s/it]\u001b[A\n","2203it [9:08:55, 115.42s/it]\u001b[A\n","2204it [9:08:56, 81.28s/it] \u001b[A\n","2205it [9:08:58, 57.37s/it]\u001b[A\n","2206it [9:08:59, 40.64s/it]\u001b[A\n","2207it [9:09:01, 28.93s/it]\u001b[A\n","2208it [9:09:03, 20.73s/it]\u001b[A\n","2209it [9:09:04, 14.99s/it]\u001b[A\n","2210it [9:09:06, 10.98s/it]\u001b[A\n","2211it [9:09:08,  8.16s/it]\u001b[A\n","2212it [9:09:09,  6.20s/it]\u001b[A\n","2213it [9:09:11,  4.82s/it]\u001b[A\n","2214it [9:09:12,  3.85s/it]\u001b[A\n","2215it [9:09:14,  3.17s/it]\u001b[A\n","2216it [9:09:16,  2.70s/it]\u001b[A\n","2217it [9:09:17,  2.38s/it]\u001b[A\n","2218it [9:09:19,  2.14s/it]\u001b[A\n","2219it [9:09:20,  1.98s/it]\u001b[A\n","2220it [9:09:22,  1.87s/it]\u001b[A\n","2221it [9:09:24,  1.79s/it]\u001b[A\n","2222it [9:09:25,  1.73s/it]\u001b[A\n","2223it [9:09:27,  1.69s/it]\u001b[A\n","2224it [9:09:28,  1.67s/it]\u001b[A\n","2225it [9:09:30,  1.65s/it]\u001b[A\n","2226it [9:09:32,  1.63s/it]\u001b[A\n","2227it [9:09:33,  1.62s/it]\u001b[A\n","2228it [9:09:35,  1.62s/it]\u001b[A\n","2229it [9:09:36,  1.61s/it]\u001b[A\n","2230it [9:09:38,  1.61s/it]\u001b[A\n","2231it [9:09:40,  1.61s/it]\u001b[A\n","2232it [9:09:41,  1.60s/it]\u001b[A\n","2233it [9:09:43,  1.60s/it]\u001b[A\n","2234it [9:09:44,  1.60s/it]\u001b[A\n","2235it [9:09:46,  1.60s/it]\u001b[A\n","2236it [9:09:48,  1.60s/it]\u001b[A\n","2237it [9:09:49,  1.60s/it]\u001b[A\n","2238it [9:09:51,  1.60s/it]\u001b[A\n","2239it [9:09:52,  1.61s/it]\u001b[A\n","2240it [9:09:54,  1.60s/it]\u001b[A\n","2241it [9:09:56,  1.60s/it]\u001b[A\n","2242it [9:09:57,  1.60s/it]\u001b[A\n","2243it [9:09:59,  1.60s/it]\u001b[A\n","2244it [9:10:00,  1.60s/it]\u001b[A\n","2245it [9:10:02,  1.60s/it]\u001b[A\n","2246it [9:10:04,  1.60s/it]\u001b[A\n","2247it [9:10:05,  1.60s/it]\u001b[A\n","2248it [9:10:07,  1.60s/it]\u001b[A\n","2249it [9:10:08,  1.60s/it]\u001b[A\n","2250it [9:10:10,  1.60s/it]\u001b[A\n","2251it [9:10:12,  1.60s/it]\u001b[A\n","2252it [9:10:13,  1.60s/it]\u001b[A\n","2253it [9:10:15,  1.60s/it]\u001b[A\n","2254it [9:10:16,  1.60s/it]\u001b[A\n","2255it [9:10:18,  1.60s/it]\u001b[A\n","2256it [9:10:20,  1.60s/it]\u001b[A\n","2257it [9:10:21,  1.60s/it]\u001b[A\n","2258it [9:10:23,  1.60s/it]\u001b[A\n","2259it [9:10:24,  1.60s/it]\u001b[A\n","2260it [9:10:26,  1.60s/it]\u001b[A\n","2261it [9:10:28,  1.60s/it]\u001b[A\n","2262it [9:10:29,  1.60s/it]\u001b[A\n","2263it [9:10:31,  1.60s/it]\u001b[A\n","2264it [9:10:32,  1.60s/it]\u001b[A\n","2265it [9:10:34,  1.61s/it]\u001b[A\n","2266it [9:10:36,  1.61s/it]\u001b[A\n","2267it [9:10:37,  1.61s/it]\u001b[A\n","2268it [9:10:39,  1.61s/it]\u001b[A\n","2269it [9:10:40,  1.62s/it]\u001b[A\n","2270it [9:10:42,  1.61s/it]\u001b[A\n","2271it [9:10:44,  1.62s/it]\u001b[A\n","2272it [9:10:45,  1.61s/it]\u001b[A\n","2273it [9:10:47,  1.61s/it]\u001b[A\n","2274it [9:10:49,  1.62s/it]\u001b[A\n","2275it [9:10:50,  1.62s/it]\u001b[A\n","2276it [9:10:52,  1.61s/it]\u001b[A\n","2277it [9:10:53,  1.61s/it]\u001b[A\n","2278it [9:10:55,  1.62s/it]\u001b[A\n","2279it [9:10:57,  1.61s/it]\u001b[A\n","2280it [9:10:58,  1.61s/it]\u001b[A\n","2281it [9:11:00,  1.62s/it]\u001b[A\n","2282it [9:11:01,  1.62s/it]\u001b[A\n","2283it [9:11:03,  1.62s/it]\u001b[A\n","2284it [9:11:05,  1.62s/it]\u001b[A\n","2285it [9:11:06,  1.61s/it]\u001b[A\n","2286it [9:11:08,  1.61s/it]\u001b[A\n","2287it [9:11:10,  1.61s/it]\u001b[A\n","2288it [9:11:11,  1.61s/it]\u001b[A\n","2289it [9:11:13,  1.61s/it]\u001b[A\n","2290it [9:11:14,  1.61s/it]\u001b[A\n","2291it [9:11:16,  1.61s/it]\u001b[A\n","2292it [9:11:18,  1.61s/it]\u001b[A\n","2293it [9:11:19,  1.61s/it]\u001b[A\n","2294it [9:11:21,  1.61s/it]\u001b[A\n","2295it [9:11:22,  1.61s/it]\u001b[A\n","2296it [9:11:24,  1.61s/it]\u001b[A\n","2297it [9:11:26,  1.61s/it]\u001b[A\n","2298it [9:11:27,  1.61s/it]\u001b[A\n","2299it [9:11:29,  1.61s/it]\u001b[A\n","2300it [9:11:30,  1.61s/it]\u001b[A\n","2301it [9:11:32,  1.61s/it]\u001b[A\n","2302it [9:11:34,  1.62s/it]\u001b[A\n","2303it [9:11:35,  1.61s/it]\u001b[A\n","2304it [9:11:37,  1.62s/it]\u001b[A\n","2305it [9:11:39,  1.62s/it]\u001b[A\n","2306it [9:11:40,  1.62s/it]\u001b[A\n","2307it [9:11:42,  1.62s/it]\u001b[A\n","2308it [9:11:43,  1.62s/it]\u001b[A\n","2309it [9:11:45,  1.62s/it]\u001b[A\n","2310it [9:11:47,  1.62s/it]\u001b[A\n","2311it [9:11:48,  1.61s/it]\u001b[A\n","2312it [9:11:50,  1.61s/it]\u001b[A\n","2313it [9:11:51,  1.62s/it]\u001b[A\n","2314it [9:11:53,  1.62s/it]\u001b[A\n","2315it [9:11:55,  1.61s/it]\u001b[A\n","2316it [9:11:56,  1.61s/it]\u001b[A\n","2317it [9:11:58,  1.62s/it]\u001b[A\n","2318it [9:12:00,  1.62s/it]\u001b[A\n","2319it [9:12:01,  1.62s/it]\u001b[A\n","2320it [9:12:03,  1.62s/it]\u001b[A\n","2321it [9:12:04,  1.62s/it]\u001b[A\n","2322it [9:12:06,  1.62s/it]\u001b[A\n","2323it [9:12:08,  1.62s/it]\u001b[A\n","2324it [9:12:09,  1.62s/it]\u001b[A\n","2325it [9:12:11,  1.61s/it]\u001b[A\n","2326it [9:12:13,  1.62s/it]\u001b[A\n","2327it [9:12:14,  1.61s/it]\u001b[A\n","2328it [9:12:16,  1.62s/it]\u001b[A\n","2329it [9:12:17,  1.62s/it]\u001b[A\n","2330it [9:12:19,  1.62s/it]\u001b[A\n","2331it [9:12:21,  1.62s/it]\u001b[A\n","2332it [9:12:22,  1.62s/it]\u001b[A\n","2333it [9:12:24,  1.62s/it]\u001b[A\n","2334it [9:12:25,  1.62s/it]\u001b[A\n","2335it [9:12:27,  1.62s/it]\u001b[A\n","2336it [9:12:29,  1.62s/it]\u001b[A\n","2337it [9:12:30,  1.62s/it]\u001b[A\n","2338it [9:12:32,  1.62s/it]\u001b[A\n","2339it [9:12:34,  1.62s/it]\u001b[A\n","2340it [9:12:35,  1.62s/it]\u001b[A\n","2341it [9:12:37,  1.61s/it]\u001b[A\n","2342it [9:12:38,  1.61s/it]\u001b[A\n","2343it [9:12:40,  1.61s/it]\u001b[A\n","2344it [9:12:42,  1.61s/it]\u001b[A\n","2345it [9:12:43,  1.62s/it]\u001b[A\n","2346it [9:12:45,  1.62s/it]\u001b[A\n","2347it [9:12:46,  1.62s/it]\u001b[A\n","2348it [9:12:48,  1.62s/it]\u001b[A\n","2349it [9:12:50,  1.62s/it]\u001b[A\n","2350it [9:12:51,  1.62s/it]\u001b[A\n","2351it [9:12:53,  1.62s/it]\u001b[A\n","2352it [9:12:55,  1.62s/it]\u001b[A\n","2353it [9:12:56,  1.62s/it]\u001b[A\n","2354it [9:12:58,  1.62s/it]\u001b[A\n","2355it [9:12:59,  1.62s/it]\u001b[A\n","2356it [9:13:01,  1.62s/it]\u001b[A\n","2357it [9:13:03,  1.62s/it]\u001b[A\n","2358it [9:13:04,  1.62s/it]\u001b[A\n","2359it [9:13:06,  1.62s/it]\u001b[A\n","2360it [9:13:07,  1.61s/it]\u001b[A\n","2361it [9:13:09,  1.62s/it]\u001b[A\n","2362it [9:13:11,  1.62s/it]\u001b[A\n","2363it [9:13:12,  1.61s/it]\u001b[A\n","2364it [9:13:14,  1.61s/it]\u001b[A\n","2365it [9:13:16,  1.61s/it]\u001b[A\n","Epoch:  70% 14/20 [9:50:57<4:13:05, 2530.85s/it]11/07/2022 22:14:54 - INFO - src.trainer -   Train loss: 0.014445911754261364\n","11/07/2022 22:18:34 - INFO - src.trainer -   Train loss: 0.010911421342329546\n","11/07/2022 22:22:15 - INFO - src.trainer -   Train loss: 0.004560990767045455\n","11/07/2022 22:25:55 - INFO - src.trainer -   Train loss: 0.004698319868607955\n","11/07/2022 22:29:35 - INFO - src.trainer -   Train loss: 0.01888483220880682\n","11/07/2022 22:33:14 - INFO - src.trainer -   Train loss: 0.04466663707386364\n","11/07/2022 22:36:54 - INFO - src.trainer -   Train loss: 0.00979336825284091\n","11/07/2022 22:40:34 - INFO - src.trainer -   Train loss: 3.190474076704545e-05\n","11/07/2022 22:44:14 - INFO - src.trainer -   Train loss: 0.006114612926136364\n","11/07/2022 22:47:54 - INFO - src.trainer -   Train loss: 0.0008961070667613636\n","\n","2367it [9:50:57, 679.04s/it]\u001b[A\n","2368it [9:50:58, 475.81s/it]\u001b[A\n","2369it [9:51:00, 333.54s/it]\u001b[A\n","2370it [9:51:02, 233.96s/it]\u001b[A\n","2371it [9:51:03, 164.25s/it]\u001b[A\n","2372it [9:51:05, 115.46s/it]\u001b[A\n","2373it [9:51:06, 81.30s/it] \u001b[A\n","2374it [9:51:08, 57.39s/it]\u001b[A\n","2375it [9:51:10, 40.66s/it]\u001b[A\n","2376it [9:51:11, 28.94s/it]\u001b[A\n","2377it [9:51:13, 20.74s/it]\u001b[A\n","2378it [9:51:14, 15.00s/it]\u001b[A\n","2379it [9:51:16, 10.98s/it]\u001b[A\n","2380it [9:51:18,  8.17s/it]\u001b[A\n","2381it [9:51:19,  6.20s/it]\u001b[A\n","2382it [9:51:21,  4.82s/it]\u001b[A\n","2383it [9:51:22,  3.85s/it]\u001b[A\n","2384it [9:51:24,  3.18s/it]\u001b[A\n","2385it [9:51:26,  2.71s/it]\u001b[A\n","2386it [9:51:27,  2.37s/it]\u001b[A\n","2387it [9:51:29,  2.14s/it]\u001b[A\n","2388it [9:51:31,  1.98s/it]\u001b[A\n","2389it [9:51:32,  1.87s/it]\u001b[A\n","2390it [9:51:34,  1.79s/it]\u001b[A\n","2391it [9:51:35,  1.73s/it]\u001b[A\n","2392it [9:51:37,  1.69s/it]\u001b[A\n","2393it [9:51:39,  1.67s/it]\u001b[A\n","2394it [9:51:40,  1.65s/it]\u001b[A\n","2395it [9:51:42,  1.63s/it]\u001b[A\n","2396it [9:51:43,  1.63s/it]\u001b[A\n","2397it [9:51:45,  1.62s/it]\u001b[A\n","2398it [9:51:47,  1.61s/it]\u001b[A\n","2399it [9:51:48,  1.61s/it]\u001b[A\n","2400it [9:51:50,  1.61s/it]\u001b[A\n","2401it [9:51:51,  1.61s/it]\u001b[A\n","2402it [9:51:53,  1.60s/it]\u001b[A\n","2403it [9:51:55,  1.60s/it]\u001b[A\n","2404it [9:51:56,  1.60s/it]\u001b[A\n","2405it [9:51:58,  1.60s/it]\u001b[A\n","2406it [9:51:59,  1.60s/it]\u001b[A\n","2407it [9:52:01,  1.61s/it]\u001b[A\n","2408it [9:52:03,  1.61s/it]\u001b[A\n","2409it [9:52:04,  1.60s/it]\u001b[A\n","2410it [9:52:06,  1.60s/it]\u001b[A\n","2411it [9:52:07,  1.60s/it]\u001b[A\n","2412it [9:52:09,  1.60s/it]\u001b[A\n","2413it [9:52:11,  1.60s/it]\u001b[A\n","2414it [9:52:12,  1.60s/it]\u001b[A\n","2415it [9:52:14,  1.60s/it]\u001b[A\n","2416it [9:52:15,  1.60s/it]\u001b[A\n","2417it [9:52:17,  1.61s/it]\u001b[A\n","2418it [9:52:19,  1.60s/it]\u001b[A\n","2419it [9:52:20,  1.60s/it]\u001b[A\n","2420it [9:52:22,  1.60s/it]\u001b[A\n","2421it [9:52:23,  1.60s/it]\u001b[A\n","2422it [9:52:25,  1.60s/it]\u001b[A\n","2423it [9:52:27,  1.60s/it]\u001b[A\n","2424it [9:52:28,  1.60s/it]\u001b[A\n","2425it [9:52:30,  1.60s/it]\u001b[A\n","2426it [9:52:31,  1.60s/it]\u001b[A\n","2427it [9:52:33,  1.60s/it]\u001b[A\n","2428it [9:52:35,  1.60s/it]\u001b[A\n","2429it [9:52:36,  1.60s/it]\u001b[A\n","2430it [9:52:38,  1.60s/it]\u001b[A\n","2431it [9:52:39,  1.60s/it]\u001b[A\n","2432it [9:52:41,  1.60s/it]\u001b[A\n","2433it [9:52:43,  1.61s/it]\u001b[A\n","2434it [9:52:44,  1.61s/it]\u001b[A\n","2435it [9:52:46,  1.61s/it]\u001b[A\n","2436it [9:52:48,  1.62s/it]\u001b[A\n","2437it [9:52:49,  1.62s/it]\u001b[A\n","2438it [9:52:51,  1.62s/it]\u001b[A\n","2439it [9:52:52,  1.62s/it]\u001b[A\n","2440it [9:52:54,  1.62s/it]\u001b[A\n","2441it [9:52:56,  1.62s/it]\u001b[A\n","2442it [9:52:57,  1.61s/it]\u001b[A\n","2443it [9:52:59,  1.62s/it]\u001b[A\n","2444it [9:53:00,  1.62s/it]\u001b[A\n","2445it [9:53:02,  1.62s/it]\u001b[A\n","2446it [9:53:04,  1.62s/it]\u001b[A\n","2447it [9:53:05,  1.62s/it]\u001b[A\n","2448it [9:53:07,  1.62s/it]\u001b[A\n","2449it [9:53:09,  1.62s/it]\u001b[A\n","2450it [9:53:10,  1.62s/it]\u001b[A\n","2451it [9:53:12,  1.62s/it]\u001b[A\n","2452it [9:53:13,  1.62s/it]\u001b[A\n","2453it [9:53:15,  1.62s/it]\u001b[A\n","2454it [9:53:17,  1.62s/it]\u001b[A\n","2455it [9:53:18,  1.62s/it]\u001b[A\n","2456it [9:53:20,  1.61s/it]\u001b[A\n","2457it [9:53:21,  1.61s/it]\u001b[A\n","2458it [9:53:23,  1.62s/it]\u001b[A\n","2459it [9:53:25,  1.62s/it]\u001b[A\n","2460it [9:53:26,  1.61s/it]\u001b[A\n","2461it [9:53:28,  1.61s/it]\u001b[A\n","2462it [9:53:30,  1.61s/it]\u001b[A\n","2463it [9:53:31,  1.61s/it]\u001b[A\n","2464it [9:53:33,  1.61s/it]\u001b[A\n","2465it [9:53:34,  1.61s/it]\u001b[A\n","2466it [9:53:36,  1.61s/it]\u001b[A\n","2467it [9:53:38,  1.61s/it]\u001b[A\n","2468it [9:53:39,  1.61s/it]\u001b[A\n","2469it [9:53:41,  1.62s/it]\u001b[A\n","2470it [9:53:42,  1.62s/it]\u001b[A\n","2471it [9:53:44,  1.62s/it]\u001b[A\n","2472it [9:53:46,  1.62s/it]\u001b[A\n","2473it [9:53:47,  1.62s/it]\u001b[A\n","2474it [9:53:49,  1.62s/it]\u001b[A\n","2475it [9:53:51,  1.62s/it]\u001b[A\n","2476it [9:53:52,  1.62s/it]\u001b[A\n","2477it [9:53:54,  1.62s/it]\u001b[A\n","2478it [9:53:55,  1.62s/it]\u001b[A\n","2479it [9:53:57,  1.62s/it]\u001b[A\n","2480it [9:53:59,  1.62s/it]\u001b[A\n","2481it [9:54:00,  1.62s/it]\u001b[A\n","2482it [9:54:02,  1.62s/it]\u001b[A\n","2483it [9:54:04,  1.62s/it]\u001b[A\n","2484it [9:54:05,  1.62s/it]\u001b[A\n","2485it [9:54:07,  1.62s/it]\u001b[A\n","2486it [9:54:08,  1.62s/it]\u001b[A\n","2487it [9:54:10,  1.62s/it]\u001b[A\n","2488it [9:54:12,  1.62s/it]\u001b[A\n","2489it [9:54:13,  1.62s/it]\u001b[A\n","2490it [9:54:15,  1.62s/it]\u001b[A\n","2491it [9:54:16,  1.62s/it]\u001b[A\n","2492it [9:54:18,  1.62s/it]\u001b[A\n","2493it [9:54:20,  1.62s/it]\u001b[A\n","2494it [9:54:21,  1.62s/it]\u001b[A\n","2495it [9:54:23,  1.62s/it]\u001b[A\n","2496it [9:54:25,  1.62s/it]\u001b[A\n","2497it [9:54:26,  1.62s/it]\u001b[A\n","2498it [9:54:28,  1.62s/it]\u001b[A\n","2499it [9:54:29,  1.62s/it]\u001b[A\n","2500it [9:54:31,  1.62s/it]\u001b[A\n","2501it [9:54:33,  1.62s/it]\u001b[A\n","2502it [9:54:34,  1.62s/it]\u001b[A\n","2503it [9:54:36,  1.62s/it]\u001b[A\n","2504it [9:54:38,  1.62s/it]\u001b[A\n","2505it [9:54:39,  1.62s/it]\u001b[A\n","2506it [9:54:41,  1.62s/it]\u001b[A\n","2507it [9:54:42,  1.62s/it]\u001b[A\n","2508it [9:54:44,  1.62s/it]\u001b[A\n","2509it [9:54:46,  1.62s/it]\u001b[A\n","2510it [9:54:47,  1.62s/it]\u001b[A\n","2511it [9:54:49,  1.62s/it]\u001b[A\n","2512it [9:54:50,  1.61s/it]\u001b[A\n","2513it [9:54:52,  1.62s/it]\u001b[A\n","2514it [9:54:54,  1.62s/it]\u001b[A\n","2515it [9:54:55,  1.62s/it]\u001b[A\n","2516it [9:54:57,  1.62s/it]\u001b[A\n","2517it [9:54:59,  1.62s/it]\u001b[A\n","2518it [9:55:00,  1.62s/it]\u001b[A\n","2519it [9:55:02,  1.62s/it]\u001b[A\n","2520it [9:55:03,  1.62s/it]\u001b[A\n","2521it [9:55:05,  1.62s/it]\u001b[A\n","2522it [9:55:07,  1.62s/it]\u001b[A\n","2523it [9:55:08,  1.62s/it]\u001b[A\n","2524it [9:55:10,  1.62s/it]\u001b[A\n","2525it [9:55:12,  1.62s/it]\u001b[A\n","2526it [9:55:13,  1.62s/it]\u001b[A\n","2527it [9:55:15,  1.62s/it]\u001b[A\n","2528it [9:55:16,  1.62s/it]\u001b[A\n","2529it [9:55:18,  1.62s/it]\u001b[A\n","2530it [9:55:20,  1.62s/it]\u001b[A\n","2531it [9:55:21,  1.62s/it]\u001b[A\n","2532it [9:55:23,  1.62s/it]\u001b[A\n","2533it [9:55:24,  1.62s/it]\u001b[A\n","2534it [9:55:26,  1.62s/it]\u001b[A\n","Epoch:  75% 15/20 [10:33:08<3:30:53, 2530.74s/it]11/07/2022 22:56:15 - INFO - src.trainer -   Train loss: 0.021452470259232956\n","11/07/2022 22:59:54 - INFO - src.trainer -   Train loss: 0.010887839577414772\n","11/07/2022 23:03:34 - INFO - src.trainer -   Train loss: 0.006844260475852273\n","11/07/2022 23:07:14 - INFO - src.trainer -   Train loss: 0.0100555419921875\n","11/07/2022 23:10:54 - INFO - src.trainer -   Train loss: 0.0011346990411931818\n","11/07/2022 23:14:34 - INFO - src.trainer -   Train loss: 0.016704212535511364\n","11/07/2022 23:18:14 - INFO - src.trainer -   Train loss: 0.02691650390625\n","11/07/2022 23:21:54 - INFO - src.trainer -   Train loss: 0.018668434836647728\n","11/07/2022 23:25:34 - INFO - src.trainer -   Train loss: 0.008421464399857954\n","11/07/2022 23:29:14 - INFO - src.trainer -   Train loss: 0.0026383833451704545\n","\n","2536it [10:33:07, 679.04s/it]\u001b[A\n","2537it [10:33:09, 475.81s/it]\u001b[A\n","2538it [10:33:11, 333.55s/it]\u001b[A\n","2539it [10:33:12, 233.96s/it]\u001b[A\n","2540it [10:33:14, 164.25s/it]\u001b[A\n","2541it [10:33:15, 115.46s/it]\u001b[A\n","2542it [10:33:17, 81.30s/it] \u001b[A\n","2543it [10:33:19, 57.39s/it]\u001b[A\n","2544it [10:33:20, 40.66s/it]\u001b[A\n","2545it [10:33:22, 28.94s/it]\u001b[A\n","2546it [10:33:23, 20.74s/it]\u001b[A\n","2547it [10:33:25, 15.00s/it]\u001b[A\n","2548it [10:33:27, 10.98s/it]\u001b[A\n","2549it [10:33:28,  8.17s/it]\u001b[A\n","2550it [10:33:30,  6.20s/it]\u001b[A\n","2551it [10:33:31,  4.82s/it]\u001b[A\n","2552it [10:33:33,  3.86s/it]\u001b[A\n","2553it [10:33:35,  3.18s/it]\u001b[A\n","2554it [10:33:36,  2.71s/it]\u001b[A\n","2555it [10:33:38,  2.38s/it]\u001b[A\n","2556it [10:33:39,  2.14s/it]\u001b[A\n","2557it [10:33:41,  1.98s/it]\u001b[A\n","2558it [10:33:43,  1.87s/it]\u001b[A\n","2559it [10:33:44,  1.79s/it]\u001b[A\n","2560it [10:33:46,  1.73s/it]\u001b[A\n","2561it [10:33:47,  1.70s/it]\u001b[A\n","2562it [10:33:49,  1.67s/it]\u001b[A\n","2563it [10:33:51,  1.65s/it]\u001b[A\n","2564it [10:33:52,  1.63s/it]\u001b[A\n","2565it [10:33:54,  1.62s/it]\u001b[A\n","2566it [10:33:55,  1.62s/it]\u001b[A\n","2567it [10:33:57,  1.61s/it]\u001b[A\n","2568it [10:33:59,  1.61s/it]\u001b[A\n","2569it [10:34:00,  1.61s/it]\u001b[A\n","2570it [10:34:02,  1.60s/it]\u001b[A\n","2571it [10:34:03,  1.60s/it]\u001b[A\n","2572it [10:34:05,  1.60s/it]\u001b[A\n","2573it [10:34:07,  1.60s/it]\u001b[A\n","2574it [10:34:08,  1.60s/it]\u001b[A\n","2575it [10:34:10,  1.60s/it]\u001b[A\n","2576it [10:34:12,  1.61s/it]\u001b[A\n","2577it [10:34:13,  1.61s/it]\u001b[A\n","2578it [10:34:15,  1.61s/it]\u001b[A\n","2579it [10:34:16,  1.61s/it]\u001b[A\n","2580it [10:34:18,  1.60s/it]\u001b[A\n","2581it [10:34:20,  1.60s/it]\u001b[A\n","2582it [10:34:21,  1.60s/it]\u001b[A\n","2583it [10:34:23,  1.60s/it]\u001b[A\n","2584it [10:34:24,  1.60s/it]\u001b[A\n","2585it [10:34:26,  1.61s/it]\u001b[A\n","2586it [10:34:28,  1.61s/it]\u001b[A\n","2587it [10:34:29,  1.61s/it]\u001b[A\n","2588it [10:34:31,  1.61s/it]\u001b[A\n","2589it [10:34:32,  1.61s/it]\u001b[A\n","2590it [10:34:34,  1.61s/it]\u001b[A\n","2591it [10:34:36,  1.60s/it]\u001b[A\n","2592it [10:34:37,  1.61s/it]\u001b[A\n","2593it [10:34:39,  1.61s/it]\u001b[A\n","2594it [10:34:40,  1.60s/it]\u001b[A\n","2595it [10:34:42,  1.60s/it]\u001b[A\n","2596it [10:34:44,  1.60s/it]\u001b[A\n","2597it [10:34:45,  1.61s/it]\u001b[A\n","2598it [10:34:47,  1.61s/it]\u001b[A\n","2599it [10:34:48,  1.61s/it]\u001b[A\n","2600it [10:34:50,  1.61s/it]\u001b[A\n","2601it [10:34:52,  1.61s/it]\u001b[A\n","2602it [10:34:53,  1.61s/it]\u001b[A\n","2603it [10:34:55,  1.61s/it]\u001b[A\n","2604it [10:34:57,  1.62s/it]\u001b[A\n","2605it [10:34:58,  1.62s/it]\u001b[A\n","2606it [10:35:00,  1.62s/it]\u001b[A\n","2607it [10:35:01,  1.62s/it]\u001b[A\n","2608it [10:35:03,  1.62s/it]\u001b[A\n","2609it [10:35:05,  1.62s/it]\u001b[A\n","2610it [10:35:06,  1.62s/it]\u001b[A\n","2611it [10:35:08,  1.62s/it]\u001b[A\n","2612it [10:35:09,  1.62s/it]\u001b[A\n","2613it [10:35:11,  1.62s/it]\u001b[A\n","2614it [10:35:13,  1.62s/it]\u001b[A\n","2615it [10:35:14,  1.62s/it]\u001b[A\n","2616it [10:35:16,  1.62s/it]\u001b[A\n","2617it [10:35:18,  1.62s/it]\u001b[A\n","2618it [10:35:19,  1.62s/it]\u001b[A\n","2619it [10:35:21,  1.62s/it]\u001b[A\n","2620it [10:35:22,  1.62s/it]\u001b[A\n","2621it [10:35:24,  1.62s/it]\u001b[A\n","2622it [10:35:26,  1.62s/it]\u001b[A\n","2623it [10:35:27,  1.62s/it]\u001b[A\n","2624it [10:35:29,  1.62s/it]\u001b[A\n","2625it [10:35:31,  1.62s/it]\u001b[A\n","2626it [10:35:32,  1.62s/it]\u001b[A\n","2627it [10:35:34,  1.62s/it]\u001b[A\n","2628it [10:35:35,  1.62s/it]\u001b[A\n","2629it [10:35:37,  1.62s/it]\u001b[A\n","2630it [10:35:39,  1.62s/it]\u001b[A\n","2631it [10:35:40,  1.62s/it]\u001b[A\n","2632it [10:35:42,  1.62s/it]\u001b[A\n","2633it [10:35:43,  1.62s/it]\u001b[A\n","2634it [10:35:45,  1.62s/it]\u001b[A\n","2635it [10:35:47,  1.62s/it]\u001b[A\n","2636it [10:35:48,  1.62s/it]\u001b[A\n","2637it [10:35:50,  1.62s/it]\u001b[A\n","2638it [10:35:52,  1.62s/it]\u001b[A\n","2639it [10:35:53,  1.62s/it]\u001b[A\n","2640it [10:35:55,  1.62s/it]\u001b[A\n","2641it [10:35:56,  1.62s/it]\u001b[A\n","2642it [10:35:58,  1.63s/it]\u001b[A\n","2643it [10:36:00,  1.62s/it]\u001b[A\n","2644it [10:36:01,  1.62s/it]\u001b[A\n","2645it [10:36:03,  1.62s/it]\u001b[A\n","2646it [10:36:05,  1.62s/it]\u001b[A\n","2647it [10:36:06,  1.62s/it]\u001b[A\n","2648it [10:36:08,  1.62s/it]\u001b[A\n","2649it [10:36:09,  1.62s/it]\u001b[A\n","2650it [10:36:11,  1.62s/it]\u001b[A\n","2651it [10:36:13,  1.62s/it]\u001b[A\n","2652it [10:36:14,  1.62s/it]\u001b[A\n","2653it [10:36:16,  1.62s/it]\u001b[A\n","2654it [10:36:18,  1.62s/it]\u001b[A\n","2655it [10:36:19,  1.62s/it]\u001b[A\n","2656it [10:36:21,  1.62s/it]\u001b[A\n","2657it [10:36:22,  1.62s/it]\u001b[A\n","2658it [10:36:24,  1.62s/it]\u001b[A\n","2659it [10:36:26,  1.62s/it]\u001b[A\n","2660it [10:36:27,  1.62s/it]\u001b[A\n","2661it [10:36:29,  1.62s/it]\u001b[A\n","2662it [10:36:30,  1.62s/it]\u001b[A\n","2663it [10:36:32,  1.62s/it]\u001b[A\n","2664it [10:36:34,  1.62s/it]\u001b[A\n","2665it [10:36:35,  1.62s/it]\u001b[A\n","2666it [10:36:37,  1.62s/it]\u001b[A\n","2667it [10:36:39,  1.62s/it]\u001b[A\n","2668it [10:36:40,  1.62s/it]\u001b[A\n","2669it [10:36:42,  1.62s/it]\u001b[A\n","2670it [10:36:43,  1.62s/it]\u001b[A\n","2671it [10:36:45,  1.62s/it]\u001b[A\n","2672it [10:36:47,  1.62s/it]\u001b[A\n","2673it [10:36:48,  1.62s/it]\u001b[A\n","2674it [10:36:50,  1.62s/it]\u001b[A\n","2675it [10:36:52,  1.62s/it]\u001b[A\n","2676it [10:36:53,  1.62s/it]\u001b[A\n","2677it [10:36:55,  1.62s/it]\u001b[A\n","2678it [10:36:56,  1.62s/it]\u001b[A\n","2679it [10:36:58,  1.62s/it]\u001b[A\n","2680it [10:37:00,  1.61s/it]\u001b[A\n","2681it [10:37:01,  1.61s/it]\u001b[A\n","2682it [10:37:03,  1.62s/it]\u001b[A\n","2683it [10:37:04,  1.62s/it]\u001b[A\n","2684it [10:37:06,  1.62s/it]\u001b[A\n","2685it [10:37:08,  1.62s/it]\u001b[A\n","2686it [10:37:09,  1.62s/it]\u001b[A\n","2687it [10:37:11,  1.62s/it]\u001b[A\n","2688it [10:37:13,  1.62s/it]\u001b[A\n","2689it [10:37:14,  1.62s/it]\u001b[A\n","2690it [10:37:16,  1.62s/it]\u001b[A\n","2691it [10:37:17,  1.62s/it]\u001b[A\n","2692it [10:37:19,  1.62s/it]\u001b[A\n","2693it [10:37:21,  1.62s/it]\u001b[A\n","2694it [10:37:22,  1.62s/it]\u001b[A\n","2695it [10:37:24,  1.62s/it]\u001b[A\n","2696it [10:37:26,  1.62s/it]\u001b[A\n","2697it [10:37:27,  1.62s/it]\u001b[A\n","2698it [10:37:29,  1.62s/it]\u001b[A\n","2699it [10:37:30,  1.62s/it]\u001b[A\n","2700it [10:37:32,  1.62s/it]\u001b[A\n","2701it [10:37:34,  1.62s/it]\u001b[A\n","2702it [10:37:35,  1.61s/it]\u001b[A\n","2703it [10:37:37,  1.61s/it]\u001b[A\n","Epoch:  80% 16/20 [11:15:18<2:48:42, 2530.74s/it]11/07/2022 23:37:35 - INFO - src.trainer -   Train loss: 0.00013594193892045453\n","11/07/2022 23:41:15 - INFO - src.trainer -   Train loss: 0.0025190873579545455\n","11/07/2022 23:44:55 - INFO - src.trainer -   Train loss: 0.0019087357954545455\n","11/07/2022 23:48:35 - INFO - src.trainer -   Train loss: 0.0009293989701704545\n","11/07/2022 23:52:15 - INFO - src.trainer -   Train loss: 0.001040371981534091\n","11/07/2022 23:55:55 - INFO - src.trainer -   Train loss: 0.008854259144176136\n","11/07/2022 23:59:35 - INFO - src.trainer -   Train loss: 0.025688864968039772\n","11/08/2022 00:03:15 - INFO - src.trainer -   Train loss: 0.01781255548650568\n","Epoch:  80% 16/20 [11:42:42<2:55:40, 2635.14s/it]\n","Traceback (most recent call last):\n","  File \"run.py\", line 647, in <module>\n","    main()\n","  File \"run.py\", line 553, in main\n","    trainer.train(model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/src/trainer.py\", line 359, in train\n","    tr_loss += self.training_step(model, inputs)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py\", line 1070, in training_step\n","    loss.backward()\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/torch/tensor.py\", line 185, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 127, in backward\n","    allow_unreachable=True)  # allow_unreachable flag\n","KeyboardInterrupt\n","2704it [11:05:15, 14.76s/it]\n"]}],"source":["!source env/bin/activate; TAG=\"v1.0_FT_batch32\" TYPE=finetune TASK=spoilers BS=32 LR=1e-5 SEED=21 MODEL=roberta-large bash run_experiment.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":443994,"status":"ok","timestamp":1667888993482,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":480},"id":"Yck8i4_g0Xx4","outputId":"96e07b13-a620-4214-c216-ebad6ef60616"},"outputs":[{"name":"stdout","output_type":"stream","text":["11/08/2022 06:22:36 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","11/08/2022 06:22:36 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/v1.0_FT_batch32', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=4500, warmup_steps=0, logging_dir='runs/Nov08_06-22-36_199c3484481b', logging_first_step=False, logging_steps=225, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=225, dataloader_num_workers=0, past_index=-1, run_name='result/v1.0_FT_batch32', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=True, save_logit_dir='result/v1.0_FT_batch32', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","11/08/2022 06:22:36 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","11/08/2022 06:22:38 - INFO - src.dataset -   Total num_sample for mode train: 1\n","11/08/2022 06:22:38 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/binary_v1.0/3615-21\n","11/08/2022 06:22:38 - INFO - filelock -   Lock 139917482576592 acquired on data/k-shot/spoilers/binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","11/08/2022 06:22:38 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers [took 0.033 s]\n","11/08/2022 06:22:38 - INFO - filelock -   Lock 139917482576592 released on data/k-shot/spoilers/binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","11/08/2022 06:22:40 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","11/08/2022 06:22:40 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/binary_v1.0/3615-21\n","11/08/2022 06:22:40 - INFO - filelock -   Lock 139917482576016 acquired on data/k-shot/spoilers/binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","11/08/2022 06:22:41 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers [took 0.428 s]\n","11/08/2022 06:22:41 - INFO - filelock -   Lock 139917482576016 released on data/k-shot/spoilers/binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","11/08/2022 06:22:41 - INFO - src.dataset -   *** Example ***\n","11/08/2022 06:22:41 - INFO - src.dataset -   guid: dev-0\n","11/08/2022 06:22:41 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 771, 523, 642, 38, 122, 33, 7036, 14, 51, 351, 17, 27, 90, 28, 37698, 24, 7, 18220, 42, 191, 4, 8133, 44412, 268, 4, 3421, 7, 8402, 9, 5, 2664, 636, 12266, 8, 471, 13, 127, 3627, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=None, label_word_list=None)\n","11/08/2022 06:22:41 - INFO - src.dataset -   text: <s>Welp I now have confirmation that they won‚Äôt be uploading it to Hulu this season. Motherfuckers. Time to dust of the tricorne and head for my ship.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n","11/08/2022 06:22:47 - INFO - src.dataset -   Total num_sample for mode test: 1\n","11/08/2022 06:22:47 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/binary_v1.0/3615-21\n","11/08/2022 06:22:47 - INFO - filelock -   Lock 139917465228944 acquired on data/k-shot/spoilers/binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers.lock\n","11/08/2022 06:22:47 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers [took 0.036 s]\n","11/08/2022 06:22:47 - INFO - filelock -   Lock 139917465228944 released on data/k-shot/spoilers/binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers.lock\n","11/08/2022 06:22:47 - INFO - src.dataset -   *** Example ***\n","11/08/2022 06:22:47 - INFO - src.dataset -   guid: test-0\n","11/08/2022 06:22:47 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 35703, 3996, 24, 4, 152, 3238, 21, 169, 357, 4, 38, 2638, 9925, 1253, 8725, 3860, 7, 2807, 227, 5, 232, 8, 1003, 73, 21290, 284, 4, 1534, 14, 12355, 8810, 37, 21, 35510, 145, 2997, 7, 116, 38, 794, 10, 14061, 15, 69, 15401, 4, 289, 10987, 21, 24282, 7735, 4, 264, 1449, 79, 965, 75, 49, 10295, 8, 172, 161, 79, 29, 202, 5, 8501, 9, 5, 2658, 19006, 4, 264, 332, 14, 70, 5, 1462, 3878, 74, 3999, 2854, 19, 18038, 53, 99, 59, 5, 32440, 6988, 661, 4, 1876, 9, 106, 58, 683, 12260, 2580, 50, 11, 5, 92, 2658, 19006, 8, 51, 1507, 19, 5, 910, 16250, 596, 74, 70, 5, 1462, 3878, 54, 129, 1467, 14, 9187, 3033, 624, 5, 6347, 1032, 13, 5, 1079, 9, 9187, 61, 1654, 106, 11, 42, 1068, 6, 17161, 102, 23737, 642, 14963, 53, 99, 38, 5324, 4, 497, 155, 35, 844, 1236, 12001, 161, 51, 1381, 13, 107, 546, 13, 5274, 4, 10426, 35, 5, 204, 12, 180, 498, 330, 1588, 4, 497, 262, 35, 4390, 289, 10987, 161, 51, 3033, 11, 5, 751, 232, 4, 152, 5072, 3137, 506, 21871, 14, 51, 1381, 15166, 4, 20, 129, 1219, 1368, 10987, 74, 33, 10, 1528, 1219, 7, 213, 74, 28, 7, 2097, 5, 910, 16250, 4, 3771, 31360, 4795, 16, 98, 3953, 24, 95, 14236, 162, 9, 821, 28261, 4795, 4, 38, 21, 2818, 821, 10810, 74, 9, 26, 4, 6553, 47, 192, 14, 1369, 116, 1586, 4691, 16, 25672, 7, 206, 381, 2558, 16, 164, 7, 912, 23, 42, 477, 4, 381, 2558, 16, 533, 7, 8439, 5, 1445, 232, 4, 21198, 2420, 11, 39996, 1026, 16, 10, 1856, 7, 70, 9, 106, 4356, 686, 4, 5363, 10720, 1223, 5101, 13, 99, 37, 222, 21, 3127, 17758, 4, 1223, 5101, 16, 35304, 744, 9287, 23, 70, 498, 38, 5170, 77, 37, 581, 1597, 4, 3791, 5, 3980, 58, 2343, 59, 501, 14200, 498, 1640, 118, 11590, 43, 20, 169, 26968, 4242, 17200, 4102, 5, 6529, 21, 5, 275, 169, 939, 115, 9, 5207, 7, 28, 2781, 4, 290, 73, 698, 13, 162, 44660, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=None, label_word_list=None)\n","11/08/2022 06:22:47 - INFO - src.dataset -   text: <s>Finally watched it. This episode was way better. I loved Jeans inner struggle to choose between the world and himself/future family. Is that Mikasa he was imagining being married to? I saw a scar on her cheek. Hange was kinda weird. She claims she isn't their superior and then says shes still the commander of the survey corps. She points that all the dead soldiers wouldnt agree with genocide but what about the yeagerist. Many of them were once cadets or in the new survey corps and they agreed with the rumbling why would all the dead soldiers who only knew that humanity lived within the walls fight for the rest of humanity which forced them in this situation, Kinda nitpicky but what I noticed. At 3:20 jean says they tried for years looking for answers. Years: the 4-year timeskip. At 7:57 Hange says they lived in the outside world. This basically comfirms that they tried diplomacy. The only reason hange would have a true reason to go would be to prevent the rumbling. Magaths argument is so weak it just reminds me of gabis argument. I was hoping gabi would of said. Did you see that happen? Armin is naive to think Eren is going to stop at this point. Eren is likely to destroy the entire world. Pieck in titan form is a threat to all of them im sure. Jean kicking Reiner for what he did was truly satisfying. Reiner is spitting death flags at all times I wonder when he'll die. Though the trees were shown about 140000 times(i counted) The way Yelena tore apart the alliance was the best way i could of hoped to be delivered. 8/10 for me :)</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","11/08/2022 06:23:33 - INFO - __main__ -   *** Validate ***\n","100% 169/169 [04:23<00:00,  1.65s/it]/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","11/08/2022 06:27:58 - INFO - __main__ -   ***** Eval results spoilers *****\n","11/08/2022 06:27:58 - INFO - __main__ -     eval_loss = 0.4411945641040802\n","11/08/2022 06:27:58 - INFO - __main__ -     eval_ap = 0.8684605956077576\n","11/08/2022 06:27:58 - INFO - __main__ -     eval_auroc = 0.9070557951927185\n","11/08/2022 06:27:58 - INFO - __main__ -     eval_recall = 0.8935140371322632\n","11/08/2022 06:27:58 - INFO - __main__ -     eval_f1 = 0.7746537327766418\n","11/08/2022 06:27:58 - INFO - root -   *** Test ***\n","238it [06:15,  1.63s/it]11/08/2022 06:29:51 - INFO - __main__ -   ***** Test results spoilers *****\n","11/08/2022 06:29:51 - INFO - __main__ -     eval_loss = 0.42585238814353943\n","11/08/2022 06:29:51 - INFO - __main__ -     eval_ap = 0.8808566927909851\n","11/08/2022 06:29:51 - INFO - __main__ -     eval_auroc = 0.9023493528366089\n","11/08/2022 06:29:51 - INFO - __main__ -     eval_recall = 0.8780487775802612\n","11/08/2022 06:29:51 - INFO - __main__ -     eval_f1 = 0.7992599606513977\n","11/08/2022 06:29:51 - INFO - filelock -   Lock 139917436823184 acquired on log.lock\n","11/08/2022 06:29:51 - INFO - filelock -   Lock 139917436823184 released on log.lock\n","238it [06:17,  1.59s/it]\n"]}],"source":["!source env/bin/activate; TAG=\"v1.0_FT_batch32\" TYPE=finetune TASK=spoilers BS=32 LR=1e-5 SEED=21 MODEL=roberta-large bash evaluate.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1482,"status":"ok","timestamp":1650439706886,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"b2FhTELXRFXq","outputId":"e81f892b-342c-46d5-85ed-747ac096cba1"},"outputs":[{"name":"stdout","output_type":"stream","text":["13-13: best dev (0.88959) test (0.88205) test2 (0.73179) | total trials: 1\n","    | gradient_accumulation_steps: 1 | learning_rate: 1e-05 | per_device_train_batch_size: 4 | eval_steps: 100 | max_steps: 1000 \n","21-21: best dev (0.86528) test (0.86851) test2 (0.76821) | total trials: 1\n","    | gradient_accumulation_steps: 1 | learning_rate: 1e-05 | per_device_train_batch_size: 4 | eval_steps: 100 | max_steps: 1000 \n","42-42: best dev (0.90591) test (0.86804) test2 (0.83444) | total trials: 1\n","    | gradient_accumulation_steps: 1 | learning_rate: 1e-05 | per_device_train_batch_size: 4 | eval_steps: 100 | max_steps: 1000 \n","87-87: best dev (0.87574) test (0.87654) test2 (0.76490) | total trials: 1\n","    | gradient_accumulation_steps: 1 | learning_rate: 1e-05 | per_device_train_batch_size: 4 | eval_steps: 100 | max_steps: 1000 \n","100-100: best dev (0.88034) test (0.87764) test2 (0.76490) | total trials: 1\n","    | gradient_accumulation_steps: 1 | learning_rate: 1e-05 | per_device_train_batch_size: 4 | eval_steps: 100 | max_steps: 1000 \n","mean +- std: 87.5 (0.5) (median 87.7)second metric: 77.3 (3.4) (median 76.5)\n"]}],"source":["# relevant, prompt-demo: 87.2\n","# relevant, prompt: 87.3\n","# this_is_mask, prompt: 88.2\n","# LR: 1e-05, BS: 4\n","!source env/bin/activate; python tools/gather_result.py --condition \"{'tag': 'it_was_mask', 'task_name': 'spoilers', 'few_shot_type': 'prompt', 'per_device_train_batch_size': 4}\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":721663,"status":"ok","timestamp":1673504862568,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":480},"id":"xzB-ceTHXuqW","outputId":"df1abb78-ea09-4488-be18-2f15fbfc7b86"},"outputs":[{"name":"stdout","output_type":"stream","text":["100% 1.31G/1.31G [01:33<00:00, 14.1MB/s]\n","spoilers-3615-21-train\n","0it [00:00, ?it/s]| [Excitement intensifies]\n","7230it [03:12, 37.58it/s]\n","spoilers-3615-21-dev\n","0it [00:00, ?it/s]| Welp I now have confirmation that they won‚Äôt be uploading it to Hulu this season. Motherfuckers. Time to dust of the tricorne and head for my ship.\n","2698it [01:11, 37.57it/s]\n","spoilers-3615-21-test\n","0it [00:00, ?it/s]| Finally watched it. This episode was way better. I loved Jeans inner struggle to choose between the world and himself/future family. Is that Mikasa he was imagining being married to? I saw a scar on her cheek. Hange was kinda weird. She claims she isn't their superior and then says shes still the commander of the survey corps. She points that all the dead soldiers wouldnt agree with genocide but what about the yeagerist. Many of them were once cadets or in the new survey corps and they agreed with the rumbling why would all the dead soldiers who only knew that humanity lived within the walls fight for the rest of humanity which forced them in this situation, Kinda nitpicky but what I noticed. At 3:20 jean says they tried for years looking for answers. Years: the 4-year timeskip. At 7:57 Hange says they lived in the outside world. This basically comfirms that they tried diplomacy. The only reason hange would have a true reason to go would be to prevent the rumbling. Magaths argument is so weak it just reminds me of gabis argument. I was hoping gabi would of said. Did you see that happen? Armin is naive to think Eren is going to stop at this point. Eren is likely to destroy the entire world. Pieck in titan form is a threat to all of them im sure. Jean kicking Reiner for what he did was truly satisfying. Reiner is spitting death flags at all times I wonder when he'll die. Though the trees were shown about 140000 times(i counted) The way Yelena tore apart the alliance was the best way i could of hoped to be delivered. 8/10 for me :)\n","1104it [00:28, 38.46it/s]\n"]}],"source":["!source env/bin/activate; python tools/get_sbert_embedding.py --do_test --sbert_model roberta-large --k 3615 --data_dir \"data/k-shot/spoilers/binary_v1.0/3615-21\" --seed 21 --task spoilers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1398202,"status":"ok","timestamp":1692902367435,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"Sq1oOn6FzHjy","outputId":"c5678541-57dc-4ae9-b421-0175a42af3a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading: 100% 1.20k/1.20k [00:00<00:00, 882kB/s]\n","Downloading: 100% 11.4G/11.4G [06:22<00:00, 29.8MB/s]\n","Downloading: 100% 792k/792k [00:00<00:00, 1.86MB/s]\n","# spoilers 16 21 100\n","| dataset examples\n","| {'label': 1, 'text': ['Bye bye levi']}\n","| {'label': 0, 'text': ['HE MADE THE OST?']}\n","\n","| mapping\n","| {0: 'relevant', 1: 'irrelevant'}\n","####### example #######\n","Bye bye levi<extra_id_0> irrelevant<extra_id_1>\n","I like how they added this lil transformation effect on the hange-eren scene<extra_id_0> irrelevant<extra_id_1>\n","I have a big feeling they'll use youseebiggirl in Warhammer's transformation reveal next ep<extra_id_0> irrelevant<extra_id_1>\n","####### example #######\n","\n","100% 18/18 [06:55<00:00, 23.08s/it]\n","####### generated results #######\n","--------------\n","score: -2.8529539108276367\n","generated ids [32099, 6, 32098, 5, 1]\n","generated text <extra_id_0>,<extra_id_1>.</s>\n","--------------\n","score: -2.979557991027832\n","generated ids [32099, 11, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅand<extra_id_1>.</s>\n","--------------\n","score: -3.0140485763549805\n","generated ids [32099, 5, 32098, 5, 1]\n","generated text <extra_id_0>.<extra_id_1>.</s>\n","--------------\n","score: -3.418804168701172\n","generated ids [32099, 19, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅis<extra_id_1>.</s>\n","--------------\n","score: -4.2345075607299805\n","generated ids [32099, 38, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅas<extra_id_1>.</s>\n","--------------\n","score: -4.420568466186523\n","generated ids [32099, 6, 32098, 58, 1]\n","generated text <extra_id_0>,<extra_id_1>?</s>\n","--------------\n","score: -4.423369407653809\n","generated ids [32099, 78, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅso<extra_id_1>.</s>\n","--------------\n","score: -4.721538543701172\n","generated ids [32099, 68, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅbut<extra_id_1>.</s>\n","--------------\n","score: -4.743102073669434\n","generated ids [32099, 6, 32098, 55, 1]\n","generated text <extra_id_0>,<extra_id_1>!</s>\n","--------------\n","score: -4.764705657958984\n","generated ids [32099, 16, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅin<extra_id_1>.</s>\n","--------------\n","score: -4.7836151123046875\n","generated ids [32099, 233, 32098, 5, 1]\n","generated text <extra_id_0>...<extra_id_1>.</s>\n","--------------\n","score: -4.829153060913086\n","generated ids [32099, 21, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅfor<extra_id_1>.</s>\n","--------------\n","score: -4.881891250610352\n","generated ids [32099, 42, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅor<extra_id_1>.</s>\n","--------------\n","score: -4.888946533203125\n","generated ids [32099, 59, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅnot<extra_id_1>.</s>\n","--------------\n","score: -4.940508842468262\n","generated ids [32099, 8, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅthe<extra_id_1>.</s>\n","--------------\n","score: -4.99543571472168\n","generated ids [32099, 12, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅto<extra_id_1>.</s>\n","--------------\n","score: -5.227717399597168\n","generated ids [32099, 24, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅthat<extra_id_1>.</s>\n","--------------\n","score: -5.278903961181641\n","generated ids [32099, 55, 32098, 5, 1]\n","generated text <extra_id_0>!<extra_id_1>.</s>\n","--------------\n","score: -5.295551300048828\n","generated ids [32099, 24, 19, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅthat‚ñÅis<extra_id_1>.</s>\n","--------------\n","score: -5.319576263427734\n","generated ids [32099, 11, 32098, 55, 1]\n","generated text <extra_id_0>‚ñÅand<extra_id_1>!</s>\n","--------------\n","score: -5.345391273498535\n","generated ids [32099, 47, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅwas<extra_id_1>.</s>\n","--------------\n","score: -5.34639835357666\n","generated ids [32099, 33, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅare<extra_id_1>.</s>\n","--------------\n","score: -5.369414329528809\n","generated ids [32099, 5, 32098, 58, 1]\n","generated text <extra_id_0>.<extra_id_1>?</s>\n","--------------\n","score: -5.44606876373291\n","generated ids [32099, 13, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅof<extra_id_1>.</s>\n","--------------\n","score: -5.50895881652832\n","generated ids [32099, 28, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅwith<extra_id_1>.</s>\n","--------------\n","score: -5.552840232849121\n","generated ids [32099, 66, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅall<extra_id_1>.</s>\n","--------------\n","score: -5.561790466308594\n","generated ids [32099, 11, 32098, 58, 1]\n","generated text <extra_id_0>‚ñÅand<extra_id_1>?</s>\n","--------------\n","score: -5.566218376159668\n","generated ids [32099, 116, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅwhen<extra_id_1>.</s>\n","--------------\n","score: -5.589694976806641\n","generated ids [32099, 72, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅmore<extra_id_1>.</s>\n","--------------\n","score: -5.594976425170898\n","generated ids [32099, 30, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅon<extra_id_1>.</s>\n","--------------\n","score: -5.6436614990234375\n","generated ids [32099, 5, 32098, 55, 1]\n","generated text <extra_id_0>.<extra_id_1>!</s>\n","--------------\n","score: -5.699522972106934\n","generated ids [32099, 58, 32098, 5, 1]\n","generated text <extra_id_0>?<extra_id_1>.</s>\n","--------------\n","score: -5.775857925415039\n","generated ids [32099, 10, 32098, 5, 1]\n","generated text <extra_id_0>:<extra_id_1>.</s>\n","--------------\n","score: -5.850682258605957\n","generated ids [32099, 182, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅvery<extra_id_1>.</s>\n","--------------\n","score: -5.912917137145996\n","generated ids [32099, 42, 32098, 58, 1]\n","generated text <extra_id_0>‚ñÅor<extra_id_1>?</s>\n","--------------\n","score: -5.9229583740234375\n","generated ids [32099, 34, 19, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅit‚ñÅis<extra_id_1>.</s>\n","--------------\n","score: -5.946376800537109\n","generated ids [32099, 84, 19, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅwhich‚ñÅis<extra_id_1>.</s>\n","--------------\n","score: -5.957042694091797\n","generated ids [32099, 271, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅbeing<extra_id_1>.</s>\n","--------------\n","score: -5.965561866760254\n","generated ids [32099, 48, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅthis<extra_id_1>.</s>\n","--------------\n","score: -5.996959686279297\n","generated ids [32099, 12, 36, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅto‚ñÅbe<extra_id_1>.</s>\n","--------------\n","score: -6.0151166915893555\n","generated ids [32099, 78, 32098, 55, 1]\n","generated text <extra_id_0>‚ñÅso<extra_id_1>!</s>\n","--------------\n","score: -6.017071723937988\n","generated ids [32099, 165, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅits<extra_id_1>.</s>\n","--------------\n","score: -6.01865291595459\n","generated ids [32099, 5, 933, 32098, 5, 1]\n","generated text <extra_id_0>.‚ñÅNot<extra_id_1>.</s>\n","--------------\n","score: -6.027393341064453\n","generated ids [32099, 19, 32098, 55, 1]\n","generated text <extra_id_0>‚ñÅis<extra_id_1>!</s>\n","--------------\n","score: -6.036293029785156\n","generated ids [32099, 233, 32098, 58, 1]\n","generated text <extra_id_0>...<extra_id_1>?</s>\n","--------------\n","score: -6.059262275695801\n","generated ids [32099, 117, 32098, 5, 1]\n","generated text <extra_id_0>;<extra_id_1>.</s>\n","--------------\n","score: -6.116189956665039\n","generated ids [32099, 5, 4242, 32098, 5, 1]\n","generated text <extra_id_0>.‚ñÅVery<extra_id_1>.</s>\n","--------------\n","score: -6.13832950592041\n","generated ids [32099, 396, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅtoo<extra_id_1>.</s>\n","--------------\n","score: -6.160573959350586\n","generated ids [32099, 213, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅwhere<extra_id_1>.</s>\n","--------------\n","score: -6.183635711669922\n","generated ids [32099, 44, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅat<extra_id_1>.</s>\n","--------------\n","score: -6.201730728149414\n","generated ids [32099, 5, 264, 32098, 5, 1]\n","generated text <extra_id_0>.‚ñÅSo<extra_id_1>.</s>\n","--------------\n","score: -6.213382720947266\n","generated ids [32099, 55, 32098, 55, 1]\n","generated text <extra_id_0>!<extra_id_1>!</s>\n","--------------\n","score: -6.229122161865234\n","generated ids [32099, 58, 32098, 58, 1]\n","generated text <extra_id_0>?<extra_id_1>?</s>\n","--------------\n","score: -6.2615861892700195\n","generated ids [32099, 233, 32098, 55, 1]\n","generated text <extra_id_0>...<extra_id_1>!</s>\n","--------------\n","score: -6.267333984375\n","generated ids [32099, 7, 32098, 5, 1]\n","generated text <extra_id_0>s<extra_id_1>.</s>\n","--------------\n","score: -6.297094345092773\n","generated ids [32099, 45, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅfrom<extra_id_1>.</s>\n","--------------\n","score: -6.370311737060547\n","generated ids [32099, 18, 32098, 5, 1]\n","generated text <extra_id_0>-<extra_id_1>.</s>\n","--------------\n","score: -6.384554862976074\n","generated ids [32099, 131, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅjust<extra_id_1>.</s>\n","--------------\n","score: -6.389375686645508\n","generated ids [32099, 57, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅby<extra_id_1>.</s>\n","--------------\n","score: -6.390478134155273\n","generated ids [32099, 230, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅnow<extra_id_1>.</s>\n","--------------\n","score: -6.430166244506836\n","generated ids [32099, 36, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅbe<extra_id_1>.</s>\n","--------------\n","score: -6.442224502563477\n","generated ids [32099, 19, 32098, 58, 1]\n","generated text <extra_id_0>‚ñÅis<extra_id_1>?</s>\n","--------------\n","score: -6.449481010437012\n","generated ids [32099, 48, 19, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅthis‚ñÅis<extra_id_1>.</s>\n","--------------\n","score: -6.454102516174316\n","generated ids [32099, 34, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅit<extra_id_1>.</s>\n","--------------\n","score: -6.476370811462402\n","generated ids [32099, 150, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅno<extra_id_1>.</s>\n","--------------\n","score: -6.52305793762207\n","generated ids [32099, 6, 68, 32098, 5, 1]\n","generated text <extra_id_0>,‚ñÅbut<extra_id_1>.</s>\n","--------------\n","score: -6.538339614868164\n","generated ids [32099, 5, 299, 32098, 5, 1]\n","generated text <extra_id_0>.‚ñÅBut<extra_id_1>.</s>\n","--------------\n","score: -6.545164108276367\n","generated ids [32099, 341, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅstill<extra_id_1>.</s>\n","--------------\n","score: -6.546990394592285\n","generated ids [32099, 21, 32098, 2081, 5, 1]\n","generated text <extra_id_0>‚ñÅfor<extra_id_1>‚ñÅreasons.</s>\n","--------------\n","score: -6.614163398742676\n","generated ids [32099, 250, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅbecause<extra_id_1>.</s>\n","--------------\n","score: -6.625177383422852\n","generated ids [32099, 92, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅalso<extra_id_1>.</s>\n","--------------\n","score: -6.653969764709473\n","generated ids [32099, 149, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅhow<extra_id_1>.</s>\n","--------------\n","score: -6.662320137023926\n","generated ids [32099, 5, 432, 32098, 5, 1]\n","generated text <extra_id_0>.‚ñÅAll<extra_id_1>.</s>\n","--------------\n","score: -6.78495979309082\n","generated ids [32099, 163, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅonly<extra_id_1>.</s>\n","--------------\n","score: -6.80489444732666\n","generated ids [32099, 38, 32098, 58, 1]\n","generated text <extra_id_0>‚ñÅas<extra_id_1>?</s>\n","--------------\n","score: -6.807009696960449\n","generated ids [32099, 167, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅmost<extra_id_1>.</s>\n","--------------\n","score: -6.820499420166016\n","generated ids [32099, 24, 47, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅthat‚ñÅwas<extra_id_1>.</s>\n","--------------\n","score: -6.821978569030762\n","generated ids [32099, 6, 78, 32098, 5, 1]\n","generated text <extra_id_0>,‚ñÅso<extra_id_1>.</s>\n","--------------\n","score: -6.829400062561035\n","generated ids [32099, 310, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅreally<extra_id_1>.</s>\n","--------------\n","score: -6.853033065795898\n","generated ids [32099, 39, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅyour<extra_id_1>.</s>\n","--------------\n","score: -6.859474182128906\n","generated ids [32099, 81, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅabout<extra_id_1>.</s>\n","--------------\n","score: -6.881559371948242\n","generated ids [32099, 114, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅlike<extra_id_1>.</s>\n","--------------\n","score: -6.8822126388549805\n","generated ids [32099, 424, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅsomething<extra_id_1>.</s>\n","--------------\n","score: -6.893820762634277\n","generated ids [32099, 55, 32098, 58, 1]\n","generated text <extra_id_0>!<extra_id_1>?</s>\n","--------------\n","score: -6.903722763061523\n","generated ids [32099, 5, 571, 32098, 5, 1]\n","generated text <extra_id_0>.‚ñÅHow<extra_id_1>.</s>\n","--------------\n","score: -6.911108016967773\n","generated ids [32099, 1330, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅseems<extra_id_1>.</s>\n","--------------\n","score: -6.91261100769043\n","generated ids [32099, 38, 32098, 55, 1]\n","generated text <extra_id_0>‚ñÅas<extra_id_1>!</s>\n","--------------\n","score: -6.974344253540039\n","generated ids [32099, 78, 32098, 58, 1]\n","generated text <extra_id_0>‚ñÅso<extra_id_1>?</s>\n","--------------\n","score: -7.0085039138793945\n","generated ids [32099, 258, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅthen<extra_id_1>.</s>\n","--------------\n","score: -7.067500114440918\n","generated ids [32099, 21, 32098, 58, 1]\n","generated text <extra_id_0>‚ñÅfor<extra_id_1>?</s>\n","--------------\n","score: -7.0735578536987305\n","generated ids [32099, 130, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅwere<extra_id_1>.</s>\n","--------------\n","score: -7.106849670410156\n","generated ids [32099, 5, 37, 32098, 5, 1]\n","generated text <extra_id_0>.‚ñÅThe<extra_id_1>.</s>\n","--------------\n","score: -7.117338180541992\n","generated ids [32099, 6, 59, 32098, 5, 1]\n","generated text <extra_id_0>,‚ñÅnot<extra_id_1>.</s>\n","--------------\n","score: -7.142725944519043\n","generated ids [32099, 6, 11, 32098, 5, 1]\n","generated text <extra_id_0>,‚ñÅand<extra_id_1>.</s>\n","--------------\n","score: -7.1690521240234375\n","generated ids [32099, 34, 47, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅit‚ñÅwas<extra_id_1>.</s>\n","--------------\n","score: -7.180925369262695\n","generated ids [32099, 5, 4886, 32098, 5, 1]\n","generated text <extra_id_0>.‚ñÅStill<extra_id_1>.</s>\n","--------------\n","score: -7.204132080078125\n","generated ids [32099, 27, 183, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅI‚ñÅam<extra_id_1>.</s>\n","--------------\n","score: -7.2365007400512695\n","generated ids [32099, 5, 27, 7, 32098, 5, 1]\n","generated text <extra_id_0>.‚ñÅIs<extra_id_1>.</s>\n","--------------\n","score: -7.248870849609375\n","generated ids [32099, 11, 8, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅand‚ñÅthe<extra_id_1>.</s>\n","--------------\n","score: -7.3839006423950195\n","generated ids [32099, 27, 7, 32098, 5, 1]\n","generated text <extra_id_0>‚ñÅIs<extra_id_1>.</s>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls**sent_0*,*mask*.*sep+*\n","*cls**sent_0*‚ñÅand*mask*.*sep+*\n","*cls**sent_0*.*mask*.*sep+*\n","*cls**sent_0*‚ñÅis*mask*.*sep+*\n","*cls**sent_0*‚ñÅas*mask*.*sep+*\n","*cls**sent_0*,*mask*?*sep+*\n","*cls**sent_0*‚ñÅso*mask*.*sep+*\n","*cls**sent_0*‚ñÅbut*mask*.*sep+*\n","*cls**sent_0*,*mask*!*sep+*\n","*cls**sent_0*‚ñÅin*mask*.*sep+*\n","*cls**sent_0*...*mask*.*sep+*\n","*cls**sent_0*‚ñÅfor*mask*.*sep+*\n","*cls**sent_0*‚ñÅor*mask*.*sep+*\n","*cls**sent_0*‚ñÅnot*mask*.*sep+*\n","*cls**sent_0*‚ñÅthe*mask*.*sep+*\n","*cls**sent_0*‚ñÅto*mask*.*sep+*\n","*cls**sent_0*‚ñÅthat*mask*.*sep+*\n","*cls**sent_0*!*mask*.*sep+*\n","*cls**sent_0*‚ñÅthat‚ñÅis*mask*.*sep+*\n","*cls**sent_0*‚ñÅand*mask*!*sep+*\n","*cls**sent_0*‚ñÅwas*mask*.*sep+*\n","*cls**sent_0*‚ñÅare*mask*.*sep+*\n","*cls**sent_0*.*mask*?*sep+*\n","*cls**sent_0*‚ñÅof*mask*.*sep+*\n","*cls**sent_0*‚ñÅwith*mask*.*sep+*\n","*cls**sent_0*‚ñÅall*mask*.*sep+*\n","*cls**sent_0*‚ñÅand*mask*?*sep+*\n","*cls**sent_0*‚ñÅwhen*mask*.*sep+*\n","*cls**sent_0*‚ñÅmore*mask*.*sep+*\n","*cls**sent_0*‚ñÅon*mask*.*sep+*\n","*cls**sent_0*.*mask*!*sep+*\n","*cls**sent_0*?*mask*.*sep+*\n","*cls**sent_0*:*mask*.*sep+*\n","*cls**sent_0*‚ñÅvery*mask*.*sep+*\n","*cls**sent_0*‚ñÅor*mask*?*sep+*\n","*cls**sent_0*‚ñÅit‚ñÅis*mask*.*sep+*\n","*cls**sent_0*‚ñÅwhich‚ñÅis*mask*.*sep+*\n","*cls**sent_0*‚ñÅbeing*mask*.*sep+*\n","*cls**sent_0*‚ñÅthis*mask*.*sep+*\n","*cls**sent_0*‚ñÅto‚ñÅbe*mask*.*sep+*\n","*cls**sent_0*‚ñÅso*mask*!*sep+*\n","*cls**sent_0*‚ñÅits*mask*.*sep+*\n","*cls**sent_0*.‚ñÅNot*mask*.*sep+*\n","*cls**sent_0*‚ñÅis*mask*!*sep+*\n","*cls**sent_0*...*mask*?*sep+*\n","*cls**sent_0*;*mask*.*sep+*\n","*cls**sent_0*.‚ñÅVery*mask*.*sep+*\n","*cls**sent_0*‚ñÅtoo*mask*.*sep+*\n","*cls**sent_0*‚ñÅwhere*mask*.*sep+*\n","*cls**sent_0*‚ñÅat*mask*.*sep+*\n","####### generated templates #######\n","\n","####### example #######\n",".<extra_id_0> irrelevant<extra_id_1> Bye bye levi\n",".<extra_id_0> irrelevant<extra_id_1> I like how they added this lil transformation effect on the hange-eren scene\n",".<extra_id_0> irrelevant<extra_id_1> I have a big feeling they'll use youseebiggirl in Warhammer's transformation reveal next ep\n","####### example #######\n","\n","100% 18/18 [07:49<00:00, 26.09s/it]\n","####### generated results #######\n","--------------\n","score: -3.7683372497558594\n","generated ids [32099, 933, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅNot<extra_id_1>.<extra_id_2>\n","--------------\n","score: -4.157559394836426\n","generated ids [32099, 100, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅThis‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -4.203014373779297\n","generated ids [32099, 432, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅAll<extra_id_1>.<extra_id_2>\n","--------------\n","score: -4.282439231872559\n","generated ids [32099, 94, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅIt‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -4.686644554138184\n","generated ids [32099, 27, 7, 34, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅIs‚ñÅit<extra_id_1>?<extra_id_2>\n","--------------\n","score: -5.103520393371582\n","generated ids [32099, 27, 7, 48, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅIs‚ñÅthis<extra_id_1>?<extra_id_2>\n","--------------\n","score: -5.160386085510254\n","generated ids [32099, 4242, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅVery<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.429224014282227\n","generated ids [32099, 264, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅSo<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.500755310058594\n","generated ids [32099, 94, 7, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅIts<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.515201568603516\n","generated ids [32099, 466, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅThat‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.571746826171875\n","generated ids [32099, 27, 7, 24, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅIs‚ñÅthat<extra_id_1>?<extra_id_2>\n","--------------\n","score: -5.595546722412109\n","generated ids [32099, 94, 31, 7, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅIt's<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.672224998474121\n","generated ids [32099, 4886, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅStill<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.714888572692871\n","generated ids [32099, 1377, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅMost<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.772037506103516\n","generated ids [32099, 94, 47, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅIt‚ñÅwas<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.853464126586914\n","generated ids [32099, 27, 7, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅIs<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.864646911621094\n","generated ids [32099, 328, 33, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅThey‚ñÅare<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.930553436279297\n","generated ids [32099, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.9486188888549805\n","generated ids [32099, 9428, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅNothing<extra_id_1>.<extra_id_2>\n","--------------\n","score: -5.971559524536133\n","generated ids [32099, 299, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅBut<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.027024269104004\n","generated ids [32099, 4073, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅWhich‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.16446590423584\n","generated ids [32099, 363, 19, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅWhat‚ñÅis<extra_id_1>?<extra_id_2>\n","--------------\n","score: -6.178217887878418\n","generated ids [32099, 1521, 79, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅAre‚ñÅthey<extra_id_1>?<extra_id_2>\n","--------------\n","score: -6.199556350708008\n","generated ids [32099, 1537, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅMore<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.258326530456543\n","generated ids [32099, 571, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅHow<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.259655952453613\n","generated ids [32099, 466, 31, 7, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅThat's<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.287297248840332\n","generated ids [32099, 506, 33, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅThese‚ñÅare<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.329953193664551\n","generated ids [32099, 5, 32098, 5, 32097]\n","generated text <extra_id_0>.<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.388821601867676\n","generated ids [32099, 275, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅAnd<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.40610408782959\n","generated ids [32099, 27, 183, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅI‚ñÅam<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.467046737670898\n","generated ids [32099, 1203, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅAlso<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.470389366149902\n","generated ids [32099, 1142, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅJust<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.520808219909668\n","generated ids [32099, 493, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅBe<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.5232696533203125\n","generated ids [32099, 1610, 51, 7, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅSeems<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.535642623901367\n","generated ids [32099, 1521, 25, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅAre‚ñÅyou<extra_id_1>?<extra_id_2>\n","--------------\n","score: -6.557099342346191\n","generated ids [32099, 148, 33, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅYou‚ñÅare<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.566082000732422\n","generated ids [32099, 94, 22, 7, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅIt‚Äôs<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.5863189697265625\n","generated ids [32099, 94, 1330, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅIt‚ñÅseems<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.653280258178711\n","generated ids [32099, 11291, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅReally<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.716004371643066\n","generated ids [32099, 7462, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅEverything‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.830849647521973\n","generated ids [32099, 571, 32098, 19, 24, 58, 32097]\n","generated text <extra_id_0>‚ñÅHow<extra_id_1>‚ñÅis‚ñÅthat?<extra_id_2>\n","--------------\n","score: -6.83371639251709\n","generated ids [32099, 156, 32098, 6, 32097]\n","generated text <extra_id_0>‚ñÅIf<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.850705146789551\n","generated ids [32099, 18268, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅPretty<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.870152473449707\n","generated ids [32099, 17112, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅSounds<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.877924919128418\n","generated ids [32099, 59, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅnot<extra_id_1>.<extra_id_2>\n","--------------\n","score: -6.936243057250977\n","generated ids [32099, 933, 32098, 55, 32097]\n","generated text <extra_id_0>‚ñÅNot<extra_id_1>!<extra_id_2>\n","--------------\n","score: -6.946358680725098\n","generated ids [32099, 2751, 34, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅWas‚ñÅit<extra_id_1>?<extra_id_2>\n","--------------\n","score: -6.958604335784912\n","generated ids [32099, 933, 32098, 6, 32097]\n","generated text <extra_id_0>‚ñÅNot<extra_id_1>,<extra_id_2>\n","--------------\n","score: -6.973685264587402\n","generated ids [32099, 933, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅNot<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.019281387329102\n","generated ids [32099, 466, 47, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅThat‚ñÅwas<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.033816337585449\n","generated ids [32099, 20510, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅAbsolutely<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.070965766906738\n","generated ids [32099, 9428, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅNothing‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.082501411437988\n","generated ids [32099, 101, 33, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅWe‚ñÅare<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.084527015686035\n","generated ids [32099, 852, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅNow<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.099616050720215\n","generated ids [32099, 432, 32098, 55, 32097]\n","generated text <extra_id_0>‚ñÅAll<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.101799964904785\n","generated ids [32099, 933, 310, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅNot‚ñÅreally<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.10227108001709\n","generated ids [32099, 2751, 48, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅWas‚ñÅthis<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.119189262390137\n","generated ids [32099, 100, 19, 32098, 55, 32097]\n","generated text <extra_id_0>‚ñÅThis‚ñÅis<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.123032569885254\n","generated ids [32099, 264, 32098, 55, 32097]\n","generated text <extra_id_0>‚ñÅSo<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.126225471496582\n","generated ids [32099, 7462, 1307, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅEverything‚ñÅelse‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.128680229187012\n","generated ids [32099, 299, 34, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅBut‚ñÅit‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.147303581237793\n","generated ids [32099, 8026, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅStay<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.172946929931641\n","generated ids [32099, 571, 32098, 19, 48, 58, 32097]\n","generated text <extra_id_0>‚ñÅHow<extra_id_1>‚ñÅis‚ñÅthis?<extra_id_2>\n","--------------\n","score: -7.174812316894531\n","generated ids [32099, 465, 1200, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅNo‚ñÅlonger<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.211454391479492\n","generated ids [32099, 299, 24, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅBut‚ñÅthat‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.234218597412109\n","generated ids [32099, 27, 253, 34, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅI‚ñÅfind‚ñÅit<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.2425642013549805\n","generated ids [32099, 3836, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅMaybe<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.284540176391602\n","generated ids [32099, 571, 32098, 55, 32097]\n","generated text <extra_id_0>‚ñÅHow<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.301651954650879\n","generated ids [32099, 94, 19, 32098, 55, 32097]\n","generated text <extra_id_0>‚ñÅIt‚ñÅis<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.311293601989746\n","generated ids [32099, 571, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅHow<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.312141418457031\n","generated ids [32099, 100, 19, 59, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅThis‚ñÅis‚ñÅnot<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.317911148071289\n","generated ids [32099, 933, 32098, 44, 66, 5, 32097]\n","generated text <extra_id_0>‚ñÅNot<extra_id_1>‚ñÅat‚ñÅall.<extra_id_2>\n","--------------\n","score: -7.333676338195801\n","generated ids [32099, 3568, 7, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅLooks<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.342568397521973\n","generated ids [32099, 66, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅall<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.374213218688965\n","generated ids [32099, 216, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅHe‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.377998352050781\n","generated ids [32099, 363, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅWhat‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.385969161987305\n","generated ids [32099, 466, 7, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅThats<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.411686897277832\n","generated ids [32099, 9647, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅToo<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.424887657165527\n","generated ids [32099, 955, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅOr<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.438077926635742\n","generated ids [32099, 94, 19, 59, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅIt‚ñÅis‚ñÅnot<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.438960075378418\n","generated ids [32099, 21345, 1307, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅAnything‚ñÅelse<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.471972465515137\n","generated ids [32099, 432, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅAll<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.492318153381348\n","generated ids [32099, 4242, 32098, 55, 32097]\n","generated text <extra_id_0>‚ñÅVery<extra_id_1>!<extra_id_2>\n","--------------\n","score: -7.531829833984375\n","generated ids [32099, 2751, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅWas<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.55588436126709\n","generated ids [32099, 4886, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅStill<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.556159973144531\n","generated ids [32099, 1318, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅIm<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.582489967346191\n","generated ids [32099, 1521, 175, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅAre‚ñÅthese<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.610995292663574\n","generated ids [32099, 156, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅIf<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.611175537109375\n","generated ids [32099, 1615, 19, 48, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅWhy‚ñÅis‚ñÅthis<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.637073516845703\n","generated ids [32099, 282, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅAs<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.653639793395996\n","generated ids [32099, 1203, 32098, 10, 32097]\n","generated text <extra_id_0>‚ñÅAlso<extra_id_1>:<extra_id_2>\n","--------------\n","score: -7.712522506713867\n","generated ids [32099, 571, 19, 48, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅHow‚ñÅis‚ñÅthis<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.713872909545898\n","generated ids [32099, 432, 24, 19, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅAll‚ñÅthat‚ñÅis<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.757518768310547\n","generated ids [32099, 571, 32098, 19, 34, 58, 32097]\n","generated text <extra_id_0>‚ñÅHow<extra_id_1>‚ñÅis‚ñÅit?<extra_id_2>\n","--------------\n","score: -7.760247230529785\n","generated ids [32099, 432, 32098, 6, 32097]\n","generated text <extra_id_0>‚ñÅAll<extra_id_1>,<extra_id_2>\n","--------------\n","score: -7.7935791015625\n","generated ids [32099, 4073, 19, 32098, 58, 32097]\n","generated text <extra_id_0>‚ñÅWhich‚ñÅis<extra_id_1>?<extra_id_2>\n","--------------\n","score: -7.842576026916504\n","generated ids [32099, 1203, 6, 32098, 5, 32097]\n","generated text <extra_id_0>‚ñÅAlso,<extra_id_1>.<extra_id_2>\n","--------------\n","score: -7.867698669433594\n","generated ids [32099, 432, 32098, 251, 5, 32097]\n","generated text <extra_id_0>‚ñÅAll<extra_id_1>‚ñÅinformation.<extra_id_2>\n","--------------\n","score: -7.867886066436768\n","generated ids [32099, 100, 19, 32098, 12, 32097]\n","generated text <extra_id_0>‚ñÅThis‚ñÅis<extra_id_1>‚ñÅto<extra_id_2>\n","--------------\n","score: -8.452643394470215\n","generated ids [32099, 94, 31, 7, 32098, 55, 32097]\n","generated text <extra_id_0>‚ñÅIt's<extra_id_1>!<extra_id_2>\n","####### generated results #######\n","\n","####### generated templates #######\n","*cls*‚ñÅNot*mask*.*+sent_0**sep+*\n","*cls*‚ñÅThis‚ñÅis*mask*.*+sent_0**sep+*\n","*cls*‚ñÅAll*mask*.*+sent_0**sep+*\n","*cls*‚ñÅIt‚ñÅis*mask*.*+sent_0**sep+*\n","*cls*‚ñÅIs‚ñÅit*mask*?*+sent_0**sep+*\n","*cls*‚ñÅIs‚ñÅthis*mask*?*+sent_0**sep+*\n","*cls*‚ñÅVery*mask*.*+sent_0**sep+*\n","*cls*‚ñÅSo*mask*.*+sent_0**sep+*\n","*cls*‚ñÅIts*mask*.*+sent_0**sep+*\n","*cls*‚ñÅThat‚ñÅis*mask*.*+sent_0**sep+*\n","*cls*‚ñÅIs‚ñÅthat*mask*?*+sent_0**sep+*\n","*cls*‚ñÅIt's*mask*.*+sent_0**sep+*\n","*cls*‚ñÅStill*mask*.*+sent_0**sep+*\n","*cls*‚ñÅMost*mask*.*+sent_0**sep+*\n","*cls*‚ñÅIt‚ñÅwas*mask*.*+sent_0**sep+*\n","*cls*‚ñÅIs*mask*.*+sent_0**sep+*\n","*cls*‚ñÅThey‚ñÅare*mask*.*+sent_0**sep+*\n","*cls*‚ñÅis*mask*.*+sent_0**sep+*\n","*cls*‚ñÅNothing*mask*.*+sent_0**sep+*\n","*cls*‚ñÅBut*mask*.*+sent_0**sep+*\n","*cls*‚ñÅWhich‚ñÅis*mask*.*+sent_0**sep+*\n","*cls*‚ñÅWhat‚ñÅis*mask*?*+sent_0**sep+*\n","*cls*‚ñÅAre‚ñÅthey*mask*?*+sent_0**sep+*\n","*cls*‚ñÅMore*mask*.*+sent_0**sep+*\n","*cls*‚ñÅHow*mask*.*+sent_0**sep+*\n","*cls*‚ñÅThat's*mask*.*+sent_0**sep+*\n","*cls*‚ñÅThese‚ñÅare*mask*.*+sent_0**sep+*\n","*cls*.*mask*.*+sent_0**sep+*\n","*cls*‚ñÅAnd*mask*.*+sent_0**sep+*\n","*cls*‚ñÅI‚ñÅam*mask*.*+sent_0**sep+*\n","*cls*‚ñÅAlso*mask*.*+sent_0**sep+*\n","*cls*‚ñÅJust*mask*.*+sent_0**sep+*\n","*cls*‚ñÅBe*mask*.*+sent_0**sep+*\n","*cls*‚ñÅSeems*mask*.*+sent_0**sep+*\n","*cls*‚ñÅAre‚ñÅyou*mask*?*+sent_0**sep+*\n","*cls*‚ñÅYou‚ñÅare*mask*.*+sent_0**sep+*\n","*cls*‚ñÅIt‚Äôs*mask*.*+sent_0**sep+*\n","*cls*‚ñÅIt‚ñÅseems*mask*.*+sent_0**sep+*\n","*cls*‚ñÅReally*mask*.*+sent_0**sep+*\n","*cls*‚ñÅEverything‚ñÅis*mask*.*+sent_0**sep+*\n","*cls*‚ñÅHow*mask*‚ñÅis‚ñÅthat?*+sent_0**sep+*\n","*cls*‚ñÅIf*mask*,*+sent_0**sep+*\n","*cls*‚ñÅPretty*mask*.*+sent_0**sep+*\n","*cls*‚ñÅSounds*mask*.*+sent_0**sep+*\n","*cls*‚ñÅnot*mask*.*+sent_0**sep+*\n","*cls*‚ñÅNot*mask*!*+sent_0**sep+*\n","*cls*‚ñÅWas‚ñÅit*mask*?*+sent_0**sep+*\n","*cls*‚ñÅNot*mask*,*+sent_0**sep+*\n","*cls*‚ñÅNot*mask*?*+sent_0**sep+*\n","*cls*‚ñÅThat‚ñÅwas*mask*.*+sent_0**sep+*\n","####### generated templates #######\n","\n"]}],"source":["!source env/bin/activate; python tools/generate_template.py \\\n","    --output_dir spoilers_auto_template \\\n","    --task_name spoilers \\\n","    --seed 21 \\\n","    --t5_model t5-3b \\\n","    --beam 100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3047311,"status":"ok","timestamp":1693267515099,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"YEg8Nol65TAU","outputId":"f43ab1e9-3269-4351-c762-a3538478a8db"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/training_args.py:337: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  warnings.warn(\n","08/28/2023 23:31:55 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/irrelevant_relevant/16-21.txt\n","08/28/2023 23:31:55 - INFO - __main__ -   Specify load the 80-th template: *cls*‚ñÅAlso*mask*.*+sent_0**sep+*\n","08/28/2023 23:31:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/28/2023 23:31:55 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/80', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=900, warmup_steps=0, logging_dir='runs/Aug28_23-31-54_3002a3a25dc1', logging_first_step=False, logging_steps=22, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=225, dataloader_num_workers=0, past_index=-1, run_name='result/80', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=True, save_logit_dir='result/80', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","08/28/2023 23:31:55 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","08/28/2023 23:31:56 - INFO - filelock -   Lock 133314853656800 acquired on /root/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748.lock\n","Downloading: 100% 482/482 [00:00<00:00, 357kB/s]\n","08/28/2023 23:31:57 - INFO - filelock -   Lock 133314853656800 released on /root/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748.lock\n","08/28/2023 23:31:58 - INFO - filelock -   Lock 133314853622640 acquired on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n","Downloading: 100% 899k/899k [00:00<00:00, 2.09MB/s]\n","08/28/2023 23:31:59 - INFO - filelock -   Lock 133314853622640 released on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n","08/28/2023 23:31:59 - INFO - filelock -   Lock 133314853622160 acquired on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n","Downloading: 100% 456k/456k [00:00<00:00, 1.34MB/s]\n","08/28/2023 23:32:00 - INFO - filelock -   Lock 133314853622160 released on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n","08/28/2023 23:32:00 - INFO - src.dataset -   Label 0 to word ƒ†relevant (4249)\n","08/28/2023 23:32:00 - INFO - src.dataset -   Label 1 to word ƒ†irrelevant (21821)\n","08/28/2023 23:32:00 - INFO - src.dataset -   Total num_sample for mode train: 1\n","08/28/2023 23:32:00 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/binary_v1.0/3615-21\n","08/28/2023 23:32:02 - INFO - filelock -   Lock 133314853684176 acquired on data/k-shot/spoilers/binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","08/28/2023 23:32:04 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers [took 2.241 s]\n","08/28/2023 23:32:04 - INFO - filelock -   Lock 133314853684176 released on data/k-shot/spoilers/binary_v1.0/3615-21/cached_train_RobertaTokenizer_512_spoilers.lock\n","08/28/2023 23:32:07 - INFO - src.dataset -   Label 0 to word ƒ†relevant (4249)\n","08/28/2023 23:32:07 - INFO - src.dataset -   Label 1 to word ƒ†irrelevant (21821)\n","08/28/2023 23:32:07 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","08/28/2023 23:32:07 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/binary_v1.0/3615-21\n","08/28/2023 23:32:07 - INFO - filelock -   Lock 133314853683600 acquired on data/k-shot/spoilers/binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","08/28/2023 23:32:08 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers [took 1.508 s]\n","08/28/2023 23:32:08 - INFO - filelock -   Lock 133314853683600 released on data/k-shot/spoilers/binary_v1.0/3615-21/cached_dev_RobertaTokenizer_512_spoilers.lock\n","08/28/2023 23:32:09 - INFO - src.dataset -   *** Example ***\n","08/28/2023 23:32:09 - INFO - src.dataset -   guid: dev-0\n","08/28/2023 23:32:09 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 22412, 50264, 4, 15565, 642, 38, 122, 33, 7036, 14, 51, 351, 17, 27, 90, 28, 37698, 24, 7, 18220, 42, 191, 4, 8133, 44412, 268, 4, 3421, 7, 8402, 9, 5, 2664, 636, 12266, 8, 471, 13, 127, 3627, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","08/28/2023 23:32:09 - INFO - src.dataset -   text: <s>‚ñÅAlso<mask>. Welp I now have confirmation that they won‚Äôt be uploading it to Hulu this season. Motherfuckers. Time to dust of the tricorne and head for my ship.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n","08/28/2023 23:32:16 - INFO - src.dataset -   Label 0 to word ƒ†relevant (4249)\n","08/28/2023 23:32:16 - INFO - src.dataset -   Label 1 to word ƒ†irrelevant (21821)\n","08/28/2023 23:32:16 - INFO - src.dataset -   Total num_sample for mode test: 1\n","08/28/2023 23:32:16 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot/spoilers/binary_v1.0/3615-21\n","08/28/2023 23:32:16 - INFO - filelock -   Lock 133314853683600 acquired on data/k-shot/spoilers/binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers.lock\n","08/28/2023 23:32:17 - INFO - src.dataset -   Loading features from cached file data/k-shot/spoilers/binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers [took 1.467 s]\n","08/28/2023 23:32:17 - INFO - filelock -   Lock 133314853683600 released on data/k-shot/spoilers/binary_v1.0/3615-21/cached_test_RobertaTokenizer_512_spoilers.lock\n","08/28/2023 23:32:18 - INFO - src.dataset -   *** Example ***\n","08/28/2023 23:32:18 - INFO - src.dataset -   guid: test-0\n","08/28/2023 23:32:18 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 22412, 50264, 4, 3347, 3996, 24, 4, 152, 3238, 21, 169, 357, 4, 38, 2638, 9925, 1253, 8725, 3860, 7, 2807, 227, 5, 232, 8, 1003, 73, 21290, 284, 4, 1534, 14, 12355, 8810, 37, 21, 35510, 145, 2997, 7, 116, 38, 794, 10, 14061, 15, 69, 15401, 4, 289, 10987, 21, 24282, 7735, 4, 264, 1449, 79, 965, 75, 49, 10295, 8, 172, 161, 79, 29, 202, 5, 8501, 9, 5, 2658, 19006, 4, 264, 332, 14, 70, 5, 1462, 3878, 74, 3999, 2854, 19, 18038, 53, 99, 59, 5, 32440, 6988, 661, 4, 1876, 9, 106, 58, 683, 12260, 2580, 50, 11, 5, 92, 2658, 19006, 8, 51, 1507, 19, 5, 910, 16250, 596, 74, 70, 5, 1462, 3878, 54, 129, 1467, 14, 9187, 3033, 624, 5, 6347, 1032, 13, 5, 1079, 9, 9187, 61, 1654, 106, 11, 42, 1068, 6, 17161, 102, 23737, 642, 14963, 53, 99, 38, 5324, 4, 497, 155, 35, 844, 1236, 12001, 161, 51, 1381, 13, 107, 546, 13, 5274, 4, 10426, 35, 5, 204, 12, 180, 498, 330, 1588, 4, 497, 262, 35, 4390, 289, 10987, 161, 51, 3033, 11, 5, 751, 232, 4, 152, 5072, 3137, 506, 21871, 14, 51, 1381, 15166, 4, 20, 129, 1219, 1368, 10987, 74, 33, 10, 1528, 1219, 7, 213, 74, 28, 7, 2097, 5, 910, 16250, 4, 3771, 31360, 4795, 16, 98, 3953, 24, 95, 14236, 162, 9, 821, 28261, 4795, 4, 38, 21, 2818, 821, 10810, 74, 9, 26, 4, 6553, 47, 192, 14, 1369, 116, 1586, 4691, 16, 25672, 7, 206, 381, 2558, 16, 164, 7, 912, 23, 42, 477, 4, 381, 2558, 16, 533, 7, 8439, 5, 1445, 232, 4, 21198, 2420, 11, 39996, 1026, 16, 10, 1856, 7, 70, 9, 106, 4356, 686, 4, 5363, 10720, 1223, 5101, 13, 99, 37, 222, 21, 3127, 17758, 4, 1223, 5101, 16, 35304, 744, 9287, 23, 70, 498, 38, 5170, 77, 37, 581, 1597, 4, 3791, 5, 3980, 58, 2343, 59, 501, 14200, 498, 1640, 118, 11590, 43, 20, 169, 26968, 4242, 17200, 4102, 5, 6529, 21, 5, 275, 169, 939, 115, 9, 5207, 7, 28, 2781, 4, 290, 73, 698, 13, 162, 44660, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[4], label_word_list=None)\n","08/28/2023 23:32:18 - INFO - src.dataset -   text: <s>‚ñÅAlso<mask>. Finally watched it. This episode was way better. I loved Jeans inner struggle to choose between the world and himself/future family. Is that Mikasa he was imagining being married to? I saw a scar on her cheek. Hange was kinda weird. She claims she isn't their superior and then says shes still the commander of the survey corps. She points that all the dead soldiers wouldnt agree with genocide but what about the yeagerist. Many of them were once cadets or in the new survey corps and they agreed with the rumbling why would all the dead soldiers who only knew that humanity lived within the walls fight for the rest of humanity which forced them in this situation, Kinda nitpicky but what I noticed. At 3:20 jean says they tried for years looking for answers. Years: the 4-year timeskip. At 7:57 Hange says they lived in the outside world. This basically comfirms that they tried diplomacy. The only reason hange would have a true reason to go would be to prevent the rumbling. Magaths argument is so weak it just reminds me of gabis argument. I was hoping gabi would of said. Did you see that happen? Armin is naive to think Eren is going to stop at this point. Eren is likely to destroy the entire world. Pieck in titan form is a threat to all of them im sure. Jean kicking Reiner for what he did was truly satisfying. Reiner is spitting death flags at all times I wonder when he'll die. Though the trees were shown about 140000 times(i counted) The way Yelena tore apart the alliance was the best way i could of hoped to be delivered. 8/10 for me :)</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","08/28/2023 23:32:20 - INFO - filelock -   Lock 133314429044912 acquired on /root/.cache/torch/transformers/2339ac1858323405dffff5156947669fed6f63a0c34cfab35bda4f78791893d2.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536.lock\n","Downloading: 100% 1.43G/1.43G [00:14<00:00, 101MB/s]\n","08/28/2023 23:32:34 - INFO - filelock -   Lock 133314429044912 released on /root/.cache/torch/transformers/2339ac1858323405dffff5156947669fed6f63a0c34cfab35bda4f78791893d2.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536.lock\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","08/28/2023 23:32:59 - INFO - src.trainer -   ***** Running training *****\n","08/28/2023 23:32:59 - INFO - src.trainer -     Num examples = 7230\n","08/28/2023 23:32:59 - INFO - src.trainer -     Num Epochs = 4\n","08/28/2023 23:32:59 - INFO - src.trainer -     Instantaneous batch size per device = 2\n","08/28/2023 23:32:59 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 32\n","08/28/2023 23:32:59 - INFO - src.trainer -     Gradient Accumulation steps = 16\n","08/28/2023 23:32:59 - INFO - src.trainer -     Total optimization steps = 900\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:   0% 0/4 [00:00<?, ?it/s]08/28/2023 23:34:25 - INFO - src.trainer -   Train loss: 0.7995116493918679\n","08/28/2023 23:35:41 - INFO - src.trainer -   Train loss: 0.5708294781771573\n","08/28/2023 23:36:58 - INFO - src.trainer -   Train loss: 0.4876331849531694\n","08/28/2023 23:38:14 - INFO - src.trainer -   Train loss: 0.44710783524946734\n","08/28/2023 23:39:31 - INFO - src.trainer -   Train loss: 0.4586271806196733\n","08/28/2023 23:40:47 - INFO - src.trainer -   Train loss: 0.42865441062233667\n","08/28/2023 23:42:04 - INFO - src.trainer -   Train loss: 0.4356176202947443\n","08/28/2023 23:43:20 - INFO - src.trainer -   Train loss: 0.4225502014160156\n","08/28/2023 23:44:36 - INFO - src.trainer -   Train loss: 0.36343799937855115\n","08/28/2023 23:45:53 - INFO - src.trainer -   Train loss: 0.39961312033913354\n","\n","  0% 0/169 [00:00<?, ?it/s]\u001b[A\n","  1% 2/169 [00:00<00:40,  4.08it/s]\u001b[A\n","  2% 3/169 [00:00<00:52,  3.14it/s]\u001b[A\n","  2% 4/169 [00:01<01:01,  2.70it/s]\u001b[A\n","  3% 5/169 [00:01<01:06,  2.46it/s]\u001b[A\n","  4% 6/169 [00:02<01:10,  2.32it/s]\u001b[A\n","  4% 7/169 [00:02<01:12,  2.23it/s]\u001b[A\n","  5% 8/169 [00:03<01:14,  2.17it/s]\u001b[A\n","  5% 9/169 [00:03<01:15,  2.13it/s]\u001b[A\n","  6% 10/169 [00:04<01:15,  2.10it/s]\u001b[A\n","  7% 11/169 [00:04<01:15,  2.08it/s]\u001b[A\n","  7% 12/169 [00:05<01:15,  2.07it/s]\u001b[A\n","  8% 13/169 [00:05<01:15,  2.06it/s]\u001b[A\n","  8% 14/169 [00:06<01:15,  2.06it/s]\u001b[A\n","  9% 15/169 [00:06<01:15,  2.05it/s]\u001b[A\n","  9% 16/169 [00:07<01:14,  2.05it/s]\u001b[A\n"," 10% 17/169 [00:07<01:14,  2.05it/s]\u001b[A\n"," 11% 18/169 [00:08<01:13,  2.05it/s]\u001b[A\n"," 11% 19/169 [00:08<01:13,  2.04it/s]\u001b[A\n"," 12% 20/169 [00:09<01:12,  2.04it/s]\u001b[A\n"," 12% 21/169 [00:09<01:12,  2.04it/s]\u001b[A\n"," 13% 22/169 [00:10<01:11,  2.04it/s]\u001b[A\n"," 14% 23/169 [00:10<01:11,  2.04it/s]\u001b[A\n"," 14% 24/169 [00:11<01:11,  2.04it/s]\u001b[A\n"," 15% 25/169 [00:11<01:10,  2.04it/s]\u001b[A\n"," 15% 26/169 [00:12<01:10,  2.04it/s]\u001b[A\n"," 16% 27/169 [00:12<01:09,  2.04it/s]\u001b[A\n"," 17% 28/169 [00:13<01:09,  2.04it/s]\u001b[A\n"," 17% 29/169 [00:13<01:08,  2.04it/s]\u001b[A\n"," 18% 30/169 [00:14<01:08,  2.04it/s]\u001b[A\n"," 18% 31/169 [00:14<01:07,  2.04it/s]\u001b[A\n"," 19% 32/169 [00:15<01:07,  2.04it/s]\u001b[A\n"," 20% 33/169 [00:15<01:06,  2.04it/s]\u001b[A\n"," 20% 34/169 [00:16<01:06,  2.04it/s]\u001b[A\n"," 21% 35/169 [00:16<01:05,  2.04it/s]\u001b[A\n"," 21% 36/169 [00:17<01:05,  2.04it/s]\u001b[A\n"," 22% 37/169 [00:17<01:04,  2.04it/s]\u001b[A\n"," 22% 38/169 [00:18<01:04,  2.04it/s]\u001b[A\n"," 23% 39/169 [00:18<01:03,  2.04it/s]\u001b[A\n"," 24% 40/169 [00:19<01:03,  2.04it/s]\u001b[A\n"," 24% 41/169 [00:19<01:02,  2.04it/s]\u001b[A\n"," 25% 42/169 [00:20<01:02,  2.04it/s]\u001b[A\n"," 25% 43/169 [00:20<01:01,  2.04it/s]\u001b[A\n"," 26% 44/169 [00:21<01:01,  2.04it/s]\u001b[A\n"," 27% 45/169 [00:21<01:00,  2.04it/s]\u001b[A\n"," 27% 46/169 [00:22<01:00,  2.04it/s]\u001b[A\n"," 28% 47/169 [00:22<00:59,  2.04it/s]\u001b[A\n"," 28% 48/169 [00:23<00:59,  2.04it/s]\u001b[A\n"," 29% 49/169 [00:23<00:58,  2.04it/s]\u001b[A\n"," 30% 50/169 [00:24<00:58,  2.04it/s]\u001b[A\n"," 30% 51/169 [00:24<00:57,  2.04it/s]\u001b[A\n"," 31% 52/169 [00:24<00:57,  2.04it/s]\u001b[A\n"," 31% 53/169 [00:25<00:56,  2.04it/s]\u001b[A\n"," 32% 54/169 [00:25<00:56,  2.04it/s]\u001b[A\n"," 33% 55/169 [00:26<00:55,  2.04it/s]\u001b[A\n"," 33% 56/169 [00:26<00:55,  2.04it/s]\u001b[A\n"," 34% 57/169 [00:27<00:54,  2.04it/s]\u001b[A\n"," 34% 58/169 [00:27<00:54,  2.04it/s]\u001b[A\n"," 35% 59/169 [00:28<00:53,  2.04it/s]\u001b[A\n"," 36% 60/169 [00:28<00:53,  2.04it/s]\u001b[A\n"," 36% 61/169 [00:29<00:52,  2.04it/s]\u001b[A\n"," 37% 62/169 [00:29<00:52,  2.04it/s]\u001b[A\n"," 37% 63/169 [00:30<00:51,  2.04it/s]\u001b[A\n"," 38% 64/169 [00:30<00:51,  2.04it/s]\u001b[A\n"," 38% 65/169 [00:31<00:50,  2.04it/s]\u001b[A\n"," 39% 66/169 [00:31<00:50,  2.04it/s]\u001b[A\n"," 40% 67/169 [00:32<00:49,  2.04it/s]\u001b[A\n"," 40% 68/169 [00:32<00:49,  2.04it/s]\u001b[A\n"," 41% 69/169 [00:33<00:49,  2.04it/s]\u001b[A\n"," 41% 70/169 [00:33<00:48,  2.04it/s]\u001b[A\n"," 42% 71/169 [00:34<00:48,  2.04it/s]\u001b[A\n"," 43% 72/169 [00:34<00:47,  2.04it/s]\u001b[A\n"," 43% 73/169 [00:35<00:47,  2.04it/s]\u001b[A\n"," 44% 74/169 [00:35<00:46,  2.04it/s]\u001b[A\n"," 44% 75/169 [00:36<00:46,  2.04it/s]\u001b[A\n"," 45% 76/169 [00:36<00:45,  2.04it/s]\u001b[A\n"," 46% 77/169 [00:37<00:45,  2.04it/s]\u001b[A\n"," 46% 78/169 [00:37<00:44,  2.04it/s]\u001b[A\n"," 47% 79/169 [00:38<00:44,  2.04it/s]\u001b[A\n"," 47% 80/169 [00:38<00:43,  2.04it/s]\u001b[A\n"," 48% 81/169 [00:39<00:43,  2.04it/s]\u001b[A\n"," 49% 82/169 [00:39<00:42,  2.04it/s]\u001b[A\n"," 49% 83/169 [00:40<00:42,  2.04it/s]\u001b[A\n"," 50% 84/169 [00:40<00:41,  2.04it/s]\u001b[A\n"," 50% 85/169 [00:41<00:41,  2.04it/s]\u001b[A\n"," 51% 86/169 [00:41<00:40,  2.04it/s]\u001b[A\n"," 51% 87/169 [00:42<00:40,  2.04it/s]\u001b[A\n"," 52% 88/169 [00:42<00:39,  2.04it/s]\u001b[A\n"," 53% 89/169 [00:43<00:39,  2.04it/s]\u001b[A\n"," 53% 90/169 [00:43<00:38,  2.04it/s]\u001b[A\n"," 54% 91/169 [00:44<00:38,  2.04it/s]\u001b[A\n"," 54% 92/169 [00:44<00:37,  2.04it/s]\u001b[A\n"," 55% 93/169 [00:45<00:37,  2.04it/s]\u001b[A\n"," 56% 94/169 [00:45<00:36,  2.04it/s]\u001b[A\n"," 56% 95/169 [00:46<00:36,  2.04it/s]\u001b[A\n"," 57% 96/169 [00:46<00:35,  2.04it/s]\u001b[A\n"," 57% 97/169 [00:47<00:35,  2.04it/s]\u001b[A\n"," 58% 98/169 [00:47<00:34,  2.04it/s]\u001b[A\n"," 59% 99/169 [00:48<00:34,  2.04it/s]\u001b[A\n"," 59% 100/169 [00:48<00:33,  2.04it/s]\u001b[A\n"," 60% 101/169 [00:48<00:33,  2.04it/s]\u001b[A\n"," 60% 102/169 [00:49<00:32,  2.04it/s]\u001b[A\n"," 61% 103/169 [00:49<00:32,  2.04it/s]\u001b[A\n"," 62% 104/169 [00:50<00:31,  2.04it/s]\u001b[A\n"," 62% 105/169 [00:50<00:31,  2.04it/s]\u001b[A\n"," 63% 106/169 [00:51<00:30,  2.04it/s]\u001b[A\n"," 63% 107/169 [00:51<00:30,  2.04it/s]\u001b[A\n"," 64% 108/169 [00:52<00:29,  2.04it/s]\u001b[A\n"," 64% 109/169 [00:52<00:29,  2.04it/s]\u001b[A\n"," 65% 110/169 [00:53<00:28,  2.04it/s]\u001b[A\n"," 66% 111/169 [00:53<00:28,  2.04it/s]\u001b[A\n"," 66% 112/169 [00:54<00:27,  2.04it/s]\u001b[A\n"," 67% 113/169 [00:54<00:27,  2.04it/s]\u001b[A\n"," 67% 114/169 [00:55<00:26,  2.04it/s]\u001b[A\n"," 68% 115/169 [00:55<00:26,  2.04it/s]\u001b[A\n"," 69% 116/169 [00:56<00:25,  2.04it/s]\u001b[A\n"," 69% 117/169 [00:56<00:25,  2.04it/s]\u001b[A\n"," 70% 118/169 [00:57<00:24,  2.04it/s]\u001b[A\n"," 70% 119/169 [00:57<00:24,  2.04it/s]\u001b[A\n"," 71% 120/169 [00:58<00:24,  2.04it/s]\u001b[A\n"," 72% 121/169 [00:58<00:23,  2.04it/s]\u001b[A\n"," 72% 122/169 [00:59<00:23,  2.04it/s]\u001b[A\n"," 73% 123/169 [00:59<00:22,  2.04it/s]\u001b[A\n"," 73% 124/169 [01:00<00:22,  2.04it/s]\u001b[A\n"," 74% 125/169 [01:00<00:21,  2.04it/s]\u001b[A\n"," 75% 126/169 [01:01<00:21,  2.04it/s]\u001b[A\n"," 75% 127/169 [01:01<00:20,  2.04it/s]\u001b[A\n"," 76% 128/169 [01:02<00:20,  2.04it/s]\u001b[A\n"," 76% 129/169 [01:02<00:19,  2.04it/s]\u001b[A\n"," 77% 130/169 [01:03<00:19,  2.04it/s]\u001b[A\n"," 78% 131/169 [01:03<00:18,  2.04it/s]\u001b[A\n"," 78% 132/169 [01:04<00:18,  2.04it/s]\u001b[A\n"," 79% 133/169 [01:04<00:17,  2.04it/s]\u001b[A\n"," 79% 134/169 [01:05<00:17,  2.04it/s]\u001b[A\n"," 80% 135/169 [01:05<00:16,  2.04it/s]\u001b[A\n"," 80% 136/169 [01:06<00:16,  2.04it/s]\u001b[A\n"," 81% 137/169 [01:06<00:15,  2.04it/s]\u001b[A\n"," 82% 138/169 [01:07<00:15,  2.04it/s]\u001b[A\n"," 82% 139/169 [01:07<00:14,  2.04it/s]\u001b[A\n"," 83% 140/169 [01:08<00:14,  2.04it/s]\u001b[A\n"," 83% 141/169 [01:08<00:13,  2.04it/s]\u001b[A\n"," 84% 142/169 [01:09<00:13,  2.04it/s]\u001b[A\n"," 85% 143/169 [01:09<00:12,  2.04it/s]\u001b[A\n"," 85% 144/169 [01:10<00:12,  2.04it/s]\u001b[A\n"," 86% 145/169 [01:10<00:11,  2.04it/s]\u001b[A\n"," 86% 146/169 [01:11<00:11,  2.04it/s]\u001b[A\n"," 87% 147/169 [01:11<00:10,  2.04it/s]\u001b[A\n"," 88% 148/169 [01:12<00:10,  2.04it/s]\u001b[A\n"," 88% 149/169 [01:12<00:09,  2.04it/s]\u001b[A\n"," 89% 150/169 [01:13<00:09,  2.04it/s]\u001b[A\n"," 89% 151/169 [01:13<00:08,  2.04it/s]\u001b[A\n"," 90% 152/169 [01:13<00:08,  2.04it/s]\u001b[A\n"," 91% 153/169 [01:14<00:07,  2.04it/s]\u001b[A\n"," 91% 154/169 [01:14<00:07,  2.04it/s]\u001b[A\n"," 92% 155/169 [01:15<00:06,  2.04it/s]\u001b[A\n"," 92% 156/169 [01:15<00:06,  2.04it/s]\u001b[A\n"," 93% 157/169 [01:16<00:05,  2.04it/s]\u001b[A\n"," 93% 158/169 [01:16<00:05,  2.04it/s]\u001b[A\n"," 94% 159/169 [01:17<00:04,  2.04it/s]\u001b[A\n"," 95% 160/169 [01:17<00:04,  2.04it/s]\u001b[A\n"," 95% 161/169 [01:18<00:03,  2.04it/s]\u001b[A\n"," 96% 162/169 [01:18<00:03,  2.04it/s]\u001b[A\n"," 96% 163/169 [01:19<00:02,  2.04it/s]\u001b[A\n"," 97% 164/169 [01:19<00:02,  2.04it/s]\u001b[A\n"," 98% 165/169 [01:20<00:01,  2.04it/s]\u001b[A\n"," 98% 166/169 [01:20<00:01,  2.04it/s]\u001b[A\n"," 99% 167/169 [01:21<00:00,  2.04it/s]\u001b[A\n"," 99% 168/169 [01:21<00:00,  2.04it/s]\u001b[A\n","100% 169/169 [01:22<00:00,  2.31it/s]\u001b[A08/28/2023 23:47:33 - INFO - src.trainer -   Best dev result: 0.9081997871398926\n","Epoch:  25% 1/4 [14:45<44:16, 885.60s/it]08/28/2023 23:48:44 - INFO - src.trainer -   Train loss: 0.3745207353071733\n","08/28/2023 23:50:01 - INFO - src.trainer -   Train loss: 0.3368398492986506\n","08/28/2023 23:51:17 - INFO - src.trainer -   Train loss: 0.33208465576171875\n","08/28/2023 23:52:33 - INFO - src.trainer -   Train loss: 0.33077864213423297\n","08/28/2023 23:53:50 - INFO - src.trainer -   Train loss: 0.3377290205522017\n","08/28/2023 23:55:06 - INFO - src.trainer -   Train loss: 0.3445712002840909\n","08/28/2023 23:56:23 - INFO - src.trainer -   Train loss: 0.32889210094105115\n","08/28/2023 23:57:39 - INFO - src.trainer -   Train loss: 0.30592484907670453\n","08/28/2023 23:58:55 - INFO - src.trainer -   Train loss: 0.3159963434392756\n","08/29/2023 00:00:12 - INFO - src.trainer -   Train loss: 0.2958595969460227\n","\n","170it [14:35, 238.40s/it]            \u001b[A\n","171it [14:36, 167.03s/it]\u001b[A\n","172it [14:36, 117.07s/it]\u001b[A\n","173it [14:37, 82.09s/it] \u001b[A\n","174it [14:37, 57.61s/it]\u001b[A\n","175it [14:38, 40.48s/it]\u001b[A\n","176it [14:38, 28.48s/it]\u001b[A\n","177it [14:39, 20.08s/it]\u001b[A\n","178it [14:39, 14.21s/it]\u001b[A\n","179it [14:40, 10.09s/it]\u001b[A\n","180it [14:40,  7.21s/it]\u001b[A\n","181it [14:41,  5.19s/it]\u001b[A\n","182it [14:41,  3.78s/it]\u001b[A\n","183it [14:42,  2.80s/it]\u001b[A\n","184it [14:42,  2.10s/it]\u001b[A\n","185it [14:43,  1.62s/it]\u001b[A\n","186it [14:43,  1.28s/it]\u001b[A\n","187it [14:44,  1.04s/it]\u001b[A\n","188it [14:44,  1.14it/s]\u001b[A\n","189it [14:45,  1.31it/s]\u001b[A\n","190it [14:45,  1.47it/s]\u001b[A\n","191it [14:46,  1.61it/s]\u001b[A\n","192it [14:46,  1.72it/s]\u001b[A\n","193it [14:47,  1.80it/s]\u001b[A\n","194it [14:47,  1.87it/s]\u001b[A\n","195it [14:48,  1.92it/s]\u001b[A\n","196it [14:48,  1.95it/s]\u001b[A\n","197it [14:49,  1.98it/s]\u001b[A\n","198it [14:49,  2.00it/s]\u001b[A\n","199it [14:50,  2.01it/s]\u001b[A\n","200it [14:50,  2.02it/s]\u001b[A\n","201it [14:50,  2.03it/s]\u001b[A\n","202it [14:51,  2.03it/s]\u001b[A\n","203it [14:51,  2.03it/s]\u001b[A\n","204it [14:52,  2.04it/s]\u001b[A\n","205it [14:52,  2.04it/s]\u001b[A\n","206it [14:53,  2.04it/s]\u001b[A\n","207it [14:53,  2.04it/s]\u001b[A\n","208it [14:54,  2.04it/s]\u001b[A\n","209it [14:54,  2.04it/s]\u001b[A\n","210it [14:55,  2.04it/s]\u001b[A\n","211it [14:55,  2.04it/s]\u001b[A\n","212it [14:56,  2.04it/s]\u001b[A\n","213it [14:56,  2.04it/s]\u001b[A\n","214it [14:57,  2.04it/s]\u001b[A\n","215it [14:57,  2.04it/s]\u001b[A\n","216it [14:58,  2.04it/s]\u001b[A\n","217it [14:58,  2.04it/s]\u001b[A\n","218it [14:59,  2.04it/s]\u001b[A\n","219it [14:59,  2.04it/s]\u001b[A\n","220it [15:00,  2.04it/s]\u001b[A\n","221it [15:00,  2.04it/s]\u001b[A\n","222it [15:01,  2.04it/s]\u001b[A\n","223it [15:01,  2.04it/s]\u001b[A\n","224it [15:02,  2.04it/s]\u001b[A\n","225it [15:02,  2.04it/s]\u001b[A\n","226it [15:03,  2.04it/s]\u001b[A\n","227it [15:03,  2.04it/s]\u001b[A\n","228it [15:04,  2.04it/s]\u001b[A\n","229it [15:04,  2.04it/s]\u001b[A\n","230it [15:05,  2.04it/s]\u001b[A\n","231it [15:05,  2.04it/s]\u001b[A\n","232it [15:06,  2.04it/s]\u001b[A\n","233it [15:06,  2.04it/s]\u001b[A\n","234it [15:07,  2.04it/s]\u001b[A\n","235it [15:07,  2.04it/s]\u001b[A\n","236it [15:08,  2.04it/s]\u001b[A\n","237it [15:08,  2.04it/s]\u001b[A\n","238it [15:09,  2.04it/s]\u001b[A\n","239it [15:09,  2.04it/s]\u001b[A\n","240it [15:10,  2.04it/s]\u001b[A\n","241it [15:10,  2.04it/s]\u001b[A\n","242it [15:11,  2.04it/s]\u001b[A\n","243it [15:11,  2.04it/s]\u001b[A\n","244it [15:12,  2.04it/s]\u001b[A\n","245it [15:12,  2.04it/s]\u001b[A\n","246it [15:13,  2.04it/s]\u001b[A\n","247it [15:13,  2.04it/s]\u001b[A\n","248it [15:14,  2.04it/s]\u001b[A\n","249it [15:14,  2.04it/s]\u001b[A\n","250it [15:15,  2.04it/s]\u001b[A\n","251it [15:15,  2.04it/s]\u001b[A\n","252it [15:15,  2.04it/s]\u001b[A\n","253it [15:16,  2.04it/s]\u001b[A\n","254it [15:16,  2.04it/s]\u001b[A\n","255it [15:17,  2.04it/s]\u001b[A\n","256it [15:17,  2.04it/s]\u001b[A\n","257it [15:18,  2.04it/s]\u001b[A\n","258it [15:18,  2.04it/s]\u001b[A\n","259it [15:19,  2.04it/s]\u001b[A\n","260it [15:19,  2.04it/s]\u001b[A\n","261it [15:20,  2.04it/s]\u001b[A\n","262it [15:20,  2.04it/s]\u001b[A\n","263it [15:21,  2.04it/s]\u001b[A\n","264it [15:21,  2.04it/s]\u001b[A\n","265it [15:22,  2.04it/s]\u001b[A\n","266it [15:22,  2.04it/s]\u001b[A\n","267it [15:23,  2.04it/s]\u001b[A\n","268it [15:23,  2.04it/s]\u001b[A\n","269it [15:24,  2.04it/s]\u001b[A\n","270it [15:24,  2.04it/s]\u001b[A\n","271it [15:25,  2.04it/s]\u001b[A\n","272it [15:25,  2.04it/s]\u001b[A\n","273it [15:26,  2.04it/s]\u001b[A\n","274it [15:26,  2.04it/s]\u001b[A\n","275it [15:27,  2.04it/s]\u001b[A\n","276it [15:27,  2.04it/s]\u001b[A\n","277it [15:28,  2.04it/s]\u001b[A\n","278it [15:28,  2.04it/s]\u001b[A\n","279it [15:29,  2.04it/s]\u001b[A\n","280it [15:29,  2.04it/s]\u001b[A\n","281it [15:30,  2.04it/s]\u001b[A\n","282it [15:30,  2.04it/s]\u001b[A\n","283it [15:31,  2.04it/s]\u001b[A\n","284it [15:31,  2.04it/s]\u001b[A\n","285it [15:32,  2.04it/s]\u001b[A\n","286it [15:32,  2.04it/s]\u001b[A\n","287it [15:33,  2.04it/s]\u001b[A\n","288it [15:33,  2.04it/s]\u001b[A\n","289it [15:34,  2.04it/s]\u001b[A\n","290it [15:34,  2.04it/s]\u001b[A\n","291it [15:35,  2.04it/s]\u001b[A\n","292it [15:35,  2.04it/s]\u001b[A\n","293it [15:36,  2.04it/s]\u001b[A\n","294it [15:36,  2.04it/s]\u001b[A\n","295it [15:37,  2.04it/s]\u001b[A\n","296it [15:37,  2.04it/s]\u001b[A\n","297it [15:38,  2.04it/s]\u001b[A\n","298it [15:38,  2.04it/s]\u001b[A\n","299it [15:39,  2.04it/s]\u001b[A\n","300it [15:39,  2.04it/s]\u001b[A\n","301it [15:40,  2.04it/s]\u001b[A\n","302it [15:40,  2.04it/s]\u001b[A\n","303it [15:40,  2.04it/s]\u001b[A\n","304it [15:41,  2.04it/s]\u001b[A\n","305it [15:41,  2.04it/s]\u001b[A\n","306it [15:42,  2.04it/s]\u001b[A\n","307it [15:42,  2.04it/s]\u001b[A\n","308it [15:43,  2.04it/s]\u001b[A\n","309it [15:43,  2.04it/s]\u001b[A\n","310it [15:44,  2.04it/s]\u001b[A\n","311it [15:44,  2.04it/s]\u001b[A\n","312it [15:45,  2.04it/s]\u001b[A\n","313it [15:45,  2.04it/s]\u001b[A\n","314it [15:46,  2.04it/s]\u001b[A\n","315it [15:46,  2.04it/s]\u001b[A\n","316it [15:47,  2.04it/s]\u001b[A\n","317it [15:47,  2.04it/s]\u001b[A\n","318it [15:48,  2.04it/s]\u001b[A\n","319it [15:48,  2.04it/s]\u001b[A\n","320it [15:49,  2.04it/s]\u001b[A\n","321it [15:49,  2.04it/s]\u001b[A\n","322it [15:50,  2.04it/s]\u001b[A\n","323it [15:50,  2.04it/s]\u001b[A\n","324it [15:51,  2.04it/s]\u001b[A\n","325it [15:51,  2.04it/s]\u001b[A\n","326it [15:52,  2.04it/s]\u001b[A\n","327it [15:52,  2.04it/s]\u001b[A\n","328it [15:53,  2.04it/s]\u001b[A\n","329it [15:53,  2.04it/s]\u001b[A\n","330it [15:54,  2.04it/s]\u001b[A\n","331it [15:54,  2.04it/s]\u001b[A\n","332it [15:55,  2.04it/s]\u001b[A\n","333it [15:55,  2.04it/s]\u001b[A\n","334it [15:56,  2.04it/s]\u001b[A\n","335it [15:56,  2.04it/s]\u001b[A\n","336it [15:57,  2.04it/s]\u001b[A\n","337it [15:57,  2.04it/s]\u001b[A\n","338it [15:57,  2.32it/s]\u001b[A08/29/2023 00:02:09 - INFO - src.trainer -   Best dev result: 0.9113559126853943\n","Epoch:  50% 2/4 [29:21<29:25, 882.61s/it]08/29/2023 00:03:02 - INFO - src.trainer -   Train loss: 0.2823964899236506\n","08/29/2023 00:04:19 - INFO - src.trainer -   Train loss: 0.2166019786487926\n","Epoch:  50% 2/4 [32:10<32:10, 965.48s/it]\n","Traceback (most recent call last):\n","  File \"run.py\", line 647, in <module>\n","    main()\n","  File \"run.py\", line 553, in main\n","    trainer.train(model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/src/trainer.py\", line 359, in train\n","    tr_loss += self.training_step(model, inputs)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/trainer.py\", line 1056, in training_step\n","    loss = self.compute_loss(model, inputs)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/trainer.py\", line 1080, in compute_loss\n","    outputs = model(**inputs)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/src/models.py\", line 151, in forward\n","    outputs = self.roberta(\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/modeling_roberta.py\", line 677, in forward\n","    encoder_outputs = self.encoder(\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/modeling_roberta.py\", line 418, in forward\n","    layer_outputs = layer_module(\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/modeling_roberta.py\", line 339, in forward\n","    self_attention_outputs = self.attention(\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/modeling_roberta.py\", line 279, in forward\n","    attention_output = self.output(self_outputs[0], hidden_states)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/transformers/modeling_roberta.py\", line 230, in forward\n","    hidden_states = self.dense(hidden_states)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 91, in forward\n","    return F.linear(input, self.weight, self.bias)\n","  File \"/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.8/site-packages/torch/nn/functional.py\", line 1678, in linear\n","    output += bias\n","KeyboardInterrupt\n","338it [19:03,  3.38s/it]\n","^C\n"]}],"source":["!source env/bin/activate; bash template_search.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XnaQH8kwjf0m"},"outputs":[],"source":["!cat template_search.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2115,"status":"ok","timestamp":1693277668911,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"SLkCkQf8-NXl","outputId":"b35996e2-3111-4b2e-cc04-a21243e3d882"},"outputs":[{"name":"stdout","output_type":"stream","text":["Seed 21 has 36 results\n"]}],"source":["!source env/bin/activate; python tools/sort_template.py --condition \"{'tag': 'irrelevant-relevant-template-v5', 'task_name': 'spoilers'}\" --template_dir spoilers_auto_template --name irrelevant_relevant"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":411,"status":"ok","timestamp":1693317800612,"user":{"displayName":"Ryan Tran","userId":"14098298700710797497"},"user_tz":420},"id":"asmOUit9r91W","outputId":"5c290ad6-285a-477a-972a-52558d009470"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ensembling template spoilers-47-test.npy\n","Ensembling template spoilers-48-test.npy\n","Ensembling template spoilers-49-test.npy\n","Ensembling template spoilers-50-test.npy\n","Ensembling template spoilers-53-test.npy\n","Ensembling template spoilers-54-test.npy\n","Ensembling template spoilers-56-test.npy\n","Ensembling template spoilers-57-test.npy\n","Ensembling template spoilers-58-test.npy\n","Ensembling template spoilers-59-test.npy\n","Ensembling template spoilers-60-test.npy\n","Ensembling template spoilers-64-test.npy\n","Ensembling template spoilers-65-test.npy\n","Ensembling template spoilers-66-test.npy\n","Ensembling template spoilers-67-test.npy\n","Ensembling template spoilers-70-test.npy\n","Ensembling template spoilers-71-test.npy\n","Ensembling template spoilers-72-test.npy\n","Ensembling template spoilers-73-test.npy\n","Ensembling template spoilers-79-test.npy\n"]}],"source":["!python average_logits.py --logits_dir ensemble_predict_results/prompt/test --sorted_templates spoilers_auto_template/irrelevant_relevant/16-21.score.txt --n_models 20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAWoDzHgIyvM"},"outputs":[],"source":["!cat average_logits.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":453451,"status":"ok","timestamp":1646650181208,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"},"user_tz":480},"id":"JG_3x0XXp0W2","outputId":"55ad3508-361f-48f8-b77e-dcecd0c8729a"},"outputs":[{"name":"stdout","output_type":"stream","text":["+ K=16\n","+ DATA_DIR=data/k-shot-10x\n","+ OUTPUT_DIR=spoilers_auto_label_mapping\n","+ MODEL_NAME=roberta-large\n","+ LOAD_TEMPLATES=true\n","+ TEMPLATE_DIR=spoilers_auto_template\n","+ TEMPLATE_NAME=yes_no\n","+ NUM_TEMPLATES=20\n","+ K_LIKELY=100\n","+ K_NEIGHBORS=30\n","+ N_PAIRS=100\n","+ TASKS=spoilers\n","+ SEEDS=21\n","+ TASK_EXTRA=\n","+ for TASK in $TASKS\n","+ for SEED in $SEEDS\n","+ case $TASK in\n","+ TEMPLATE='*cls**sent_0*._Spoiler?*mask*.*sep+*'\n","+ MAPPING='{0:'\\''No'\\'',1:'\\''Yes'\\''}'\n","+ [[ true = \\t\\r\\u\\e ]]\n","+ FILENAME=spoilers_auto_template/yes_no/16-21.sort.txt\n","++ head -n 20 spoilers_auto_template/yes_no/16-21.sort.txt\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*.*mask*s.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:42:13 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:42:13 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-42-13_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:42:13 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:42:29 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:42:29 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:42:29 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:42:29 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:42:29 - INFO - filelock -   Lock 140193110928720 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:42:29 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:42:29 - INFO - src.dataset -   Saving features into cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.004 s]\n","03/07/2022 10:42:29 - INFO - filelock -   Lock 140193110928720 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:02<00:00,  1.14s/it]03/07/2022 10:42:32 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:42:32 - INFO - src.label_search -   \t| Label 0: ƒ†pl, ƒ†Pl, ƒ†b, ƒ†Your, ƒ†Opp, ƒ†B, ƒ†%, ƒ†Troll, ƒ†sm, ƒ†C\n","03/07/2022 10:42:32 - INFO - src.label_search -   \t| Label 1: ƒ†pl, ƒ†b, ƒ†Pl, ƒ†Opp, ƒ†Your, ƒ†sm, ƒ†B, ƒ†Sh, ƒ†Troll, ƒ†%\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 26% 2581/10000 [00:00<00:00, 25484.11it/s]\u001b[A\n"," 58% 5831/10000 [00:00<00:00, 27231.78it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 31185.85it/s]\n","03/07/2022 10:42:33 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:42:33 - INFO - src.label_search -   \t| ƒ†Photograph ƒ†Gr (acc = 0.81)\n","03/07/2022 10:42:33 - INFO - src.label_search -   \t| ƒ†Kind ƒ†80 (acc = 0.81)\n","03/07/2022 10:42:33 - INFO - src.label_search -   \t| ƒ†If ƒ†Gr (acc = 0.81)\n","03/07/2022 10:42:33 - INFO - src.label_search -   \t| ƒ†Script ƒ†Aw (acc = 0.81)\n","03/07/2022 10:42:33 - INFO - src.label_search -   \t| ƒ†Photograph ƒ†Aw (acc = 0.78)\n","100% 2/2 [00:02<00:00,  1.45s/it]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*‚ñÅJust*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:42:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:42:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-42-39_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:42:39 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:42:54 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:42:54 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:42:54 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:42:54 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:42:54 - INFO - filelock -   Lock 140207555928336 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:42:54 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:42:54 - INFO - filelock -   Lock 140207555928336 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.42it/s]03/07/2022 10:42:55 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:42:55 - INFO - src.label_search -   \t| Label 0: ƒ†kidding, ƒ†wait, ƒ†saying, ƒ†that, ƒ†because, ƒ†wow, ƒ†think, ƒ†joking, ƒ†no, ƒ†say\n","03/07/2022 10:42:55 - INFO - src.label_search -   \t| Label 1: ƒ†kidding, ƒ†saying, ƒ†wow, ƒ†wait, ƒ†joking, ƒ†think, ƒ†because, ƒ†great, ƒ†that, ƒ†watch\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 31% 3131/10000 [00:00<00:00, 31230.51it/s]\u001b[A\n"," 63% 6271/10000 [00:00<00:00, 31228.23it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 31187.48it/s]\n","03/07/2022 10:42:56 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:42:56 - INFO - src.label_search -   \t| ƒ†Wait ƒ†funny (acc = 0.84)\n","03/07/2022 10:42:56 - INFO - src.label_search -   \t| ƒ†repeat ƒ†funny (acc = 0.81)\n","03/07/2022 10:42:56 - INFO - src.label_search -   \t| ƒ†reading ƒ†beautiful (acc = 0.81)\n","03/07/2022 10:42:56 - INFO - src.label_search -   \t| ƒ†click ƒ†beautiful (acc = 0.81)\n","03/07/2022 10:42:56 - INFO - src.label_search -   \t| ƒ†reading ƒ†funny (acc = 0.81)\n","100% 2/2 [00:01<00:00,  1.89it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*.*mask*?*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:43:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:43:01 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-43-01_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:43:01 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:43:16 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:43:16 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:43:16 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:43:16 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:43:16 - INFO - filelock -   Lock 140276202999568 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:43:16 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:43:16 - INFO - filelock -   Lock 140276202999568 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:43:17 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:43:17 - INFO - src.label_search -   \t| Label 0: ƒ†What, ƒ†Really, ƒ†Why, ƒ†Huh, ƒ†See, ƒ†Right, ƒ†., ƒ†No, ƒ†How, ƒ†Seriously\n","03/07/2022 10:43:17 - INFO - src.label_search -   \t| Label 1: ƒ†What, ƒ†Really, ƒ†Why, ƒ†Huh, ƒ†See, ƒ†., ƒ†what, ƒ†really, ƒ†Right, ƒ†right\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 30% 3041/10000 [00:00<00:00, 30330.12it/s]\u001b[A\n"," 64% 6381/10000 [00:00<00:00, 31163.04it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32846.52it/s]\n","03/07/2022 10:43:18 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:43:18 - INFO - src.label_search -   \t| ƒ†Yes ƒ†huh (acc = 0.81)\n","03/07/2022 10:43:18 - INFO - src.label_search -   \t| ƒ†Then ƒ†eh (acc = 0.78)\n","03/07/2022 10:43:18 - INFO - src.label_search -   \t| ƒ†You ƒ†anyone (acc = 0.78)\n","03/07/2022 10:43:18 - INFO - src.label_search -   \t| ƒ†Sorry ƒ†Wow (acc = 0.78)\n","03/07/2022 10:43:18 - INFO - src.label_search -   \t| ƒ†This ƒ†now (acc = 0.78)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*‚ñÅfor*mask*.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:43:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:43:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-43-23_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:43:23 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:43:39 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:43:39 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:43:39 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:43:39 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:43:39 - INFO - filelock -   Lock 140614948910288 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:43:39 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:43:39 - INFO - filelock -   Lock 140614948910288 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:43:40 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| Label 0: ƒ†now, ƒ†me, ƒ†sure, ƒ†real, ƒ†you, ƒ†good, ƒ†example, ƒ†everyone, ƒ†it, ƒ†free\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| Label 1: ƒ†now, ƒ†me, ƒ†sure, ƒ†real, ƒ†example, ƒ†fun, ƒ†good, ƒ†once, ƒ†free, ƒ†you\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 32% 3191/10000 [00:00<00:00, 31807.20it/s]\u001b[A\n"," 65% 6461/10000 [00:00<00:00, 32002.52it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32186.23it/s]\n","03/07/2022 10:43:40 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| ƒ†√¢ƒ¢¬¶ ƒ†god (acc = 0.84)\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| ƒ†√¢ƒ¢¬¶ ƒ†joy (acc = 0.81)\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| ƒ†Now ƒ†joy (acc = 0.81)\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| ƒ†Now ƒ†god (acc = 0.81)\n","03/07/2022 10:43:40 - INFO - src.label_search -   \t| ƒ†discussion ƒ†simplicity (acc = 0.78)\n","100% 2/2 [00:01<00:00,  1.89it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*‚ñÅThe*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:43:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:43:46 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-43-46_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:43:46 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:44:01 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:44:01 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:44:01 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:44:01 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:44:01 - INFO - filelock -   Lock 140068756929168 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:44:01 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:44:01 - INFO - filelock -   Lock 140068756929168 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:44:02 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:44:02 - INFO - src.label_search -   \t| Label 0: ƒ†end, ƒ†rest, ƒ†End, ƒ†game, ƒ†movie, ƒ†world, ƒ†future, ƒ†story, ƒ†film, ƒ†show\n","03/07/2022 10:44:02 - INFO - src.label_search -   \t| Label 1: ƒ†end, ƒ†ending, ƒ†rest, ƒ†End, ƒ†game, ƒ†story, ƒ†world, ƒ†future, ƒ†finale, ƒ†best\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 32% 3181/10000 [00:00<00:00, 31737.62it/s]\u001b[A\n"," 61% 6071/10000 [00:00<00:00, 30688.33it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 30567.99it/s]\n","03/07/2022 10:44:03 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:44:03 - INFO - src.label_search -   \t| ƒ†series ƒ†sword (acc = 0.94)\n","03/07/2022 10:44:03 - INFO - src.label_search -   \t| ƒ†Internet ƒ†dead (acc = 0.91)\n","03/07/2022 10:44:03 - INFO - src.label_search -   \t| ƒ†Internet ƒ†death (acc = 0.91)\n","03/07/2022 10:44:03 - INFO - src.label_search -   \t| ƒ†Internet ƒ†good (acc = 0.91)\n","03/07/2022 10:44:03 - INFO - src.label_search -   \t| ƒ†Show ƒ†sword (acc = 0.91)\n","100% 2/2 [00:01<00:00,  1.89it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*‚ñÅwith*mask*.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:44:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:44:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-44-08_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:44:08 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:44:24 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:44:24 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:44:24 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:44:24 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:44:24 - INFO - filelock -   Lock 139990429371792 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:44:24 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:44:24 - INFO - filelock -   Lock 139990429371792 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:44:25 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| Label 0: ƒ†it, ƒ†me, ƒ†this, ƒ†that, ƒ†you, ƒ†him, ƒ†them, ƒ†us, ƒ†love, ƒ†everything\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| Label 1: ƒ†me, ƒ†it, ƒ†us, ƒ†him, ƒ†you, ƒ†this, ƒ†love, ƒ†that, ƒ†them, ƒ†everything\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2881/10000 [00:00<00:00, 28801.58it/s]\u001b[A\n"," 63% 6281/10000 [00:00<00:00, 30183.25it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32130.36it/s]\n","03/07/2022 10:44:25 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| ƒ†work ƒ†weapons (acc = 0.84)\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| ƒ†less ƒ†death (acc = 0.84)\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| ƒ†photos ƒ†weapons (acc = 0.84)\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| ƒ†A ƒ†flowers (acc = 0.84)\n","03/07/2022 10:44:25 - INFO - src.label_search -   \t| ƒ†questions ƒ†humor (acc = 0.81)\n","100% 2/2 [00:01<00:00,  1.88it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*.*mask*?*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:44:31 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:44:31 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-44-31_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:44:31 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:44:46 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:44:46 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:44:46 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:44:46 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:44:46 - INFO - filelock -   Lock 140438203924560 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:44:46 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:44:46 - INFO - filelock -   Lock 140438203924560 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.44it/s]03/07/2022 10:44:47 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:44:47 - INFO - src.label_search -   \t| Label 0: ƒ†Why, ƒ†What, ƒ†Right, ƒ†Really, ƒ†How, ƒ†Huh, ƒ†Seriously, ƒ†No, ƒ†See, ƒ†Okay\n","03/07/2022 10:44:47 - INFO - src.label_search -   \t| Label 1: ƒ†Why, ƒ†Right, ƒ†Really, ƒ†What, ƒ†Huh, ƒ†How, ƒ†Thoughts, ƒ†No, ƒ†right, ƒ†Seriously\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 30% 3041/10000 [00:00<00:00, 30272.75it/s]\u001b[A\n"," 64% 6401/10000 [00:00<00:00, 31192.00it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32132.06it/s]\n","03/07/2022 10:44:48 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:44:48 - INFO - src.label_search -   \t| ƒ†Okay ƒ†Remember (acc = 0.78)\n","03/07/2022 10:44:48 - INFO - src.label_search -   \t| ƒ†1 ƒ†eh (acc = 0.78)\n","03/07/2022 10:44:48 - INFO - src.label_search -   \t| ƒ†Okay ƒ†Eh (acc = 0.75)\n","03/07/2022 10:44:48 - INFO - src.label_search -   \t| ƒ†OK ƒ†Eh (acc = 0.75)\n","03/07/2022 10:44:48 - INFO - src.label_search -   \t| ƒ†1 ƒ†: (acc = 0.75)\n","100% 2/2 [00:01<00:00,  1.89it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*‚ñÅis*mask*.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:44:53 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:44:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-44-53_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:44:53 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:45:09 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:45:09 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:45:09 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:45:09 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:45:09 - INFO - filelock -   Lock 140655652146192 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:45:09 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:45:09 - INFO - filelock -   Lock 140655652146192 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.41it/s]03/07/2022 10:45:10 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| Label 0: ƒ†it, ƒ†all, ƒ†here, ƒ†good, ƒ†not, ƒ†right, ƒ†true, ƒ†me, ƒ†there, ƒ†that\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| Label 1: ƒ†it, ƒ†all, ƒ†good, ƒ†right, ƒ†here, ƒ†not, ƒ†me, ƒ†awesome, ƒ†great, ƒ†there\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2931/10000 [00:00<00:00, 29164.29it/s]\u001b[A\n"," 61% 6131/10000 [00:00<00:00, 29954.22it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32295.82it/s]\n","03/07/2022 10:45:10 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| ƒ†again ƒ†face (acc = 0.81)\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| ƒ†new ƒ†name (acc = 0.78)\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| ƒ†fine ƒ†cute (acc = 0.78)\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| ƒ†possible ƒ†lol (acc = 0.78)\n","03/07/2022 10:45:10 - INFO - src.label_search -   \t| ƒ†fine ƒ†nice (acc = 0.78)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*.*mask*‚ñÅthanks.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:45:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:45:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-45-16_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:45:16 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:45:31 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:45:31 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:45:31 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:45:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:45:31 - INFO - filelock -   Lock 140077024103824 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:45:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:45:31 - INFO - filelock -   Lock 140077024103824 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.40it/s]03/07/2022 10:45:32 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:45:32 - INFO - src.label_search -   \t| Label 0: ƒ†, ƒ†:, ƒ†:), ƒ†\", ƒ†c, ƒ†(, ƒ†>, ƒ†#, ƒ†I, ƒ†and\n","03/07/2022 10:45:32 - INFO - src.label_search -   \t| Label 1: ƒ†, ƒ†:, ƒ†:), ƒ†>, ƒ†c, ƒ†(, ƒ†#, ƒ†and, ƒ†+, ƒ†k\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2941/10000 [00:00<00:00, 29343.08it/s]\u001b[A\n"," 62% 6151/10000 [00:00<00:00, 30063.38it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 31544.42it/s]\n","03/07/2022 10:45:33 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:45:33 - INFO - src.label_search -   \t| ƒ†* ƒ†^ (acc = 0.75)\n","03/07/2022 10:45:33 - INFO - src.label_search -   \t| ƒ†x ƒ†thanks (acc = 0.72)\n","03/07/2022 10:45:33 - INFO - src.label_search -   \t| ƒ†e ƒ†t (acc = 0.72)\n","03/07/2022 10:45:33 - INFO - src.label_search -   \t| ƒ†' ƒ†, (acc = 0.72)\n","03/07/2022 10:45:33 - INFO - src.label_search -   \t| ƒ†' ƒ†?? (acc = 0.72)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*‚ñÅAnd*mask*!*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:45:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:45:38 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-45-38_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:45:38 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:45:54 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:45:54 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:45:54 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:45:54 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:45:54 - INFO - filelock -   Lock 139993514275088 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:45:54 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:45:54 - INFO - filelock -   Lock 139993514275088 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.42it/s]03/07/2022 10:45:55 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| Label 0: ƒ†now, ƒ†finally, ƒ†hey, ƒ†wait, ƒ†look, ƒ†then, ƒ†yes, ƒ†yet, ƒ†yeah, ƒ†so\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| Label 1: ƒ†finally, ƒ†hey, ƒ†now, ƒ†look, ƒ†wait, ƒ†yes, ƒ†then, ƒ†yeah, ƒ†wow, ƒ†oh\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 31% 3121/10000 [00:00<00:00, 27788.58it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32534.14it/s]\n","03/07/2022 10:45:55 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| ƒ†No ƒ†bonus (acc = 0.81)\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| ƒ†Bravo ƒ†great (acc = 0.81)\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| ƒ†soon ƒ†final (acc = 0.81)\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| ƒ†√¢ƒ¢¬¶ ƒ†final (acc = 0.81)\n","03/07/2022 10:45:55 - INFO - src.label_search -   \t| ƒ†Done ƒ†final (acc = 0.81)\n","100% 2/2 [00:00<00:00,  2.07it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*‚ñÅOh*mask*!*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:46:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:46:01 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-46-01_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:46:01 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:46:16 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:46:16 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:46:16 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:46:16 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:46:16 - INFO - filelock -   Lock 140553886015568 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:46:16 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:46:16 - INFO - filelock -   Lock 140553886015568 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.36it/s]03/07/2022 10:46:17 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:46:17 - INFO - src.label_search -   \t| Label 0: ƒ†no, ƒ†yeah, ƒ†boy, ƒ†my, ƒ†hey, ƒ†yes, ƒ†wait, ƒ†No, ƒ†man, ƒ†shit\n","03/07/2022 10:46:17 - INFO - src.label_search -   \t| Label 1: ƒ†no, ƒ†yeah, ƒ†boy, ƒ†hey, ƒ†my, ƒ†man, ƒ†wow, ƒ†yes, ƒ†wait, ƒ†dear\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2871/10000 [00:00<00:00, 28662.33it/s]\u001b[A\n"," 62% 6191/10000 [00:00<00:00, 29867.67it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32328.61it/s]\n","03/07/2022 10:46:18 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:46:18 - INFO - src.label_search -   \t| ƒ†Hell ƒ†bye (acc = 0.81)\n","03/07/2022 10:46:18 - INFO - src.label_search -   \t| ƒ†fuck ƒ†cool (acc = 0.78)\n","03/07/2022 10:46:18 - INFO - src.label_search -   \t| ƒ†Brother ƒ†sweet (acc = 0.78)\n","03/07/2022 10:46:18 - INFO - src.label_search -   \t| ƒ†Hell ƒ†dad (acc = 0.75)\n","03/07/2022 10:46:18 - INFO - src.label_search -   \t| ƒ†Brother ƒ†fun (acc = 0.75)\n","100% 2/2 [00:01<00:00,  1.86it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*).*mask*.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:46:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:46:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-46-23_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:46:23 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:46:39 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:46:39 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:46:39 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:46:39 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:46:39 - INFO - filelock -   Lock 140329352908368 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:46:39 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:46:39 - INFO - filelock -   Lock 140329352908368 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.31it/s]03/07/2022 10:46:40 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| Label 0: ƒ†2, ƒ†3, ƒ†1, ƒ†5, ƒ†4, ƒ†etc, ƒ†6, ƒ†7, ƒ†8, ƒ†√¢ƒ¢¬¶\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| Label 1: ƒ†2, ƒ†3, ƒ†1, ƒ†etc, ƒ†5, ƒ†4, ƒ†here, ƒ†lol, ƒ†6, ƒ†√¢ƒ¢¬¶\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2911/10000 [00:00<00:00, 28894.33it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 33388.93it/s]\n","03/07/2022 10:46:40 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| ƒ†Ever ƒ†thanks (acc = 0.81)\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| ƒ†None ƒ†wow (acc = 0.78)\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| ƒ†None ƒ†thanks (acc = 0.78)\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| ƒ†Now ƒ†finally (acc = 0.78)\n","03/07/2022 10:46:40 - INFO - src.label_search -   \t| ƒ†Now ƒ†thanks (acc = 0.78)\n","100% 2/2 [00:00<00:00,  2.04it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*‚ñÅfrom*mask*.*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:46:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:46:46 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-46-46_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:46:46 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:47:01 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:47:01 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:47:01 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:47:01 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:47:01 - INFO - filelock -   Lock 140642451383440 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:47:01 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:47:01 - INFO - filelock -   Lock 140642451383440 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.37it/s]03/07/2022 10:47:02 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:47:02 - INFO - src.label_search -   \t| Label 0: ƒ†here, ƒ†there, ƒ†it, ƒ†me, ƒ†above, ƒ†this, ƒ†now, ƒ†below, ƒ†today, ƒ†Wikipedia\n","03/07/2022 10:47:02 - INFO - src.label_search -   \t| Label 1: ƒ†here, ƒ†there, ƒ†above, ƒ†it, ƒ†me, ƒ†hell, ƒ†this, ƒ†below, ƒ†nowhere, ƒ†Japan\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 29% 2923/10000 [00:00<00:00, 29229.61it/s]\u001b[A\n"," 62% 6231/10000 [00:00<00:00, 30264.84it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32425.56it/s]\n","03/07/2022 10:47:03 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:47:03 - INFO - src.label_search -   \t| ƒ†2008 ƒ†heaven (acc = 0.81)\n","03/07/2022 10:47:03 - INFO - src.label_search -   \t| ƒ†√¢ƒ¢¬¶ ƒ†god (acc = 0.81)\n","03/07/2022 10:47:03 - INFO - src.label_search -   \t| ƒ†A ƒ†god (acc = 0.81)\n","03/07/2022 10:47:03 - INFO - src.label_search -   \t| ƒ†2009 ƒ†heaven (acc = 0.81)\n","03/07/2022 10:47:03 - INFO - src.label_search -   \t| ƒ†2013 ƒ†heaven (acc = 0.81)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*:*mask*!*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:47:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:47:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-47-08_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:47:08 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:47:24 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:47:24 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:47:24 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:47:24 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:47:24 - INFO - filelock -   Lock 140220184513936 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:47:24 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:47:24 - INFO - filelock -   Lock 140220184513936 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.39it/s]03/07/2022 10:47:25 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| Label 0: ƒ†Yes, ƒ†Thanks, ƒ†No, ƒ†Wow, ƒ†Yeah, ƒ†Sorry, ƒ†YES, ƒ†Enjoy, ƒ†Oh, ƒ†NO\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| Label 1: ƒ†Enjoy, ƒ†Thanks, ƒ†Wow, ƒ†LOL, ƒ†lol, ƒ†awesome, ƒ†Yeah, ƒ†Yes, ƒ†wow, ƒ†thanks\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 33% 3281/10000 [00:00<00:00, 29322.55it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 33013.33it/s]\n","03/07/2022 10:47:25 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| ƒ†Again ƒ†GG (acc = 0.84)\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| ƒ†Here ƒ†GG (acc = 0.81)\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| ƒ†0 ƒ†die (acc = 0.81)\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| ƒ†Okay ƒ†GG (acc = 0.81)\n","03/07/2022 10:47:25 - INFO - src.label_search -   \t| ƒ†HELP ƒ†POW (acc = 0.81)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*‚ñÅAnd*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:47:31 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:47:31 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-47-31_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:47:31 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:47:46 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:47:46 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:47:46 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:47:46 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:47:46 - INFO - filelock -   Lock 139790228980944 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:47:46 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:47:46 - INFO - filelock -   Lock 139790228980944 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:47:47 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| Label 0: ƒ†2, ƒ†finally, ƒ†3, ƒ†4, ƒ†then, ƒ†now, ƒ†5, ƒ†yes, ƒ†1, ƒ†so\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| Label 1: ƒ†finally, ƒ†2, ƒ†3, ƒ†then, ƒ†4, ƒ†yes, ƒ†now, ƒ†5, ƒ†1, ƒ†no\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 33% 3271/10000 [00:00<00:00, 32684.24it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 33991.10it/s]\n","03/07/2022 10:47:47 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| ƒ†Now ƒ†oh (acc = 0.84)\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| ƒ†anyway ƒ†oh (acc = 0.84)\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| ƒ†So ƒ†ah (acc = 0.84)\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| ƒ†I ƒ†oh (acc = 0.81)\n","03/07/2022 10:47:47 - INFO - src.label_search -   \t| ƒ†it ƒ†second (acc = 0.81)\n","100% 2/2 [00:00<00:00,  2.09it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*‚ñÅNo*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:47:53 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:47:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-47-53_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:47:53 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:48:08 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:48:08 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:48:08 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:48:08 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:48:08 - INFO - filelock -   Lock 140541305159952 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:48:08 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:48:08 - INFO - filelock -   Lock 140541305159952 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.40it/s]03/07/2022 10:48:09 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:48:09 - INFO - src.label_search -   \t| Label 0: ƒ†way, ƒ†comment, ƒ†comments, ƒ†problem, ƒ†kidding, ƒ†more, ƒ†no, ƒ†joke, ƒ†matter, ƒ†wait\n","03/07/2022 10:48:09 - INFO - src.label_search -   \t| Label 1: ƒ†kidding, ƒ†way, ƒ†joke, ƒ†problem, ƒ†no, ƒ†comment, ƒ†wait, ƒ†matter, ƒ†worries, ƒ†comments\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 32% 3194/10000 [00:00<00:00, 31939.95it/s]\u001b[A\n"," 64% 6351/10000 [00:00<00:00, 31803.03it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 33080.32it/s]\n","03/07/2022 10:48:10 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:48:10 - INFO - src.label_search -   \t| ƒ†Thanks ƒ†fair (acc = 0.81)\n","03/07/2022 10:48:10 - INFO - src.label_search -   \t| ƒ†contact ƒ†go (acc = 0.78)\n","03/07/2022 10:48:10 - INFO - src.label_search -   \t| ƒ†info ƒ†go (acc = 0.78)\n","03/07/2022 10:48:10 - INFO - src.label_search -   \t| ƒ†details ƒ†scene (acc = 0.78)\n","03/07/2022 10:48:10 - INFO - src.label_search -   \t| ƒ†Comment ƒ†joking (acc = 0.78)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*‚ñÅ>*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:48:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:48:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-48-16_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:48:16 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:48:31 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:48:31 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:48:31 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:48:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:48:31 - INFO - filelock -   Lock 140444342324560 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:48:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.002 s]\n","03/07/2022 10:48:31 - INFO - filelock -   Lock 140444342324560 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.41it/s]03/07/2022 10:48:32 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| Label 0: ƒ†1, ƒ†2, ƒ†3, ƒ†5, ƒ†4, ƒ†No, ƒ†6, ƒ†A, ƒ†8, ƒ†10\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| Label 1: ƒ†1, ƒ†2, ƒ†3, ƒ†5, ƒ†4, ƒ†6, ƒ†No, ƒ†8, ƒ†7, ƒ†10\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 30% 3001/10000 [00:00<00:00, 29844.66it/s]\u001b[A\n"," 62% 6181/10000 [00:00<00:00, 30396.05it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32245.47it/s]\n","03/07/2022 10:48:32 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| ƒ†yeah ƒ†wow (acc = 0.75)\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| ƒ†X ƒ†Oops (acc = 0.75)\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| ƒ†U ƒ†Indeed (acc = 0.75)\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| ƒ†... ƒ†Oops (acc = 0.75)\n","03/07/2022 10:48:32 - INFO - src.label_search -   \t| ƒ†M ƒ†vs (acc = 0.72)\n","100% 2/2 [00:01<00:00,  1.88it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*‚ñÅ4.*mask*.*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:48:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:48:38 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-48-38_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:48:38 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:48:53 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:48:53 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:48:53 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:48:53 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:48:53 - INFO - filelock -   Lock 140247307999376 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:48:53 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:48:53 - INFO - filelock -   Lock 140247307999376 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.42it/s]03/07/2022 10:48:54 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:48:54 - INFO - src.label_search -   \t| Label 0: ƒ†2, ƒ†1, ƒ†3, ƒ†5, ƒ†4, ƒ†A, ƒ†6, ƒ†8, ƒ†7, ƒ†B\n","03/07/2022 10:48:54 - INFO - src.label_search -   \t| Label 1: ƒ†2, ƒ†1, ƒ†5, ƒ†3, ƒ†4, ƒ†6, ƒ†A, ƒ†8, ƒ†7, ƒ†9\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 27% 2691/10000 [00:00<00:00, 26872.67it/s]\u001b[A\n"," 60% 6001/10000 [00:00<00:00, 28295.14it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 31764.59it/s]\n","03/07/2022 10:48:55 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:48:55 - INFO - src.label_search -   \t| ƒ†All ƒ†Ah (acc = 0.81)\n","03/07/2022 10:48:55 - INFO - src.label_search -   \t| ƒ†U ƒ†Z (acc = 0.78)\n","03/07/2022 10:48:55 - INFO - src.label_search -   \t| ƒ†2018 ƒ†Amazing (acc = 0.78)\n","03/07/2022 10:48:55 - INFO - src.label_search -   \t| ƒ†O ƒ†Z (acc = 0.75)\n","03/07/2022 10:48:55 - INFO - src.label_search -   \t| ƒ†All ƒ†Okay (acc = 0.75)\n","100% 2/2 [00:01<00:00,  1.87it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls*‚ñÅYes!*mask*!*+sent_0**sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:49:00 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:49:00 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-49-00_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:49:00 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:49:16 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:49:16 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:49:16 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:49:16 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:49:16 - INFO - filelock -   Lock 139837995482512 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:49:16 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:49:16 - INFO - filelock -   Lock 139837995482512 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.43it/s]03/07/2022 10:49:17 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| Label 0: ƒ†Yes, ƒ†No, ƒ†YES, ƒ†yes, ƒ†Please, ƒ†Yeah, ƒ†Finally, ƒ†Now, ƒ†Absolutely, ƒ†Indeed\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| Label 1: ƒ†Yes, ƒ†yes, ƒ†No, ƒ†YES, ƒ†Yeah, ƒ†Please, ƒ†Finally, ƒ†Absolutely, ƒ†Indeed, ƒ†Now\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 35% 3491/10000 [00:00<00:00, 34659.16it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 34711.63it/s]\n","03/07/2022 10:49:17 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| ƒ†We ƒ†Bad (acc = 0.81)\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| ƒ†Here ƒ†Bad (acc = 0.81)\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| ƒ†You ƒ†Go (acc = 0.81)\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| ƒ†It ƒ†Bad (acc = 0.78)\n","03/07/2022 10:49:17 - INFO - src.label_search -   \t| ƒ†Actually ƒ†Perfect (acc = 0.78)\n","100% 2/2 [00:00<00:00,  2.10it/s]\n","+ for TEMPLATE in $(head -n $NUM_TEMPLATES $FILENAME)\n","+ python tools/generate_labels.py --overwrite_output_dir --output_dir /tmp/output --model_name_or_path roberta-large --output_file spoilers_auto_label_mapping/auto_template/spoilers/16-21.txt --append_output_file --write_template --template '*cls**sent_0*,*mask*?*sep+*' --mapping '{0:'\\''No'\\'',1:'\\''Yes'\\''}' --task_name spoilers --data_dir data/k-shot-10x/spoilers/16-21 --k_likely 100 --k_neighbors 30 --n_pairs 5 --max_seq_len 256 --per_device_eval_batch_size 16\n","03/07/2022 10:49:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/07/2022 10:49:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_10-49-23_0983a71847ad', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","03/07/2022 10:49:23 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/07/2022 10:49:38 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/07/2022 10:49:38 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/07/2022 10:49:38 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/07/2022 10:49:38 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/07/2022 10:49:38 - INFO - filelock -   Lock 139694936154384 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","03/07/2022 10:49:38 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers [took 0.001 s]\n","03/07/2022 10:49:38 - INFO - filelock -   Lock 139694936154384 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_256_spoilers.lock\n","100% 2/2 [00:00<00:00,  4.44it/s]03/07/2022 10:49:39 - INFO - src.label_search -   Top labels (conditional) per class:\n","03/07/2022 10:49:39 - INFO - src.label_search -   \t| Label 0: ƒ†right, ƒ†huh, ƒ†no, ƒ†eh, ƒ†okay, ƒ†OK, ƒ†ok, ƒ†yeah, ƒ†what, ƒ†yes\n","03/07/2022 10:49:39 - INFO - src.label_search -   \t| Label 1: ƒ†right, ƒ†no, ƒ†huh, ƒ†eh, ƒ†yeah, ƒ†yes, ƒ†ok, ƒ†okay, ƒ†what, ƒ†anyone\n","\n","  0% 0/10000 [00:00<?, ?it/s]\u001b[A\n"," 32% 3151/10000 [00:00<00:00, 31421.26it/s]\u001b[A\n"," 63% 6261/10000 [00:00<00:00, 31316.46it/s]\u001b[A\n","100% 10000/10000 [00:00<00:00, 32861.48it/s]\n","03/07/2022 10:49:40 - INFO - src.label_search -   Automatically searched pairings:\n","03/07/2022 10:49:40 - INFO - src.label_search -   \t| ƒ†Bro ƒ†true (acc = 0.81)\n","03/07/2022 10:49:40 - INFO - src.label_search -   \t| ƒ†Dad ƒ†still (acc = 0.81)\n","03/07/2022 10:49:40 - INFO - src.label_search -   \t| ƒ†it ƒ†true (acc = 0.78)\n","03/07/2022 10:49:40 - INFO - src.label_search -   \t| ƒ†Dad ƒ†actually (acc = 0.78)\n","03/07/2022 10:49:40 - INFO - src.label_search -   \t| ƒ†people ƒ†ha (acc = 0.78)\n","100% 2/2 [00:01<00:00,  1.88it/s]\n"]}],"source":["# !source env/bin/activate; bash tools/run_generate_labels.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29807,"status":"ok","timestamp":1646729802969,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"},"user_tz":480},"id":"W-fgYh11vGYF","outputId":"28103d17-215a-4014-bd98-a2ca50ebf27c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 05:11:42 - INFO - src.trainer -   Best dev result: 0.9383749961853027\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.14s/it]            \u001b[A\n","14it [00:52, 10.67s/it]\u001b[A\n","15it [00:52,  7.54s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.81s/it]\u001b[A\n","18it [00:53,  2.74s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/08/2022 05:12:34 - INFO - src.trainer -   Best dev result: 0.9388749599456787\n","Epoch:  30% 74/250 [02:23<04:52,  1.66s/it]\n","25it [01:43, 15.03s/it]\u001b[A\n","26it [01:44, 10.59s/it]\u001b[A\n","27it [01:44,  7.48s/it]\u001b[A\n","28it [01:44,  5.31s/it]\u001b[A\n","29it [01:44,  3.78s/it]\u001b[A\n","30it [01:44,  2.72s/it]\u001b[A\n","31it [01:45,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:46,  1.54it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:10,  1.66s/it]\n","37it [02:27, 12.86s/it]\u001b[A\n","38it [02:28,  9.07s/it]\u001b[A\n","39it [02:28,  6.42s/it]\u001b[A\n","40it [02:28,  4.56s/it]\u001b[A\n","41it [02:28,  3.26s/it]\u001b[A\n","42it [02:29,  2.35s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.27s/it]\u001b[A\n","45it [02:29,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:29,  1.67s/it]\n","49it [03:12, 12.89s/it]\u001b[A\n","50it [03:12,  9.09s/it]\u001b[A\n","51it [03:12,  6.43s/it]\u001b[A\n","52it [03:13,  4.57s/it]\u001b[A\n","53it [03:13,  3.27s/it]\u001b[A\n","54it [03:13,  2.36s/it]\u001b[A\n","55it [03:13,  1.72s/it]\u001b[A\n","56it [03:13,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:36<02:48,  1.67s/it]\n","61it [03:56, 12.88s/it]\u001b[A\n","62it [03:56,  9.09s/it]\u001b[A\n","63it [03:57,  6.43s/it]\u001b[A\n","64it [03:57,  4.57s/it]\u001b[A\n","65it [03:57,  3.27s/it]\u001b[A\n","66it [03:57,  2.36s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:58,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.67s/it]\n","73it [04:40, 12.88s/it]\u001b[A\n","74it [04:41,  9.09s/it]\u001b[A\n","75it [04:41,  6.43s/it]\u001b[A\n","76it [04:41,  4.57s/it]\u001b[A\n","77it [04:41,  3.27s/it]\u001b[A\n","78it [04:42,  2.36s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:25,  1.67s/it]\n","85it [05:25, 12.88s/it]\u001b[A\n","86it [05:25,  9.09s/it]\u001b[A\n","87it [05:25,  6.43s/it]\u001b[A\n","88it [05:26,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:09, 12.87s/it]\u001b[A\n","98it [06:09,  9.08s/it]\u001b[A\n","99it [06:10,  6.43s/it]\u001b[A\n","100it [06:10,  4.57s/it]\u001b[A\n","101it [06:10,  3.27s/it]\u001b[A\n","102it [06:10,  2.36s/it]\u001b[A\n","103it [06:11,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.67s/it]\n","109it [06:53, 12.88s/it]\u001b[A\n","110it [06:54,  9.08s/it]\u001b[A\n","111it [06:54,  6.43s/it]\u001b[A\n","112it [06:54,  4.57s/it]\u001b[A\n","113it [06:54,  3.27s/it]\u001b[A\n","114it [06:55,  2.36s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:56,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 05:18:36 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 05:18:50 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.49s/it]\u001b[A\n","122it [07:10,  3.22s/it]\u001b[A\n","123it [07:10,  2.32s/it]\u001b[A\n","124it [07:10,  1.69s/it]\u001b[A\n","125it [07:11,  1.25s/it]\u001b[A\n","126it [07:11,  1.06it/s]\u001b[A\n","127it [07:11,  1.36it/s]\u001b[A\n","128it [07:11,  1.72it/s]\u001b[A\n","129it [07:12,  2.10it/s]\u001b[A\n","130it [07:12,  2.48it/s]\u001b[A\n","131it [07:12,  2.84it/s]\u001b[A\n","132it [07:12,  3.52it/s]\u001b[A03/08/2022 05:18:52 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 05:18:52 - INFO - __main__ -     eval_loss = 2.5238890647888184\n","03/08/2022 05:18:52 - INFO - __main__ -     eval_auroc = 0.9388749599456787\n","03/08/2022 05:18:52 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 05:18:52 - INFO - __main__ -     eval_f1 = 0.5952380895614624\n","03/08/2022 05:18:52 - INFO - filelock -   Lock 139850988202896 acquired on log.lock\n","03/08/2022 05:18:52 - INFO - filelock -   Lock 139850988202896 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 05:18:58 - INFO - __main__ -   Specify load the 74-th prompt: *cls*‚ñÅAnd*mask*.*+sent_0**sep+* | {0: \"it\", 1: \"second\"}\n","03/08/2022 05:18:58 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 05:18:58 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-22833', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_05-18-58_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-22833', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 05:18:58 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 05:18:59 - INFO - src.dataset -   Label 0 to word ƒ†it (24)\n","03/08/2022 05:18:59 - INFO - src.dataset -   Label 1 to word ƒ†second (200)\n","03/08/2022 05:18:59 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 05:18:59 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:18:59 - INFO - filelock -   Lock 139997109647120 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:18:59 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 05:18:59 - INFO - filelock -   Lock 139997109647120 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:18:59 - INFO - src.dataset -   Label 0 to word ƒ†it (24)\n","03/08/2022 05:18:59 - INFO - src.dataset -   Label 1 to word ƒ†second (200)\n","03/08/2022 05:18:59 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 05:18:59 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:18:59 - INFO - filelock -   Lock 139997109646800 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:18:59 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 05:18:59 - INFO - filelock -   Lock 139997109646800 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:18:59 - INFO - src.dataset -   *** Example ***\n","03/08/2022 05:18:59 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 05:18:59 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 2409, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 05:18:59 - INFO - src.dataset -   text: <s>‚ñÅAnd<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 05:19:15 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 05:19:15 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 05:19:15 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 05:19:15 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 05:19:15 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 05:19:15 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 05:19:15 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.90it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.70it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.58it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.44it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 05:19:59 - INFO - src.trainer -   Best dev result: 0.8761249780654907\n","Epoch:  20% 49/250 [01:31<05:41,  1.70s/it]\n","13it [00:51, 14.92s/it]            \u001b[A\n","14it [00:51, 10.51s/it]\u001b[A\n","15it [00:52,  7.43s/it]\u001b[A\n","16it [00:52,  5.27s/it]\u001b[A\n","17it [00:52,  3.76s/it]\u001b[A\n","18it [00:52,  2.70s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.55it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:35, 12.89s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:36,  6.43s/it]\u001b[A\n","28it [01:36,  4.57s/it]\u001b[A\n","29it [01:36,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:20, 12.84s/it]\u001b[A\n","38it [02:20,  9.06s/it]\u001b[A\n","39it [02:20,  6.41s/it]\u001b[A\n","40it [02:20,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:21,  1.71s/it]\u001b[A\n","44it [02:21,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:29,  1.66s/it]\n","49it [03:04, 12.85s/it]\u001b[A\n","50it [03:04,  9.06s/it]\u001b[A\n","51it [03:04,  6.41s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:05,  1.72s/it]\u001b[A\n","56it [03:05,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:28<02:47,  1.66s/it]\n","61it [03:48, 12.84s/it]\u001b[A\n","62it [03:48,  9.06s/it]\u001b[A\n","63it [03:49,  6.41s/it]\u001b[A\n","64it [03:49,  4.56s/it]\u001b[A\n","65it [03:49,  3.26s/it]\u001b[A\n","66it [03:49,  2.35s/it]\u001b[A\n","67it [03:49,  1.72s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:50,  1.04it/s]\u001b[A\n","70it [03:50,  1.35it/s]\u001b[A\n","71it [03:50,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:12<02:06,  1.66s/it]\n","73it [04:32, 12.84s/it]\u001b[A\n","74it [04:32,  9.06s/it]\u001b[A\n","75it [04:33,  6.41s/it]\u001b[A\n","76it [04:33,  4.56s/it]\u001b[A\n","77it [04:33,  3.26s/it]\u001b[A\n","78it [04:33,  2.35s/it]\u001b[A\n","79it [04:34,  1.72s/it]\u001b[A\n","80it [04:34,  1.27s/it]\u001b[A\n","81it [04:34,  1.04it/s]\u001b[A\n","82it [04:34,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","84it [04:35,  2.23it/s]\u001b[A03/08/2022 05:24:32 - INFO - src.trainer -   Best dev result: 0.8776249289512634\n","Epoch:  80% 199/250 [06:05<01:24,  1.66s/it]\n","85it [05:24, 15.24s/it]\u001b[A\n","86it [05:25, 10.74s/it]\u001b[A\n","87it [05:25,  7.59s/it]\u001b[A\n","88it [05:25,  5.38s/it]\u001b[A\n","89it [05:25,  3.84s/it]\u001b[A\n","90it [05:26,  2.75s/it]\u001b[A\n","91it [05:26,  2.00s/it]\u001b[A\n","92it [05:26,  1.47s/it]\u001b[A\n","93it [05:26,  1.10s/it]\u001b[A\n","94it [05:27,  1.19it/s]\u001b[A\n","95it [05:27,  1.53it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:09, 12.87s/it]\u001b[A\n","98it [06:09,  9.08s/it]\u001b[A\n","99it [06:09,  6.42s/it]\u001b[A\n","100it [06:09,  4.57s/it]\u001b[A\n","101it [06:10,  3.27s/it]\u001b[A\n","102it [06:10,  2.36s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:10,  1.27s/it]\u001b[A\n","105it [06:10,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:33<00:01,  1.66s/it]\n","109it [06:53, 12.83s/it]\u001b[A\n","110it [06:53,  9.05s/it]\u001b[A\n","111it [06:53,  6.41s/it]\u001b[A\n","112it [06:53,  4.55s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:54,  1.71s/it]\u001b[A\n","116it [06:54,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:55,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:37<00:00,  1.83s/it]\n","03/08/2022 05:26:53 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 05:27:07 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:09,  4.56s/it]\u001b[A\n","122it [07:10,  3.26s/it]\u001b[A\n","123it [07:10,  2.35s/it]\u001b[A\n","124it [07:10,  1.72s/it]\u001b[A\n","125it [07:10,  1.27s/it]\u001b[A\n","126it [07:11,  1.04it/s]\u001b[A\n","127it [07:11,  1.35it/s]\u001b[A\n","128it [07:11,  1.70it/s]\u001b[A\n","129it [07:11,  2.08it/s]\u001b[A\n","130it [07:11,  2.46it/s]\u001b[A\n","131it [07:12,  2.83it/s]\u001b[A\n","132it [07:12,  3.51it/s]\u001b[A03/08/2022 05:27:09 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 05:27:09 - INFO - __main__ -     eval_loss = 5.557480812072754\n","03/08/2022 05:27:09 - INFO - __main__ -     eval_auroc = 0.8776249289512634\n","03/08/2022 05:27:09 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 05:27:09 - INFO - __main__ -     eval_f1 = 0.44859808683395386\n","03/08/2022 05:27:09 - INFO - filelock -   Lock 139997111084944 acquired on log.lock\n","03/08/2022 05:27:09 - INFO - filelock -   Lock 139997111084944 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 05:27:15 - INFO - __main__ -   Specify load the 75-th prompt: *cls*‚ñÅNo*mask*.*+sent_0**sep+* | {0: \"Thanks\", 1: \"fair\"}\n","03/08/2022 05:27:15 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 05:27:15 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-32396', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_05-27-15_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-32396', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 05:27:15 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 05:27:17 - INFO - src.dataset -   Label 0 to word ƒ†Thanks (4557)\n","03/08/2022 05:27:17 - INFO - src.dataset -   Label 1 to word ƒ†fair (2105)\n","03/08/2022 05:27:17 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 05:27:17 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:27:17 - INFO - filelock -   Lock 139997601188112 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:27:17 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 05:27:17 - INFO - filelock -   Lock 139997601188112 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:27:17 - INFO - src.dataset -   Label 0 to word ƒ†Thanks (4557)\n","03/08/2022 05:27:17 - INFO - src.dataset -   Label 1 to word ƒ†fair (2105)\n","03/08/2022 05:27:17 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 05:27:17 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:27:17 - INFO - filelock -   Lock 139997599745808 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:27:17 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 05:27:17 - INFO - filelock -   Lock 139997599745808 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:27:17 - INFO - src.dataset -   *** Example ***\n","03/08/2022 05:27:17 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 05:27:17 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 3084, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 05:27:17 - INFO - src.dataset -   text: <s>‚ñÅNo<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 05:27:32 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 05:27:32 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 05:27:32 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 05:27:32 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 05:27:32 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 05:27:32 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 05:27:32 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.11it/s]\u001b[A03/08/2022 05:28:16 - INFO - src.trainer -   Best dev result: 0.9000000357627869\n","Epoch:  20% 49/250 [01:32<05:34,  1.67s/it]\n","13it [00:51, 14.96s/it]            \u001b[A\n","14it [00:52, 10.54s/it]\u001b[A\n","15it [00:52,  7.45s/it]\u001b[A\n","16it [00:52,  5.28s/it]\u001b[A\n","17it [00:52,  3.77s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.55it/s]\u001b[A\n","24it [00:54,  2.04it/s]\u001b[A03/08/2022 05:29:08 - INFO - src.trainer -   Best dev result: 0.9117500185966492\n","Epoch:  30% 74/250 [02:23<04:54,  1.67s/it]\n","25it [01:43, 15.04s/it]\u001b[A\n","26it [01:43, 10.59s/it]\u001b[A\n","27it [01:43,  7.49s/it]\u001b[A\n","28it [01:43,  5.31s/it]\u001b[A\n","29it [01:44,  3.79s/it]\u001b[A\n","30it [01:44,  2.72s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:44,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","36it [01:45,  2.03it/s]\u001b[A03/08/2022 05:30:00 - INFO - src.trainer -   Best dev result: 0.9122499823570251\n","Epoch:  40% 99/250 [03:15<04:10,  1.66s/it]\n","37it [02:35, 15.16s/it]\u001b[A\n","38it [02:35, 10.68s/it]\u001b[A\n","39it [02:35,  7.55s/it]\u001b[A\n","40it [02:35,  5.35s/it]\u001b[A\n","41it [02:36,  3.81s/it]\u001b[A\n","42it [02:36,  2.74s/it]\u001b[A\n","43it [02:36,  1.99s/it]\u001b[A\n","44it [02:36,  1.46s/it]\u001b[A\n","45it [02:36,  1.09s/it]\u001b[A\n","46it [02:37,  1.20it/s]\u001b[A\n","47it [02:37,  1.53it/s]\u001b[A\n","Epoch:  50% 124/250 [03:59<03:28,  1.66s/it]\n","49it [03:19, 12.84s/it]\u001b[A\n","50it [03:19,  9.05s/it]\u001b[A\n","51it [03:19,  6.41s/it]\u001b[A\n","52it [03:19,  4.55s/it]\u001b[A\n","53it [03:20,  3.26s/it]\u001b[A\n","54it [03:20,  2.35s/it]\u001b[A\n","55it [03:20,  1.71s/it]\u001b[A\n","56it [03:20,  1.27s/it]\u001b[A\n","57it [03:20,  1.04it/s]\u001b[A\n","58it [03:21,  1.35it/s]\u001b[A\n","59it [03:21,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:43<02:47,  1.66s/it]\n","61it [04:03, 12.81s/it]\u001b[A\n","62it [04:03,  9.04s/it]\u001b[A\n","63it [04:03,  6.39s/it]\u001b[A\n","64it [04:03,  4.55s/it]\u001b[A\n","65it [04:04,  3.25s/it]\u001b[A\n","66it [04:04,  2.34s/it]\u001b[A\n","67it [04:04,  1.71s/it]\u001b[A\n","68it [04:04,  1.27s/it]\u001b[A\n","69it [04:05,  1.05it/s]\u001b[A\n","70it [04:05,  1.35it/s]\u001b[A\n","71it [04:05,  1.71it/s]\u001b[A\n","Epoch:  70% 174/250 [05:27<02:05,  1.66s/it]\n","73it [04:47, 12.81s/it]\u001b[A\n","74it [04:47,  9.04s/it]\u001b[A\n","75it [04:47,  6.39s/it]\u001b[A\n","76it [04:48,  4.55s/it]\u001b[A\n","77it [04:48,  3.25s/it]\u001b[A\n","78it [04:48,  2.34s/it]\u001b[A\n","79it [04:48,  1.71s/it]\u001b[A\n","80it [04:48,  1.27s/it]\u001b[A\n","81it [04:49,  1.05it/s]\u001b[A\n","82it [04:49,  1.35it/s]\u001b[A\n","83it [04:49,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:11<01:24,  1.66s/it]\n","85it [05:31, 12.82s/it]\u001b[A\n","86it [05:31,  9.04s/it]\u001b[A\n","87it [05:31,  6.40s/it]\u001b[A\n","88it [05:32,  4.55s/it]\u001b[A\n","89it [05:32,  3.25s/it]\u001b[A\n","90it [05:32,  2.35s/it]\u001b[A\n","91it [05:32,  1.71s/it]\u001b[A\n","92it [05:33,  1.27s/it]\u001b[A\n","93it [05:33,  1.05it/s]\u001b[A\n","94it [05:33,  1.35it/s]\u001b[A\n","95it [05:33,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:55<00:43,  1.66s/it]\n","97it [06:15, 12.82s/it]\u001b[A\n","98it [06:15,  9.04s/it]\u001b[A\n","99it [06:16,  6.40s/it]\u001b[A\n","100it [06:16,  4.55s/it]\u001b[A\n","101it [06:16,  3.25s/it]\u001b[A\n","102it [06:16,  2.35s/it]\u001b[A\n","103it [06:16,  1.71s/it]\u001b[A\n","104it [06:17,  1.27s/it]\u001b[A\n","105it [06:17,  1.05it/s]\u001b[A\n","106it [06:17,  1.35it/s]\u001b[A\n","107it [06:17,  1.70it/s]\u001b[A\n","108it [06:17,  2.23it/s]\u001b[A03/08/2022 05:34:32 - INFO - src.trainer -   Best dev result: 0.9124999642372131\n","Epoch: 100% 249/250 [07:47<00:01,  1.67s/it]\n","109it [07:07, 15.11s/it]\u001b[A\n","110it [07:07, 10.64s/it]\u001b[A\n","111it [07:07,  7.52s/it]\u001b[A\n","112it [07:07,  5.33s/it]\u001b[A\n","113it [07:08,  3.80s/it]\u001b[A\n","114it [07:08,  2.73s/it]\u001b[A\n","115it [07:08,  1.98s/it]\u001b[A\n","116it [07:08,  1.46s/it]\u001b[A\n","117it [07:09,  1.09s/it]\u001b[A\n","118it [07:09,  1.20it/s]\u001b[A\n","119it [07:09,  1.54it/s]\u001b[A\n","Epoch: 100% 250/250 [07:51<00:00,  1.89s/it]\n","03/08/2022 05:35:24 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 05:35:38 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:24,  4.63s/it]\u001b[A\n","122it [07:24,  3.31s/it]\u001b[A\n","123it [07:24,  2.39s/it]\u001b[A\n","124it [07:24,  1.74s/it]\u001b[A\n","125it [07:24,  1.29s/it]\u001b[A\n","126it [07:25,  1.03it/s]\u001b[A\n","127it [07:25,  1.34it/s]\u001b[A\n","128it [07:25,  1.69it/s]\u001b[A\n","129it [07:25,  2.06it/s]\u001b[A\n","130it [07:26,  2.45it/s]\u001b[A\n","131it [07:26,  2.81it/s]\u001b[A\n","132it [07:26,  3.49it/s]\u001b[A03/08/2022 05:35:40 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 05:35:40 - INFO - __main__ -     eval_loss = 2.6098716259002686\n","03/08/2022 05:35:40 - INFO - __main__ -     eval_auroc = 0.9124999642372131\n","03/08/2022 05:35:40 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 05:35:40 - INFO - __main__ -     eval_f1 = 0.5274725556373596\n","03/08/2022 05:35:40 - INFO - filelock -   Lock 139997601187856 acquired on log.lock\n","03/08/2022 05:35:40 - INFO - filelock -   Lock 139997601187856 released on log.lock\n","132it [07:26,  3.38s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 05:35:46 - INFO - __main__ -   Specify load the 76-th prompt: *cls*‚ñÅNo*mask*.*+sent_0**sep+* | {0: \"contact\", 1: \"go\"}\n","03/08/2022 05:35:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 05:35:46 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-4908', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_05-35-46_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-4908', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 05:35:46 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 05:35:48 - INFO - src.dataset -   Label 0 to word ƒ†contact (1511)\n","03/08/2022 05:35:48 - INFO - src.dataset -   Label 1 to word ƒ†go (213)\n","03/08/2022 05:35:48 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 05:35:48 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:35:48 - INFO - filelock -   Lock 140219659973840 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:35:48 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 05:35:48 - INFO - filelock -   Lock 140219659973840 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:35:48 - INFO - src.dataset -   Label 0 to word ƒ†contact (1511)\n","03/08/2022 05:35:48 - INFO - src.dataset -   Label 1 to word ƒ†go (213)\n","03/08/2022 05:35:48 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 05:35:48 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:35:48 - INFO - filelock -   Lock 140219659973776 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:35:48 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 05:35:48 - INFO - filelock -   Lock 140219659973776 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:35:48 - INFO - src.dataset -   *** Example ***\n","03/08/2022 05:35:48 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 05:35:48 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 3084, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 05:35:48 - INFO - src.dataset -   text: <s>‚ñÅNo<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 05:36:03 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 05:36:03 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 05:36:03 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 05:36:03 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 05:36:03 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 05:36:03 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 05:36:03 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 05:36:47 - INFO - src.trainer -   Best dev result: 0.9597499370574951\n","Epoch:  20% 49/250 [01:32<05:38,  1.69s/it]\n","13it [00:51, 14.97s/it]            \u001b[A\n","14it [00:52, 10.55s/it]\u001b[A\n","15it [00:52,  7.45s/it]\u001b[A\n","16it [00:52,  5.29s/it]\u001b[A\n","17it [00:52,  3.77s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.04it/s]\u001b[A03/08/2022 05:37:39 - INFO - src.trainer -   Best dev result: 0.9663749933242798\n","Epoch:  30% 74/250 [02:23<04:53,  1.67s/it]\n","25it [01:43, 15.11s/it]\u001b[A\n","26it [01:43, 10.64s/it]\u001b[A\n","27it [01:43,  7.52s/it]\u001b[A\n","28it [01:44,  5.33s/it]\u001b[A\n","29it [01:44,  3.80s/it]\u001b[A\n","30it [01:44,  2.73s/it]\u001b[A\n","31it [01:44,  1.98s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:45,  1.20it/s]\u001b[A\n","35it [01:45,  1.53it/s]\u001b[A\n","36it [01:45,  2.02it/s]\u001b[A03/08/2022 05:38:31 - INFO - src.trainer -   Best dev result: 0.9753749966621399\n","Epoch:  40% 99/250 [03:15<04:11,  1.67s/it]\n","37it [02:35, 15.27s/it]\u001b[A\n","38it [02:35, 10.76s/it]\u001b[A\n","39it [02:36,  7.60s/it]\u001b[A\n","40it [02:36,  5.39s/it]\u001b[A\n","41it [02:36,  3.84s/it]\u001b[A\n","42it [02:36,  2.76s/it]\u001b[A\n","43it [02:37,  2.00s/it]\u001b[A\n","44it [02:37,  1.47s/it]\u001b[A\n","45it [02:37,  1.10s/it]\u001b[A\n","46it [02:37,  1.19it/s]\u001b[A\n","47it [02:38,  1.52it/s]\u001b[A\n","48it [02:38,  2.01it/s]\u001b[A03/08/2022 05:39:23 - INFO - src.trainer -   Best dev result: 0.9763749837875366\n","Epoch:  50% 124/250 [04:08<03:33,  1.69s/it]\n","49it [03:27, 15.30s/it]\u001b[A\n","50it [03:28, 10.78s/it]\u001b[A\n","51it [03:28,  7.61s/it]\u001b[A\n","52it [03:28,  5.40s/it]\u001b[A\n","53it [03:28,  3.85s/it]\u001b[A\n","54it [03:29,  2.76s/it]\u001b[A\n","55it [03:29,  2.00s/it]\u001b[A\n","56it [03:29,  1.47s/it]\u001b[A\n","57it [03:29,  1.10s/it]\u001b[A\n","58it [03:30,  1.19it/s]\u001b[A\n","59it [03:30,  1.52it/s]\u001b[A\n","60it [03:30,  2.01it/s]\u001b[A03/08/2022 05:40:15 - INFO - src.trainer -   Best dev result: 0.9763750433921814\n","Epoch:  60% 149/250 [05:00<02:49,  1.67s/it]\n","61it [04:19, 15.20s/it]\u001b[A\n","62it [04:20, 10.71s/it]\u001b[A\n","63it [04:20,  7.56s/it]\u001b[A\n","64it [04:20,  5.36s/it]\u001b[A\n","65it [04:20,  3.82s/it]\u001b[A\n","66it [04:21,  2.75s/it]\u001b[A\n","67it [04:21,  1.99s/it]\u001b[A\n","68it [04:21,  1.46s/it]\u001b[A\n","69it [04:21,  1.09s/it]\u001b[A\n","70it [04:21,  1.20it/s]\u001b[A\n","71it [04:22,  1.53it/s]\u001b[A\n","72it [04:22,  2.02it/s]\u001b[A03/08/2022 05:41:07 - INFO - src.trainer -   Best dev result: 0.9766249656677246\n","Epoch:  70% 174/250 [05:52<02:06,  1.67s/it]\n","73it [05:12, 15.35s/it]\u001b[A\n","74it [05:12, 10.82s/it]\u001b[A\n","75it [05:12,  7.64s/it]\u001b[A\n","76it [05:13,  5.42s/it]\u001b[A\n","77it [05:13,  3.86s/it]\u001b[A\n","78it [05:13,  2.77s/it]\u001b[A\n","79it [05:13,  2.01s/it]\u001b[A\n","80it [05:13,  1.48s/it]\u001b[A\n","81it [05:14,  1.10s/it]\u001b[A\n","82it [05:14,  1.19it/s]\u001b[A\n","83it [05:14,  1.52it/s]\u001b[A\n","Epoch:  80% 199/250 [06:36<01:24,  1.66s/it]\n","85it [05:56, 12.88s/it]\u001b[A\n","86it [05:56,  9.09s/it]\u001b[A\n","87it [05:57,  6.43s/it]\u001b[A\n","88it [05:57,  4.57s/it]\u001b[A\n","89it [05:57,  3.27s/it]\u001b[A\n","90it [05:57,  2.36s/it]\u001b[A\n","91it [05:57,  1.72s/it]\u001b[A\n","92it [05:58,  1.27s/it]\u001b[A\n","93it [05:58,  1.04it/s]\u001b[A\n","94it [05:58,  1.35it/s]\u001b[A\n","95it [05:58,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:21<00:43,  1.66s/it]\n","97it [06:40, 12.84s/it]\u001b[A\n","98it [06:40,  9.06s/it]\u001b[A\n","99it [06:41,  6.41s/it]\u001b[A\n","100it [06:41,  4.56s/it]\u001b[A\n","101it [06:41,  3.26s/it]\u001b[A\n","102it [06:41,  2.35s/it]\u001b[A\n","103it [06:42,  1.71s/it]\u001b[A\n","104it [06:42,  1.27s/it]\u001b[A\n","105it [06:42,  1.04it/s]\u001b[A\n","106it [06:42,  1.35it/s]\u001b[A\n","107it [06:43,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [08:05<00:01,  1.66s/it]\n","109it [07:24, 12.84s/it]\u001b[A\n","110it [07:25,  9.06s/it]\u001b[A\n","111it [07:25,  6.41s/it]\u001b[A\n","112it [07:25,  4.56s/it]\u001b[A\n","113it [07:25,  3.26s/it]\u001b[A\n","114it [07:26,  2.35s/it]\u001b[A\n","115it [07:26,  1.71s/it]\u001b[A\n","116it [07:26,  1.27s/it]\u001b[A\n","117it [07:26,  1.04it/s]\u001b[A\n","118it [07:27,  1.35it/s]\u001b[A\n","119it [07:27,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [08:09<00:00,  1.96s/it]\n","03/08/2022 05:44:12 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 05:44:26 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:41,  4.62s/it]\u001b[A\n","122it [07:41,  3.30s/it]\u001b[A\n","123it [07:42,  2.38s/it]\u001b[A\n","124it [07:42,  1.74s/it]\u001b[A\n","125it [07:42,  1.28s/it]\u001b[A\n","126it [07:42,  1.03it/s]\u001b[A\n","127it [07:43,  1.34it/s]\u001b[A\n","128it [07:43,  1.69it/s]\u001b[A\n","129it [07:43,  2.07it/s]\u001b[A\n","130it [07:43,  2.45it/s]\u001b[A\n","131it [07:44,  2.81it/s]\u001b[A\n","132it [07:44,  3.49it/s]\u001b[A03/08/2022 05:44:29 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 05:44:29 - INFO - __main__ -     eval_loss = 0.7703326940536499\n","03/08/2022 05:44:29 - INFO - __main__ -     eval_auroc = 0.9766249656677246\n","03/08/2022 05:44:29 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 05:44:29 - INFO - __main__ -     eval_f1 = 0.7272726893424988\n","03/08/2022 05:44:29 - INFO - filelock -   Lock 140219643571664 acquired on log.lock\n","03/08/2022 05:44:29 - INFO - filelock -   Lock 140219643571664 released on log.lock\n","132it [07:44,  3.52s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 05:44:35 - INFO - __main__ -   Specify load the 77-th prompt: *cls*‚ñÅNo*mask*.*+sent_0**sep+* | {0: \"info\", 1: \"go\"}\n","03/08/2022 05:44:35 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 05:44:35 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-2523', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_05-44-35_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-2523', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 05:44:35 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 05:44:36 - INFO - src.dataset -   Label 0 to word ƒ†info (8574)\n","03/08/2022 05:44:36 - INFO - src.dataset -   Label 1 to word ƒ†go (213)\n","03/08/2022 05:44:36 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 05:44:36 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:44:36 - INFO - filelock -   Lock 140331303923600 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:44:36 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 05:44:36 - INFO - filelock -   Lock 140331303923600 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:44:36 - INFO - src.dataset -   Label 0 to word ƒ†info (8574)\n","03/08/2022 05:44:36 - INFO - src.dataset -   Label 1 to word ƒ†go (213)\n","03/08/2022 05:44:36 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 05:44:36 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:44:36 - INFO - filelock -   Lock 140331303922576 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:44:36 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/08/2022 05:44:36 - INFO - filelock -   Lock 140331303922576 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:44:36 - INFO - src.dataset -   *** Example ***\n","03/08/2022 05:44:36 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 05:44:36 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 3084, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 05:44:36 - INFO - src.dataset -   text: <s>‚ñÅNo<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 05:44:52 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 05:44:52 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 05:44:52 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 05:44:52 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 05:44:52 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 05:44:52 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 05:44:52 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 05:45:36 - INFO - src.trainer -   Best dev result: 0.9300000071525574\n","Epoch:  20% 49/250 [01:32<05:34,  1.66s/it]\n","13it [00:52, 15.09s/it]            \u001b[A\n","14it [00:52, 10.63s/it]\u001b[A\n","15it [00:52,  7.51s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 05:46:29 - INFO - src.trainer -   Best dev result: 0.9307499527931213\n","Epoch:  30% 74/250 [02:23<04:55,  1.68s/it]\n","25it [01:43, 15.04s/it]\u001b[A\n","26it [01:43, 10.60s/it]\u001b[A\n","27it [01:44,  7.49s/it]\u001b[A\n","28it [01:44,  5.31s/it]\u001b[A\n","29it [01:44,  3.79s/it]\u001b[A\n","30it [01:44,  2.72s/it]\u001b[A\n","31it [01:45,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:46,  1.54it/s]\u001b[A\n","36it [01:46,  2.03it/s]\u001b[A03/08/2022 05:47:20 - INFO - src.trainer -   Best dev result: 0.9373749494552612\n","Epoch:  40% 99/250 [03:15<04:12,  1.67s/it]\n","37it [02:35, 15.16s/it]\u001b[A\n","38it [02:35, 10.68s/it]\u001b[A\n","39it [02:35,  7.55s/it]\u001b[A\n","40it [02:36,  5.35s/it]\u001b[A\n","41it [02:36,  3.82s/it]\u001b[A\n","42it [02:36,  2.74s/it]\u001b[A\n","43it [02:36,  1.99s/it]\u001b[A\n","44it [02:37,  1.46s/it]\u001b[A\n","45it [02:37,  1.09s/it]\u001b[A\n","46it [02:37,  1.20it/s]\u001b[A\n","47it [02:37,  1.53it/s]\u001b[A\n","48it [02:37,  2.02it/s]\u001b[A03/08/2022 05:48:12 - INFO - src.trainer -   Best dev result: 0.9467499852180481\n","Epoch:  50% 124/250 [04:08<03:29,  1.67s/it]\n","49it [03:27, 15.33s/it]\u001b[A\n","50it [03:28, 10.80s/it]\u001b[A\n","51it [03:28,  7.63s/it]\u001b[A\n","52it [03:28,  5.41s/it]\u001b[A\n","53it [03:28,  3.86s/it]\u001b[A\n","54it [03:29,  2.77s/it]\u001b[A\n","55it [03:29,  2.01s/it]\u001b[A\n","56it [03:29,  1.47s/it]\u001b[A\n","57it [03:29,  1.10s/it]\u001b[A\n","58it [03:29,  1.19it/s]\u001b[A\n","59it [03:30,  1.52it/s]\u001b[A\n","Epoch:  60% 149/250 [04:52<02:47,  1.66s/it]\n","61it [04:12, 12.86s/it]\u001b[A\n","62it [04:12,  9.07s/it]\u001b[A\n","63it [04:12,  6.42s/it]\u001b[A\n","64it [04:12,  4.56s/it]\u001b[A\n","65it [04:12,  3.26s/it]\u001b[A\n","66it [04:13,  2.35s/it]\u001b[A\n","67it [04:13,  1.72s/it]\u001b[A\n","68it [04:13,  1.27s/it]\u001b[A\n","69it [04:13,  1.04it/s]\u001b[A\n","70it [04:14,  1.35it/s]\u001b[A\n","71it [04:14,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:36<02:05,  1.66s/it]\n","73it [04:56, 12.82s/it]\u001b[A\n","74it [04:56,  9.04s/it]\u001b[A\n","75it [04:56,  6.40s/it]\u001b[A\n","76it [04:56,  4.55s/it]\u001b[A\n","77it [04:57,  3.25s/it]\u001b[A\n","78it [04:57,  2.35s/it]\u001b[A\n","79it [04:57,  1.71s/it]\u001b[A\n","80it [04:57,  1.27s/it]\u001b[A\n","81it [04:58,  1.05it/s]\u001b[A\n","82it [04:58,  1.35it/s]\u001b[A\n","83it [04:58,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:20<01:24,  1.66s/it]\n","85it [05:40, 12.81s/it]\u001b[A\n","86it [05:40,  9.04s/it]\u001b[A\n","87it [05:40,  6.39s/it]\u001b[A\n","88it [05:40,  4.55s/it]\u001b[A\n","89it [05:41,  3.25s/it]\u001b[A\n","90it [05:41,  2.35s/it]\u001b[A\n","91it [05:41,  1.71s/it]\u001b[A\n","92it [05:41,  1.27s/it]\u001b[A\n","93it [05:42,  1.05it/s]\u001b[A\n","94it [05:42,  1.35it/s]\u001b[A\n","95it [05:42,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [07:04<00:43,  1.66s/it]\n","97it [06:24, 12.81s/it]\u001b[A\n","98it [06:24,  9.04s/it]\u001b[A\n","99it [06:24,  6.40s/it]\u001b[A\n","100it [06:25,  4.55s/it]\u001b[A\n","101it [06:25,  3.25s/it]\u001b[A\n","102it [06:25,  2.35s/it]\u001b[A\n","103it [06:25,  1.71s/it]\u001b[A\n","104it [06:25,  1.27s/it]\u001b[A\n","105it [06:26,  1.05it/s]\u001b[A\n","106it [06:26,  1.35it/s]\u001b[A\n","107it [06:26,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:48<00:01,  1.66s/it]\n","109it [07:08, 12.82s/it]\u001b[A\n","110it [07:08,  9.04s/it]\u001b[A\n","111it [07:08,  6.40s/it]\u001b[A\n","112it [07:09,  4.55s/it]\u001b[A\n","113it [07:09,  3.25s/it]\u001b[A\n","114it [07:09,  2.35s/it]\u001b[A\n","115it [07:09,  1.71s/it]\u001b[A\n","116it [07:10,  1.27s/it]\u001b[A\n","117it [07:10,  1.05it/s]\u001b[A\n","118it [07:10,  1.35it/s]\u001b[A\n","119it [07:10,  1.71it/s]\u001b[A\n","Epoch: 100% 250/250 [07:52<00:00,  1.89s/it]\n","03/08/2022 05:52:45 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 05:52:59 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:25,  4.65s/it]\u001b[A\n","122it [07:25,  3.33s/it]\u001b[A\n","123it [07:25,  2.40s/it]\u001b[A\n","124it [07:26,  1.75s/it]\u001b[A\n","125it [07:26,  1.29s/it]\u001b[A\n","126it [07:26,  1.03it/s]\u001b[A\n","127it [07:26,  1.33it/s]\u001b[A\n","128it [07:26,  1.68it/s]\u001b[A\n","129it [07:27,  2.06it/s]\u001b[A\n","130it [07:27,  2.44it/s]\u001b[A\n","131it [07:27,  2.81it/s]\u001b[A\n","132it [07:27,  3.49it/s]\u001b[A03/08/2022 05:53:02 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 05:53:02 - INFO - __main__ -     eval_loss = 1.7783153057098389\n","03/08/2022 05:53:02 - INFO - __main__ -     eval_auroc = 0.9467499852180481\n","03/08/2022 05:53:02 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 05:53:02 - INFO - __main__ -     eval_f1 = 0.6024096608161926\n","03/08/2022 05:53:02 - INFO - filelock -   Lock 140331304148368 acquired on log.lock\n","03/08/2022 05:53:02 - INFO - filelock -   Lock 140331304148368 released on log.lock\n","132it [07:27,  3.39s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 05:53:08 - INFO - __main__ -   Specify load the 78-th prompt: *cls*‚ñÅNo*mask*.*+sent_0**sep+* | {0: \"details\", 1: \"scene\"}\n","03/08/2022 05:53:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 05:53:08 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-11013', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_05-53-08_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-11013', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 05:53:08 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 05:53:10 - INFO - src.dataset -   Label 0 to word ƒ†details (1254)\n","03/08/2022 05:53:10 - INFO - src.dataset -   Label 1 to word ƒ†scene (1310)\n","03/08/2022 05:53:10 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 05:53:10 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:53:10 - INFO - filelock -   Lock 140377742976784 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:53:10 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 05:53:10 - INFO - filelock -   Lock 140377742976784 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:53:10 - INFO - src.dataset -   Label 0 to word ƒ†details (1254)\n","03/08/2022 05:53:10 - INFO - src.dataset -   Label 1 to word ƒ†scene (1310)\n","03/08/2022 05:53:10 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 05:53:10 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 05:53:10 - INFO - filelock -   Lock 140377714542928 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:53:10 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 05:53:10 - INFO - filelock -   Lock 140377714542928 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 05:53:10 - INFO - src.dataset -   *** Example ***\n","03/08/2022 05:53:10 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 05:53:10 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 3084, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 05:53:10 - INFO - src.dataset -   text: <s>‚ñÅNo<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 05:53:25 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 05:53:25 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 05:53:25 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 05:53:25 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 05:53:25 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 05:53:25 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 05:53:25 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.64it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.08it/s]\u001b[A03/08/2022 05:54:10 - INFO - src.trainer -   Best dev result: 0.9197499752044678\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.12s/it]            \u001b[A\n","14it [00:52, 10.66s/it]\u001b[A\n","15it [00:52,  7.53s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.81s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/08/2022 05:55:02 - INFO - src.trainer -   Best dev result: 0.9332499504089355\n","Epoch:  30% 74/250 [02:24<04:56,  1.69s/it]\n","25it [01:44, 15.13s/it]\u001b[A\n","26it [01:44, 10.66s/it]\u001b[A\n","27it [01:44,  7.53s/it]\u001b[A\n","28it [01:44,  5.34s/it]\u001b[A\n","29it [01:45,  3.81s/it]\u001b[A\n","30it [01:45,  2.74s/it]\u001b[A\n","31it [01:45,  1.98s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:46,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:11,  1.66s/it]\n","37it [02:28, 12.89s/it]\u001b[A\n","38it [02:28,  9.09s/it]\u001b[A\n","39it [02:28,  6.43s/it]\u001b[A\n","40it [02:29,  4.57s/it]\u001b[A\n","41it [02:29,  3.27s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.27s/it]\u001b[A\n","45it [02:30,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:29,  1.66s/it]\n","49it [03:12, 12.85s/it]\u001b[A\n","50it [03:12,  9.07s/it]\u001b[A\n","51it [03:13,  6.42s/it]\u001b[A\n","52it [03:13,  4.56s/it]\u001b[A\n","53it [03:13,  3.26s/it]\u001b[A\n","54it [03:13,  2.35s/it]\u001b[A\n","55it [03:13,  1.72s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:36<02:47,  1.66s/it]\n","61it [03:56, 12.85s/it]\u001b[A\n","62it [03:57,  9.06s/it]\u001b[A\n","63it [03:57,  6.41s/it]\u001b[A\n","64it [03:57,  4.56s/it]\u001b[A\n","65it [03:57,  3.26s/it]\u001b[A\n","66it [03:57,  2.35s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:59,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.66s/it]\n","73it [04:41, 12.85s/it]\u001b[A\n","74it [04:41,  9.06s/it]\u001b[A\n","75it [04:41,  6.41s/it]\u001b[A\n","76it [04:41,  4.56s/it]\u001b[A\n","77it [04:41,  3.26s/it]\u001b[A\n","78it [04:42,  2.35s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:24,  1.66s/it]\n","85it [05:25, 12.86s/it]\u001b[A\n","86it [05:25,  9.07s/it]\u001b[A\n","87it [05:25,  6.42s/it]\u001b[A\n","88it [05:25,  4.56s/it]\u001b[A\n","89it [05:26,  3.26s/it]\u001b[A\n","90it [05:26,  2.35s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:09, 12.85s/it]\u001b[A\n","98it [06:09,  9.06s/it]\u001b[A\n","99it [06:09,  6.41s/it]\u001b[A\n","100it [06:10,  4.56s/it]\u001b[A\n","101it [06:10,  3.26s/it]\u001b[A\n","102it [06:10,  2.35s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:33<00:01,  1.66s/it]\n","109it [06:53, 12.85s/it]\u001b[A\n","110it [06:53,  9.06s/it]\u001b[A\n","111it [06:54,  6.41s/it]\u001b[A\n","112it [06:54,  4.56s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 06:01:04 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:01:18 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.61s/it]\u001b[A\n","122it [07:10,  3.30s/it]\u001b[A\n","123it [07:10,  2.38s/it]\u001b[A\n","124it [07:11,  1.73s/it]\u001b[A\n","125it [07:11,  1.28s/it]\u001b[A\n","126it [07:11,  1.03it/s]\u001b[A\n","127it [07:11,  1.34it/s]\u001b[A\n","128it [07:12,  1.69it/s]\u001b[A\n","129it [07:12,  2.07it/s]\u001b[A\n","130it [07:12,  2.45it/s]\u001b[A\n","131it [07:12,  2.82it/s]\u001b[A\n","132it [07:12,  3.50it/s]\u001b[A03/08/2022 06:01:20 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:01:20 - INFO - __main__ -     eval_loss = 1.5492570400238037\n","03/08/2022 06:01:20 - INFO - __main__ -     eval_auroc = 0.9332499504089355\n","03/08/2022 06:01:20 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/08/2022 06:01:20 - INFO - __main__ -     eval_f1 = 0.6388888955116272\n","03/08/2022 06:01:20 - INFO - filelock -   Lock 140378233225808 acquired on log.lock\n","03/08/2022 06:01:20 - INFO - filelock -   Lock 140378233225808 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:01:26 - INFO - __main__ -   Specify load the 79-th prompt: *cls*‚ñÅNo*mask*.*+sent_0**sep+* | {0: \"Comment\", 1: \"joking\"}\n","03/08/2022 06:01:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:01:26 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-272', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-01-26_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-272', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:01:26 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:01:28 - INFO - src.dataset -   Label 0 to word ƒ†Comment (14642)\n","03/08/2022 06:01:28 - INFO - src.dataset -   Label 1 to word ƒ†joking (22024)\n","03/08/2022 06:01:28 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:01:28 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:01:28 - INFO - filelock -   Lock 139628890232656 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:01:28 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 06:01:28 - INFO - filelock -   Lock 139628890232656 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:01:28 - INFO - src.dataset -   Label 0 to word ƒ†Comment (14642)\n","03/08/2022 06:01:28 - INFO - src.dataset -   Label 1 to word ƒ†joking (22024)\n","03/08/2022 06:01:28 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:01:28 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:01:28 - INFO - filelock -   Lock 139628565614544 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:01:28 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:01:28 - INFO - filelock -   Lock 139628565614544 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:01:28 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:01:28 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:01:28 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 3084, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:01:28 - INFO - src.dataset -   text: <s>‚ñÅNo<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:01:43 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:01:43 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:01:43 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:01:43 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:01:43 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:01:43 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:01:43 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.63it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 06:02:28 - INFO - src.trainer -   Best dev result: 0.9497499465942383\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.04s/it]            \u001b[A\n","14it [00:52, 10.59s/it]\u001b[A\n","15it [00:52,  7.48s/it]\u001b[A\n","16it [00:52,  5.31s/it]\u001b[A\n","17it [00:53,  3.79s/it]\u001b[A\n","18it [00:53,  2.72s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.87s/it]\u001b[A\n","26it [01:36,  9.08s/it]\u001b[A\n","27it [01:36,  6.42s/it]\u001b[A\n","28it [01:36,  4.56s/it]\u001b[A\n","29it [01:37,  3.26s/it]\u001b[A\n","30it [01:37,  2.35s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:20, 12.84s/it]\u001b[A\n","38it [02:20,  9.06s/it]\u001b[A\n","39it [02:20,  6.41s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:21,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:29,  1.66s/it]\n","49it [03:04, 12.84s/it]\u001b[A\n","50it [03:04,  9.06s/it]\u001b[A\n","51it [03:05,  6.41s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:06,  1.71s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:28<02:47,  1.66s/it]\n","61it [03:48, 12.83s/it]\u001b[A\n","62it [03:49,  9.05s/it]\u001b[A\n","63it [03:49,  6.40s/it]\u001b[A\n","64it [03:49,  4.55s/it]\u001b[A\n","65it [03:49,  3.26s/it]\u001b[A\n","66it [03:49,  2.35s/it]\u001b[A\n","67it [03:50,  1.71s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:50,  1.04it/s]\u001b[A\n","70it [03:50,  1.35it/s]\u001b[A\n","71it [03:51,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:13<02:06,  1.66s/it]\n","73it [04:33, 12.84s/it]\u001b[A\n","74it [04:33,  9.06s/it]\u001b[A\n","75it [04:33,  6.41s/it]\u001b[A\n","76it [04:33,  4.56s/it]\u001b[A\n","77it [04:33,  3.26s/it]\u001b[A\n","78it [04:34,  2.35s/it]\u001b[A\n","79it [04:34,  1.71s/it]\u001b[A\n","80it [04:34,  1.27s/it]\u001b[A\n","81it [04:34,  1.04it/s]\u001b[A\n","82it [04:35,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","84it [04:35,  2.23it/s]\u001b[A03/08/2022 06:07:01 - INFO - src.trainer -   Best dev result: 0.9537500143051147\n","Epoch:  80% 199/250 [06:04<01:25,  1.67s/it]\n","85it [05:24, 14.98s/it]\u001b[A\n","86it [05:24, 10.56s/it]\u001b[A\n","87it [05:24,  7.46s/it]\u001b[A\n","88it [05:25,  5.29s/it]\u001b[A\n","89it [05:25,  3.77s/it]\u001b[A\n","90it [05:25,  2.71s/it]\u001b[A\n","91it [05:25,  1.97s/it]\u001b[A\n","92it [05:25,  1.45s/it]\u001b[A\n","93it [05:26,  1.08s/it]\u001b[A\n","94it [05:26,  1.21it/s]\u001b[A\n","95it [05:26,  1.54it/s]\u001b[A\n","Epoch:  90% 224/250 [06:48<00:43,  1.66s/it]\n","97it [06:08, 12.86s/it]\u001b[A\n","98it [06:08,  9.07s/it]\u001b[A\n","99it [06:08,  6.42s/it]\u001b[A\n","100it [06:09,  4.56s/it]\u001b[A\n","101it [06:09,  3.26s/it]\u001b[A\n","102it [06:09,  2.35s/it]\u001b[A\n","103it [06:09,  1.72s/it]\u001b[A\n","104it [06:10,  1.27s/it]\u001b[A\n","105it [06:10,  1.04it/s]\u001b[A\n","106it [06:10,  1.35it/s]\u001b[A\n","107it [06:10,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:32<00:01,  1.66s/it]\n","109it [06:52, 12.83s/it]\u001b[A\n","110it [06:52,  9.05s/it]\u001b[A\n","111it [06:53,  6.40s/it]\u001b[A\n","112it [06:53,  4.55s/it]\u001b[A\n","113it [06:53,  3.26s/it]\u001b[A\n","114it [06:53,  2.35s/it]\u001b[A\n","115it [06:54,  1.71s/it]\u001b[A\n","116it [06:54,  1.27s/it]\u001b[A\n","117it [06:54,  1.04it/s]\u001b[A\n","118it [06:54,  1.35it/s]\u001b[A\n","119it [06:54,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:37<00:00,  1.83s/it]\n","03/08/2022 06:09:20 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:09:34 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:09,  4.56s/it]\u001b[A\n","122it [07:09,  3.26s/it]\u001b[A\n","123it [07:09,  2.35s/it]\u001b[A\n","124it [07:09,  1.71s/it]\u001b[A\n","125it [07:10,  1.27s/it]\u001b[A\n","126it [07:10,  1.04it/s]\u001b[A\n","127it [07:10,  1.35it/s]\u001b[A\n","128it [07:10,  1.70it/s]\u001b[A\n","129it [07:11,  2.08it/s]\u001b[A\n","130it [07:11,  2.46it/s]\u001b[A\n","131it [07:11,  2.83it/s]\u001b[A\n","132it [07:11,  3.50it/s]\u001b[A03/08/2022 06:09:37 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:09:37 - INFO - __main__ -     eval_loss = 0.8776593208312988\n","03/08/2022 06:09:37 - INFO - __main__ -     eval_auroc = 0.9537500143051147\n","03/08/2022 06:09:37 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/08/2022 06:09:37 - INFO - __main__ -     eval_f1 = 0.7457627058029175\n","03/08/2022 06:09:37 - INFO - filelock -   Lock 139628565614544 acquired on log.lock\n","03/08/2022 06:09:37 - INFO - filelock -   Lock 139628565614544 released on log.lock\n","132it [07:11,  3.27s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:09:43 - INFO - __main__ -   Specify load the 80-th prompt: *cls*‚ñÅ>*mask*.*+sent_0**sep+* | {0: \"yeah\", 1: \"wow\"}\n","03/08/2022 06:09:43 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:09:43 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-14167', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-09-43_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-14167', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:09:43 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:09:44 - INFO - src.dataset -   Label 0 to word ƒ†yeah (11380)\n","03/08/2022 06:09:44 - INFO - src.dataset -   Label 1 to word ƒ†wow (26388)\n","03/08/2022 06:09:44 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:09:44 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:09:44 - INFO - filelock -   Lock 140274189581200 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:09:44 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 06:09:44 - INFO - filelock -   Lock 140274189581200 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:09:44 - INFO - src.dataset -   Label 0 to word ƒ†yeah (11380)\n","03/08/2022 06:09:44 - INFO - src.dataset -   Label 1 to word ƒ†wow (26388)\n","03/08/2022 06:09:44 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:09:44 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:09:44 - INFO - filelock -   Lock 140273901432656 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:09:44 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:09:44 - INFO - filelock -   Lock 140273901432656 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:09:44 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:09:44 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:09:44 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 15698, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:09:44 - INFO - src.dataset -   text: <s>‚ñÅ><mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:09:59 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:09:59 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:09:59 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:09:59 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:09:59 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:09:59 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:09:59 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 06:10:44 - INFO - src.trainer -   Best dev result: 0.9353750348091125\n","Epoch:  20% 49/250 [01:32<05:34,  1.67s/it]\n","13it [00:52, 15.14s/it]            \u001b[A\n","14it [00:52, 10.66s/it]\u001b[A\n","15it [00:52,  7.53s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.81s/it]\u001b[A\n","18it [00:53,  2.74s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/08/2022 06:11:36 - INFO - src.trainer -   Best dev result: 0.937749981880188\n","Epoch:  30% 74/250 [02:23<04:53,  1.67s/it]\n","25it [01:43, 15.05s/it]\u001b[A\n","26it [01:44, 10.60s/it]\u001b[A\n","27it [01:44,  7.49s/it]\u001b[A\n","28it [01:44,  5.31s/it]\u001b[A\n","29it [01:44,  3.79s/it]\u001b[A\n","30it [01:45,  2.72s/it]\u001b[A\n","31it [01:45,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:46,  1.54it/s]\u001b[A\n","36it [01:46,  2.03it/s]\u001b[A03/08/2022 06:12:28 - INFO - src.trainer -   Best dev result: 0.9387499690055847\n","Epoch:  40% 99/250 [03:16<04:11,  1.66s/it]\n","37it [02:36, 15.28s/it]\u001b[A\n","38it [02:36, 10.76s/it]\u001b[A\n","39it [02:36,  7.60s/it]\u001b[A\n","40it [02:36,  5.39s/it]\u001b[A\n","41it [02:37,  3.84s/it]\u001b[A\n","42it [02:37,  2.76s/it]\u001b[A\n","43it [02:37,  2.00s/it]\u001b[A\n","44it [02:37,  1.47s/it]\u001b[A\n","45it [02:37,  1.10s/it]\u001b[A\n","46it [02:38,  1.19it/s]\u001b[A\n","47it [02:38,  1.52it/s]\u001b[A\n","48it [02:38,  2.01it/s]\u001b[A03/08/2022 06:13:20 - INFO - src.trainer -   Best dev result: 0.9444999694824219\n","Epoch:  50% 124/250 [04:08<03:30,  1.67s/it]\n","49it [03:28, 15.23s/it]\u001b[A\n","50it [03:28, 10.73s/it]\u001b[A\n","51it [03:28,  7.58s/it]\u001b[A\n","52it [03:28,  5.38s/it]\u001b[A\n","53it [03:29,  3.83s/it]\u001b[A\n","54it [03:29,  2.75s/it]\u001b[A\n","55it [03:29,  2.00s/it]\u001b[A\n","56it [03:29,  1.47s/it]\u001b[A\n","57it [03:29,  1.10s/it]\u001b[A\n","58it [03:30,  1.20it/s]\u001b[A\n","59it [03:30,  1.53it/s]\u001b[A\n","Epoch:  60% 149/250 [04:52<02:47,  1.66s/it]\n","61it [04:12, 12.88s/it]\u001b[A\n","62it [04:12,  9.09s/it]\u001b[A\n","63it [04:12,  6.43s/it]\u001b[A\n","64it [04:13,  4.57s/it]\u001b[A\n","65it [04:13,  3.27s/it]\u001b[A\n","66it [04:13,  2.36s/it]\u001b[A\n","67it [04:13,  1.72s/it]\u001b[A\n","68it [04:13,  1.27s/it]\u001b[A\n","69it [04:14,  1.04it/s]\u001b[A\n","70it [04:14,  1.35it/s]\u001b[A\n","71it [04:14,  1.70it/s]\u001b[A\n","72it [04:14,  2.22it/s]\u001b[A03/08/2022 06:14:56 - INFO - src.trainer -   Best dev result: 0.9448750019073486\n","Epoch:  70% 174/250 [05:44<02:06,  1.66s/it]\n","73it [05:04, 15.33s/it]\u001b[A\n","74it [05:05, 10.80s/it]\u001b[A\n","75it [05:05,  7.63s/it]\u001b[A\n","76it [05:05,  5.41s/it]\u001b[A\n","77it [05:05,  3.86s/it]\u001b[A\n","78it [05:05,  2.77s/it]\u001b[A\n","79it [05:06,  2.01s/it]\u001b[A\n","80it [05:06,  1.47s/it]\u001b[A\n","81it [05:06,  1.10s/it]\u001b[A\n","82it [05:06,  1.19it/s]\u001b[A\n","83it [05:07,  1.52it/s]\u001b[A\n","84it [05:07,  2.01it/s]\u001b[A03/08/2022 06:15:49 - INFO - src.trainer -   Best dev result: 0.9449999928474426\n","Epoch:  80% 199/250 [06:36<01:25,  1.67s/it]\n","85it [05:56, 15.25s/it]\u001b[A\n","86it [05:57, 10.74s/it]\u001b[A\n","87it [05:57,  7.59s/it]\u001b[A\n","88it [05:57,  5.38s/it]\u001b[A\n","89it [05:57,  3.84s/it]\u001b[A\n","90it [05:58,  2.76s/it]\u001b[A\n","91it [05:58,  2.00s/it]\u001b[A\n","92it [05:58,  1.47s/it]\u001b[A\n","93it [05:58,  1.10s/it]\u001b[A\n","94it [05:59,  1.19it/s]\u001b[A\n","95it [05:59,  1.53it/s]\u001b[A\n","Epoch:  90% 224/250 [07:21<00:43,  1.66s/it]\n","97it [06:41, 12.87s/it]\u001b[A\n","98it [06:41,  9.08s/it]\u001b[A\n","99it [06:41,  6.42s/it]\u001b[A\n","100it [06:41,  4.57s/it]\u001b[A\n","101it [06:42,  3.27s/it]\u001b[A\n","102it [06:42,  2.36s/it]\u001b[A\n","103it [06:42,  1.72s/it]\u001b[A\n","104it [06:42,  1.27s/it]\u001b[A\n","105it [06:42,  1.04it/s]\u001b[A\n","106it [06:43,  1.35it/s]\u001b[A\n","107it [06:43,  1.70it/s]\u001b[A\n","108it [06:43,  2.22it/s]\u001b[A03/08/2022 06:17:25 - INFO - src.trainer -   Best dev result: 0.9472500085830688\n","Epoch: 100% 249/250 [08:13<00:01,  1.67s/it]\n","109it [07:33, 15.23s/it]\u001b[A\n","110it [07:33, 10.73s/it]\u001b[A\n","111it [07:33,  7.58s/it]\u001b[A\n","112it [07:33,  5.38s/it]\u001b[A\n","113it [07:34,  3.83s/it]\u001b[A\n","114it [07:34,  2.75s/it]\u001b[A\n","115it [07:34,  2.00s/it]\u001b[A\n","116it [07:34,  1.47s/it]\u001b[A\n","117it [07:35,  1.10s/it]\u001b[A\n","118it [07:35,  1.20it/s]\u001b[A\n","119it [07:35,  1.53it/s]\u001b[A\n","Epoch: 100% 250/250 [08:17<00:00,  1.99s/it]\n","03/08/2022 06:18:17 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:18:31 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:49,  4.60s/it]\u001b[A\n","122it [07:50,  3.29s/it]\u001b[A\n","123it [07:50,  2.37s/it]\u001b[A\n","124it [07:50,  1.73s/it]\u001b[A\n","125it [07:50,  1.28s/it]\u001b[A\n","126it [07:51,  1.04it/s]\u001b[A\n","127it [07:51,  1.34it/s]\u001b[A\n","128it [07:51,  1.69it/s]\u001b[A\n","129it [07:51,  2.07it/s]\u001b[A\n","130it [07:51,  2.45it/s]\u001b[A\n","131it [07:52,  2.82it/s]\u001b[A\n","132it [07:52,  3.50it/s]\u001b[A03/08/2022 06:18:34 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:18:34 - INFO - __main__ -     eval_loss = 1.7733235359191895\n","03/08/2022 06:18:34 - INFO - __main__ -     eval_auroc = 0.9472500085830688\n","03/08/2022 06:18:34 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 06:18:34 - INFO - __main__ -     eval_f1 = 0.5999999642372131\n","03/08/2022 06:18:34 - INFO - filelock -   Lock 140273914162640 acquired on log.lock\n","03/08/2022 06:18:34 - INFO - filelock -   Lock 140273914162640 released on log.lock\n","132it [07:52,  3.58s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:18:40 - INFO - __main__ -   Specify load the 81-th prompt: *cls*‚ñÅ>*mask*.*+sent_0**sep+* | {0: \"X\", 1: \"Oops\"}\n","03/08/2022 06:18:40 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:18:40 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-1179', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-18-40_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-1179', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:18:40 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:18:41 - INFO - src.dataset -   Label 0 to word ƒ†X (1577)\n","03/08/2022 06:18:41 - INFO - src.dataset -   Label 1 to word ƒ†Oops (44007)\n","03/08/2022 06:18:41 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:18:41 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:18:41 - INFO - filelock -   Lock 139726880442384 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:18:41 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 06:18:41 - INFO - filelock -   Lock 139726880442384 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:18:41 - INFO - src.dataset -   Label 0 to word ƒ†X (1577)\n","03/08/2022 06:18:41 - INFO - src.dataset -   Label 1 to word ƒ†Oops (44007)\n","03/08/2022 06:18:41 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:18:41 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:18:41 - INFO - filelock -   Lock 139726884424848 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:18:41 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:18:41 - INFO - filelock -   Lock 139726884424848 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:18:41 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:18:41 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:18:41 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 15698, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:18:41 - INFO - src.dataset -   text: <s>‚ñÅ><mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:18:57 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:18:57 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:18:57 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:18:57 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:18:57 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:18:57 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:18:57 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.71it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.58it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 06:19:41 - INFO - src.trainer -   Best dev result: 0.8892499804496765\n","Epoch:  20% 49/250 [01:33<05:37,  1.68s/it]\n","13it [00:52, 15.27s/it]            \u001b[A\n","14it [00:53, 10.76s/it]\u001b[A\n","15it [00:53,  7.60s/it]\u001b[A\n","16it [00:53,  5.39s/it]\u001b[A\n","17it [00:53,  3.84s/it]\u001b[A\n","18it [00:54,  2.76s/it]\u001b[A\n","19it [00:54,  2.00s/it]\u001b[A\n","20it [00:54,  1.47s/it]\u001b[A\n","21it [00:54,  1.10s/it]\u001b[A\n","22it [00:54,  1.19it/s]\u001b[A\n","23it [00:55,  1.52it/s]\u001b[A\n","24it [00:55,  2.01it/s]\u001b[A03/08/2022 06:20:34 - INFO - src.trainer -   Best dev result: 0.893500030040741\n","Epoch:  30% 74/250 [02:24<04:54,  1.67s/it]\n","25it [01:44, 15.03s/it]\u001b[A\n","26it [01:44, 10.59s/it]\u001b[A\n","27it [01:44,  7.48s/it]\u001b[A\n","28it [01:44,  5.31s/it]\u001b[A\n","29it [01:45,  3.78s/it]\u001b[A\n","30it [01:45,  2.72s/it]\u001b[A\n","31it [01:45,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:46,  1.08s/it]\u001b[A\n","34it [01:46,  1.21it/s]\u001b[A\n","35it [01:46,  1.54it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:10,  1.66s/it]\n","37it [02:28, 12.87s/it]\u001b[A\n","38it [02:28,  9.08s/it]\u001b[A\n","39it [02:28,  6.43s/it]\u001b[A\n","40it [02:29,  4.57s/it]\u001b[A\n","41it [02:29,  3.27s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:30,  1.27s/it]\u001b[A\n","45it [02:30,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","48it [02:30,  2.23it/s]\u001b[A03/08/2022 06:22:10 - INFO - src.trainer -   Best dev result: 0.9008750319480896\n","Epoch:  50% 124/250 [04:00<03:30,  1.67s/it]\n","49it [03:20, 15.18s/it]\u001b[A\n","50it [03:20, 10.70s/it]\u001b[A\n","51it [03:20,  7.56s/it]\u001b[A\n","52it [03:21,  5.36s/it]\u001b[A\n","53it [03:21,  3.82s/it]\u001b[A\n","54it [03:21,  2.74s/it]\u001b[A\n","55it [03:21,  1.99s/it]\u001b[A\n","56it [03:22,  1.46s/it]\u001b[A\n","57it [03:22,  1.09s/it]\u001b[A\n","58it [03:22,  1.20it/s]\u001b[A\n","59it [03:22,  1.53it/s]\u001b[A\n","60it [03:22,  2.02it/s]\u001b[A03/08/2022 06:23:02 - INFO - src.trainer -   Best dev result: 0.9011250138282776\n","Epoch:  60% 149/250 [04:52<02:48,  1.67s/it]\n","61it [04:12, 15.29s/it]\u001b[A\n","62it [04:12, 10.77s/it]\u001b[A\n","63it [04:13,  7.61s/it]\u001b[A\n","64it [04:13,  5.40s/it]\u001b[A\n","65it [04:13,  3.85s/it]\u001b[A\n","66it [04:13,  2.76s/it]\u001b[A\n","67it [04:14,  2.00s/it]\u001b[A\n","68it [04:14,  1.47s/it]\u001b[A\n","69it [04:14,  1.10s/it]\u001b[A\n","70it [04:14,  1.19it/s]\u001b[A\n","71it [04:15,  1.52it/s]\u001b[A\n","Epoch:  70% 174/250 [05:37<02:06,  1.66s/it]\n","73it [04:56, 12.87s/it]\u001b[A\n","74it [04:57,  9.08s/it]\u001b[A\n","75it [04:57,  6.42s/it]\u001b[A\n","76it [04:57,  4.57s/it]\u001b[A\n","77it [04:57,  3.27s/it]\u001b[A\n","78it [04:58,  2.36s/it]\u001b[A\n","79it [04:58,  1.72s/it]\u001b[A\n","80it [04:58,  1.27s/it]\u001b[A\n","81it [04:58,  1.04it/s]\u001b[A\n","82it [04:58,  1.35it/s]\u001b[A\n","83it [04:59,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:21<01:24,  1.66s/it]\n","85it [05:41, 12.84s/it]\u001b[A\n","86it [05:41,  9.05s/it]\u001b[A\n","87it [05:41,  6.41s/it]\u001b[A\n","88it [05:41,  4.55s/it]\u001b[A\n","89it [05:41,  3.26s/it]\u001b[A\n","90it [05:42,  2.35s/it]\u001b[A\n","91it [05:42,  1.71s/it]\u001b[A\n","92it [05:42,  1.27s/it]\u001b[A\n","93it [05:42,  1.04it/s]\u001b[A\n","94it [05:43,  1.35it/s]\u001b[A\n","95it [05:43,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:05<00:43,  1.66s/it]\n","97it [06:25, 12.84s/it]\u001b[A\n","98it [06:25,  9.06s/it]\u001b[A\n","99it [06:25,  6.41s/it]\u001b[A\n","100it [06:25,  4.56s/it]\u001b[A\n","101it [06:26,  3.26s/it]\u001b[A\n","102it [06:26,  2.35s/it]\u001b[A\n","103it [06:26,  1.71s/it]\u001b[A\n","104it [06:26,  1.27s/it]\u001b[A\n","105it [06:27,  1.04it/s]\u001b[A\n","106it [06:27,  1.35it/s]\u001b[A\n","107it [06:27,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:49<00:01,  1.66s/it]\n","109it [07:09, 12.84s/it]\u001b[A\n","110it [07:09,  9.05s/it]\u001b[A\n","111it [07:09,  6.41s/it]\u001b[A\n","112it [07:10,  4.55s/it]\u001b[A\n","113it [07:10,  3.26s/it]\u001b[A\n","114it [07:10,  2.35s/it]\u001b[A\n","115it [07:10,  1.71s/it]\u001b[A\n","116it [07:11,  1.27s/it]\u001b[A\n","117it [07:11,  1.04it/s]\u001b[A\n","118it [07:11,  1.35it/s]\u001b[A\n","119it [07:11,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:53<00:00,  1.90s/it]\n","03/08/2022 06:26:51 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:27:05 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:26,  4.59s/it]\u001b[A\n","122it [07:26,  3.28s/it]\u001b[A\n","123it [07:26,  2.37s/it]\u001b[A\n","124it [07:26,  1.73s/it]\u001b[A\n","125it [07:27,  1.28s/it]\u001b[A\n","126it [07:27,  1.04it/s]\u001b[A\n","127it [07:27,  1.34it/s]\u001b[A\n","128it [07:27,  1.69it/s]\u001b[A\n","129it [07:27,  2.07it/s]\u001b[A\n","130it [07:28,  2.45it/s]\u001b[A\n","131it [07:28,  2.82it/s]\u001b[A\n","132it [07:28,  3.50it/s]\u001b[A03/08/2022 06:27:07 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:27:07 - INFO - __main__ -     eval_loss = 2.772655487060547\n","03/08/2022 06:27:07 - INFO - __main__ -     eval_auroc = 0.9011250138282776\n","03/08/2022 06:27:07 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 06:27:07 - INFO - __main__ -     eval_f1 = 0.4897959232330322\n","03/08/2022 06:27:08 - INFO - filelock -   Lock 139726941507920 acquired on log.lock\n","03/08/2022 06:27:08 - INFO - filelock -   Lock 139726941507920 released on log.lock\n","132it [07:28,  3.40s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:27:13 - INFO - __main__ -   Specify load the 82-th prompt: *cls*‚ñÅ>*mask*.*+sent_0**sep+* | {0: \"U\", 1: \"Indeed\"}\n","03/08/2022 06:27:13 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:27:13 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-18016', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-27-13_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-18016', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:27:13 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:27:15 - INFO - src.dataset -   Label 0 to word ƒ†U (121)\n","03/08/2022 06:27:15 - INFO - src.dataset -   Label 1 to word ƒ†Indeed (7908)\n","03/08/2022 06:27:15 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:27:15 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:27:15 - INFO - filelock -   Lock 140343389837456 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:27:15 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 06:27:15 - INFO - filelock -   Lock 140343389837456 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:27:15 - INFO - src.dataset -   Label 0 to word ƒ†U (121)\n","03/08/2022 06:27:15 - INFO - src.dataset -   Label 1 to word ƒ†Indeed (7908)\n","03/08/2022 06:27:15 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:27:15 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:27:15 - INFO - filelock -   Lock 140343389750224 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:27:15 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:27:15 - INFO - filelock -   Lock 140343389750224 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:27:15 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:27:15 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:27:15 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 15698, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:27:15 - INFO - src.dataset -   text: <s>‚ñÅ><mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:27:30 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:27:30 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:27:30 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:27:30 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:27:30 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:27:30 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:27:30 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.71it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.71it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.58it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 06:28:15 - INFO - src.trainer -   Best dev result: 0.8996250033378601\n","Epoch:  20% 49/250 [01:32<05:36,  1.67s/it]\n","13it [00:51, 14.98s/it]            \u001b[A\n","14it [00:52, 10.55s/it]\u001b[A\n","15it [00:52,  7.46s/it]\u001b[A\n","16it [00:52,  5.29s/it]\u001b[A\n","17it [00:52,  3.77s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.04it/s]\u001b[A03/08/2022 06:29:06 - INFO - src.trainer -   Best dev result: 0.9086250066757202\n","Epoch:  30% 74/250 [02:24<04:53,  1.67s/it]\n","25it [01:44, 15.25s/it]\u001b[A\n","26it [01:44, 10.75s/it]\u001b[A\n","27it [01:44,  7.59s/it]\u001b[A\n","28it [01:44,  5.38s/it]\u001b[A\n","29it [01:44,  3.84s/it]\u001b[A\n","30it [01:45,  2.76s/it]\u001b[A\n","31it [01:45,  2.00s/it]\u001b[A\n","32it [01:45,  1.47s/it]\u001b[A\n","33it [01:45,  1.10s/it]\u001b[A\n","34it [01:46,  1.19it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","36it [01:46,  2.02it/s]\u001b[A03/08/2022 06:29:59 - INFO - src.trainer -   Best dev result: 0.9106249213218689\n","Epoch:  40% 99/250 [03:15<04:12,  1.67s/it]\n","37it [02:35, 15.14s/it]\u001b[A\n","38it [02:36, 10.67s/it]\u001b[A\n","39it [02:36,  7.54s/it]\u001b[A\n","40it [02:36,  5.35s/it]\u001b[A\n","41it [02:36,  3.81s/it]\u001b[A\n","42it [02:36,  2.74s/it]\u001b[A\n","43it [02:37,  1.99s/it]\u001b[A\n","44it [02:37,  1.46s/it]\u001b[A\n","45it [02:37,  1.09s/it]\u001b[A\n","46it [02:37,  1.20it/s]\u001b[A\n","47it [02:38,  1.53it/s]\u001b[A\n","Epoch:  50% 124/250 [04:00<03:28,  1.66s/it]\n","49it [03:19, 12.84s/it]\u001b[A\n","50it [03:20,  9.06s/it]\u001b[A\n","51it [03:20,  6.41s/it]\u001b[A\n","52it [03:20,  4.56s/it]\u001b[A\n","53it [03:20,  3.26s/it]\u001b[A\n","54it [03:21,  2.35s/it]\u001b[A\n","55it [03:21,  1.71s/it]\u001b[A\n","56it [03:21,  1.27s/it]\u001b[A\n","57it [03:21,  1.04it/s]\u001b[A\n","58it [03:21,  1.35it/s]\u001b[A\n","59it [03:22,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:44<02:47,  1.66s/it]\n","61it [04:03, 12.80s/it]\u001b[A\n","62it [04:04,  9.03s/it]\u001b[A\n","63it [04:04,  6.39s/it]\u001b[A\n","64it [04:04,  4.54s/it]\u001b[A\n","65it [04:04,  3.25s/it]\u001b[A\n","66it [04:05,  2.34s/it]\u001b[A\n","67it [04:05,  1.71s/it]\u001b[A\n","68it [04:05,  1.27s/it]\u001b[A\n","69it [04:05,  1.05it/s]\u001b[A\n","70it [04:06,  1.35it/s]\u001b[A\n","71it [04:06,  1.71it/s]\u001b[A\n","Epoch:  70% 174/250 [05:28<02:06,  1.66s/it]\n","73it [04:48, 12.81s/it]\u001b[A\n","74it [04:48,  9.04s/it]\u001b[A\n","75it [04:48,  6.39s/it]\u001b[A\n","76it [04:48,  4.55s/it]\u001b[A\n","77it [04:48,  3.25s/it]\u001b[A\n","78it [04:49,  2.35s/it]\u001b[A\n","79it [04:49,  1.71s/it]\u001b[A\n","80it [04:49,  1.27s/it]\u001b[A\n","81it [04:49,  1.05it/s]\u001b[A\n","82it [04:50,  1.35it/s]\u001b[A\n","83it [04:50,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:12<01:24,  1.66s/it]\n","85it [05:32, 12.81s/it]\u001b[A\n","86it [05:32,  9.03s/it]\u001b[A\n","87it [05:32,  6.39s/it]\u001b[A\n","88it [05:32,  4.54s/it]\u001b[A\n","89it [05:33,  3.25s/it]\u001b[A\n","90it [05:33,  2.34s/it]\u001b[A\n","91it [05:33,  1.71s/it]\u001b[A\n","92it [05:33,  1.27s/it]\u001b[A\n","93it [05:33,  1.05it/s]\u001b[A\n","94it [05:34,  1.35it/s]\u001b[A\n","95it [05:34,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [06:56<00:43,  1.66s/it]\n","97it [06:16, 12.83s/it]\u001b[A\n","98it [06:16,  9.05s/it]\u001b[A\n","99it [06:16,  6.41s/it]\u001b[A\n","100it [06:16,  4.55s/it]\u001b[A\n","101it [06:17,  3.26s/it]\u001b[A\n","102it [06:17,  2.35s/it]\u001b[A\n","103it [06:17,  1.71s/it]\u001b[A\n","104it [06:17,  1.27s/it]\u001b[A\n","105it [06:18,  1.04it/s]\u001b[A\n","106it [06:18,  1.35it/s]\u001b[A\n","107it [06:18,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:40<00:01,  1.66s/it]\n","109it [07:00, 12.83s/it]\u001b[A\n","110it [07:00,  9.05s/it]\u001b[A\n","111it [07:00,  6.41s/it]\u001b[A\n","112it [07:01,  4.55s/it]\u001b[A\n","113it [07:01,  3.26s/it]\u001b[A\n","114it [07:01,  2.35s/it]\u001b[A\n","115it [07:01,  1.71s/it]\u001b[A\n","116it [07:02,  1.27s/it]\u001b[A\n","117it [07:02,  1.04it/s]\u001b[A\n","118it [07:02,  1.35it/s]\u001b[A\n","119it [07:02,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:44<00:00,  1.86s/it]\n","03/08/2022 06:35:15 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:35:29 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:17,  4.62s/it]\u001b[A\n","122it [07:17,  3.30s/it]\u001b[A\n","123it [07:17,  2.38s/it]\u001b[A\n","124it [07:17,  1.74s/it]\u001b[A\n","125it [07:18,  1.29s/it]\u001b[A\n","126it [07:18,  1.03it/s]\u001b[A\n","127it [07:18,  1.34it/s]\u001b[A\n","128it [07:18,  1.69it/s]\u001b[A\n","129it [07:19,  2.06it/s]\u001b[A\n","130it [07:19,  2.45it/s]\u001b[A\n","131it [07:19,  2.81it/s]\u001b[A\n","132it [07:19,  3.49it/s]\u001b[A03/08/2022 06:35:32 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:35:32 - INFO - __main__ -     eval_loss = 1.949631929397583\n","03/08/2022 06:35:32 - INFO - __main__ -     eval_auroc = 0.9106249213218689\n","03/08/2022 06:35:32 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 06:35:32 - INFO - __main__ -     eval_f1 = 0.5274725556373596\n","03/08/2022 06:35:32 - INFO - filelock -   Lock 140343767938192 acquired on log.lock\n","03/08/2022 06:35:32 - INFO - filelock -   Lock 140343767938192 released on log.lock\n","132it [07:19,  3.33s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:35:38 - INFO - __main__ -   Specify load the 83-th prompt: *cls*‚ñÅ>*mask*.*+sent_0**sep+* | {0: \"...\", 1: \"Oops\"}\n","03/08/2022 06:35:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:35:38 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-19511', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-35-38_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-19511', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:35:38 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:35:39 - INFO - src.dataset -   Label 0 to word ... (734)\n","03/08/2022 06:35:39 - INFO - src.dataset -   Label 1 to word ƒ†Oops (44007)\n","03/08/2022 06:35:39 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:35:39 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:35:39 - INFO - filelock -   Lock 140315071707280 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:35:39 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 06:35:39 - INFO - filelock -   Lock 140315071707280 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:35:39 - INFO - src.dataset -   Label 0 to word ... (734)\n","03/08/2022 06:35:39 - INFO - src.dataset -   Label 1 to word ƒ†Oops (44007)\n","03/08/2022 06:35:39 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:35:39 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:35:39 - INFO - filelock -   Lock 140315057500624 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:35:39 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:35:39 - INFO - filelock -   Lock 140315057500624 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:35:39 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:35:39 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:35:39 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 15698, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:35:39 - INFO - src.dataset -   text: <s>‚ñÅ><mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:35:54 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:35:54 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:35:54 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:35:54 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:35:54 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:35:54 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:35:54 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.73it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 06:36:39 - INFO - src.trainer -   Best dev result: 0.9127500057220459\n","Epoch:  20% 49/250 [01:31<05:41,  1.70s/it]\n","13it [00:51, 14.90s/it]            \u001b[A\n","14it [00:51, 10.50s/it]\u001b[A\n","15it [00:52,  7.42s/it]\u001b[A\n","16it [00:52,  5.26s/it]\u001b[A\n","17it [00:52,  3.75s/it]\u001b[A\n","18it [00:52,  2.70s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.22it/s]\u001b[A\n","23it [00:53,  1.55it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:51,  1.66s/it]\n","25it [01:35, 12.86s/it]\u001b[A\n","26it [01:36,  9.07s/it]\u001b[A\n","27it [01:36,  6.42s/it]\u001b[A\n","28it [01:36,  4.56s/it]\u001b[A\n","29it [01:36,  3.26s/it]\u001b[A\n","30it [01:36,  2.35s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.04it/s]\u001b[A\n","34it [01:37,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:19, 12.84s/it]\u001b[A\n","38it [02:20,  9.05s/it]\u001b[A\n","39it [02:20,  6.41s/it]\u001b[A\n","40it [02:20,  4.55s/it]\u001b[A\n","41it [02:20,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:21,  1.71s/it]\u001b[A\n","44it [02:21,  1.27s/it]\u001b[A\n","45it [02:21,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:29,  1.66s/it]\n","49it [03:04, 12.84s/it]\u001b[A\n","50it [03:04,  9.06s/it]\u001b[A\n","51it [03:04,  6.41s/it]\u001b[A\n","52it [03:04,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:05,  1.71s/it]\u001b[A\n","56it [03:05,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:28<02:47,  1.66s/it]\n","61it [03:48, 12.84s/it]\u001b[A\n","62it [03:48,  9.06s/it]\u001b[A\n","63it [03:48,  6.41s/it]\u001b[A\n","64it [03:49,  4.56s/it]\u001b[A\n","65it [03:49,  3.26s/it]\u001b[A\n","66it [03:49,  2.35s/it]\u001b[A\n","67it [03:49,  1.71s/it]\u001b[A\n","68it [03:49,  1.27s/it]\u001b[A\n","69it [03:50,  1.04it/s]\u001b[A\n","70it [03:50,  1.35it/s]\u001b[A\n","71it [03:50,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:12<02:06,  1.66s/it]\n","73it [04:32, 12.85s/it]\u001b[A\n","74it [04:32,  9.07s/it]\u001b[A\n","75it [04:33,  6.41s/it]\u001b[A\n","76it [04:33,  4.56s/it]\u001b[A\n","77it [04:33,  3.26s/it]\u001b[A\n","78it [04:33,  2.35s/it]\u001b[A\n","79it [04:33,  1.72s/it]\u001b[A\n","80it [04:34,  1.27s/it]\u001b[A\n","81it [04:34,  1.04it/s]\u001b[A\n","82it [04:34,  1.35it/s]\u001b[A\n","83it [04:34,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:57<01:24,  1.66s/it]\n","85it [05:16, 12.84s/it]\u001b[A\n","86it [05:17,  9.06s/it]\u001b[A\n","87it [05:17,  6.41s/it]\u001b[A\n","88it [05:17,  4.56s/it]\u001b[A\n","89it [05:17,  3.26s/it]\u001b[A\n","90it [05:17,  2.35s/it]\u001b[A\n","91it [05:18,  1.72s/it]\u001b[A\n","92it [05:18,  1.27s/it]\u001b[A\n","93it [05:18,  1.04it/s]\u001b[A\n","94it [05:18,  1.35it/s]\u001b[A\n","95it [05:19,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:41<00:43,  1.66s/it]\n","97it [06:01, 12.85s/it]\u001b[A\n","98it [06:01,  9.06s/it]\u001b[A\n","99it [06:01,  6.41s/it]\u001b[A\n","100it [06:01,  4.56s/it]\u001b[A\n","101it [06:01,  3.26s/it]\u001b[A\n","102it [06:02,  2.35s/it]\u001b[A\n","103it [06:02,  1.72s/it]\u001b[A\n","104it [06:02,  1.27s/it]\u001b[A\n","105it [06:02,  1.04it/s]\u001b[A\n","106it [06:03,  1.35it/s]\u001b[A\n","107it [06:03,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:25<00:01,  1.66s/it]\n","109it [06:45, 12.84s/it]\u001b[A\n","110it [06:45,  9.06s/it]\u001b[A\n","111it [06:45,  6.41s/it]\u001b[A\n","112it [06:45,  4.56s/it]\u001b[A\n","113it [06:46,  3.26s/it]\u001b[A\n","114it [06:46,  2.35s/it]\u001b[A\n","115it [06:46,  1.71s/it]\u001b[A\n","116it [06:46,  1.27s/it]\u001b[A\n","117it [06:47,  1.04it/s]\u001b[A\n","118it [06:47,  1.35it/s]\u001b[A\n","119it [06:47,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:29<00:00,  1.80s/it]\n","03/08/2022 06:43:24 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:43:37 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:00,  4.29s/it]\u001b[A\n","122it [07:01,  3.07s/it]\u001b[A\n","123it [07:01,  2.22s/it]\u001b[A\n","124it [07:01,  1.62s/it]\u001b[A\n","125it [07:01,  1.21s/it]\u001b[A\n","126it [07:02,  1.09it/s]\u001b[A\n","127it [07:02,  1.41it/s]\u001b[A\n","128it [07:02,  1.77it/s]\u001b[A\n","129it [07:02,  2.15it/s]\u001b[A\n","130it [07:02,  2.53it/s]\u001b[A\n","131it [07:03,  2.89it/s]\u001b[A\n","132it [07:03,  3.57it/s]\u001b[A03/08/2022 06:43:40 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:43:40 - INFO - __main__ -     eval_loss = 1.601298451423645\n","03/08/2022 06:43:40 - INFO - __main__ -     eval_auroc = 0.9127500057220459\n","03/08/2022 06:43:40 - INFO - __main__ -     eval_recall = 0.8399999737739563\n","03/08/2022 06:43:40 - INFO - __main__ -     eval_f1 = 0.5600000023841858\n","03/08/2022 06:43:40 - INFO - filelock -   Lock 140315070574288 acquired on log.lock\n","03/08/2022 06:43:40 - INFO - filelock -   Lock 140315070574288 released on log.lock\n","132it [07:03,  3.21s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:43:46 - INFO - __main__ -   Specify load the 84-th prompt: *cls*‚ñÅ>*mask*.*+sent_0**sep+* | {0: \"M\", 1: \"vs\"}\n","03/08/2022 06:43:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:43:46 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-11278', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-43-46_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-11278', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:43:46 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:43:47 - INFO - src.dataset -   Label 0 to word ƒ†M (256)\n","03/08/2022 06:43:47 - INFO - src.dataset -   Label 1 to word ƒ†vs (1954)\n","03/08/2022 06:43:47 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:43:47 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:43:47 - INFO - filelock -   Lock 140344354618064 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:43:47 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 06:43:47 - INFO - filelock -   Lock 140344354618064 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:43:47 - INFO - src.dataset -   Label 0 to word ƒ†M (256)\n","03/08/2022 06:43:47 - INFO - src.dataset -   Label 1 to word ƒ†vs (1954)\n","03/08/2022 06:43:47 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:43:47 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:43:47 - INFO - filelock -   Lock 140344326900368 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:43:47 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:43:47 - INFO - filelock -   Lock 140344326900368 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:43:47 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:43:47 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:43:47 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 15698, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[4], label_word_list=None)\n","03/08/2022 06:43:47 - INFO - src.dataset -   text: <s>‚ñÅ><mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:44:02 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:44:02 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:44:02 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:44:02 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:44:02 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:44:02 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:44:02 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 06:44:47 - INFO - src.trainer -   Best dev result: 0.9200000166893005\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.14s/it]            \u001b[A\n","14it [00:52, 10.66s/it]\u001b[A\n","15it [00:52,  7.53s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.81s/it]\u001b[A\n","18it [00:53,  2.74s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.88s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:37,  6.43s/it]\u001b[A\n","28it [01:37,  4.57s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:38,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:01<04:10,  1.66s/it]\n","37it [02:20, 12.85s/it]\u001b[A\n","38it [02:21,  9.06s/it]\u001b[A\n","39it [02:21,  6.41s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:22,  2.35s/it]\u001b[A\n","43it [02:22,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:23,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:29,  1.66s/it]\n","49it [03:05, 12.85s/it]\u001b[A\n","50it [03:05,  9.06s/it]\u001b[A\n","51it [03:05,  6.41s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:06,  3.26s/it]\u001b[A\n","54it [03:06,  2.35s/it]\u001b[A\n","55it [03:06,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:07,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:29<02:47,  1.66s/it]\n","61it [03:49, 12.84s/it]\u001b[A\n","62it [03:49,  9.06s/it]\u001b[A\n","63it [03:49,  6.41s/it]\u001b[A\n","64it [03:49,  4.56s/it]\u001b[A\n","65it [03:50,  3.26s/it]\u001b[A\n","66it [03:50,  2.35s/it]\u001b[A\n","67it [03:50,  1.71s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:51,  1.04it/s]\u001b[A\n","70it [03:51,  1.35it/s]\u001b[A\n","71it [03:51,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:13<02:06,  1.66s/it]\n","73it [04:33, 12.85s/it]\u001b[A\n","74it [04:33,  9.06s/it]\u001b[A\n","75it [04:33,  6.41s/it]\u001b[A\n","76it [04:34,  4.56s/it]\u001b[A\n","77it [04:34,  3.26s/it]\u001b[A\n","78it [04:34,  2.35s/it]\u001b[A\n","79it [04:34,  1.72s/it]\u001b[A\n","80it [04:35,  1.27s/it]\u001b[A\n","81it [04:35,  1.04it/s]\u001b[A\n","82it [04:35,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:57<01:24,  1.66s/it]\n","85it [05:17, 12.83s/it]\u001b[A\n","86it [05:17,  9.05s/it]\u001b[A\n","87it [05:18,  6.40s/it]\u001b[A\n","88it [05:18,  4.55s/it]\u001b[A\n","89it [05:18,  3.26s/it]\u001b[A\n","90it [05:18,  2.35s/it]\u001b[A\n","91it [05:19,  1.71s/it]\u001b[A\n","92it [05:19,  1.27s/it]\u001b[A\n","93it [05:19,  1.04it/s]\u001b[A\n","94it [05:19,  1.35it/s]\u001b[A\n","95it [05:19,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:41<00:43,  1.66s/it]\n","97it [06:01, 12.81s/it]\u001b[A\n","98it [06:01,  9.04s/it]\u001b[A\n","99it [06:02,  6.39s/it]\u001b[A\n","100it [06:02,  4.55s/it]\u001b[A\n","101it [06:02,  3.25s/it]\u001b[A\n","102it [06:02,  2.34s/it]\u001b[A\n","103it [06:03,  1.71s/it]\u001b[A\n","104it [06:03,  1.27s/it]\u001b[A\n","105it [06:03,  1.05it/s]\u001b[A\n","106it [06:03,  1.35it/s]\u001b[A\n","107it [06:04,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:26<00:01,  1.66s/it]\n","109it [06:45, 12.82s/it]\u001b[A\n","110it [06:46,  9.04s/it]\u001b[A\n","111it [06:46,  6.40s/it]\u001b[A\n","112it [06:46,  4.55s/it]\u001b[A\n","113it [06:46,  3.25s/it]\u001b[A\n","114it [06:47,  2.35s/it]\u001b[A\n","115it [06:47,  1.71s/it]\u001b[A\n","116it [06:47,  1.27s/it]\u001b[A\n","117it [06:47,  1.05it/s]\u001b[A\n","118it [06:47,  1.35it/s]\u001b[A\n","119it [06:48,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:30<00:00,  1.80s/it]\n","03/08/2022 06:51:32 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 06:51:45 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:01,  4.31s/it]\u001b[A\n","122it [07:01,  3.08s/it]\u001b[A\n","123it [07:02,  2.23s/it]\u001b[A\n","124it [07:02,  1.63s/it]\u001b[A\n","125it [07:02,  1.21s/it]\u001b[A\n","126it [07:02,  1.09it/s]\u001b[A\n","127it [07:02,  1.41it/s]\u001b[A\n","128it [07:03,  1.76it/s]\u001b[A\n","129it [07:03,  2.15it/s]\u001b[A\n","130it [07:03,  2.53it/s]\u001b[A\n","131it [07:03,  2.89it/s]\u001b[A\n","132it [07:04,  3.56it/s]\u001b[A03/08/2022 06:51:48 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 06:51:48 - INFO - __main__ -     eval_loss = 2.6093661785125732\n","03/08/2022 06:51:48 - INFO - __main__ -     eval_auroc = 0.9200000166893005\n","03/08/2022 06:51:48 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 06:51:48 - INFO - __main__ -     eval_f1 = 0.555555522441864\n","03/08/2022 06:51:48 - INFO - filelock -   Lock 140344338222736 acquired on log.lock\n","03/08/2022 06:51:48 - INFO - filelock -   Lock 140344338222736 released on log.lock\n","132it [07:04,  3.21s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 06:51:54 - INFO - __main__ -   Specify load the 85-th prompt: *cls*‚ñÅ4.*mask*.*+sent_0**sep+* | {0: \"All\", 1: \"Ah\"}\n","03/08/2022 06:51:54 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 06:51:54 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-28401', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_06-51-54_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-28401', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 06:51:54 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 06:51:55 - INFO - src.dataset -   Label 0 to word ƒ†All (404)\n","03/08/2022 06:51:55 - INFO - src.dataset -   Label 1 to word ƒ†Ah (7746)\n","03/08/2022 06:51:55 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 06:51:55 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:51:55 - INFO - filelock -   Lock 140409204248656 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:51:55 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 06:51:55 - INFO - filelock -   Lock 140409204248656 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:51:55 - INFO - src.dataset -   Label 0 to word ƒ†All (404)\n","03/08/2022 06:51:55 - INFO - src.dataset -   Label 1 to word ƒ†Ah (7746)\n","03/08/2022 06:51:55 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 06:51:55 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 06:51:55 - INFO - filelock -   Lock 140408877392144 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:51:55 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 06:51:55 - INFO - filelock -   Lock 140408877392144 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 06:51:55 - INFO - src.dataset -   *** Example ***\n","03/08/2022 06:51:55 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 06:51:55 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 306, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 06:51:55 - INFO - src.dataset -   text: <s>‚ñÅ4.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 06:52:11 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 06:52:11 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 06:52:11 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 06:52:11 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 06:52:11 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 06:52:11 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 06:52:11 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.64it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.90it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.71it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.58it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.44it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.40it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.08it/s]\u001b[A03/08/2022 06:52:55 - INFO - src.trainer -   Best dev result: 0.8520000576972961\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.06s/it]            \u001b[A\n","14it [00:52, 10.61s/it]\u001b[A\n","15it [00:52,  7.50s/it]\u001b[A\n","16it [00:52,  5.32s/it]\u001b[A\n","17it [00:53,  3.79s/it]\u001b[A\n","18it [00:53,  2.72s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 06:53:47 - INFO - src.trainer -   Best dev result: 0.9036250114440918\n","Epoch:  30% 74/250 [02:24<04:53,  1.67s/it]\n","25it [01:44, 15.25s/it]\u001b[A\n","26it [01:44, 10.75s/it]\u001b[A\n","27it [01:44,  7.59s/it]\u001b[A\n","28it [01:45,  5.39s/it]\u001b[A\n","29it [01:45,  3.84s/it]\u001b[A\n","30it [01:45,  2.76s/it]\u001b[A\n","31it [01:45,  2.00s/it]\u001b[A\n","32it [01:45,  1.47s/it]\u001b[A\n","33it [01:46,  1.10s/it]\u001b[A\n","34it [01:46,  1.19it/s]\u001b[A\n","35it [01:46,  1.52it/s]\u001b[A\n","36it [01:46,  2.01it/s]\u001b[A03/08/2022 06:54:39 - INFO - src.trainer -   Best dev result: 0.9076249599456787\n","Epoch:  40% 99/250 [03:16<04:12,  1.67s/it]\n","37it [02:36, 15.25s/it]\u001b[A\n","38it [02:36, 10.75s/it]\u001b[A\n","39it [02:36,  7.59s/it]\u001b[A\n","40it [02:37,  5.38s/it]\u001b[A\n","41it [02:37,  3.84s/it]\u001b[A\n","42it [02:37,  2.76s/it]\u001b[A\n","43it [02:37,  2.00s/it]\u001b[A\n","44it [02:38,  1.47s/it]\u001b[A\n","45it [02:38,  1.10s/it]\u001b[A\n","46it [02:38,  1.19it/s]\u001b[A\n","47it [02:38,  1.52it/s]\u001b[A\n","48it [02:38,  2.01it/s]\u001b[A03/08/2022 06:55:31 - INFO - src.trainer -   Best dev result: 0.909375011920929\n","Epoch:  50% 124/250 [04:08<03:33,  1.70s/it]\n","49it [03:28, 15.23s/it]\u001b[A\n","50it [03:28, 10.73s/it]\u001b[A\n","51it [03:28,  7.58s/it]\u001b[A\n","52it [03:29,  5.38s/it]\u001b[A\n","53it [03:29,  3.83s/it]\u001b[A\n","54it [03:29,  2.75s/it]\u001b[A\n","55it [03:29,  2.00s/it]\u001b[A\n","56it [03:30,  1.47s/it]\u001b[A\n","57it [03:30,  1.10s/it]\u001b[A\n","58it [03:30,  1.19it/s]\u001b[A\n","59it [03:30,  1.53it/s]\u001b[A\n","Epoch:  60% 149/250 [04:52<02:47,  1.66s/it]\n","61it [04:12, 12.89s/it]\u001b[A\n","62it [04:12,  9.09s/it]\u001b[A\n","63it [04:13,  6.43s/it]\u001b[A\n","64it [04:13,  4.57s/it]\u001b[A\n","65it [04:13,  3.27s/it]\u001b[A\n","66it [04:13,  2.36s/it]\u001b[A\n","67it [04:14,  1.72s/it]\u001b[A\n","68it [04:14,  1.27s/it]\u001b[A\n","69it [04:14,  1.04it/s]\u001b[A\n","70it [04:14,  1.35it/s]\u001b[A\n","71it [04:15,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:37<02:06,  1.66s/it]\n","73it [04:56, 12.84s/it]\u001b[A\n","74it [04:57,  9.06s/it]\u001b[A\n","75it [04:57,  6.41s/it]\u001b[A\n","76it [04:57,  4.56s/it]\u001b[A\n","77it [04:57,  3.26s/it]\u001b[A\n","78it [04:58,  2.35s/it]\u001b[A\n","79it [04:58,  1.72s/it]\u001b[A\n","80it [04:58,  1.27s/it]\u001b[A\n","81it [04:58,  1.04it/s]\u001b[A\n","82it [04:59,  1.35it/s]\u001b[A\n","83it [04:59,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:21<01:24,  1.66s/it]\n","85it [05:41, 12.85s/it]\u001b[A\n","86it [05:41,  9.06s/it]\u001b[A\n","87it [05:41,  6.41s/it]\u001b[A\n","88it [05:41,  4.56s/it]\u001b[A\n","89it [05:42,  3.26s/it]\u001b[A\n","90it [05:42,  2.35s/it]\u001b[A\n","91it [05:42,  1.72s/it]\u001b[A\n","92it [05:42,  1.27s/it]\u001b[A\n","93it [05:43,  1.04it/s]\u001b[A\n","94it [05:43,  1.35it/s]\u001b[A\n","95it [05:43,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:05<00:43,  1.66s/it]\n","97it [06:25, 12.85s/it]\u001b[A\n","98it [06:25,  9.06s/it]\u001b[A\n","99it [06:25,  6.41s/it]\u001b[A\n","100it [06:26,  4.56s/it]\u001b[A\n","101it [06:26,  3.26s/it]\u001b[A\n","102it [06:26,  2.35s/it]\u001b[A\n","103it [06:26,  1.72s/it]\u001b[A\n","104it [06:27,  1.27s/it]\u001b[A\n","105it [06:27,  1.04it/s]\u001b[A\n","106it [06:27,  1.35it/s]\u001b[A\n","107it [06:27,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:49<00:01,  1.66s/it]\n","109it [07:09, 12.85s/it]\u001b[A\n","110it [07:09,  9.06s/it]\u001b[A\n","111it [07:10,  6.41s/it]\u001b[A\n","112it [07:10,  4.56s/it]\u001b[A\n","113it [07:10,  3.26s/it]\u001b[A\n","114it [07:10,  2.35s/it]\u001b[A\n","115it [07:11,  1.72s/it]\u001b[A\n","116it [07:11,  1.27s/it]\u001b[A\n","117it [07:11,  1.04it/s]\u001b[A\n","118it [07:11,  1.35it/s]\u001b[A\n","119it [07:11,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:54<00:00,  1.90s/it]\n","03/08/2022 07:00:05 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:00:19 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:26,  4.64s/it]\u001b[A\n","122it [07:26,  3.32s/it]\u001b[A\n","123it [07:26,  2.39s/it]\u001b[A\n","124it [07:27,  1.74s/it]\u001b[A\n","125it [07:27,  1.29s/it]\u001b[A\n","126it [07:27,  1.03it/s]\u001b[A\n","127it [07:27,  1.33it/s]\u001b[A\n","128it [07:28,  1.68it/s]\u001b[A\n","129it [07:28,  2.06it/s]\u001b[A\n","130it [07:28,  2.44it/s]\u001b[A\n","131it [07:28,  2.81it/s]\u001b[A\n","132it [07:28,  3.48it/s]\u001b[A03/08/2022 07:00:21 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:00:21 - INFO - __main__ -     eval_loss = 3.832409381866455\n","03/08/2022 07:00:21 - INFO - __main__ -     eval_auroc = 0.909375011920929\n","03/08/2022 07:00:21 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 07:00:21 - INFO - __main__ -     eval_f1 = 0.4761904776096344\n","03/08/2022 07:00:22 - INFO - filelock -   Lock 140408901061392 acquired on log.lock\n","03/08/2022 07:00:22 - INFO - filelock -   Lock 140408901061392 released on log.lock\n","132it [07:28,  3.40s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:00:27 - INFO - __main__ -   Specify load the 86-th prompt: *cls*‚ñÅ4.*mask*.*+sent_0**sep+* | {0: \"U\", 1: \"Z\"}\n","03/08/2022 07:00:27 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:00:27 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-29226', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-00-27_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-29226', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:00:27 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:00:29 - INFO - src.dataset -   Label 0 to word ƒ†U (121)\n","03/08/2022 07:00:29 - INFO - src.dataset -   Label 1 to word ƒ†Z (525)\n","03/08/2022 07:00:29 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:00:29 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:00:29 - INFO - filelock -   Lock 139870407921232 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:00:29 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 07:00:29 - INFO - filelock -   Lock 139870407921232 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:00:29 - INFO - src.dataset -   Label 0 to word ƒ†U (121)\n","03/08/2022 07:00:29 - INFO - src.dataset -   Label 1 to word ƒ†Z (525)\n","03/08/2022 07:00:29 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:00:29 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:00:29 - INFO - filelock -   Lock 139870395081168 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:00:29 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:00:29 - INFO - filelock -   Lock 139870395081168 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:00:29 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:00:29 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:00:29 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 306, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:00:29 - INFO - src.dataset -   text: <s>‚ñÅ4.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:00:44 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:00:44 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:00:44 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:00:44 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:00:44 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:00:44 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:00:44 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.23it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 07:01:28 - INFO - src.trainer -   Best dev result: 0.934249997138977\n","Epoch:  20% 49/250 [01:31<05:34,  1.66s/it]\n","13it [00:51, 14.91s/it]            \u001b[A\n","14it [00:51, 10.51s/it]\u001b[A\n","15it [00:52,  7.42s/it]\u001b[A\n","16it [00:52,  5.27s/it]\u001b[A\n","17it [00:52,  3.76s/it]\u001b[A\n","18it [00:52,  2.70s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:53,  1.55it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:35, 12.87s/it]\u001b[A\n","26it [01:36,  9.08s/it]\u001b[A\n","27it [01:36,  6.42s/it]\u001b[A\n","28it [01:36,  4.57s/it]\u001b[A\n","29it [01:36,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.04it/s]\u001b[A\n","34it [01:37,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:20, 12.83s/it]\u001b[A\n","38it [02:20,  9.05s/it]\u001b[A\n","39it [02:20,  6.41s/it]\u001b[A\n","40it [02:20,  4.55s/it]\u001b[A\n","41it [02:20,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:21,  1.71s/it]\u001b[A\n","44it [02:21,  1.27s/it]\u001b[A\n","45it [02:21,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:44<03:29,  1.66s/it]\n","49it [03:04, 12.84s/it]\u001b[A\n","50it [03:04,  9.05s/it]\u001b[A\n","51it [03:04,  6.41s/it]\u001b[A\n","52it [03:04,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:05,  1.71s/it]\u001b[A\n","56it [03:05,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:06,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:28<02:47,  1.66s/it]\n","61it [03:48, 12.85s/it]\u001b[A\n","62it [03:48,  9.07s/it]\u001b[A\n","63it [03:48,  6.42s/it]\u001b[A\n","64it [03:49,  4.56s/it]\u001b[A\n","65it [03:49,  3.26s/it]\u001b[A\n","66it [03:49,  2.35s/it]\u001b[A\n","67it [03:49,  1.72s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:50,  1.04it/s]\u001b[A\n","70it [03:50,  1.35it/s]\u001b[A\n","71it [03:50,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:12<02:06,  1.66s/it]\n","73it [04:32, 12.84s/it]\u001b[A\n","74it [04:32,  9.06s/it]\u001b[A\n","75it [04:33,  6.41s/it]\u001b[A\n","76it [04:33,  4.55s/it]\u001b[A\n","77it [04:33,  3.26s/it]\u001b[A\n","78it [04:33,  2.35s/it]\u001b[A\n","79it [04:34,  1.71s/it]\u001b[A\n","80it [04:34,  1.27s/it]\u001b[A\n","81it [04:34,  1.04it/s]\u001b[A\n","82it [04:34,  1.35it/s]\u001b[A\n","83it [04:34,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:57<01:24,  1.66s/it]\n","85it [05:16, 12.84s/it]\u001b[A\n","86it [05:17,  9.06s/it]\u001b[A\n","87it [05:17,  6.41s/it]\u001b[A\n","88it [05:17,  4.56s/it]\u001b[A\n","89it [05:17,  3.26s/it]\u001b[A\n","90it [05:17,  2.35s/it]\u001b[A\n","91it [05:18,  1.71s/it]\u001b[A\n","92it [05:18,  1.27s/it]\u001b[A\n","93it [05:18,  1.04it/s]\u001b[A\n","94it [05:18,  1.35it/s]\u001b[A\n","95it [05:19,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:41<00:43,  1.66s/it]\n","97it [06:01, 12.84s/it]\u001b[A\n","98it [06:01,  9.06s/it]\u001b[A\n","99it [06:01,  6.41s/it]\u001b[A\n","100it [06:01,  4.56s/it]\u001b[A\n","101it [06:01,  3.26s/it]\u001b[A\n","102it [06:02,  2.35s/it]\u001b[A\n","103it [06:02,  1.71s/it]\u001b[A\n","104it [06:02,  1.27s/it]\u001b[A\n","105it [06:02,  1.04it/s]\u001b[A\n","106it [06:03,  1.35it/s]\u001b[A\n","107it [06:03,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:25<00:01,  1.66s/it]\n","109it [06:45, 12.83s/it]\u001b[A\n","110it [06:45,  9.05s/it]\u001b[A\n","111it [06:45,  6.41s/it]\u001b[A\n","112it [06:45,  4.55s/it]\u001b[A\n","113it [06:46,  3.26s/it]\u001b[A\n","114it [06:46,  2.35s/it]\u001b[A\n","115it [06:46,  1.71s/it]\u001b[A\n","116it [06:46,  1.27s/it]\u001b[A\n","117it [06:47,  1.04it/s]\u001b[A\n","118it [06:47,  1.35it/s]\u001b[A\n","119it [06:47,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:29<00:00,  1.80s/it]\n","03/08/2022 07:08:14 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:08:27 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:00,  4.32s/it]\u001b[A\n","122it [07:01,  3.09s/it]\u001b[A\n","123it [07:01,  2.23s/it]\u001b[A\n","124it [07:01,  1.63s/it]\u001b[A\n","125it [07:01,  1.21s/it]\u001b[A\n","126it [07:02,  1.09it/s]\u001b[A\n","127it [07:02,  1.40it/s]\u001b[A\n","128it [07:02,  1.76it/s]\u001b[A\n","129it [07:02,  2.14it/s]\u001b[A\n","130it [07:03,  2.52it/s]\u001b[A\n","131it [07:03,  2.88it/s]\u001b[A\n","132it [07:03,  3.57it/s]\u001b[A03/08/2022 07:08:29 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:08:29 - INFO - __main__ -     eval_loss = 3.132761001586914\n","03/08/2022 07:08:29 - INFO - __main__ -     eval_auroc = 0.934249997138977\n","03/08/2022 07:08:29 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 07:08:29 - INFO - __main__ -     eval_f1 = 0.5333333611488342\n","03/08/2022 07:08:29 - INFO - filelock -   Lock 139870407857296 acquired on log.lock\n","03/08/2022 07:08:29 - INFO - filelock -   Lock 139870407857296 released on log.lock\n","132it [07:03,  3.21s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:08:35 - INFO - __main__ -   Specify load the 87-th prompt: *cls*‚ñÅ4.*mask*.*+sent_0**sep+* | {0: \"2018\", 1: \"Amazing\"}\n","03/08/2022 07:08:35 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:08:35 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-27407', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-08-35_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-27407', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:08:35 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:08:37 - INFO - src.dataset -   Label 0 to word ƒ†2018 (199)\n","03/08/2022 07:08:37 - INFO - src.dataset -   Label 1 to word ƒ†Amazing (24361)\n","03/08/2022 07:08:37 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:08:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:08:37 - INFO - filelock -   Lock 139697252100816 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:08:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 07:08:37 - INFO - filelock -   Lock 139697252100816 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:08:37 - INFO - src.dataset -   Label 0 to word ƒ†2018 (199)\n","03/08/2022 07:08:37 - INFO - src.dataset -   Label 1 to word ƒ†Amazing (24361)\n","03/08/2022 07:08:37 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:08:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:08:37 - INFO - filelock -   Lock 139697220452304 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:08:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:08:37 - INFO - filelock -   Lock 139697220452304 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:08:37 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:08:37 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:08:37 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 306, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:08:37 - INFO - src.dataset -   text: <s>‚ñÅ4.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:08:52 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:08:52 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:08:52 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:08:52 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:08:52 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:08:52 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:08:52 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.11it/s]\u001b[A03/08/2022 07:09:36 - INFO - src.trainer -   Best dev result: 0.9626249670982361\n","Epoch:  20% 49/250 [01:31<05:34,  1.66s/it]\n","13it [00:51, 14.81s/it]            \u001b[A\n","14it [00:51, 10.44s/it]\u001b[A\n","15it [00:51,  7.38s/it]\u001b[A\n","16it [00:52,  5.23s/it]\u001b[A\n","17it [00:52,  3.73s/it]\u001b[A\n","18it [00:52,  2.68s/it]\u001b[A\n","19it [00:52,  1.95s/it]\u001b[A\n","20it [00:52,  1.43s/it]\u001b[A\n","21it [00:53,  1.07s/it]\u001b[A\n","22it [00:53,  1.22it/s]\u001b[A\n","23it [00:53,  1.56it/s]\u001b[A\n","Epoch:  30% 74/250 [02:15<04:51,  1.66s/it]\n","25it [01:35, 12.82s/it]\u001b[A\n","26it [01:35,  9.05s/it]\u001b[A\n","27it [01:35,  6.40s/it]\u001b[A\n","28it [01:36,  4.55s/it]\u001b[A\n","29it [01:36,  3.25s/it]\u001b[A\n","30it [01:36,  2.35s/it]\u001b[A\n","31it [01:36,  1.71s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.05it/s]\u001b[A\n","34it [01:37,  1.35it/s]\u001b[A\n","35it [01:37,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [02:59<04:10,  1.66s/it]\n","37it [02:19, 12.80s/it]\u001b[A\n","38it [02:19,  9.03s/it]\u001b[A\n","39it [02:19,  6.39s/it]\u001b[A\n","40it [02:20,  4.54s/it]\u001b[A\n","41it [02:20,  3.25s/it]\u001b[A\n","42it [02:20,  2.34s/it]\u001b[A\n","43it [02:20,  1.71s/it]\u001b[A\n","44it [02:21,  1.27s/it]\u001b[A\n","45it [02:21,  1.05it/s]\u001b[A\n","46it [02:21,  1.36it/s]\u001b[A\n","47it [02:21,  1.71it/s]\u001b[A\n","Epoch:  50% 124/250 [03:43<03:28,  1.65s/it]\n","49it [03:03, 12.80s/it]\u001b[A\n","50it [03:03,  9.03s/it]\u001b[A\n","51it [03:03,  6.39s/it]\u001b[A\n","52it [03:04,  4.54s/it]\u001b[A\n","53it [03:04,  3.25s/it]\u001b[A\n","54it [03:04,  2.34s/it]\u001b[A\n","55it [03:04,  1.71s/it]\u001b[A\n","56it [03:05,  1.27s/it]\u001b[A\n","57it [03:05,  1.05it/s]\u001b[A\n","58it [03:05,  1.35it/s]\u001b[A\n","59it [03:05,  1.71it/s]\u001b[A\n","Epoch:  60% 149/250 [04:27<02:47,  1.66s/it]\n","61it [03:47, 12.81s/it]\u001b[A\n","62it [03:47,  9.04s/it]\u001b[A\n","63it [03:48,  6.39s/it]\u001b[A\n","64it [03:48,  4.55s/it]\u001b[A\n","65it [03:48,  3.25s/it]\u001b[A\n","66it [03:48,  2.34s/it]\u001b[A\n","67it [03:48,  1.71s/it]\u001b[A\n","68it [03:49,  1.27s/it]\u001b[A\n","69it [03:49,  1.05it/s]\u001b[A\n","70it [03:49,  1.35it/s]\u001b[A\n","71it [03:49,  1.71it/s]\u001b[A\n","Epoch:  70% 174/250 [05:11<02:06,  1.66s/it]\n","73it [04:31, 12.83s/it]\u001b[A\n","74it [04:31,  9.05s/it]\u001b[A\n","75it [04:32,  6.40s/it]\u001b[A\n","76it [04:32,  4.55s/it]\u001b[A\n","77it [04:32,  3.25s/it]\u001b[A\n","78it [04:32,  2.35s/it]\u001b[A\n","79it [04:33,  1.71s/it]\u001b[A\n","80it [04:33,  1.27s/it]\u001b[A\n","81it [04:33,  1.04it/s]\u001b[A\n","82it [04:33,  1.35it/s]\u001b[A\n","83it [04:34,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:56<01:24,  1.66s/it]\n","85it [05:15, 12.83s/it]\u001b[A\n","86it [05:16,  9.05s/it]\u001b[A\n","87it [05:16,  6.41s/it]\u001b[A\n","88it [05:16,  4.55s/it]\u001b[A\n","89it [05:16,  3.26s/it]\u001b[A\n","90it [05:17,  2.35s/it]\u001b[A\n","91it [05:17,  1.71s/it]\u001b[A\n","92it [05:17,  1.27s/it]\u001b[A\n","93it [05:17,  1.04it/s]\u001b[A\n","94it [05:17,  1.35it/s]\u001b[A\n","95it [05:18,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:40<00:43,  1.66s/it]\n","97it [06:00, 12.83s/it]\u001b[A\n","98it [06:00,  9.05s/it]\u001b[A\n","99it [06:00,  6.40s/it]\u001b[A\n","100it [06:00,  4.55s/it]\u001b[A\n","101it [06:00,  3.26s/it]\u001b[A\n","102it [06:01,  2.35s/it]\u001b[A\n","103it [06:01,  1.71s/it]\u001b[A\n","104it [06:01,  1.27s/it]\u001b[A\n","105it [06:01,  1.04it/s]\u001b[A\n","106it [06:02,  1.35it/s]\u001b[A\n","107it [06:02,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:24<00:01,  1.66s/it]\n","109it [06:44, 12.84s/it]\u001b[A\n","110it [06:44,  9.05s/it]\u001b[A\n","111it [06:44,  6.41s/it]\u001b[A\n","112it [06:44,  4.55s/it]\u001b[A\n","113it [06:45,  3.26s/it]\u001b[A\n","114it [06:45,  2.35s/it]\u001b[A\n","115it [06:45,  1.71s/it]\u001b[A\n","116it [06:45,  1.27s/it]\u001b[A\n","117it [06:46,  1.04it/s]\u001b[A\n","118it [06:46,  1.35it/s]\u001b[A\n","119it [06:46,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:28<00:00,  1.79s/it]\n","03/08/2022 07:16:21 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:16:34 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:00,  4.34s/it]\u001b[A\n","122it [07:00,  3.10s/it]\u001b[A\n","123it [07:00,  2.24s/it]\u001b[A\n","124it [07:00,  1.64s/it]\u001b[A\n","125it [07:00,  1.22s/it]\u001b[A\n","126it [07:01,  1.09it/s]\u001b[A\n","127it [07:01,  1.40it/s]\u001b[A\n","128it [07:01,  1.76it/s]\u001b[A\n","129it [07:01,  2.14it/s]\u001b[A\n","130it [07:02,  2.52it/s]\u001b[A\n","131it [07:02,  2.88it/s]\u001b[A\n","132it [07:02,  3.56it/s]\u001b[A03/08/2022 07:16:36 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:16:36 - INFO - __main__ -     eval_loss = 1.1777856349945068\n","03/08/2022 07:16:36 - INFO - __main__ -     eval_auroc = 0.9626249670982361\n","03/08/2022 07:16:36 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 07:16:36 - INFO - __main__ -     eval_f1 = 0.64000004529953\n","03/08/2022 07:16:36 - INFO - filelock -   Lock 139697201192592 acquired on log.lock\n","03/08/2022 07:16:36 - INFO - filelock -   Lock 139697201192592 released on log.lock\n","132it [07:02,  3.20s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:16:42 - INFO - __main__ -   Specify load the 88-th prompt: *cls*‚ñÅ4.*mask*.*+sent_0**sep+* | {0: \"O\", 1: \"Z\"}\n","03/08/2022 07:16:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:16:42 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-3624', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-16-42_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-3624', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:16:42 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:16:44 - INFO - src.dataset -   Label 0 to word ƒ†O (384)\n","03/08/2022 07:16:44 - INFO - src.dataset -   Label 1 to word ƒ†Z (525)\n","03/08/2022 07:16:44 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:16:44 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:16:44 - INFO - filelock -   Lock 140701726471952 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:16:44 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 07:16:44 - INFO - filelock -   Lock 140701726471952 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:16:44 - INFO - src.dataset -   Label 0 to word ƒ†O (384)\n","03/08/2022 07:16:44 - INFO - src.dataset -   Label 1 to word ƒ†Z (525)\n","03/08/2022 07:16:44 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:16:44 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:16:44 - INFO - filelock -   Lock 140701697314768 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:16:44 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.004 s]\n","03/08/2022 07:16:44 - INFO - filelock -   Lock 140701697314768 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:16:44 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:16:44 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:16:44 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 306, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:16:44 - INFO - src.dataset -   text: <s>‚ñÅ4.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:16:59 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:16:59 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:16:59 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:16:59 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:16:59 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:16:59 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:16:59 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 07:17:44 - INFO - src.trainer -   Best dev result: 0.9627500176429749\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.14s/it]            \u001b[A\n","14it [00:52, 10.66s/it]\u001b[A\n","15it [00:52,  7.53s/it]\u001b[A\n","16it [00:53,  5.34s/it]\u001b[A\n","17it [00:53,  3.81s/it]\u001b[A\n","18it [00:53,  2.74s/it]\u001b[A\n","19it [00:53,  1.99s/it]\u001b[A\n","20it [00:54,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.90s/it]\u001b[A\n","26it [01:36,  9.10s/it]\u001b[A\n","27it [01:37,  6.44s/it]\u001b[A\n","28it [01:37,  4.58s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:38,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:39,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:01<04:11,  1.66s/it]\n","37it [02:20, 12.85s/it]\u001b[A\n","38it [02:21,  9.07s/it]\u001b[A\n","39it [02:21,  6.42s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:22,  2.35s/it]\u001b[A\n","43it [02:22,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:23,  1.35it/s]\u001b[A\n","47it [02:23,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:29,  1.66s/it]\n","49it [03:05, 12.85s/it]\u001b[A\n","50it [03:05,  9.06s/it]\u001b[A\n","51it [03:05,  6.41s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:06,  3.26s/it]\u001b[A\n","54it [03:06,  2.35s/it]\u001b[A\n","55it [03:06,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:07,  1.04it/s]\u001b[A\n","58it [03:07,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:29<02:47,  1.66s/it]\n","61it [03:49, 12.84s/it]\u001b[A\n","62it [03:49,  9.05s/it]\u001b[A\n","63it [03:49,  6.41s/it]\u001b[A\n","64it [03:50,  4.55s/it]\u001b[A\n","65it [03:50,  3.26s/it]\u001b[A\n","66it [03:50,  2.35s/it]\u001b[A\n","67it [03:50,  1.71s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:51,  1.04it/s]\u001b[A\n","70it [03:51,  1.35it/s]\u001b[A\n","71it [03:51,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:13<02:06,  1.66s/it]\n","73it [04:33, 12.84s/it]\u001b[A\n","74it [04:33,  9.06s/it]\u001b[A\n","75it [04:34,  6.41s/it]\u001b[A\n","76it [04:34,  4.56s/it]\u001b[A\n","77it [04:34,  3.26s/it]\u001b[A\n","78it [04:34,  2.35s/it]\u001b[A\n","79it [04:34,  1.71s/it]\u001b[A\n","80it [04:35,  1.27s/it]\u001b[A\n","81it [04:35,  1.04it/s]\u001b[A\n","82it [04:35,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:57<01:24,  1.66s/it]\n","85it [05:17, 12.84s/it]\u001b[A\n","86it [05:17,  9.06s/it]\u001b[A\n","87it [05:18,  6.41s/it]\u001b[A\n","88it [05:18,  4.56s/it]\u001b[A\n","89it [05:18,  3.26s/it]\u001b[A\n","90it [05:18,  2.35s/it]\u001b[A\n","91it [05:19,  1.72s/it]\u001b[A\n","92it [05:19,  1.27s/it]\u001b[A\n","93it [05:19,  1.04it/s]\u001b[A\n","94it [05:19,  1.35it/s]\u001b[A\n","95it [05:20,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:42<00:43,  1.67s/it]\n","97it [06:02, 12.86s/it]\u001b[A\n","98it [06:02,  9.07s/it]\u001b[A\n","99it [06:02,  6.42s/it]\u001b[A\n","100it [06:02,  4.56s/it]\u001b[A\n","101it [06:02,  3.26s/it]\u001b[A\n","102it [06:03,  2.35s/it]\u001b[A\n","103it [06:03,  1.72s/it]\u001b[A\n","104it [06:03,  1.27s/it]\u001b[A\n","105it [06:03,  1.04it/s]\u001b[A\n","106it [06:04,  1.35it/s]\u001b[A\n","107it [06:04,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:26<00:01,  1.67s/it]\n","109it [06:46, 12.88s/it]\u001b[A\n","110it [06:46,  9.08s/it]\u001b[A\n","111it [06:46,  6.43s/it]\u001b[A\n","112it [06:47,  4.57s/it]\u001b[A\n","113it [06:47,  3.27s/it]\u001b[A\n","114it [06:47,  2.36s/it]\u001b[A\n","115it [06:47,  1.72s/it]\u001b[A\n","116it [06:47,  1.27s/it]\u001b[A\n","117it [06:48,  1.04it/s]\u001b[A\n","118it [06:48,  1.35it/s]\u001b[A\n","119it [06:48,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:30<00:00,  1.80s/it]\n","03/08/2022 07:24:30 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:24:43 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:01,  4.27s/it]\u001b[A\n","122it [07:02,  3.06s/it]\u001b[A\n","123it [07:02,  2.21s/it]\u001b[A\n","124it [07:02,  1.62s/it]\u001b[A\n","125it [07:02,  1.20s/it]\u001b[A\n","126it [07:03,  1.10it/s]\u001b[A\n","127it [07:03,  1.42it/s]\u001b[A\n","128it [07:03,  1.77it/s]\u001b[A\n","129it [07:03,  2.15it/s]\u001b[A\n","130it [07:04,  2.53it/s]\u001b[A\n","131it [07:04,  2.89it/s]\u001b[A\n","132it [07:04,  3.57it/s]\u001b[A03/08/2022 07:24:46 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:24:46 - INFO - __main__ -     eval_loss = 1.488381266593933\n","03/08/2022 07:24:46 - INFO - __main__ -     eval_auroc = 0.9627500176429749\n","03/08/2022 07:24:46 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 07:24:46 - INFO - __main__ -     eval_f1 = 0.6153846383094788\n","03/08/2022 07:24:46 - INFO - filelock -   Lock 140701707760784 acquired on log.lock\n","03/08/2022 07:24:46 - INFO - filelock -   Lock 140701707760784 released on log.lock\n","132it [07:04,  3.22s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:24:52 - INFO - __main__ -   Specify load the 89-th prompt: *cls*‚ñÅ4.*mask*.*+sent_0**sep+* | {0: \"All\", 1: \"Okay\"}\n","03/08/2022 07:24:52 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:24:52 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-20976', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-24-52_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-20976', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:24:52 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:24:53 - INFO - src.dataset -   Label 0 to word ƒ†All (404)\n","03/08/2022 07:24:53 - INFO - src.dataset -   Label 1 to word ƒ†Okay (8487)\n","03/08/2022 07:24:53 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:24:53 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:24:53 - INFO - filelock -   Lock 139718637637904 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:24:53 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 07:24:53 - INFO - filelock -   Lock 139718637637904 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:24:53 - INFO - src.dataset -   Label 0 to word ƒ†All (404)\n","03/08/2022 07:24:53 - INFO - src.dataset -   Label 1 to word ƒ†Okay (8487)\n","03/08/2022 07:24:53 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:24:53 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:24:53 - INFO - filelock -   Lock 139718637655440 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:24:53 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:24:53 - INFO - filelock -   Lock 139718637655440 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:24:53 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:24:53 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:24:53 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 306, 4, 50264, 4, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:24:53 - INFO - src.dataset -   text: <s>‚ñÅ4.<mask>. Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:25:08 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:25:08 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:25:08 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:25:08 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:25:08 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:25:08 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:25:08 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:17,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.64it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.44it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 07:25:53 - INFO - src.trainer -   Best dev result: 0.8708750605583191\n","Epoch:  20% 49/250 [01:32<05:40,  1.70s/it]\n","13it [00:51, 14.96s/it]            \u001b[A\n","14it [00:52, 10.54s/it]\u001b[A\n","15it [00:52,  7.45s/it]\u001b[A\n","16it [00:52,  5.28s/it]\u001b[A\n","17it [00:52,  3.77s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:53,  1.67s/it]\n","25it [01:36, 12.99s/it]\u001b[A\n","26it [01:36,  9.16s/it]\u001b[A\n","27it [01:36,  6.48s/it]\u001b[A\n","28it [01:37,  4.61s/it]\u001b[A\n","29it [01:37,  3.29s/it]\u001b[A\n","30it [01:37,  2.38s/it]\u001b[A\n","31it [01:37,  1.73s/it]\u001b[A\n","32it [01:38,  1.28s/it]\u001b[A\n","33it [01:38,  1.03it/s]\u001b[A\n","34it [01:38,  1.34it/s]\u001b[A\n","35it [01:38,  1.69it/s]\u001b[A\n","Epoch:  40% 99/250 [03:01<04:11,  1.66s/it]\n","37it [02:20, 12.87s/it]\u001b[A\n","38it [02:20,  9.08s/it]\u001b[A\n","39it [02:21,  6.43s/it]\u001b[A\n","40it [02:21,  4.57s/it]\u001b[A\n","41it [02:21,  3.27s/it]\u001b[A\n","42it [02:21,  2.36s/it]\u001b[A\n","43it [02:22,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:23,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:30,  1.67s/it]\n","49it [03:05, 12.88s/it]\u001b[A\n","50it [03:05,  9.09s/it]\u001b[A\n","51it [03:05,  6.43s/it]\u001b[A\n","52it [03:05,  4.57s/it]\u001b[A\n","53it [03:05,  3.27s/it]\u001b[A\n","54it [03:06,  2.36s/it]\u001b[A\n","55it [03:06,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:07,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:29<02:48,  1.66s/it]\n","61it [03:49, 12.87s/it]\u001b[A\n","62it [03:49,  9.08s/it]\u001b[A\n","63it [03:49,  6.42s/it]\u001b[A\n","64it [03:50,  4.57s/it]\u001b[A\n","65it [03:50,  3.27s/it]\u001b[A\n","66it [03:50,  2.36s/it]\u001b[A\n","67it [03:50,  1.72s/it]\u001b[A\n","68it [03:50,  1.27s/it]\u001b[A\n","69it [03:51,  1.04it/s]\u001b[A\n","70it [03:51,  1.35it/s]\u001b[A\n","71it [03:51,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:14<02:06,  1.67s/it]\n","73it [04:33, 12.87s/it]\u001b[A\n","74it [04:33,  9.08s/it]\u001b[A\n","75it [04:34,  6.42s/it]\u001b[A\n","76it [04:34,  4.57s/it]\u001b[A\n","77it [04:34,  3.27s/it]\u001b[A\n","78it [04:34,  2.35s/it]\u001b[A\n","79it [04:35,  1.72s/it]\u001b[A\n","80it [04:35,  1.27s/it]\u001b[A\n","81it [04:35,  1.04it/s]\u001b[A\n","82it [04:35,  1.35it/s]\u001b[A\n","83it [04:35,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [05:58<01:24,  1.67s/it]\n","85it [05:17, 12.87s/it]\u001b[A\n","86it [05:18,  9.08s/it]\u001b[A\n","87it [05:18,  6.43s/it]\u001b[A\n","88it [05:18,  4.57s/it]\u001b[A\n","89it [05:18,  3.27s/it]\u001b[A\n","90it [05:19,  2.36s/it]\u001b[A\n","91it [05:19,  1.72s/it]\u001b[A\n","92it [05:19,  1.27s/it]\u001b[A\n","93it [05:19,  1.04it/s]\u001b[A\n","94it [05:20,  1.35it/s]\u001b[A\n","95it [05:20,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:42<00:43,  1.66s/it]\n","97it [06:02, 12.88s/it]\u001b[A\n","98it [06:02,  9.09s/it]\u001b[A\n","99it [06:02,  6.43s/it]\u001b[A\n","100it [06:02,  4.57s/it]\u001b[A\n","101it [06:03,  3.27s/it]\u001b[A\n","102it [06:03,  2.36s/it]\u001b[A\n","103it [06:03,  1.72s/it]\u001b[A\n","104it [06:03,  1.27s/it]\u001b[A\n","105it [06:04,  1.04it/s]\u001b[A\n","106it [06:04,  1.35it/s]\u001b[A\n","107it [06:04,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:26<00:01,  1.66s/it]\n","109it [06:46, 12.85s/it]\u001b[A\n","110it [06:46,  9.07s/it]\u001b[A\n","111it [06:46,  6.42s/it]\u001b[A\n","112it [06:47,  4.56s/it]\u001b[A\n","113it [06:47,  3.26s/it]\u001b[A\n","114it [06:47,  2.35s/it]\u001b[A\n","115it [06:47,  1.72s/it]\u001b[A\n","116it [06:48,  1.27s/it]\u001b[A\n","117it [06:48,  1.04it/s]\u001b[A\n","118it [06:48,  1.35it/s]\u001b[A\n","119it [06:48,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:31<00:00,  1.80s/it]\n","03/08/2022 07:32:40 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:32:53 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:02,  4.25s/it]\u001b[A\n","122it [07:02,  3.05s/it]\u001b[A\n","123it [07:02,  2.20s/it]\u001b[A\n","124it [07:02,  1.61s/it]\u001b[A\n","125it [07:02,  1.20s/it]\u001b[A\n","126it [07:03,  1.10it/s]\u001b[A\n","127it [07:03,  1.42it/s]\u001b[A\n","128it [07:03,  1.78it/s]\u001b[A\n","129it [07:03,  2.16it/s]\u001b[A\n","130it [07:04,  2.54it/s]\u001b[A\n","131it [07:04,  2.90it/s]\u001b[A\n","132it [07:04,  3.58it/s]\u001b[A03/08/2022 07:32:55 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:32:55 - INFO - __main__ -     eval_loss = 3.1585376262664795\n","03/08/2022 07:32:55 - INFO - __main__ -     eval_auroc = 0.8708750605583191\n","03/08/2022 07:32:55 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/08/2022 07:32:55 - INFO - __main__ -     eval_f1 = 0.5176470875740051\n","03/08/2022 07:32:55 - INFO - filelock -   Lock 139718620827024 acquired on log.lock\n","03/08/2022 07:32:55 - INFO - filelock -   Lock 139718620827024 released on log.lock\n","132it [07:04,  3.22s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:33:01 - INFO - __main__ -   Specify load the 90-th prompt: *cls*‚ñÅYes!*mask*!*+sent_0**sep+* | {0: \"We\", 1: \"Bad\"}\n","03/08/2022 07:33:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:33:01 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-21526', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-33-01_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-21526', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:33:01 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:33:02 - INFO - src.dataset -   Label 0 to word ƒ†We (166)\n","03/08/2022 07:33:02 - INFO - src.dataset -   Label 1 to word ƒ†Bad (5654)\n","03/08/2022 07:33:02 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:33:02 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:33:02 - INFO - filelock -   Lock 140041202415312 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:33:02 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 07:33:02 - INFO - filelock -   Lock 140041202415312 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:33:02 - INFO - src.dataset -   Label 0 to word ƒ†We (166)\n","03/08/2022 07:33:02 - INFO - src.dataset -   Label 1 to word ƒ†Bad (5654)\n","03/08/2022 07:33:02 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:33:02 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:33:02 - INFO - filelock -   Lock 140041169181456 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:33:02 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:33:02 - INFO - filelock -   Lock 140041169181456 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:33:02 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:33:02 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:33:02 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:33:02 - INFO - src.dataset -   text: <s>‚ñÅYes!<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:33:18 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:33:18 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:33:18 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:33:18 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:33:18 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:33:18 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:33:18 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.64it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.90it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.71it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.58it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.50it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.44it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 07:34:02 - INFO - src.trainer -   Best dev result: 0.9154999852180481\n","Epoch:  20% 49/250 [01:32<05:37,  1.68s/it]\n","13it [00:51, 15.00s/it]            \u001b[A\n","14it [00:52, 10.57s/it]\u001b[A\n","15it [00:52,  7.47s/it]\u001b[A\n","16it [00:52,  5.30s/it]\u001b[A\n","17it [00:52,  3.78s/it]\u001b[A\n","18it [00:53,  2.71s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 07:34:54 - INFO - src.trainer -   Best dev result: 0.9516249895095825\n","Epoch:  30% 74/250 [02:24<04:54,  1.67s/it]\n","25it [01:43, 15.16s/it]\u001b[A\n","26it [01:44, 10.68s/it]\u001b[A\n","27it [01:44,  7.55s/it]\u001b[A\n","28it [01:44,  5.35s/it]\u001b[A\n","29it [01:44,  3.82s/it]\u001b[A\n","30it [01:44,  2.74s/it]\u001b[A\n","31it [01:45,  1.99s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:45,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:11,  1.66s/it]\n","37it [02:28, 12.90s/it]\u001b[A\n","38it [02:28,  9.10s/it]\u001b[A\n","39it [02:28,  6.44s/it]\u001b[A\n","40it [02:28,  4.58s/it]\u001b[A\n","41it [02:29,  3.27s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.28s/it]\u001b[A\n","45it [02:29,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:29,  1.67s/it]\n","49it [03:12, 12.87s/it]\u001b[A\n","50it [03:12,  9.08s/it]\u001b[A\n","51it [03:12,  6.43s/it]\u001b[A\n","52it [03:13,  4.57s/it]\u001b[A\n","53it [03:13,  3.27s/it]\u001b[A\n","54it [03:13,  2.36s/it]\u001b[A\n","55it [03:13,  1.72s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:37<02:48,  1.66s/it]\n","61it [03:56, 12.87s/it]\u001b[A\n","62it [03:56,  9.08s/it]\u001b[A\n","63it [03:57,  6.42s/it]\u001b[A\n","64it [03:57,  4.57s/it]\u001b[A\n","65it [03:57,  3.27s/it]\u001b[A\n","66it [03:57,  2.36s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:59,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.67s/it]\n","73it [04:41, 12.88s/it]\u001b[A\n","74it [04:41,  9.08s/it]\u001b[A\n","75it [04:41,  6.43s/it]\u001b[A\n","76it [04:41,  4.57s/it]\u001b[A\n","77it [04:41,  3.27s/it]\u001b[A\n","78it [04:42,  2.36s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:25,  1.67s/it]\n","85it [05:25, 12.88s/it]\u001b[A\n","86it [05:25,  9.08s/it]\u001b[A\n","87it [05:25,  6.43s/it]\u001b[A\n","88it [05:26,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.66s/it]\n","97it [06:09, 12.86s/it]\u001b[A\n","98it [06:09,  9.07s/it]\u001b[A\n","99it [06:10,  6.42s/it]\u001b[A\n","100it [06:10,  4.56s/it]\u001b[A\n","101it [06:10,  3.26s/it]\u001b[A\n","102it [06:10,  2.35s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.66s/it]\n","109it [06:53, 12.85s/it]\u001b[A\n","110it [06:54,  9.07s/it]\u001b[A\n","111it [06:54,  6.42s/it]\u001b[A\n","112it [06:54,  4.56s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 07:40:56 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:41:10 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.49s/it]\u001b[A\n","122it [07:10,  3.21s/it]\u001b[A\n","123it [07:10,  2.32s/it]\u001b[A\n","124it [07:10,  1.69s/it]\u001b[A\n","125it [07:11,  1.25s/it]\u001b[A\n","126it [07:11,  1.06it/s]\u001b[A\n","127it [07:11,  1.37it/s]\u001b[A\n","128it [07:11,  1.72it/s]\u001b[A\n","129it [07:12,  2.10it/s]\u001b[A\n","130it [07:12,  2.48it/s]\u001b[A\n","131it [07:12,  2.84it/s]\u001b[A\n","132it [07:12,  3.52it/s]\u001b[A03/08/2022 07:41:13 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:41:13 - INFO - __main__ -     eval_loss = 1.792115569114685\n","03/08/2022 07:41:13 - INFO - __main__ -     eval_auroc = 0.9516249895095825\n","03/08/2022 07:41:13 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/08/2022 07:41:13 - INFO - __main__ -     eval_f1 = 0.6478872895240784\n","03/08/2022 07:41:13 - INFO - filelock -   Lock 140041169411664 acquired on log.lock\n","03/08/2022 07:41:13 - INFO - filelock -   Lock 140041169411664 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:41:18 - INFO - __main__ -   Specify load the 91-th prompt: *cls*‚ñÅYes!*mask*!*+sent_0**sep+* | {0: \"Here\", 1: \"Bad\"}\n","03/08/2022 07:41:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:41:18 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-9298', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-41-18_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-9298', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:41:18 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:41:20 - INFO - src.dataset -   Label 0 to word ƒ†Here (1398)\n","03/08/2022 07:41:20 - INFO - src.dataset -   Label 1 to word ƒ†Bad (5654)\n","03/08/2022 07:41:20 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:41:20 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:41:20 - INFO - filelock -   Lock 140665396606928 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:41:20 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 07:41:20 - INFO - filelock -   Lock 140665396606928 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:41:20 - INFO - src.dataset -   Label 0 to word ƒ†Here (1398)\n","03/08/2022 07:41:20 - INFO - src.dataset -   Label 1 to word ƒ†Bad (5654)\n","03/08/2022 07:41:20 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:41:20 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:41:20 - INFO - filelock -   Lock 140665442245776 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:41:20 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:41:20 - INFO - filelock -   Lock 140665442245776 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:41:20 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:41:20 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:41:20 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:41:20 - INFO - src.dataset -   text: <s>‚ñÅYes!<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:41:35 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:41:35 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:41:35 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:41:35 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:41:35 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:41:35 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:41:35 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 07:42:20 - INFO - src.trainer -   Best dev result: 0.8959999680519104\n","Epoch:  20% 49/250 [01:32<05:40,  1.69s/it]\n","13it [00:52, 15.04s/it]            \u001b[A\n","14it [00:52, 10.60s/it]\u001b[A\n","15it [00:52,  7.49s/it]\u001b[A\n","16it [00:52,  5.31s/it]\u001b[A\n","17it [00:53,  3.79s/it]\u001b[A\n","18it [00:53,  2.72s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.90s/it]\u001b[A\n","26it [01:36,  9.10s/it]\u001b[A\n","27it [01:36,  6.44s/it]\u001b[A\n","28it [01:37,  4.58s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:38,  1.28s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:01<04:10,  1.66s/it]\n","37it [02:20, 12.85s/it]\u001b[A\n","38it [02:20,  9.07s/it]\u001b[A\n","39it [02:21,  6.42s/it]\u001b[A\n","40it [02:21,  4.56s/it]\u001b[A\n","41it [02:21,  3.26s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:22,  1.72s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:29,  1.67s/it]\n","49it [03:04, 12.86s/it]\u001b[A\n","50it [03:05,  9.07s/it]\u001b[A\n","51it [03:05,  6.42s/it]\u001b[A\n","52it [03:05,  4.56s/it]\u001b[A\n","53it [03:05,  3.26s/it]\u001b[A\n","54it [03:06,  2.35s/it]\u001b[A\n","55it [03:06,  1.72s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.04it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","60it [03:07,  2.22it/s]\u001b[A03/08/2022 07:45:24 - INFO - src.trainer -   Best dev result: 0.9273750185966492\n","Epoch:  60% 149/250 [04:36<02:52,  1.71s/it]\n","61it [03:56, 15.10s/it]\u001b[A\n","62it [03:56, 10.64s/it]\u001b[A\n","63it [03:57,  7.52s/it]\u001b[A\n","64it [03:57,  5.33s/it]\u001b[A\n","65it [03:57,  3.80s/it]\u001b[A\n","66it [03:57,  2.73s/it]\u001b[A\n","67it [03:58,  1.98s/it]\u001b[A\n","68it [03:58,  1.46s/it]\u001b[A\n","69it [03:58,  1.09s/it]\u001b[A\n","70it [03:58,  1.20it/s]\u001b[A\n","71it [03:58,  1.54it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.67s/it]\n","73it [04:40, 12.90s/it]\u001b[A\n","74it [04:41,  9.10s/it]\u001b[A\n","75it [04:41,  6.44s/it]\u001b[A\n","76it [04:41,  4.58s/it]\u001b[A\n","77it [04:41,  3.27s/it]\u001b[A\n","78it [04:42,  2.36s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.28s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:24,  1.67s/it]\n","85it [05:25, 12.88s/it]\u001b[A\n","86it [05:25,  9.08s/it]\u001b[A\n","87it [05:25,  6.43s/it]\u001b[A\n","88it [05:25,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:49<00:43,  1.67s/it]\n","97it [06:09, 12.88s/it]\u001b[A\n","98it [06:09,  9.08s/it]\u001b[A\n","99it [06:10,  6.43s/it]\u001b[A\n","100it [06:10,  4.57s/it]\u001b[A\n","101it [06:10,  3.27s/it]\u001b[A\n","102it [06:10,  2.36s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:11,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.67s/it]\n","109it [06:53, 12.87s/it]\u001b[A\n","110it [06:54,  9.08s/it]\u001b[A\n","111it [06:54,  6.42s/it]\u001b[A\n","112it [06:54,  4.57s/it]\u001b[A\n","113it [06:54,  3.27s/it]\u001b[A\n","114it [06:55,  2.36s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:55,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 07:49:13 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:49:27 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.58s/it]\u001b[A\n","122it [07:10,  3.28s/it]\u001b[A\n","123it [07:10,  2.36s/it]\u001b[A\n","124it [07:11,  1.72s/it]\u001b[A\n","125it [07:11,  1.28s/it]\u001b[A\n","126it [07:11,  1.04it/s]\u001b[A\n","127it [07:11,  1.34it/s]\u001b[A\n","128it [07:12,  1.70it/s]\u001b[A\n","129it [07:12,  2.07it/s]\u001b[A\n","130it [07:12,  2.46it/s]\u001b[A\n","131it [07:12,  2.82it/s]\u001b[A\n","132it [07:12,  3.50it/s]\u001b[A03/08/2022 07:49:30 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:49:30 - INFO - __main__ -     eval_loss = 2.6156797409057617\n","03/08/2022 07:49:30 - INFO - __main__ -     eval_auroc = 0.9273750185966492\n","03/08/2022 07:49:30 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 07:49:30 - INFO - __main__ -     eval_f1 = 0.5263157486915588\n","03/08/2022 07:49:30 - INFO - filelock -   Lock 140665442245776 acquired on log.lock\n","03/08/2022 07:49:30 - INFO - filelock -   Lock 140665442245776 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:49:36 - INFO - __main__ -   Specify load the 92-th prompt: *cls*‚ñÅYes!*mask*!*+sent_0**sep+* | {0: \"You\", 1: \"Go\"}\n","03/08/2022 07:49:36 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:49:36 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-14837', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-49-36_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-14837', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:49:36 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:49:37 - INFO - src.dataset -   Label 0 to word ƒ†You (370)\n","03/08/2022 07:49:37 - INFO - src.dataset -   Label 1 to word ƒ†Go (2381)\n","03/08/2022 07:49:37 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:49:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:49:37 - INFO - filelock -   Lock 140142323509456 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:49:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 07:49:37 - INFO - filelock -   Lock 140142323509456 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:49:37 - INFO - src.dataset -   Label 0 to word ƒ†You (370)\n","03/08/2022 07:49:37 - INFO - src.dataset -   Label 1 to word ƒ†Go (2381)\n","03/08/2022 07:49:37 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:49:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:49:37 - INFO - filelock -   Lock 140142322150992 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:49:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:49:37 - INFO - filelock -   Lock 140142322150992 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:49:37 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:49:37 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:49:37 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:49:37 - INFO - src.dataset -   text: <s>‚ñÅYes!<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:49:52 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:49:52 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:49:52 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:49:52 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:49:52 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:49:52 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:49:52 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 07:50:37 - INFO - src.trainer -   Best dev result: 0.9526249170303345\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.09s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.51s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.88s/it]\u001b[A\n","26it [01:36,  9.09s/it]\u001b[A\n","27it [01:36,  6.43s/it]\u001b[A\n","28it [01:37,  4.57s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","Epoch:  40% 99/250 [03:00<04:10,  1.66s/it]\n","37it [02:20, 12.82s/it]\u001b[A\n","38it [02:20,  9.05s/it]\u001b[A\n","39it [02:21,  6.40s/it]\u001b[A\n","40it [02:21,  4.55s/it]\u001b[A\n","41it [02:21,  3.25s/it]\u001b[A\n","42it [02:21,  2.35s/it]\u001b[A\n","43it [02:22,  1.71s/it]\u001b[A\n","44it [02:22,  1.27s/it]\u001b[A\n","45it [02:22,  1.04it/s]\u001b[A\n","46it [02:22,  1.35it/s]\u001b[A\n","47it [02:22,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:45<03:28,  1.66s/it]\n","49it [03:04, 12.82s/it]\u001b[A\n","50it [03:04,  9.04s/it]\u001b[A\n","51it [03:05,  6.40s/it]\u001b[A\n","52it [03:05,  4.55s/it]\u001b[A\n","53it [03:05,  3.25s/it]\u001b[A\n","54it [03:05,  2.35s/it]\u001b[A\n","55it [03:06,  1.71s/it]\u001b[A\n","56it [03:06,  1.27s/it]\u001b[A\n","57it [03:06,  1.05it/s]\u001b[A\n","58it [03:06,  1.35it/s]\u001b[A\n","59it [03:07,  1.70it/s]\u001b[A\n","60it [03:07,  2.23it/s]\u001b[A03/08/2022 07:53:42 - INFO - src.trainer -   Best dev result: 0.9574999809265137\n","Epoch:  60% 149/250 [04:37<02:48,  1.67s/it]\n","61it [03:57, 15.32s/it]\u001b[A\n","62it [03:57, 10.79s/it]\u001b[A\n","63it [03:57,  7.62s/it]\u001b[A\n","64it [03:57,  5.41s/it]\u001b[A\n","65it [03:58,  3.85s/it]\u001b[A\n","66it [03:58,  2.77s/it]\u001b[A\n","67it [03:58,  2.01s/it]\u001b[A\n","68it [03:58,  1.47s/it]\u001b[A\n","69it [03:59,  1.10s/it]\u001b[A\n","70it [03:59,  1.19it/s]\u001b[A\n","71it [03:59,  1.52it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.66s/it]\n","73it [04:41, 12.89s/it]\u001b[A\n","74it [04:41,  9.09s/it]\u001b[A\n","75it [04:41,  6.43s/it]\u001b[A\n","76it [04:42,  4.57s/it]\u001b[A\n","77it [04:42,  3.27s/it]\u001b[A\n","78it [04:42,  2.36s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:43,  1.27s/it]\u001b[A\n","81it [04:43,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:06<01:24,  1.67s/it]\n","85it [05:25, 12.87s/it]\u001b[A\n","86it [05:25,  9.08s/it]\u001b[A\n","87it [05:26,  6.42s/it]\u001b[A\n","88it [05:26,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:27,  1.72s/it]\u001b[A\n","92it [05:27,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:28,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:50<00:43,  1.66s/it]\n","97it [06:09, 12.86s/it]\u001b[A\n","98it [06:10,  9.07s/it]\u001b[A\n","99it [06:10,  6.42s/it]\u001b[A\n","100it [06:10,  4.56s/it]\u001b[A\n","101it [06:10,  3.26s/it]\u001b[A\n","102it [06:11,  2.35s/it]\u001b[A\n","103it [06:11,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:12,  1.35it/s]\u001b[A\n","107it [06:12,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.66s/it]\n","109it [06:54, 12.86s/it]\u001b[A\n","110it [06:54,  9.07s/it]\u001b[A\n","111it [06:54,  6.42s/it]\u001b[A\n","112it [06:54,  4.56s/it]\u001b[A\n","113it [06:55,  3.26s/it]\u001b[A\n","114it [06:55,  2.35s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:56,  1.04it/s]\u001b[A\n","118it [06:56,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.84s/it]\n","03/08/2022 07:57:31 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 07:57:45 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.59s/it]\u001b[A\n","122it [07:11,  3.28s/it]\u001b[A\n","123it [07:11,  2.37s/it]\u001b[A\n","124it [07:11,  1.72s/it]\u001b[A\n","125it [07:11,  1.28s/it]\u001b[A\n","126it [07:12,  1.04it/s]\u001b[A\n","127it [07:12,  1.34it/s]\u001b[A\n","128it [07:12,  1.69it/s]\u001b[A\n","129it [07:12,  2.07it/s]\u001b[A\n","130it [07:12,  2.46it/s]\u001b[A\n","131it [07:13,  2.82it/s]\u001b[A\n","132it [07:13,  3.50it/s]\u001b[A03/08/2022 07:57:48 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 07:57:48 - INFO - __main__ -     eval_loss = 1.7647958993911743\n","03/08/2022 07:57:48 - INFO - __main__ -     eval_auroc = 0.9574999809265137\n","03/08/2022 07:57:48 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 07:57:48 - INFO - __main__ -     eval_f1 = 0.6666666865348816\n","03/08/2022 07:57:48 - INFO - filelock -   Lock 140142329991312 acquired on log.lock\n","03/08/2022 07:57:48 - INFO - filelock -   Lock 140142329991312 released on log.lock\n","132it [07:13,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 07:57:54 - INFO - __main__ -   Specify load the 93-th prompt: *cls*‚ñÅYes!*mask*!*+sent_0**sep+* | {0: \"It\", 1: \"Bad\"}\n","03/08/2022 07:57:54 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 07:57:54 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-30438', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_07-57-54_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-30438', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 07:57:54 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 07:57:55 - INFO - src.dataset -   Label 0 to word ƒ†It (85)\n","03/08/2022 07:57:55 - INFO - src.dataset -   Label 1 to word ƒ†Bad (5654)\n","03/08/2022 07:57:55 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 07:57:55 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:57:55 - INFO - filelock -   Lock 140064765446224 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:57:55 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 07:57:55 - INFO - filelock -   Lock 140064765446224 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:57:55 - INFO - src.dataset -   Label 0 to word ƒ†It (85)\n","03/08/2022 07:57:55 - INFO - src.dataset -   Label 1 to word ƒ†Bad (5654)\n","03/08/2022 07:57:55 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 07:57:55 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 07:57:55 - INFO - filelock -   Lock 140064764003728 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:57:55 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 07:57:55 - INFO - filelock -   Lock 140064764003728 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 07:57:55 - INFO - src.dataset -   *** Example ***\n","03/08/2022 07:57:55 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 07:57:55 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 07:57:55 - INFO - src.dataset -   text: <s>‚ñÅYes!<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 07:58:10 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 07:58:10 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 07:58:10 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 07:58:10 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 07:58:10 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 07:58:10 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 07:58:10 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:16,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 07:58:55 - INFO - src.trainer -   Best dev result: 0.9196249842643738\n","Epoch:  20% 49/250 [01:32<05:36,  1.67s/it]\n","13it [00:52, 15.10s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:53,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 07:59:47 - INFO - src.trainer -   Best dev result: 0.921750009059906\n","Epoch:  30% 74/250 [02:24<04:56,  1.68s/it]\n","25it [01:44, 15.18s/it]\u001b[A\n","26it [01:44, 10.69s/it]\u001b[A\n","27it [01:44,  7.55s/it]\u001b[A\n","28it [01:44,  5.36s/it]\u001b[A\n","29it [01:45,  3.82s/it]\u001b[A\n","30it [01:45,  2.74s/it]\u001b[A\n","31it [01:45,  1.99s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:46,  1.09s/it]\u001b[A\n","34it [01:46,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:11,  1.66s/it]\n","37it [02:28, 12.89s/it]\u001b[A\n","38it [02:28,  9.09s/it]\u001b[A\n","39it [02:28,  6.43s/it]\u001b[A\n","40it [02:29,  4.57s/it]\u001b[A\n","41it [02:29,  3.27s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:30,  1.27s/it]\u001b[A\n","45it [02:30,  1.04it/s]\u001b[A\n","46it [02:30,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:53<03:29,  1.66s/it]\n","49it [03:12, 12.86s/it]\u001b[A\n","50it [03:12,  9.07s/it]\u001b[A\n","51it [03:13,  6.42s/it]\u001b[A\n","52it [03:13,  4.56s/it]\u001b[A\n","53it [03:13,  3.26s/it]\u001b[A\n","54it [03:13,  2.35s/it]\u001b[A\n","55it [03:14,  1.72s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:15,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:37<02:47,  1.66s/it]\n","61it [03:56, 12.85s/it]\u001b[A\n","62it [03:57,  9.06s/it]\u001b[A\n","63it [03:57,  6.41s/it]\u001b[A\n","64it [03:57,  4.56s/it]\u001b[A\n","65it [03:57,  3.26s/it]\u001b[A\n","66it [03:58,  2.35s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:59,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.67s/it]\n","73it [04:41, 12.86s/it]\u001b[A\n","74it [04:41,  9.07s/it]\u001b[A\n","75it [04:41,  6.42s/it]\u001b[A\n","76it [04:41,  4.56s/it]\u001b[A\n","77it [04:42,  3.26s/it]\u001b[A\n","78it [04:42,  2.35s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:24,  1.66s/it]\n","85it [05:25, 12.87s/it]\u001b[A\n","86it [05:25,  9.08s/it]\u001b[A\n","87it [05:25,  6.42s/it]\u001b[A\n","88it [05:26,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:27,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:50<00:43,  1.66s/it]\n","97it [06:09, 12.87s/it]\u001b[A\n","98it [06:09,  9.08s/it]\u001b[A\n","99it [06:10,  6.42s/it]\u001b[A\n","100it [06:10,  4.57s/it]\u001b[A\n","101it [06:10,  3.27s/it]\u001b[A\n","102it [06:10,  2.36s/it]\u001b[A\n","103it [06:11,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:12,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.67s/it]\n","109it [06:53, 12.86s/it]\u001b[A\n","110it [06:54,  9.07s/it]\u001b[A\n","111it [06:54,  6.42s/it]\u001b[A\n","112it [06:54,  4.56s/it]\u001b[A\n","113it [06:54,  3.26s/it]\u001b[A\n","114it [06:55,  2.35s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:56,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 08:05:49 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:06:03 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.62s/it]\u001b[A\n","122it [07:10,  3.30s/it]\u001b[A\n","123it [07:11,  2.38s/it]\u001b[A\n","124it [07:11,  1.74s/it]\u001b[A\n","125it [07:11,  1.29s/it]\u001b[A\n","126it [07:11,  1.03it/s]\u001b[A\n","127it [07:12,  1.34it/s]\u001b[A\n","128it [07:12,  1.69it/s]\u001b[A\n","129it [07:12,  2.07it/s]\u001b[A\n","130it [07:12,  2.45it/s]\u001b[A\n","131it [07:13,  2.81it/s]\u001b[A\n","132it [07:13,  3.49it/s]\u001b[A03/08/2022 08:06:06 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:06:06 - INFO - __main__ -     eval_loss = 3.329766035079956\n","03/08/2022 08:06:06 - INFO - __main__ -     eval_auroc = 0.921750009059906\n","03/08/2022 08:06:06 - INFO - __main__ -     eval_recall = 1.0\n","03/08/2022 08:06:06 - INFO - __main__ -     eval_f1 = 0.5154638886451721\n","03/08/2022 08:06:06 - INFO - filelock -   Lock 140065146110352 acquired on log.lock\n","03/08/2022 08:06:06 - INFO - filelock -   Lock 140065146110352 released on log.lock\n","132it [07:13,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:06:12 - INFO - __main__ -   Specify load the 94-th prompt: *cls*‚ñÅYes!*mask*!*+sent_0**sep+* | {0: \"Actually\", 1: \"Perfect\"}\n","03/08/2022 08:06:12 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:06:12 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-31989', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-06-12_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-31989', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:06:12 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:06:13 - INFO - src.dataset -   Label 0 to word ƒ†Actually (18894)\n","03/08/2022 08:06:13 - INFO - src.dataset -   Label 1 to word ƒ†Perfect (17586)\n","03/08/2022 08:06:13 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:06:13 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:06:13 - INFO - filelock -   Lock 139854804813904 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:06:13 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 08:06:13 - INFO - filelock -   Lock 139854804813904 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:06:13 - INFO - src.dataset -   Label 0 to word ƒ†Actually (18894)\n","03/08/2022 08:06:13 - INFO - src.dataset -   Label 1 to word ƒ†Perfect (17586)\n","03/08/2022 08:06:13 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:06:13 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:06:13 - INFO - filelock -   Lock 139854775655376 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:06:13 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:06:13 - INFO - filelock -   Lock 139854775655376 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:06:13 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:06:13 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:06:13 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/08/2022 08:06:13 - INFO - src.dataset -   text: <s>‚ñÅYes!<mask>! Guys, why goth mikasa was shown?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:06:28 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:06:28 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:06:28 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:06:28 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:06:28 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:06:28 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:06:28 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.63it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.63it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.71it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.20it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.90it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.70it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.57it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.49it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.43it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.39it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.07it/s]\u001b[A03/08/2022 08:07:13 - INFO - src.trainer -   Best dev result: 0.9322500228881836\n","Epoch:  20% 49/250 [01:32<05:36,  1.67s/it]\n","13it [00:52, 15.09s/it]            \u001b[A\n","14it [00:52, 10.63s/it]\u001b[A\n","15it [00:52,  7.51s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.53it/s]\u001b[A\n","24it [00:54,  2.02it/s]\u001b[A03/08/2022 08:08:05 - INFO - src.trainer -   Best dev result: 0.9361250400543213\n","Epoch:  30% 74/250 [02:24<04:54,  1.67s/it]\n","25it [01:43, 15.08s/it]\u001b[A\n","26it [01:44, 10.63s/it]\u001b[A\n","27it [01:44,  7.51s/it]\u001b[A\n","28it [01:44,  5.33s/it]\u001b[A\n","29it [01:44,  3.80s/it]\u001b[A\n","30it [01:45,  2.73s/it]\u001b[A\n","31it [01:45,  1.98s/it]\u001b[A\n","32it [01:45,  1.46s/it]\u001b[A\n","33it [01:45,  1.09s/it]\u001b[A\n","34it [01:45,  1.20it/s]\u001b[A\n","35it [01:46,  1.53it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:11,  1.67s/it]\n","37it [02:28, 12.91s/it]\u001b[A\n","38it [02:28,  9.10s/it]\u001b[A\n","39it [02:28,  6.44s/it]\u001b[A\n","40it [02:28,  4.58s/it]\u001b[A\n","41it [02:29,  3.28s/it]\u001b[A\n","42it [02:29,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.28s/it]\u001b[A\n","45it [02:30,  1.04it/s]\u001b[A\n","46it [02:30,  1.34it/s]\u001b[A\n","47it [02:30,  1.69it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:29,  1.67s/it]\n","49it [03:12, 12.89s/it]\u001b[A\n","50it [03:12,  9.09s/it]\u001b[A\n","51it [03:12,  6.43s/it]\u001b[A\n","52it [03:13,  4.57s/it]\u001b[A\n","53it [03:13,  3.27s/it]\u001b[A\n","54it [03:13,  2.36s/it]\u001b[A\n","55it [03:13,  1.72s/it]\u001b[A\n","56it [03:14,  1.27s/it]\u001b[A\n","57it [03:14,  1.04it/s]\u001b[A\n","58it [03:14,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:37<02:47,  1.66s/it]\n","61it [03:56, 12.86s/it]\u001b[A\n","62it [03:57,  9.07s/it]\u001b[A\n","63it [03:57,  6.42s/it]\u001b[A\n","64it [03:57,  4.56s/it]\u001b[A\n","65it [03:57,  3.26s/it]\u001b[A\n","66it [03:57,  2.35s/it]\u001b[A\n","67it [03:58,  1.72s/it]\u001b[A\n","68it [03:58,  1.27s/it]\u001b[A\n","69it [03:58,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:59,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:21<02:06,  1.67s/it]\n","73it [04:41, 12.87s/it]\u001b[A\n","74it [04:41,  9.08s/it]\u001b[A\n","75it [04:41,  6.42s/it]\u001b[A\n","76it [04:41,  4.57s/it]\u001b[A\n","77it [04:42,  3.27s/it]\u001b[A\n","78it [04:42,  2.36s/it]\u001b[A\n","79it [04:42,  1.72s/it]\u001b[A\n","80it [04:42,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:43,  1.35it/s]\u001b[A\n","83it [04:43,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:05<01:25,  1.67s/it]\n","85it [05:25, 12.87s/it]\u001b[A\n","86it [05:25,  9.08s/it]\u001b[A\n","87it [05:25,  6.43s/it]\u001b[A\n","88it [05:26,  4.57s/it]\u001b[A\n","89it [05:26,  3.27s/it]\u001b[A\n","90it [05:26,  2.36s/it]\u001b[A\n","91it [05:26,  1.72s/it]\u001b[A\n","92it [05:27,  1.27s/it]\u001b[A\n","93it [05:27,  1.04it/s]\u001b[A\n","94it [05:27,  1.35it/s]\u001b[A\n","95it [05:27,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:50<00:43,  1.66s/it]\n","97it [06:09, 12.87s/it]\u001b[A\n","98it [06:09,  9.08s/it]\u001b[A\n","99it [06:10,  6.42s/it]\u001b[A\n","100it [06:10,  4.57s/it]\u001b[A\n","101it [06:10,  3.27s/it]\u001b[A\n","102it [06:10,  2.36s/it]\u001b[A\n","103it [06:11,  1.72s/it]\u001b[A\n","104it [06:11,  1.27s/it]\u001b[A\n","105it [06:11,  1.04it/s]\u001b[A\n","106it [06:11,  1.35it/s]\u001b[A\n","107it [06:12,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:34<00:01,  1.67s/it]\n","109it [06:54, 12.87s/it]\u001b[A\n","110it [06:54,  9.08s/it]\u001b[A\n","111it [06:54,  6.43s/it]\u001b[A\n","112it [06:54,  4.57s/it]\u001b[A\n","113it [06:54,  3.27s/it]\u001b[A\n","114it [06:55,  2.36s/it]\u001b[A\n","115it [06:55,  1.72s/it]\u001b[A\n","116it [06:55,  1.27s/it]\u001b[A\n","117it [06:55,  1.04it/s]\u001b[A\n","118it [06:56,  1.35it/s]\u001b[A\n","119it [06:56,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:38<00:00,  1.83s/it]\n","03/08/2022 08:14:07 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:14:21 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:10,  4.51s/it]\u001b[A\n","122it [07:10,  3.23s/it]\u001b[A\n","123it [07:10,  2.33s/it]\u001b[A\n","124it [07:11,  1.70s/it]\u001b[A\n","125it [07:11,  1.26s/it]\u001b[A\n","126it [07:11,  1.05it/s]\u001b[A\n","127it [07:11,  1.36it/s]\u001b[A\n","128it [07:12,  1.71it/s]\u001b[A\n","129it [07:12,  2.09it/s]\u001b[A\n","130it [07:12,  2.47it/s]\u001b[A\n","131it [07:12,  2.83it/s]\u001b[A\n","132it [07:12,  3.51it/s]\u001b[A03/08/2022 08:14:24 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:14:24 - INFO - __main__ -     eval_loss = 2.023900270462036\n","03/08/2022 08:14:24 - INFO - __main__ -     eval_auroc = 0.9361250400543213\n","03/08/2022 08:14:24 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/08/2022 08:14:24 - INFO - __main__ -     eval_f1 = 0.6388888955116272\n","03/08/2022 08:14:24 - INFO - filelock -   Lock 139854804814032 acquired on log.lock\n","03/08/2022 08:14:24 - INFO - filelock -   Lock 139854804814032 released on log.lock\n","132it [07:12,  3.28s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:14:29 - INFO - __main__ -   Specify load the 95-th prompt: *cls**sent_0*,*mask*?*sep+* | {0: \"Bro\", 1: \"true\"}\n","03/08/2022 08:14:29 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:14:29 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-24490', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-14-29_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-24490', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:14:29 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:14:31 - INFO - src.dataset -   Label 0 to word ƒ†Bro (4994)\n","03/08/2022 08:14:31 - INFO - src.dataset -   Label 1 to word ƒ†true (1528)\n","03/08/2022 08:14:31 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:14:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:14:31 - INFO - filelock -   Lock 139881790509136 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:14:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 08:14:31 - INFO - filelock -   Lock 139881790509136 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:14:31 - INFO - src.dataset -   Label 0 to word ƒ†Bro (4994)\n","03/08/2022 08:14:31 - INFO - src.dataset -   Label 1 to word ƒ†true (1528)\n","03/08/2022 08:14:31 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:14:31 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:14:31 - INFO - filelock -   Lock 139881777824016 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:14:31 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:14:31 - INFO - filelock -   Lock 139881777824016 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:14:31 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:14:31 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:14:31 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/08/2022 08:14:31 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:14:46 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:14:46 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:14:46 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:14:46 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:14:46 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:14:46 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:14:46 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:17,  1.67s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.72it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.21it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 08:15:31 - INFO - src.trainer -   Best dev result: 0.8817499876022339\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.07s/it]            \u001b[A\n","14it [00:52, 10.62s/it]\u001b[A\n","15it [00:52,  7.50s/it]\u001b[A\n","16it [00:52,  5.32s/it]\u001b[A\n","17it [00:53,  3.79s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 08:16:23 - INFO - src.trainer -   Best dev result: 0.9022500514984131\n","Epoch:  30% 74/250 [02:23<04:53,  1.67s/it]\n","25it [01:43, 15.01s/it]\u001b[A\n","26it [01:43, 10.58s/it]\u001b[A\n","27it [01:44,  7.48s/it]\u001b[A\n","28it [01:44,  5.30s/it]\u001b[A\n","29it [01:44,  3.78s/it]\u001b[A\n","30it [01:44,  2.72s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:10,  1.66s/it]\n","37it [02:27, 12.87s/it]\u001b[A\n","38it [02:27,  9.08s/it]\u001b[A\n","39it [02:28,  6.42s/it]\u001b[A\n","40it [02:28,  4.57s/it]\u001b[A\n","41it [02:28,  3.27s/it]\u001b[A\n","42it [02:28,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.27s/it]\u001b[A\n","45it [02:29,  1.04it/s]\u001b[A\n","46it [02:29,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","Epoch:  50% 124/250 [03:52<03:28,  1.66s/it]\n","49it [03:11, 12.83s/it]\u001b[A\n","50it [03:12,  9.05s/it]\u001b[A\n","51it [03:12,  6.41s/it]\u001b[A\n","52it [03:12,  4.55s/it]\u001b[A\n","53it [03:12,  3.26s/it]\u001b[A\n","54it [03:13,  2.35s/it]\u001b[A\n","55it [03:13,  1.71s/it]\u001b[A\n","56it [03:13,  1.27s/it]\u001b[A\n","57it [03:13,  1.04it/s]\u001b[A\n","58it [03:13,  1.35it/s]\u001b[A\n","59it [03:14,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:36<02:47,  1.66s/it]\n","61it [03:56, 12.82s/it]\u001b[A\n","62it [03:56,  9.05s/it]\u001b[A\n","63it [03:56,  6.40s/it]\u001b[A\n","64it [03:56,  4.55s/it]\u001b[A\n","65it [03:56,  3.25s/it]\u001b[A\n","66it [03:57,  2.35s/it]\u001b[A\n","67it [03:57,  1.71s/it]\u001b[A\n","68it [03:57,  1.27s/it]\u001b[A\n","69it [03:57,  1.04it/s]\u001b[A\n","70it [03:58,  1.35it/s]\u001b[A\n","71it [03:58,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:20<02:06,  1.66s/it]\n","73it [04:40, 12.83s/it]\u001b[A\n","74it [04:40,  9.05s/it]\u001b[A\n","75it [04:40,  6.41s/it]\u001b[A\n","76it [04:40,  4.55s/it]\u001b[A\n","77it [04:41,  3.26s/it]\u001b[A\n","78it [04:41,  2.35s/it]\u001b[A\n","79it [04:41,  1.71s/it]\u001b[A\n","80it [04:41,  1.27s/it]\u001b[A\n","81it [04:42,  1.04it/s]\u001b[A\n","82it [04:42,  1.35it/s]\u001b[A\n","83it [04:42,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:04<01:24,  1.66s/it]\n","85it [05:24, 12.84s/it]\u001b[A\n","86it [05:24,  9.06s/it]\u001b[A\n","87it [05:24,  6.41s/it]\u001b[A\n","88it [05:25,  4.56s/it]\u001b[A\n","89it [05:25,  3.26s/it]\u001b[A\n","90it [05:25,  2.35s/it]\u001b[A\n","91it [05:25,  1.72s/it]\u001b[A\n","92it [05:26,  1.27s/it]\u001b[A\n","93it [05:26,  1.04it/s]\u001b[A\n","94it [05:26,  1.35it/s]\u001b[A\n","95it [05:26,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:48<00:43,  1.66s/it]\n","97it [06:08, 12.85s/it]\u001b[A\n","98it [06:08,  9.06s/it]\u001b[A\n","99it [06:09,  6.41s/it]\u001b[A\n","100it [06:09,  4.56s/it]\u001b[A\n","101it [06:09,  3.26s/it]\u001b[A\n","102it [06:09,  2.35s/it]\u001b[A\n","103it [06:10,  1.72s/it]\u001b[A\n","104it [06:10,  1.27s/it]\u001b[A\n","105it [06:10,  1.04it/s]\u001b[A\n","106it [06:10,  1.35it/s]\u001b[A\n","107it [06:10,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:33<00:01,  1.66s/it]\n","109it [06:52, 12.85s/it]\u001b[A\n","110it [06:53,  9.06s/it]\u001b[A\n","111it [06:53,  6.41s/it]\u001b[A\n","112it [06:53,  4.56s/it]\u001b[A\n","113it [06:53,  3.26s/it]\u001b[A\n","114it [06:54,  2.35s/it]\u001b[A\n","115it [06:54,  1.72s/it]\u001b[A\n","116it [06:54,  1.27s/it]\u001b[A\n","117it [06:54,  1.04it/s]\u001b[A\n","118it [06:54,  1.35it/s]\u001b[A\n","119it [06:55,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:37<00:00,  1.83s/it]\n","03/08/2022 08:22:23 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:22:37 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:09,  4.51s/it]\u001b[A\n","122it [07:09,  3.23s/it]\u001b[A\n","123it [07:09,  2.33s/it]\u001b[A\n","124it [07:09,  1.70s/it]\u001b[A\n","125it [07:10,  1.26s/it]\u001b[A\n","126it [07:10,  1.05it/s]\u001b[A\n","127it [07:10,  1.36it/s]\u001b[A\n","128it [07:10,  1.71it/s]\u001b[A\n","129it [07:11,  2.09it/s]\u001b[A\n","130it [07:11,  2.47it/s]\u001b[A\n","131it [07:11,  2.84it/s]\u001b[A\n","132it [07:11,  3.52it/s]\u001b[A03/08/2022 08:22:40 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:22:40 - INFO - __main__ -     eval_loss = 2.094346046447754\n","03/08/2022 08:22:40 - INFO - __main__ -     eval_auroc = 0.9022500514984131\n","03/08/2022 08:22:40 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/08/2022 08:22:40 - INFO - __main__ -     eval_f1 = 0.5111111402511597\n","03/08/2022 08:22:40 - INFO - filelock -   Lock 139881790603856 acquired on log.lock\n","03/08/2022 08:22:40 - INFO - filelock -   Lock 139881790603856 released on log.lock\n","132it [07:11,  3.27s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:22:46 - INFO - __main__ -   Specify load the 96-th prompt: *cls**sent_0*,*mask*?*sep+* | {0: \"Dad\", 1: \"still\"}\n","03/08/2022 08:22:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:22:46 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-26474', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-22-46_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-26474', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:22:46 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:22:47 - INFO - src.dataset -   Label 0 to word ƒ†Dad (13404)\n","03/08/2022 08:22:47 - INFO - src.dataset -   Label 1 to word ƒ†still (202)\n","03/08/2022 08:22:47 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:22:47 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:22:47 - INFO - filelock -   Lock 140100595468688 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:22:47 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 08:22:47 - INFO - filelock -   Lock 140100595468688 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:22:47 - INFO - src.dataset -   Label 0 to word ƒ†Dad (13404)\n","03/08/2022 08:22:47 - INFO - src.dataset -   Label 1 to word ƒ†still (202)\n","03/08/2022 08:22:47 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:22:47 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:22:47 - INFO - filelock -   Lock 140100567658448 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:22:47 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:22:47 - INFO - filelock -   Lock 140100567658448 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:22:47 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:22:47 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:22:47 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/08/2022 08:22:47 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:23:03 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:23:03 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:23:03 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:23:03 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:23:03 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:23:03 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:23:03 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.67it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.74it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.92it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.46it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 08:23:47 - INFO - src.trainer -   Best dev result: 0.8742499351501465\n","Epoch:  20% 49/250 [01:32<05:40,  1.70s/it]\n","13it [00:52, 15.10s/it]            \u001b[A\n","14it [00:52, 10.64s/it]\u001b[A\n","15it [00:52,  7.52s/it]\u001b[A\n","16it [00:52,  5.33s/it]\u001b[A\n","17it [00:53,  3.80s/it]\u001b[A\n","18it [00:53,  2.73s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.46s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","Epoch:  30% 74/250 [02:16<04:52,  1.66s/it]\n","25it [01:36, 12.90s/it]\u001b[A\n","26it [01:36,  9.10s/it]\u001b[A\n","27it [01:37,  6.44s/it]\u001b[A\n","28it [01:37,  4.58s/it]\u001b[A\n","29it [01:37,  3.27s/it]\u001b[A\n","30it [01:37,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:38,  1.27s/it]\u001b[A\n","33it [01:38,  1.04it/s]\u001b[A\n","34it [01:38,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","36it [01:39,  2.22it/s]\u001b[A03/08/2022 08:25:24 - INFO - src.trainer -   Best dev result: 0.9222500324249268\n","Epoch:  40% 99/250 [03:08<04:11,  1.67s/it]\n","37it [02:28, 15.05s/it]\u001b[A\n","38it [02:28, 10.60s/it]\u001b[A\n","39it [02:28,  7.49s/it]\u001b[A\n","40it [02:28,  5.31s/it]\u001b[A\n","41it [02:29,  3.79s/it]\u001b[A\n","42it [02:29,  2.72s/it]\u001b[A\n","43it [02:29,  1.97s/it]\u001b[A\n","44it [02:29,  1.45s/it]\u001b[A\n","45it [02:29,  1.09s/it]\u001b[A\n","46it [02:30,  1.21it/s]\u001b[A\n","47it [02:30,  1.54it/s]\u001b[A\n","48it [02:30,  2.03it/s]\u001b[A03/08/2022 08:26:15 - INFO - src.trainer -   Best dev result: 0.9269999861717224\n","Epoch:  50% 124/250 [04:00<03:34,  1.70s/it]\n","49it [03:20, 15.29s/it]\u001b[A\n","50it [03:20, 10.77s/it]\u001b[A\n","51it [03:20,  7.61s/it]\u001b[A\n","52it [03:21,  5.40s/it]\u001b[A\n","53it [03:21,  3.85s/it]\u001b[A\n","54it [03:21,  2.76s/it]\u001b[A\n","55it [03:21,  2.00s/it]\u001b[A\n","56it [03:22,  1.47s/it]\u001b[A\n","57it [03:22,  1.10s/it]\u001b[A\n","58it [03:22,  1.19it/s]\u001b[A\n","59it [03:22,  1.52it/s]\u001b[A\n","Epoch:  60% 149/250 [04:44<02:47,  1.66s/it]\n","61it [04:04, 12.88s/it]\u001b[A\n","62it [04:04,  9.09s/it]\u001b[A\n","63it [04:05,  6.43s/it]\u001b[A\n","64it [04:05,  4.57s/it]\u001b[A\n","65it [04:05,  3.27s/it]\u001b[A\n","66it [04:05,  2.36s/it]\u001b[A\n","67it [04:06,  1.72s/it]\u001b[A\n","68it [04:06,  1.27s/it]\u001b[A\n","69it [04:06,  1.04it/s]\u001b[A\n","70it [04:06,  1.35it/s]\u001b[A\n","71it [04:06,  1.70it/s]\u001b[A\n","72it [04:07,  2.22it/s]\u001b[A03/08/2022 08:27:52 - INFO - src.trainer -   Best dev result: 0.9284999966621399\n","Epoch:  70% 174/250 [05:36<02:07,  1.67s/it]\n","73it [04:56, 15.16s/it]\u001b[A\n","74it [04:56, 10.68s/it]\u001b[A\n","75it [04:57,  7.55s/it]\u001b[A\n","76it [04:57,  5.35s/it]\u001b[A\n","77it [04:57,  3.82s/it]\u001b[A\n","78it [04:57,  2.74s/it]\u001b[A\n","79it [04:57,  1.99s/it]\u001b[A\n","80it [04:58,  1.46s/it]\u001b[A\n","81it [04:58,  1.09s/it]\u001b[A\n","82it [04:58,  1.20it/s]\u001b[A\n","83it [04:58,  1.53it/s]\u001b[A\n","Epoch:  80% 199/250 [06:21<01:24,  1.66s/it]\n","85it [05:40, 12.88s/it]\u001b[A\n","86it [05:40,  9.08s/it]\u001b[A\n","87it [05:41,  6.43s/it]\u001b[A\n","88it [05:41,  4.57s/it]\u001b[A\n","89it [05:41,  3.27s/it]\u001b[A\n","90it [05:41,  2.36s/it]\u001b[A\n","91it [05:42,  1.72s/it]\u001b[A\n","92it [05:42,  1.27s/it]\u001b[A\n","93it [05:42,  1.04it/s]\u001b[A\n","94it [05:42,  1.35it/s]\u001b[A\n","95it [05:43,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [07:05<00:43,  1.66s/it]\n","97it [06:24, 12.85s/it]\u001b[A\n","98it [06:25,  9.06s/it]\u001b[A\n","99it [06:25,  6.41s/it]\u001b[A\n","100it [06:25,  4.56s/it]\u001b[A\n","101it [06:25,  3.26s/it]\u001b[A\n","102it [06:26,  2.35s/it]\u001b[A\n","103it [06:26,  1.72s/it]\u001b[A\n","104it [06:26,  1.27s/it]\u001b[A\n","105it [06:26,  1.04it/s]\u001b[A\n","106it [06:27,  1.35it/s]\u001b[A\n","107it [06:27,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:49<00:01,  1.66s/it]\n","109it [07:09, 12.85s/it]\u001b[A\n","110it [07:09,  9.06s/it]\u001b[A\n","111it [07:09,  6.41s/it]\u001b[A\n","112it [07:09,  4.56s/it]\u001b[A\n","113it [07:10,  3.26s/it]\u001b[A\n","114it [07:10,  2.35s/it]\u001b[A\n","115it [07:10,  1.72s/it]\u001b[A\n","116it [07:10,  1.27s/it]\u001b[A\n","117it [07:11,  1.04it/s]\u001b[A\n","118it [07:11,  1.35it/s]\u001b[A\n","119it [07:11,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:53<00:00,  1.90s/it]\n","03/08/2022 08:30:57 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:31:11 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:25,  4.61s/it]\u001b[A\n","122it [07:26,  3.30s/it]\u001b[A\n","123it [07:26,  2.38s/it]\u001b[A\n","124it [07:26,  1.73s/it]\u001b[A\n","125it [07:26,  1.28s/it]\u001b[A\n","126it [07:27,  1.03it/s]\u001b[A\n","127it [07:27,  1.34it/s]\u001b[A\n","128it [07:27,  1.69it/s]\u001b[A\n","129it [07:27,  2.07it/s]\u001b[A\n","130it [07:28,  2.45it/s]\u001b[A\n","131it [07:28,  2.82it/s]\u001b[A\n","132it [07:28,  3.49it/s]\u001b[A03/08/2022 08:31:13 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:31:13 - INFO - __main__ -     eval_loss = 1.7660127878189087\n","03/08/2022 08:31:13 - INFO - __main__ -     eval_auroc = 0.9284999966621399\n","03/08/2022 08:31:13 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/08/2022 08:31:13 - INFO - __main__ -     eval_f1 = 0.5714285373687744\n","03/08/2022 08:31:13 - INFO - filelock -   Lock 140100567658448 acquired on log.lock\n","03/08/2022 08:31:13 - INFO - filelock -   Lock 140100567658448 released on log.lock\n","132it [07:28,  3.40s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:31:19 - INFO - __main__ -   Specify load the 97-th prompt: *cls**sent_0*,*mask*?*sep+* | {0: \"it\", 1: \"true\"}\n","03/08/2022 08:31:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:31:19 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-23171', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-31-19_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-23171', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:31:19 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:31:21 - INFO - src.dataset -   Label 0 to word ƒ†it (24)\n","03/08/2022 08:31:21 - INFO - src.dataset -   Label 1 to word ƒ†true (1528)\n","03/08/2022 08:31:21 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:31:21 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:31:21 - INFO - filelock -   Lock 139966460970448 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:31:21 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/08/2022 08:31:21 - INFO - filelock -   Lock 139966460970448 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:31:21 - INFO - src.dataset -   Label 0 to word ƒ†it (24)\n","03/08/2022 08:31:21 - INFO - src.dataset -   Label 1 to word ƒ†true (1528)\n","03/08/2022 08:31:21 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:31:21 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:31:21 - INFO - filelock -   Lock 139966460971216 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:31:21 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:31:21 - INFO - filelock -   Lock 139966460971216 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:31:21 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:31:21 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:31:21 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/08/2022 08:31:21 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:31:36 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:31:36 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:31:36 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:31:36 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:31:36 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:31:36 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:31:36 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.65it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 08:32:20 - INFO - src.trainer -   Best dev result: 0.8577499985694885\n","Epoch:  20% 49/250 [01:31<05:34,  1.67s/it]\n","13it [00:51, 14.90s/it]            \u001b[A\n","14it [00:51, 10.50s/it]\u001b[A\n","15it [00:52,  7.42s/it]\u001b[A\n","16it [00:52,  5.26s/it]\u001b[A\n","17it [00:52,  3.75s/it]\u001b[A\n","18it [00:52,  2.70s/it]\u001b[A\n","19it [00:53,  1.96s/it]\u001b[A\n","20it [00:53,  1.44s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:53,  1.22it/s]\u001b[A\n","23it [00:53,  1.55it/s]\u001b[A\n","Epoch:  30% 74/250 [02:15<04:52,  1.66s/it]\n","25it [01:35, 12.87s/it]\u001b[A\n","26it [01:36,  9.08s/it]\u001b[A\n","27it [01:36,  6.43s/it]\u001b[A\n","28it [01:36,  4.57s/it]\u001b[A\n","29it [01:36,  3.27s/it]\u001b[A\n","30it [01:36,  2.36s/it]\u001b[A\n","31it [01:37,  1.72s/it]\u001b[A\n","32it [01:37,  1.27s/it]\u001b[A\n","33it [01:37,  1.04it/s]\u001b[A\n","34it [01:37,  1.35it/s]\u001b[A\n","35it [01:38,  1.70it/s]\u001b[A\n","36it [01:38,  2.22it/s]\u001b[A03/08/2022 08:33:56 - INFO - src.trainer -   Best dev result: 0.8730000257492065\n","Epoch:  40% 99/250 [03:08<04:11,  1.67s/it]\n","37it [02:27, 15.22s/it]\u001b[A\n","38it [02:28, 10.73s/it]\u001b[A\n","39it [02:28,  7.58s/it]\u001b[A\n","40it [02:28,  5.37s/it]\u001b[A\n","41it [02:28,  3.83s/it]\u001b[A\n","42it [02:29,  2.75s/it]\u001b[A\n","43it [02:29,  2.00s/it]\u001b[A\n","44it [02:29,  1.47s/it]\u001b[A\n","45it [02:29,  1.10s/it]\u001b[A\n","46it [02:30,  1.20it/s]\u001b[A\n","47it [02:30,  1.53it/s]\u001b[A\n","48it [02:30,  2.02it/s]\u001b[A03/08/2022 08:34:48 - INFO - src.trainer -   Best dev result: 0.8731250166893005\n","Epoch:  50% 124/250 [03:59<03:32,  1.69s/it]\n","49it [03:19, 15.13s/it]\u001b[A\n","50it [03:19, 10.66s/it]\u001b[A\n","51it [03:20,  7.53s/it]\u001b[A\n","52it [03:20,  5.34s/it]\u001b[A\n","53it [03:20,  3.81s/it]\u001b[A\n","54it [03:20,  2.73s/it]\u001b[A\n","55it [03:21,  1.98s/it]\u001b[A\n","56it [03:21,  1.46s/it]\u001b[A\n","57it [03:21,  1.09s/it]\u001b[A\n","58it [03:21,  1.20it/s]\u001b[A\n","59it [03:21,  1.53it/s]\u001b[A\n","60it [03:22,  2.02it/s]\u001b[A03/08/2022 08:35:40 - INFO - src.trainer -   Best dev result: 0.8762500882148743\n","Epoch:  60% 149/250 [04:51<02:48,  1.67s/it]\n","61it [04:11, 15.25s/it]\u001b[A\n","62it [04:12, 10.74s/it]\u001b[A\n","63it [04:12,  7.59s/it]\u001b[A\n","64it [04:12,  5.38s/it]\u001b[A\n","65it [04:12,  3.84s/it]\u001b[A\n","66it [04:12,  2.75s/it]\u001b[A\n","67it [04:13,  2.00s/it]\u001b[A\n","68it [04:13,  1.47s/it]\u001b[A\n","69it [04:13,  1.10s/it]\u001b[A\n","70it [04:13,  1.19it/s]\u001b[A\n","71it [04:14,  1.53it/s]\u001b[A\n","Epoch:  70% 174/250 [05:36<02:06,  1.66s/it]\n","73it [04:55, 12.88s/it]\u001b[A\n","74it [04:56,  9.09s/it]\u001b[A\n","75it [04:56,  6.43s/it]\u001b[A\n","76it [04:56,  4.57s/it]\u001b[A\n","77it [04:56,  3.27s/it]\u001b[A\n","78it [04:57,  2.36s/it]\u001b[A\n","79it [04:57,  1.72s/it]\u001b[A\n","80it [04:57,  1.27s/it]\u001b[A\n","81it [04:57,  1.04it/s]\u001b[A\n","82it [04:58,  1.35it/s]\u001b[A\n","83it [04:58,  1.70it/s]\u001b[A\n","84it [04:58,  2.22it/s]\u001b[A03/08/2022 08:37:16 - INFO - src.trainer -   Best dev result: 0.8763750195503235\n","Epoch:  80% 199/250 [06:28<01:25,  1.67s/it]\n","85it [05:48, 15.19s/it]\u001b[A\n","86it [05:48, 10.70s/it]\u001b[A\n","87it [05:48,  7.56s/it]\u001b[A\n","88it [05:48,  5.36s/it]\u001b[A\n","89it [05:48,  3.82s/it]\u001b[A\n","90it [05:49,  2.75s/it]\u001b[A\n","91it [05:49,  1.99s/it]\u001b[A\n","92it [05:49,  1.46s/it]\u001b[A\n","93it [05:49,  1.09s/it]\u001b[A\n","94it [05:50,  1.20it/s]\u001b[A\n","95it [05:50,  1.53it/s]\u001b[A\n","Epoch:  90% 224/250 [07:12<00:43,  1.66s/it]\n","97it [06:32, 12.87s/it]\u001b[A\n","98it [06:32,  9.08s/it]\u001b[A\n","99it [06:32,  6.42s/it]\u001b[A\n","100it [06:32,  4.57s/it]\u001b[A\n","101it [06:33,  3.27s/it]\u001b[A\n","102it [06:33,  2.36s/it]\u001b[A\n","103it [06:33,  1.72s/it]\u001b[A\n","104it [06:33,  1.27s/it]\u001b[A\n","105it [06:34,  1.04it/s]\u001b[A\n","106it [06:34,  1.35it/s]\u001b[A\n","107it [06:34,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:56<00:01,  1.66s/it]\n","109it [07:16, 12.83s/it]\u001b[A\n","110it [07:16,  9.05s/it]\u001b[A\n","111it [07:16,  6.41s/it]\u001b[A\n","112it [07:17,  4.55s/it]\u001b[A\n","113it [07:17,  3.26s/it]\u001b[A\n","114it [07:17,  2.35s/it]\u001b[A\n","115it [07:17,  1.71s/it]\u001b[A\n","116it [07:17,  1.27s/it]\u001b[A\n","117it [07:18,  1.04it/s]\u001b[A\n","118it [07:18,  1.35it/s]\u001b[A\n","119it [07:18,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [08:00<00:00,  1.92s/it]\n","03/08/2022 08:39:37 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:39:51 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:33,  4.64s/it]\u001b[A\n","122it [07:33,  3.32s/it]\u001b[A\n","123it [07:33,  2.39s/it]\u001b[A\n","124it [07:33,  1.74s/it]\u001b[A\n","125it [07:34,  1.29s/it]\u001b[A\n","126it [07:34,  1.03it/s]\u001b[A\n","127it [07:34,  1.33it/s]\u001b[A\n","128it [07:34,  1.68it/s]\u001b[A\n","129it [07:35,  2.06it/s]\u001b[A\n","130it [07:35,  2.44it/s]\u001b[A\n","131it [07:35,  2.81it/s]\u001b[A\n","132it [07:35,  3.49it/s]\u001b[A03/08/2022 08:39:54 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:39:54 - INFO - __main__ -     eval_loss = 4.714199066162109\n","03/08/2022 08:39:54 - INFO - __main__ -     eval_auroc = 0.8763750195503235\n","03/08/2022 08:39:54 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 08:39:54 - INFO - __main__ -     eval_f1 = 0.4363636374473572\n","03/08/2022 08:39:54 - INFO - filelock -   Lock 139966460971216 acquired on log.lock\n","03/08/2022 08:39:54 - INFO - filelock -   Lock 139966460971216 released on log.lock\n","132it [07:35,  3.45s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:40:00 - INFO - __main__ -   Specify load the 98-th prompt: *cls**sent_0*,*mask*?*sep+* | {0: \"Dad\", 1: \"actually\"}\n","03/08/2022 08:40:00 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:40:00 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-29777', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-40-00_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-29777', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:40:00 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:40:01 - INFO - src.dataset -   Label 0 to word ƒ†Dad (13404)\n","03/08/2022 08:40:01 - INFO - src.dataset -   Label 1 to word ƒ†actually (888)\n","03/08/2022 08:40:01 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:40:01 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:40:01 - INFO - filelock -   Lock 140056565926800 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:40:01 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 08:40:01 - INFO - filelock -   Lock 140056565926800 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:40:01 - INFO - src.dataset -   Label 0 to word ƒ†Dad (13404)\n","03/08/2022 08:40:01 - INFO - src.dataset -   Label 1 to word ƒ†actually (888)\n","03/08/2022 08:40:01 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:40:01 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:40:01 - INFO - filelock -   Lock 140056565926800 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:40:01 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:40:01 - INFO - filelock -   Lock 140056565926800 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:40:01 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:40:01 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:40:01 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/08/2022 08:40:01 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:40:16 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:40:16 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:40:16 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:40:16 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:40:16 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:40:16 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:40:16 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:15,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.66it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.59it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.41it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.09it/s]\u001b[A03/08/2022 08:41:01 - INFO - src.trainer -   Best dev result: 0.8803749680519104\n","Epoch:  20% 49/250 [01:32<05:34,  1.67s/it]\n","13it [00:52, 15.06s/it]            \u001b[A\n","14it [00:52, 10.61s/it]\u001b[A\n","15it [00:52,  7.50s/it]\u001b[A\n","16it [00:52,  5.32s/it]\u001b[A\n","17it [00:53,  3.79s/it]\u001b[A\n","18it [00:53,  2.72s/it]\u001b[A\n","19it [00:53,  1.98s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:54,  1.09s/it]\u001b[A\n","22it [00:54,  1.20it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 08:41:53 - INFO - src.trainer -   Best dev result: 0.8812499642372131\n","Epoch:  30% 74/250 [02:23<04:53,  1.67s/it]\n","25it [01:43, 15.01s/it]\u001b[A\n","26it [01:43, 10.57s/it]\u001b[A\n","27it [01:43,  7.47s/it]\u001b[A\n","28it [01:44,  5.30s/it]\u001b[A\n","29it [01:44,  3.78s/it]\u001b[A\n","30it [01:44,  2.71s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:45,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","Epoch:  40% 99/250 [03:08<04:10,  1.66s/it]\n","37it [02:27, 12.88s/it]\u001b[A\n","38it [02:27,  9.09s/it]\u001b[A\n","39it [02:28,  6.43s/it]\u001b[A\n","40it [02:28,  4.57s/it]\u001b[A\n","41it [02:28,  3.27s/it]\u001b[A\n","42it [02:28,  2.36s/it]\u001b[A\n","43it [02:29,  1.72s/it]\u001b[A\n","44it [02:29,  1.27s/it]\u001b[A\n","45it [02:29,  1.04it/s]\u001b[A\n","46it [02:29,  1.35it/s]\u001b[A\n","47it [02:30,  1.70it/s]\u001b[A\n","48it [02:30,  2.22it/s]\u001b[A03/08/2022 08:43:29 - INFO - src.trainer -   Best dev result: 0.8830000162124634\n","Epoch:  50% 124/250 [04:00<03:30,  1.67s/it]\n","49it [03:19, 15.23s/it]\u001b[A\n","50it [03:20, 10.73s/it]\u001b[A\n","51it [03:20,  7.58s/it]\u001b[A\n","52it [03:20,  5.38s/it]\u001b[A\n","53it [03:20,  3.83s/it]\u001b[A\n","54it [03:21,  2.75s/it]\u001b[A\n","55it [03:21,  2.00s/it]\u001b[A\n","56it [03:21,  1.47s/it]\u001b[A\n","57it [03:21,  1.10s/it]\u001b[A\n","58it [03:21,  1.20it/s]\u001b[A\n","59it [03:22,  1.53it/s]\u001b[A\n","Epoch:  60% 149/250 [04:44<02:47,  1.66s/it]\n","61it [04:04, 12.87s/it]\u001b[A\n","62it [04:04,  9.08s/it]\u001b[A\n","63it [04:04,  6.43s/it]\u001b[A\n","64it [04:04,  4.57s/it]\u001b[A\n","65it [04:05,  3.27s/it]\u001b[A\n","66it [04:05,  2.36s/it]\u001b[A\n","67it [04:05,  1.72s/it]\u001b[A\n","68it [04:05,  1.27s/it]\u001b[A\n","69it [04:05,  1.04it/s]\u001b[A\n","70it [04:06,  1.35it/s]\u001b[A\n","71it [04:06,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:28<02:06,  1.66s/it]\n","73it [04:48, 12.84s/it]\u001b[A\n","74it [04:48,  9.06s/it]\u001b[A\n","75it [04:48,  6.41s/it]\u001b[A\n","76it [04:48,  4.56s/it]\u001b[A\n","77it [04:49,  3.26s/it]\u001b[A\n","78it [04:49,  2.35s/it]\u001b[A\n","79it [04:49,  1.71s/it]\u001b[A\n","80it [04:49,  1.27s/it]\u001b[A\n","81it [04:50,  1.04it/s]\u001b[A\n","82it [04:50,  1.35it/s]\u001b[A\n","83it [04:50,  1.70it/s]\u001b[A\n","Epoch:  80% 199/250 [06:12<01:24,  1.66s/it]\n","85it [05:32, 12.84s/it]\u001b[A\n","86it [05:32,  9.06s/it]\u001b[A\n","87it [05:32,  6.41s/it]\u001b[A\n","88it [05:33,  4.56s/it]\u001b[A\n","89it [05:33,  3.26s/it]\u001b[A\n","90it [05:33,  2.35s/it]\u001b[A\n","91it [05:33,  1.72s/it]\u001b[A\n","92it [05:34,  1.27s/it]\u001b[A\n","93it [05:34,  1.04it/s]\u001b[A\n","94it [05:34,  1.35it/s]\u001b[A\n","95it [05:34,  1.70it/s]\u001b[A\n","Epoch:  90% 224/250 [06:57<00:43,  1.66s/it]\n","97it [06:16, 12.85s/it]\u001b[A\n","98it [06:16,  9.07s/it]\u001b[A\n","99it [06:17,  6.42s/it]\u001b[A\n","100it [06:17,  4.56s/it]\u001b[A\n","101it [06:17,  3.26s/it]\u001b[A\n","102it [06:17,  2.35s/it]\u001b[A\n","103it [06:18,  1.72s/it]\u001b[A\n","104it [06:18,  1.27s/it]\u001b[A\n","105it [06:18,  1.04it/s]\u001b[A\n","106it [06:18,  1.35it/s]\u001b[A\n","107it [06:19,  1.70it/s]\u001b[A\n","Epoch: 100% 249/250 [07:41<00:01,  1.66s/it]\n","109it [07:00, 12.84s/it]\u001b[A\n","110it [07:01,  9.06s/it]\u001b[A\n","111it [07:01,  6.41s/it]\u001b[A\n","112it [07:01,  4.56s/it]\u001b[A\n","113it [07:01,  3.26s/it]\u001b[A\n","114it [07:02,  2.35s/it]\u001b[A\n","115it [07:02,  1.71s/it]\u001b[A\n","116it [07:02,  1.27s/it]\u001b[A\n","117it [07:02,  1.04it/s]\u001b[A\n","118it [07:02,  1.35it/s]\u001b[A\n","119it [07:03,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:45<00:00,  1.86s/it]\n","03/08/2022 08:48:02 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:48:16 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:17,  4.67s/it]\u001b[A\n","122it [07:18,  3.34s/it]\u001b[A\n","123it [07:18,  2.41s/it]\u001b[A\n","124it [07:18,  1.75s/it]\u001b[A\n","125it [07:18,  1.30s/it]\u001b[A\n","126it [07:19,  1.02it/s]\u001b[A\n","127it [07:19,  1.33it/s]\u001b[A\n","128it [07:19,  1.68it/s]\u001b[A\n","129it [07:19,  2.05it/s]\u001b[A\n","130it [07:19,  2.44it/s]\u001b[A\n","131it [07:20,  2.80it/s]\u001b[A\n","132it [07:20,  3.48it/s]\u001b[A03/08/2022 08:48:19 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:48:19 - INFO - __main__ -     eval_loss = 3.3439133167266846\n","03/08/2022 08:48:19 - INFO - __main__ -     eval_auroc = 0.8830000162124634\n","03/08/2022 08:48:19 - INFO - __main__ -     eval_recall = 0.9599999785423279\n","03/08/2022 08:48:19 - INFO - __main__ -     eval_f1 = 0.4528301954269409\n","03/08/2022 08:48:19 - INFO - filelock -   Lock 140056477513808 acquired on log.lock\n","03/08/2022 08:48:19 - INFO - filelock -   Lock 140056477513808 released on log.lock\n","132it [07:20,  3.34s/it]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/08/2022 08:48:25 - INFO - __main__ -   Specify load the 99-th prompt: *cls**sent_0*,*mask*?*sep+* | {0: \"people\", 1: \"ha\"}\n","03/08/2022 08:48:25 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/08/2022 08:48:25 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-16-21-roberta-large-27119', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar08_08-48-25_147a699b497f', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-16-21-roberta-large-27119', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=True)\n","03/08/2022 08:48:25 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/08/2022 08:48:26 - INFO - src.dataset -   Label 0 to word ƒ†people (82)\n","03/08/2022 08:48:26 - INFO - src.dataset -   Label 1 to word ƒ†ha (2489)\n","03/08/2022 08:48:26 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/08/2022 08:48:26 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:48:26 - INFO - filelock -   Lock 139700923133392 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:48:26 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/08/2022 08:48:26 - INFO - filelock -   Lock 139700923133392 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:48:26 - INFO - src.dataset -   Label 0 to word ƒ†people (82)\n","03/08/2022 08:48:26 - INFO - src.dataset -   Label 1 to word ƒ†ha (2489)\n","03/08/2022 08:48:26 - INFO - src.dataset -   Total num_sample for mode dev: 1\n","03/08/2022 08:48:26 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/08/2022 08:48:26 - INFO - filelock -   Lock 139700927142608 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:48:26 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/08/2022 08:48:26 - INFO - filelock -   Lock 139700927142608 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/08/2022 08:48:26 - INFO - src.dataset -   *** Example ***\n","03/08/2022 08:48:26 - INFO - src.dataset -   guid: dev-0\n","03/08/2022 08:48:26 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/08/2022 08:48:26 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/08/2022 08:48:42 - INFO - src.trainer -   ***** Running training *****\n","03/08/2022 08:48:42 - INFO - src.trainer -     Num examples = 32\n","03/08/2022 08:48:42 - INFO - src.trainer -     Num Epochs = 250\n","03/08/2022 08:48:42 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/08/2022 08:48:42 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/08/2022 08:48:42 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/08/2022 08:48:42 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:14,  1.66s/it]\n","  0% 0/12 [00:00<?, ?it/s]\u001b[A\n"," 17% 2/12 [00:00<00:01,  8.66it/s]\u001b[A\n"," 25% 3/12 [00:00<00:01,  6.65it/s]\u001b[A\n"," 33% 4/12 [00:00<00:01,  5.73it/s]\u001b[A\n"," 42% 5/12 [00:00<00:01,  5.22it/s]\u001b[A\n"," 50% 6/12 [00:01<00:01,  4.91it/s]\u001b[A\n"," 58% 7/12 [00:01<00:01,  4.72it/s]\u001b[A\n"," 67% 8/12 [00:01<00:00,  4.60it/s]\u001b[A\n"," 75% 9/12 [00:01<00:00,  4.51it/s]\u001b[A\n"," 83% 10/12 [00:02<00:00,  4.45it/s]\u001b[A\n"," 92% 11/12 [00:02<00:00,  4.42it/s]\u001b[A\n","100% 12/12 [00:02<00:00,  5.10it/s]\u001b[A03/08/2022 08:49:26 - INFO - src.trainer -   Best dev result: 0.8546249866485596\n","Epoch:  20% 49/250 [01:32<05:35,  1.67s/it]\n","13it [00:52, 15.02s/it]            \u001b[A\n","14it [00:52, 10.58s/it]\u001b[A\n","15it [00:52,  7.48s/it]\u001b[A\n","16it [00:52,  5.30s/it]\u001b[A\n","17it [00:52,  3.78s/it]\u001b[A\n","18it [00:53,  2.72s/it]\u001b[A\n","19it [00:53,  1.97s/it]\u001b[A\n","20it [00:53,  1.45s/it]\u001b[A\n","21it [00:53,  1.08s/it]\u001b[A\n","22it [00:54,  1.21it/s]\u001b[A\n","23it [00:54,  1.54it/s]\u001b[A\n","24it [00:54,  2.03it/s]\u001b[A03/08/2022 08:50:18 - INFO - src.trainer -   Best dev result: 0.8596249222755432\n","Epoch:  30% 74/250 [02:23<04:53,  1.67s/it]\n","25it [01:43, 15.00s/it]\u001b[A\n","26it [01:43, 10.57s/it]\u001b[A\n","27it [01:43,  7.47s/it]\u001b[A\n","28it [01:44,  5.30s/it]\u001b[A\n","29it [01:44,  3.78s/it]\u001b[A\n","30it [01:44,  2.71s/it]\u001b[A\n","31it [01:44,  1.97s/it]\u001b[A\n","32it [01:44,  1.45s/it]\u001b[A\n","33it [01:45,  1.08s/it]\u001b[A\n","34it [01:45,  1.21it/s]\u001b[A\n","35it [01:45,  1.54it/s]\u001b[A\n","36it [01:45,  2.03it/s]\u001b[A03/08/2022 08:51:09 - INFO - src.trainer -   Best dev result: 0.8643750548362732\n","Epoch:  40% 99/250 [03:14<04:11,  1.67s/it]\n","37it [02:34, 15.01s/it]\u001b[A\n","38it [02:34, 10.58s/it]\u001b[A\n","39it [02:35,  7.47s/it]\u001b[A\n","40it [02:35,  5.30s/it]\u001b[A\n","41it [02:35,  3.78s/it]\u001b[A\n","42it [02:35,  2.72s/it]\u001b[A\n","43it [02:36,  1.97s/it]\u001b[A\n","44it [02:36,  1.45s/it]\u001b[A\n","45it [02:36,  1.08s/it]\u001b[A\n","46it [02:36,  1.21it/s]\u001b[A\n","47it [02:36,  1.54it/s]\u001b[A\n","Epoch:  50% 124/250 [03:58<03:28,  1.66s/it]\n","49it [03:18, 12.85s/it]\u001b[A\n","50it [03:19,  9.06s/it]\u001b[A\n","51it [03:19,  6.41s/it]\u001b[A\n","52it [03:19,  4.56s/it]\u001b[A\n","53it [03:19,  3.26s/it]\u001b[A\n","54it [03:19,  2.35s/it]\u001b[A\n","55it [03:20,  1.72s/it]\u001b[A\n","56it [03:20,  1.27s/it]\u001b[A\n","57it [03:20,  1.04it/s]\u001b[A\n","58it [03:20,  1.35it/s]\u001b[A\n","59it [03:21,  1.70it/s]\u001b[A\n","Epoch:  60% 149/250 [04:43<02:48,  1.66s/it]\n","61it [04:02, 12.83s/it]\u001b[A\n","62it [04:03,  9.05s/it]\u001b[A\n","63it [04:03,  6.40s/it]\u001b[A\n","64it [04:03,  4.55s/it]\u001b[A\n","65it [04:03,  3.26s/it]\u001b[A\n","66it [04:04,  2.35s/it]\u001b[A\n","67it [04:04,  1.71s/it]\u001b[A\n","68it [04:04,  1.27s/it]\u001b[A\n","69it [04:04,  1.04it/s]\u001b[A\n","70it [04:05,  1.35it/s]\u001b[A\n","71it [04:05,  1.70it/s]\u001b[A\n","Epoch:  70% 174/250 [05:27<02:05,  1.65s/it]\n","73it [04:47, 12.80s/it]\u001b[A\n","74it [04:47,  9.03s/it]\u001b[A\n","75it [04:47,  6.39s/it]\u001b[A\n","76it [04:47,  4.54s/it]\u001b[A\n","77it [04:47,  3.25s/it]\u001b[A\n","78it [04:48,  2.34s/it]\u001b[A\n","79it [04:48,  1.71s/it]\u001b[A\n","80it [04:48,  1.27s/it]\u001b[A\n","81it [04:48,  1.05it/s]\u001b[A\n","82it [04:49,  1.35it/s]\u001b[A\n","83it [04:49,  1.71it/s]\u001b[A\n","Epoch:  80% 199/250 [06:11<01:24,  1.66s/it]\n","85it [05:31, 12.79s/it]\u001b[A\n","86it [05:31,  9.02s/it]\u001b[A\n","87it [05:31,  6.39s/it]\u001b[A\n","88it [05:31,  4.54s/it]\u001b[A\n","89it [05:31,  3.25s/it]\u001b[A\n","90it [05:32,  2.34s/it]\u001b[A\n","91it [05:32,  1.71s/it]\u001b[A\n","92it [05:32,  1.27s/it]\u001b[A\n","93it [05:32,  1.05it/s]\u001b[A\n","94it [05:33,  1.36it/s]\u001b[A\n","95it [05:33,  1.71it/s]\u001b[A\n","Epoch:  90% 224/250 [06:55<00:42,  1.65s/it]\n","97it [06:15, 12.78s/it]\u001b[A\n","98it [06:15,  9.02s/it]\u001b[A\n","99it [06:15,  6.38s/it]\u001b[A\n","100it [06:15,  4.54s/it]\u001b[A\n","101it [06:15,  3.24s/it]\u001b[A\n","102it [06:16,  2.34s/it]\u001b[A\n","103it [06:16,  1.71s/it]\u001b[A\n","104it [06:16,  1.26s/it]\u001b[A\n","105it [06:16,  1.05it/s]\u001b[A\n","106it [06:17,  1.36it/s]\u001b[A\n","107it [06:17,  1.71it/s]\u001b[A\n","Epoch: 100% 249/250 [07:39<00:01,  1.66s/it]\n","109it [06:59, 12.83s/it]\u001b[A\n","110it [06:59,  9.05s/it]\u001b[A\n","111it [06:59,  6.40s/it]\u001b[A\n","112it [06:59,  4.55s/it]\u001b[A\n","113it [07:00,  3.26s/it]\u001b[A\n","114it [07:00,  2.35s/it]\u001b[A\n","115it [07:00,  1.71s/it]\u001b[A\n","116it [07:00,  1.27s/it]\u001b[A\n","117it [07:01,  1.04it/s]\u001b[A\n","118it [07:01,  1.35it/s]\u001b[A\n","119it [07:01,  1.70it/s]\u001b[A\n","Epoch: 100% 250/250 [07:43<00:00,  1.85s/it]\n","03/08/2022 08:56:25 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/08/2022 08:56:39 - INFO - __main__ -   *** Validate ***\n","\n","121it [07:15,  4.61s/it]\u001b[A\n","122it [07:16,  3.30s/it]\u001b[A\n","123it [07:16,  2.38s/it]\u001b[A\n","124it [07:16,  1.73s/it]\u001b[A\n","125it [07:16,  1.28s/it]\u001b[A\n","126it [07:17,  1.03it/s]\u001b[A\n","127it [07:17,  1.34it/s]\u001b[A\n","128it [07:17,  1.69it/s]\u001b[A\n","129it [07:17,  2.07it/s]\u001b[A\n","130it [07:18,  2.45it/s]\u001b[A\n","131it [07:18,  2.82it/s]\u001b[A\n","132it [07:18,  3.49it/s]\u001b[A03/08/2022 08:56:42 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/08/2022 08:56:42 - INFO - __main__ -     eval_loss = 2.8205087184906006\n","03/08/2022 08:56:42 - INFO - __main__ -     eval_auroc = 0.8643750548362732\n","03/08/2022 08:56:42 - INFO - __main__ -     eval_recall = 0.8799999952316284\n","03/08/2022 08:56:42 - INFO - __main__ -     eval_f1 = 0.4888889193534851\n","03/08/2022 08:56:42 - INFO - filelock -   Lock 139700863701840 acquired on log.lock\n","03/08/2022 08:56:42 - INFO - filelock -   Lock 139700863701840 released on log.lock\n","132it [07:18,  3.32s/it]\n"]}],"source":["# !source env/bin/activate; bash label_search.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4147529,"status":"ok","timestamp":1647251470532,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"},"user_tz":420},"id":"6Or6nfVP3VjV","outputId":"0964e5fc-7905-487d-a561-4b055a95a2ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","2224it [15:36,  4.33it/s]\u001b[A\n","2225it [15:36,  4.33it/s]\u001b[A\n","2226it [15:36,  4.33it/s]\u001b[A\n","2227it [15:36,  4.33it/s]\u001b[A\n","2228it [15:37,  4.33it/s]\u001b[A\n","2229it [15:37,  4.33it/s]\u001b[A\n","2230it [15:37,  4.33it/s]\u001b[A\n","2231it [15:37,  4.33it/s]\u001b[A\n","2232it [15:38,  4.33it/s]\u001b[A\n","2233it [15:38,  4.33it/s]\u001b[A\n","2234it [15:38,  4.33it/s]\u001b[A\n","2235it [15:38,  4.33it/s]\u001b[A\n","2236it [15:39,  4.33it/s]\u001b[A\n","2237it [15:39,  4.33it/s]\u001b[A\n","2238it [15:39,  4.33it/s]\u001b[A\n","2239it [15:39,  4.33it/s]\u001b[A\n","2240it [15:39,  4.33it/s]\u001b[A\n","2241it [15:40,  4.33it/s]\u001b[A\n","2242it [15:40,  4.33it/s]\u001b[A\n","2243it [15:40,  4.33it/s]\u001b[A\n","2244it [15:40,  4.33it/s]\u001b[A\n","2245it [15:41,  4.33it/s]\u001b[A\n","2246it [15:41,  4.33it/s]\u001b[A\n","2247it [15:41,  4.33it/s]\u001b[A\n","2248it [15:41,  4.33it/s]\u001b[A\n","2249it [15:42,  4.33it/s]\u001b[A\n","2250it [15:42,  4.33it/s]\u001b[A\n","2251it [15:42,  4.33it/s]\u001b[A\n","2252it [15:42,  4.33it/s]\u001b[A\n","2253it [15:42,  4.33it/s]\u001b[A\n","2254it [15:43,  4.33it/s]\u001b[A\n","2255it [15:43,  4.33it/s]\u001b[A\n","2256it [15:43,  4.33it/s]\u001b[A\n","2257it [15:43,  4.33it/s]\u001b[A\n","2258it [15:44,  4.33it/s]\u001b[A\n","2259it [15:44,  4.33it/s]\u001b[A\n","2260it [15:44,  4.33it/s]\u001b[A\n","2261it [15:44,  4.33it/s]\u001b[A\n","2262it [15:45,  4.33it/s]\u001b[A\n","2263it [15:45,  4.33it/s]\u001b[A\n","2264it [15:45,  4.33it/s]\u001b[A\n","2265it [15:45,  4.33it/s]\u001b[A\n","2266it [15:45,  4.33it/s]\u001b[A\n","2267it [15:46,  4.33it/s]\u001b[A\n","2268it [15:46,  4.33it/s]\u001b[A\n","2269it [15:46,  4.33it/s]\u001b[A\n","2270it [15:46,  4.33it/s]\u001b[A\n","2271it [15:47,  4.33it/s]\u001b[A\n","2272it [15:47,  4.33it/s]\u001b[A\n","2273it [15:47,  4.33it/s]\u001b[A\n","2274it [15:47,  4.33it/s]\u001b[A\n","2275it [15:48,  4.33it/s]\u001b[A\n","2276it [15:48,  4.33it/s]\u001b[A\n","2277it [15:48,  4.33it/s]\u001b[A\n","2278it [15:48,  4.33it/s]\u001b[A\n","2279it [15:48,  4.33it/s]\u001b[A\n","2280it [15:49,  4.33it/s]\u001b[A\n","2281it [15:49,  4.33it/s]\u001b[A\n","2282it [15:49,  4.33it/s]\u001b[A\n","2283it [15:49,  4.33it/s]\u001b[A\n","2284it [15:50,  4.33it/s]\u001b[A\n","2285it [15:50,  4.33it/s]\u001b[A\n","2286it [15:50,  4.33it/s]\u001b[A\n","2287it [15:50,  4.33it/s]\u001b[A\n","2288it [15:51,  4.33it/s]\u001b[A\n","2289it [15:51,  4.33it/s]\u001b[A\n","2290it [15:51,  4.33it/s]\u001b[A\n","2291it [15:51,  4.33it/s]\u001b[A\n","2292it [15:51,  4.33it/s]\u001b[A\n","2293it [15:52,  4.33it/s]\u001b[A\n","2294it [15:52,  4.33it/s]\u001b[A\n","2295it [15:52,  4.33it/s]\u001b[A\n","2296it [15:52,  4.33it/s]\u001b[A\n","2297it [15:53,  4.33it/s]\u001b[A\n","2298it [15:53,  4.33it/s]\u001b[A\n","2299it [15:53,  4.33it/s]\u001b[A\n","2300it [15:53,  4.33it/s]\u001b[A\n","2301it [15:54,  4.33it/s]\u001b[A\n","2302it [15:54,  4.33it/s]\u001b[A\n","2303it [15:54,  4.33it/s]\u001b[A\n","2304it [15:54,  4.33it/s]\u001b[A\n","2305it [15:54,  4.33it/s]\u001b[A\n","2306it [15:55,  4.33it/s]\u001b[A\n","2307it [15:55,  4.33it/s]\u001b[A\n","2308it [15:55,  4.33it/s]\u001b[A\n","2309it [15:55,  4.33it/s]\u001b[A\n","2310it [15:56,  4.33it/s]\u001b[A\n","2311it [15:56,  4.33it/s]\u001b[A\n","2312it [15:56,  4.33it/s]\u001b[A\n","2313it [15:56,  4.33it/s]\u001b[A\n","2314it [15:57,  4.33it/s]\u001b[A\n","2315it [15:57,  4.33it/s]\u001b[A\n","2316it [15:57,  4.33it/s]\u001b[A\n","2317it [15:57,  4.33it/s]\u001b[A\n","2318it [15:57,  4.33it/s]\u001b[A\n","2319it [15:58,  4.33it/s]\u001b[A\n","2320it [15:58,  4.33it/s]\u001b[A\n","2321it [15:58,  4.33it/s]\u001b[A\n","2322it [15:58,  4.33it/s]\u001b[A\n","2323it [15:59,  4.33it/s]\u001b[A\n","2324it [15:59,  4.33it/s]\u001b[A\n","2325it [15:59,  4.33it/s]\u001b[A\n","2326it [15:59,  4.33it/s]\u001b[A\n","2327it [16:00,  4.33it/s]\u001b[A\n","2328it [16:00,  4.33it/s]\u001b[A\n","2329it [16:00,  4.33it/s]\u001b[A\n","2330it [16:00,  4.33it/s]\u001b[A\n","2331it [16:00,  4.33it/s]\u001b[A\n","2332it [16:01,  4.33it/s]\u001b[A\n","2333it [16:01,  4.33it/s]\u001b[A\n","2334it [16:01,  4.33it/s]\u001b[A\n","2335it [16:01,  4.33it/s]\u001b[A\n","2336it [16:02,  4.33it/s]\u001b[A\n","2337it [16:02,  4.33it/s]\u001b[A\n","2338it [16:02,  4.33it/s]\u001b[A\n","2339it [16:02,  4.33it/s]\u001b[A\n","2340it [16:03,  4.33it/s]\u001b[A\n","2341it [16:03,  4.33it/s]\u001b[A\n","2342it [16:03,  4.33it/s]\u001b[A\n","2343it [16:03,  4.33it/s]\u001b[A\n","2344it [16:03,  4.33it/s]\u001b[A\n","2345it [16:04,  4.33it/s]\u001b[A\n","2346it [16:04,  4.33it/s]\u001b[A\n","2347it [16:04,  4.33it/s]\u001b[A\n","2348it [16:04,  4.34it/s]\u001b[A\n","2349it [16:05,  4.34it/s]\u001b[A\n","2350it [16:05,  4.33it/s]\u001b[A\n","2351it [16:05,  4.33it/s]\u001b[A\n","2352it [16:05,  4.33it/s]\u001b[A\n","2353it [16:06,  4.33it/s]\u001b[A\n","2354it [16:06,  4.33it/s]\u001b[A03/14/2022 09:17:01 - INFO - __main__ -   ***** Test results spoilers *****\n","03/14/2022 09:17:01 - INFO - __main__ -     eval_loss = 5.440009593963623\n","03/14/2022 09:17:01 - INFO - __main__ -     eval_auroc = 0.824364423751831\n","03/14/2022 09:17:01 - INFO - __main__ -     eval_recall = 0.9166666865348816\n","03/14/2022 09:17:01 - INFO - __main__ -     eval_f1 = 0.24444447457790375\n","03/14/2022 09:17:01 - INFO - filelock -   Lock 140358899154512 acquired on log.lock\n","03/14/2022 09:17:02 - INFO - filelock -   Lock 140358899154512 released on log.lock\n","2354it [16:06,  2.44it/s]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/14/2022 09:17:08 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.sort.txt\n","03/14/2022 09:17:08 - INFO - __main__ -   Specify load the 18-th template: *cls*‚ñÅYes!*mask*!*+sent_0**sep+*\n","03/14/2022 09:17:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/14/2022 09:17:08 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-demo-16-21-roberta-large-13811', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar14_09-17-08_eda9c4d80d5c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-demo-16-21-roberta-large-13811', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=0, model_id=18, save_logit=True, save_logit_dir='ensemble_predict_results', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","03/14/2022 09:17:08 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/14/2022 09:17:08 - INFO - __main__ -   Automatically convert the template to using demonstrations.\n","03/14/2022 09:17:08 - INFO - __main__ -   | *cls*‚ñÅYes!*mask*!*+sent_0**sep+* => *cls*‚ñÅYes!*mask*!*+sent_0**sep+*‚ñÅYes!*label_0*!*+sent_1**sep+*‚ñÅYes!*label_1*!*+sent_2**sep+*\n","03/14/2022 09:17:09 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:17:09 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/14/2022 09:17:09 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/14/2022 09:17:09 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/14/2022 09:17:09 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:17:09 - INFO - filelock -   Lock 140288453070416 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:09 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.001 s]\n","03/14/2022 09:17:09 - INFO - filelock -   Lock 140288453070416 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:09 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:17:09 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/14/2022 09:17:09 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/14/2022 09:17:09 - INFO - src.dataset -   Total num_sample for mode dev: 16\n","03/14/2022 09:17:09 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:17:09 - INFO - filelock -   Lock 140288457060944 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:09 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/14/2022 09:17:09 - INFO - filelock -   Lock 140288457060944 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:09 - INFO - src.dataset -   *** Example ***\n","03/14/2022 09:17:09 - INFO - src.dataset -   guid: dev-0\n","03/14/2022 09:17:09 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 28361, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 2, 48584, 10172, 9904, 328, 440, 328, 61, 74, 28, 6474, 9724, 2, 48584, 10172, 9904, 328, 3216, 328, 821, 10810, 31985, 364, 329, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[5], label_word_list=None)\n","03/14/2022 09:17:09 - INFO - src.dataset -   text: <s>‚ñÅYes!<mask>! Guys, why goth mikasa was shown?</s>‚ñÅYes! No! which would be lit af</s>‚ñÅYes! Yes! gabi sniper ez</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","03/14/2022 09:17:12 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:17:12 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/14/2022 09:17:12 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/14/2022 09:17:12 - INFO - src.dataset -   Total num_sample for mode test: 16\n","03/14/2022 09:17:12 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:17:12 - INFO - filelock -   Lock 140288453070416 acquired on data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:12 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/14/2022 09:17:12 - INFO - filelock -   Lock 140288453070416 released on data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:17:12 - INFO - src.dataset -   *** Example ***\n","03/14/2022 09:17:12 - INFO - src.dataset -   guid: test-0\n","03/14/2022 09:17:12 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 48584, 10172, 9904, 328, 50264, 328, 364, 2558, 18, 269, 15867, 154, 123, 2, 48584, 10172, 9904, 328, 440, 328, 2230, 2, 48584, 10172, 9904, 328, 3216, 328, 821, 10810, 31985, 364, 329, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[5], label_word_list=None)\n","03/14/2022 09:17:12 - INFO - src.dataset -   text: <s>‚ñÅYes!<mask>! eren's really battering him</s>‚ñÅYes! No! exactly</s>‚ñÅYes! Yes! gabi sniper ez</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/14/2022 09:17:34 - INFO - src.trainer -   ***** Running training *****\n","03/14/2022 09:17:34 - INFO - src.trainer -     Num examples = 32\n","03/14/2022 09:17:34 - INFO - src.trainer -     Num Epochs = 250\n","03/14/2022 09:17:34 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/14/2022 09:17:34 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/14/2022 09:17:34 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/14/2022 09:17:34 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:19,  1.68s/it]\n","  0% 0/185 [00:00<?, ?it/s]\u001b[A\n","  1% 2/185 [00:00<00:21,  8.68it/s]\u001b[A\n","  2% 3/185 [00:00<00:27,  6.67it/s]\u001b[A\n","  2% 4/185 [00:00<00:31,  5.74it/s]\u001b[A\n","  3% 5/185 [00:00<00:34,  5.23it/s]\u001b[A\n","  3% 6/185 [00:01<00:36,  4.93it/s]\u001b[A\n","  4% 7/185 [00:01<00:37,  4.73it/s]\u001b[A\n","  4% 8/185 [00:01<00:38,  4.60it/s]\u001b[A\n","  5% 9/185 [00:01<00:38,  4.52it/s]\u001b[A\n","  5% 10/185 [00:02<00:39,  4.46it/s]\u001b[A\n","  6% 11/185 [00:02<00:39,  4.42it/s]\u001b[A\n","  6% 12/185 [00:02<00:39,  4.39it/s]\u001b[A\n","  7% 13/185 [00:02<00:39,  4.37it/s]\u001b[A\n","  8% 14/185 [00:03<00:39,  4.36it/s]\u001b[A\n","  8% 15/185 [00:03<00:39,  4.35it/s]\u001b[A\n","  9% 16/185 [00:03<00:38,  4.35it/s]\u001b[A\n","  9% 17/185 [00:03<00:38,  4.34it/s]\u001b[A\n"," 10% 18/185 [00:03<00:38,  4.34it/s]\u001b[A\n"," 10% 19/185 [00:04<00:38,  4.34it/s]\u001b[A\n"," 11% 20/185 [00:04<00:38,  4.34it/s]\u001b[A\n"," 11% 21/185 [00:04<00:37,  4.33it/s]\u001b[A\n"," 12% 22/185 [00:04<00:37,  4.33it/s]\u001b[A\n"," 12% 23/185 [00:05<00:37,  4.33it/s]\u001b[A\n"," 13% 24/185 [00:05<00:37,  4.33it/s]\u001b[A\n"," 14% 25/185 [00:05<00:36,  4.33it/s]\u001b[A\n"," 14% 26/185 [00:05<00:36,  4.33it/s]\u001b[A\n"," 15% 27/185 [00:06<00:36,  4.33it/s]\u001b[A\n"," 15% 28/185 [00:06<00:36,  4.33it/s]\u001b[A\n"," 16% 29/185 [00:06<00:36,  4.33it/s]\u001b[A\n"," 16% 30/185 [00:06<00:35,  4.33it/s]\u001b[A\n"," 17% 31/185 [00:06<00:35,  4.33it/s]\u001b[A\n"," 17% 32/185 [00:07<00:35,  4.33it/s]\u001b[A\n"," 18% 33/185 [00:07<00:35,  4.33it/s]\u001b[A\n"," 18% 34/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 35/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 36/185 [00:08<00:34,  4.33it/s]\u001b[A\n"," 20% 37/185 [00:08<00:34,  4.33it/s]\u001b[A\n"," 21% 38/185 [00:08<00:33,  4.33it/s]\u001b[A\n"," 21% 39/185 [00:08<00:33,  4.33it/s]\u001b[A\n"," 22% 40/185 [00:09<00:33,  4.33it/s]\u001b[A\n"," 22% 41/185 [00:09<00:33,  4.33it/s]\u001b[A\n"," 23% 42/185 [00:09<00:33,  4.33it/s]\u001b[A\n"," 23% 43/185 [00:09<00:32,  4.33it/s]\u001b[A\n"," 24% 44/185 [00:09<00:32,  4.33it/s]\u001b[A\n"," 24% 45/185 [00:10<00:32,  4.33it/s]\u001b[A\n"," 25% 46/185 [00:10<00:32,  4.33it/s]\u001b[A\n"," 25% 47/185 [00:10<00:31,  4.33it/s]\u001b[A\n"," 26% 48/185 [00:10<00:31,  4.33it/s]\u001b[A\n"," 26% 49/185 [00:11<00:31,  4.33it/s]\u001b[A\n"," 27% 50/185 [00:11<00:31,  4.33it/s]\u001b[A\n"," 28% 51/185 [00:11<00:30,  4.33it/s]\u001b[A\n"," 28% 52/185 [00:11<00:30,  4.33it/s]\u001b[A\n"," 29% 53/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 29% 54/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 30% 55/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 30% 56/185 [00:12<00:29,  4.33it/s]\u001b[A\n"," 31% 57/185 [00:12<00:29,  4.33it/s]\u001b[A\n"," 31% 58/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 59/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 60/185 [00:13<00:28,  4.33it/s]\u001b[A\n"," 33% 61/185 [00:13<00:28,  4.33it/s]\u001b[A\n"," 34% 62/185 [00:14<00:28,  4.33it/s]\u001b[A\n"," 34% 63/185 [00:14<00:28,  4.33it/s]\u001b[A\n"," 35% 64/185 [00:14<00:27,  4.33it/s]\u001b[A\n"," 35% 65/185 [00:14<00:27,  4.33it/s]\u001b[A\n"," 36% 66/185 [00:15<00:27,  4.32it/s]\u001b[A\n"," 36% 67/185 [00:15<00:27,  4.32it/s]\u001b[A\n"," 37% 68/185 [00:15<00:27,  4.32it/s]\u001b[A\n"," 37% 69/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 38% 70/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 38% 71/185 [00:16<00:26,  4.33it/s]\u001b[A\n"," 39% 72/185 [00:16<00:26,  4.33it/s]\u001b[A\n"," 39% 73/185 [00:16<00:25,  4.33it/s]\u001b[A\n"," 40% 74/185 [00:16<00:25,  4.33it/s]\u001b[A\n"," 41% 75/185 [00:17<00:25,  4.33it/s]\u001b[A\n"," 41% 76/185 [00:17<00:25,  4.33it/s]\u001b[A\n"," 42% 77/185 [00:17<00:24,  4.33it/s]\u001b[A\n"," 42% 78/185 [00:17<00:24,  4.33it/s]\u001b[A\n"," 43% 79/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 43% 80/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 44% 81/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 44% 82/185 [00:18<00:23,  4.33it/s]\u001b[A\n"," 45% 83/185 [00:18<00:23,  4.33it/s]\u001b[A\n"," 45% 84/185 [00:19<00:23,  4.33it/s]\u001b[A\n"," 46% 85/185 [00:19<00:23,  4.33it/s]\u001b[A\n"," 46% 86/185 [00:19<00:22,  4.33it/s]\u001b[A\n"," 47% 87/185 [00:19<00:22,  4.33it/s]\u001b[A\n"," 48% 88/185 [00:20<00:22,  4.33it/s]\u001b[A\n"," 48% 89/185 [00:20<00:22,  4.33it/s]\u001b[A\n"," 49% 90/185 [00:20<00:21,  4.33it/s]\u001b[A\n"," 49% 91/185 [00:20<00:21,  4.33it/s]\u001b[A\n"," 50% 92/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 50% 93/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 51% 94/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 51% 95/185 [00:21<00:20,  4.33it/s]\u001b[A\n"," 52% 96/185 [00:21<00:20,  4.33it/s]\u001b[A\n"," 52% 97/185 [00:22<00:20,  4.33it/s]\u001b[A\n"," 53% 98/185 [00:22<00:20,  4.33it/s]\u001b[A\n"," 54% 99/185 [00:22<00:19,  4.33it/s]\u001b[A\n"," 54% 100/185 [00:22<00:19,  4.33it/s]\u001b[A\n"," 55% 101/185 [00:23<00:19,  4.33it/s]\u001b[A\n"," 55% 102/185 [00:23<00:19,  4.33it/s]\u001b[A\n"," 56% 103/185 [00:23<00:18,  4.33it/s]\u001b[A\n"," 56% 104/185 [00:23<00:18,  4.33it/s]\u001b[A\n"," 57% 105/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 57% 106/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 58% 107/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 58% 108/185 [00:24<00:17,  4.33it/s]\u001b[A\n"," 59% 109/185 [00:24<00:17,  4.33it/s]\u001b[A\n"," 59% 110/185 [00:25<00:17,  4.33it/s]\u001b[A\n"," 60% 111/185 [00:25<00:17,  4.33it/s]\u001b[A\n"," 61% 112/185 [00:25<00:16,  4.33it/s]\u001b[A\n"," 61% 113/185 [00:25<00:16,  4.33it/s]\u001b[A\n"," 62% 114/185 [00:26<00:16,  4.33it/s]\u001b[A\n"," 62% 115/185 [00:26<00:16,  4.33it/s]\u001b[A\n"," 63% 116/185 [00:26<00:15,  4.33it/s]\u001b[A\n"," 63% 117/185 [00:26<00:15,  4.33it/s]\u001b[A\n"," 64% 118/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 64% 119/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 65% 120/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 65% 121/185 [00:27<00:14,  4.33it/s]\u001b[A\n"," 66% 122/185 [00:27<00:14,  4.33it/s]\u001b[A\n"," 66% 123/185 [00:28<00:14,  4.33it/s]\u001b[A\n"," 67% 124/185 [00:28<00:14,  4.33it/s]\u001b[A\n"," 68% 125/185 [00:28<00:13,  4.33it/s]\u001b[A\n"," 68% 126/185 [00:28<00:13,  4.33it/s]\u001b[A\n"," 69% 127/185 [00:29<00:13,  4.33it/s]\u001b[A\n"," 69% 128/185 [00:29<00:13,  4.33it/s]\u001b[A\n"," 70% 129/185 [00:29<00:12,  4.33it/s]\u001b[A\n"," 70% 130/185 [00:29<00:12,  4.33it/s]\u001b[A\n"," 71% 131/185 [00:30<00:12,  4.33it/s]\u001b[A\n"," 71% 132/185 [00:30<00:12,  4.33it/s]\u001b[A\n"," 72% 133/185 [00:30<00:12,  4.33it/s]\u001b[A\n"," 72% 134/185 [00:30<00:11,  4.33it/s]\u001b[A\n"," 73% 135/185 [00:30<00:11,  4.33it/s]\u001b[A\n"," 74% 136/185 [00:31<00:11,  4.33it/s]\u001b[A\n"," 74% 137/185 [00:31<00:11,  4.33it/s]\u001b[A\n"," 75% 138/185 [00:31<00:10,  4.33it/s]\u001b[A\n"," 75% 139/185 [00:31<00:10,  4.33it/s]\u001b[A\n"," 76% 140/185 [00:32<00:10,  4.33it/s]\u001b[A\n"," 76% 141/185 [00:32<00:10,  4.33it/s]\u001b[A\n"," 77% 142/185 [00:32<00:09,  4.33it/s]\u001b[A\n"," 77% 143/185 [00:32<00:09,  4.33it/s]\u001b[A\n"," 78% 144/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 78% 145/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 79% 146/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 79% 147/185 [00:33<00:08,  4.33it/s]\u001b[A\n"," 80% 148/185 [00:33<00:08,  4.33it/s]\u001b[A\n"," 81% 149/185 [00:34<00:08,  4.33it/s]\u001b[A\n"," 81% 150/185 [00:34<00:08,  4.33it/s]\u001b[A\n"," 82% 151/185 [00:34<00:07,  4.33it/s]\u001b[A\n"," 82% 152/185 [00:34<00:07,  4.33it/s]\u001b[A\n"," 83% 153/185 [00:35<00:07,  4.33it/s]\u001b[A\n"," 83% 154/185 [00:35<00:07,  4.33it/s]\u001b[A\n"," 84% 155/185 [00:35<00:06,  4.33it/s]\u001b[A\n"," 84% 156/185 [00:35<00:06,  4.33it/s]\u001b[A\n"," 85% 157/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 85% 158/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 86% 159/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 86% 160/185 [00:36<00:05,  4.33it/s]\u001b[A\n"," 87% 161/185 [00:36<00:05,  4.33it/s]\u001b[A\n"," 88% 162/185 [00:37<00:05,  4.33it/s]\u001b[A\n"," 88% 163/185 [00:37<00:05,  4.33it/s]\u001b[A\n"," 89% 164/185 [00:37<00:04,  4.33it/s]\u001b[A\n"," 89% 165/185 [00:37<00:04,  4.33it/s]\u001b[A\n"," 90% 166/185 [00:38<00:04,  4.33it/s]\u001b[A\n"," 90% 167/185 [00:38<00:04,  4.33it/s]\u001b[A\n"," 91% 168/185 [00:38<00:03,  4.33it/s]\u001b[A\n"," 91% 169/185 [00:38<00:03,  4.33it/s]\u001b[A\n"," 92% 170/185 [00:39<00:03,  4.33it/s]\u001b[A\n"," 92% 171/185 [00:39<00:03,  4.33it/s]\u001b[A\n"," 93% 172/185 [00:39<00:03,  4.33it/s]\u001b[A\n"," 94% 173/185 [00:39<00:02,  4.33it/s]\u001b[A\n"," 94% 174/185 [00:39<00:02,  4.33it/s]\u001b[A\n"," 95% 175/185 [00:40<00:02,  4.33it/s]\u001b[A\n"," 95% 176/185 [00:40<00:02,  4.33it/s]\u001b[A\n"," 96% 177/185 [00:40<00:01,  4.33it/s]\u001b[A\n"," 96% 178/185 [00:40<00:01,  4.33it/s]\u001b[A\n"," 97% 179/185 [00:41<00:01,  4.33it/s]\u001b[A\n"," 97% 180/185 [00:41<00:01,  4.33it/s]\u001b[A\n"," 98% 181/185 [00:41<00:00,  4.33it/s]\u001b[A\n"," 98% 182/185 [00:41<00:00,  4.33it/s]\u001b[A\n"," 99% 183/185 [00:42<00:00,  4.34it/s]\u001b[A\n"," 99% 184/185 [00:42<00:00,  4.33it/s]\u001b[A\n","100% 185/185 [00:42<00:00,  4.33it/s]\u001b[A03/14/2022 09:18:59 - INFO - src.trainer -   Best dev result: 0.872749924659729\n","Epoch:  20% 49/250 [02:13<05:38,  1.69s/it]\n","186it [01:32, 15.21s/it]             \u001b[A\n","187it [01:32, 10.72s/it]\u001b[A\n","188it [01:33,  7.57s/it]\u001b[A\n","189it [01:33,  5.37s/it]\u001b[A\n","190it [01:33,  3.83s/it]\u001b[A\n","191it [01:33,  2.75s/it]\u001b[A\n","192it [01:34,  1.99s/it]\u001b[A\n","193it [01:34,  1.46s/it]\u001b[A\n","194it [01:34,  1.09s/it]\u001b[A\n","195it [01:34,  1.20it/s]\u001b[A\n","196it [01:34,  1.53it/s]\u001b[A\n","197it [01:35,  1.90it/s]\u001b[A\n","198it [01:35,  2.28it/s]\u001b[A\n","199it [01:35,  2.66it/s]\u001b[A\n","200it [01:35,  3.01it/s]\u001b[A\n","201it [01:36,  3.31it/s]\u001b[A\n","202it [01:36,  3.56it/s]\u001b[A\n","203it [01:36,  3.76it/s]\u001b[A\n","204it [01:36,  3.92it/s]\u001b[A\n","205it [01:37,  4.03it/s]\u001b[A\n","206it [01:37,  4.12it/s]\u001b[A\n","207it [01:37,  4.18it/s]\u001b[A\n","208it [01:37,  4.22it/s]\u001b[A\n","209it [01:37,  4.26it/s]\u001b[A\n","210it [01:38,  4.28it/s]\u001b[A\n","211it [01:38,  4.29it/s]\u001b[A\n","212it [01:38,  4.30it/s]\u001b[A\n","213it [01:38,  4.31it/s]\u001b[A\n","214it [01:39,  4.32it/s]\u001b[A\n","215it [01:39,  4.32it/s]\u001b[A\n","216it [01:39,  4.32it/s]\u001b[A\n","217it [01:39,  4.33it/s]\u001b[A\n","218it [01:40,  4.33it/s]\u001b[A\n","219it [01:40,  4.32it/s]\u001b[A\n","220it [01:40,  4.33it/s]\u001b[A\n","221it [01:40,  4.33it/s]\u001b[A\n","222it [01:40,  4.33it/s]\u001b[A\n","223it [01:41,  4.33it/s]\u001b[A\n","224it [01:41,  4.33it/s]\u001b[A\n","225it [01:41,  4.33it/s]\u001b[A\n","226it [01:41,  4.33it/s]\u001b[A\n","227it [01:42,  4.33it/s]\u001b[A\n","228it [01:42,  4.33it/s]\u001b[A\n","229it [01:42,  4.33it/s]\u001b[A\n","230it [01:42,  4.33it/s]\u001b[A\n","231it [01:43,  4.33it/s]\u001b[A\n","232it [01:43,  4.33it/s]\u001b[A\n","233it [01:43,  4.33it/s]\u001b[A\n","234it [01:43,  4.33it/s]\u001b[A\n","235it [01:43,  4.33it/s]\u001b[A\n","236it [01:44,  4.33it/s]\u001b[A\n","237it [01:44,  4.33it/s]\u001b[A\n","238it [01:44,  4.33it/s]\u001b[A\n","239it [01:44,  4.33it/s]\u001b[A\n","240it [01:45,  4.33it/s]\u001b[A\n","241it [01:45,  4.33it/s]\u001b[A\n","242it [01:45,  4.33it/s]\u001b[A\n","243it [01:45,  4.33it/s]\u001b[A\n","244it [01:46,  4.33it/s]\u001b[A\n","245it [01:46,  4.33it/s]\u001b[A\n","246it [01:46,  4.33it/s]\u001b[A\n","247it [01:46,  4.33it/s]\u001b[A\n","248it [01:46,  4.33it/s]\u001b[A\n","249it [01:47,  4.33it/s]\u001b[A\n","250it [01:47,  4.33it/s]\u001b[A\n","251it [01:47,  4.33it/s]\u001b[A\n","252it [01:47,  4.33it/s]\u001b[A\n","253it [01:48,  4.33it/s]\u001b[A\n","254it [01:48,  4.33it/s]\u001b[A\n","255it [01:48,  4.33it/s]\u001b[A\n","256it [01:48,  4.33it/s]\u001b[A\n","257it [01:49,  4.33it/s]\u001b[A\n","258it [01:49,  4.33it/s]\u001b[A\n","259it [01:49,  4.33it/s]\u001b[A\n","260it [01:49,  4.33it/s]\u001b[A\n","261it [01:49,  4.33it/s]\u001b[A\n","262it [01:50,  4.33it/s]\u001b[A\n","263it [01:50,  4.33it/s]\u001b[A\n","264it [01:50,  4.33it/s]\u001b[A\n","265it [01:50,  4.33it/s]\u001b[A\n","266it [01:51,  4.33it/s]\u001b[A\n","267it [01:51,  4.33it/s]\u001b[A\n","268it [01:51,  4.33it/s]\u001b[A\n","269it [01:51,  4.33it/s]\u001b[A\n","270it [01:52,  4.33it/s]\u001b[A\n","271it [01:52,  4.33it/s]\u001b[A\n","272it [01:52,  4.33it/s]\u001b[A\n","273it [01:52,  4.33it/s]\u001b[A\n","274it [01:52,  4.33it/s]\u001b[A\n","275it [01:53,  4.33it/s]\u001b[A\n","276it [01:53,  4.33it/s]\u001b[A\n","277it [01:53,  4.33it/s]\u001b[A\n","278it [01:53,  4.33it/s]\u001b[A\n","279it [01:54,  4.33it/s]\u001b[A\n","280it [01:54,  4.33it/s]\u001b[A\n","281it [01:54,  4.33it/s]\u001b[A\n","282it [01:54,  4.33it/s]\u001b[A\n","283it [01:55,  4.33it/s]\u001b[A\n","284it [01:55,  4.33it/s]\u001b[A\n","285it [01:55,  4.33it/s]\u001b[A\n","286it [01:55,  4.33it/s]\u001b[A\n","287it [01:55,  4.33it/s]\u001b[A\n","288it [01:56,  4.33it/s]\u001b[A\n","289it [01:56,  4.33it/s]\u001b[A\n","290it [01:56,  4.33it/s]\u001b[A\n","291it [01:56,  4.33it/s]\u001b[A\n","292it [01:57,  4.33it/s]\u001b[A\n","293it [01:57,  4.33it/s]\u001b[A\n","294it [01:57,  4.33it/s]\u001b[A\n","295it [01:57,  4.33it/s]\u001b[A\n","296it [01:58,  4.33it/s]\u001b[A\n","297it [01:58,  4.33it/s]\u001b[A\n","298it [01:58,  4.33it/s]\u001b[A\n","299it [01:58,  4.33it/s]\u001b[A\n","300it [01:58,  4.33it/s]\u001b[A\n","301it [01:59,  4.33it/s]\u001b[A\n","302it [01:59,  4.33it/s]\u001b[A\n","303it [01:59,  4.33it/s]\u001b[A\n","304it [01:59,  4.33it/s]\u001b[A\n","305it [02:00,  4.33it/s]\u001b[A\n","306it [02:00,  4.33it/s]\u001b[A\n","307it [02:00,  4.33it/s]\u001b[A\n","308it [02:00,  4.33it/s]\u001b[A\n","309it [02:01,  4.33it/s]\u001b[A\n","310it [02:01,  4.33it/s]\u001b[A\n","311it [02:01,  4.33it/s]\u001b[A\n","312it [02:01,  4.33it/s]\u001b[A\n","313it [02:01,  4.33it/s]\u001b[A\n","314it [02:02,  4.33it/s]\u001b[A\n","315it [02:02,  4.33it/s]\u001b[A\n","316it [02:02,  4.33it/s]\u001b[A\n","317it [02:02,  4.33it/s]\u001b[A\n","318it [02:03,  4.33it/s]\u001b[A\n","319it [02:03,  4.34it/s]\u001b[A\n","320it [02:03,  4.33it/s]\u001b[A\n","321it [02:03,  4.33it/s]\u001b[A\n","322it [02:04,  4.33it/s]\u001b[A\n","323it [02:04,  4.33it/s]\u001b[A\n","324it [02:04,  4.33it/s]\u001b[A\n","325it [02:04,  4.33it/s]\u001b[A\n","326it [02:04,  4.33it/s]\u001b[A\n","327it [02:05,  4.33it/s]\u001b[A\n","328it [02:05,  4.33it/s]\u001b[A\n","329it [02:05,  4.33it/s]\u001b[A\n","330it [02:05,  4.33it/s]\u001b[A\n","331it [02:06,  4.33it/s]\u001b[A\n","332it [02:06,  4.33it/s]\u001b[A\n","333it [02:06,  4.33it/s]\u001b[A\n","334it [02:06,  4.33it/s]\u001b[A\n","335it [02:07,  4.33it/s]\u001b[A\n","336it [02:07,  4.33it/s]\u001b[A\n","337it [02:07,  4.33it/s]\u001b[A\n","338it [02:07,  4.33it/s]\u001b[A\n","339it [02:07,  4.33it/s]\u001b[A\n","340it [02:08,  4.33it/s]\u001b[A\n","341it [02:08,  4.33it/s]\u001b[A\n","342it [02:08,  4.33it/s]\u001b[A\n","343it [02:08,  4.33it/s]\u001b[A\n","344it [02:09,  4.33it/s]\u001b[A\n","345it [02:09,  4.33it/s]\u001b[A\n","346it [02:09,  4.33it/s]\u001b[A\n","347it [02:09,  4.33it/s]\u001b[A\n","348it [02:10,  4.33it/s]\u001b[A\n","349it [02:10,  4.33it/s]\u001b[A\n","350it [02:10,  4.33it/s]\u001b[A\n","351it [02:10,  4.33it/s]\u001b[A\n","352it [02:10,  4.33it/s]\u001b[A\n","353it [02:11,  4.33it/s]\u001b[A\n","354it [02:11,  4.33it/s]\u001b[A\n","355it [02:11,  4.33it/s]\u001b[A\n","356it [02:11,  4.33it/s]\u001b[A\n","357it [02:12,  4.33it/s]\u001b[A\n","358it [02:12,  4.33it/s]\u001b[A\n","359it [02:12,  4.33it/s]\u001b[A\n","360it [02:12,  4.33it/s]\u001b[A\n","361it [02:13,  4.33it/s]\u001b[A\n","362it [02:13,  4.33it/s]\u001b[A\n","363it [02:13,  4.33it/s]\u001b[A\n","364it [02:13,  4.33it/s]\u001b[A\n","365it [02:13,  4.33it/s]\u001b[A\n","366it [02:14,  4.33it/s]\u001b[A\n","367it [02:14,  4.33it/s]\u001b[A\n","368it [02:14,  4.33it/s]\u001b[A\n","369it [02:14,  4.33it/s]\u001b[A\n","370it [02:15,  4.33it/s]\u001b[A03/14/2022 09:20:32 - INFO - src.trainer -   Best dev result: 0.8730000853538513\n","Epoch:  30% 74/250 [03:46<04:56,  1.69s/it]\n","371it [03:05, 15.22s/it]\u001b[A\n","372it [03:05, 10.73s/it]\u001b[A\n","373it [03:05,  7.58s/it]\u001b[A\n","374it [03:06,  5.37s/it]\u001b[A\n","375it [03:06,  3.83s/it]\u001b[A\n","376it [03:06,  2.75s/it]\u001b[A\n","377it [03:06,  1.99s/it]\u001b[A\n","378it [03:06,  1.47s/it]\u001b[A\n","379it [03:07,  1.10s/it]\u001b[A\n","380it [03:07,  1.20it/s]\u001b[A\n","381it [03:07,  1.53it/s]\u001b[A\n","382it [03:07,  1.90it/s]\u001b[A\n","383it [03:08,  2.28it/s]\u001b[A\n","384it [03:08,  2.66it/s]\u001b[A\n","385it [03:08,  3.01it/s]\u001b[A\n","386it [03:08,  3.31it/s]\u001b[A\n","387it [03:09,  3.56it/s]\u001b[A\n","388it [03:09,  3.76it/s]\u001b[A\n","389it [03:09,  3.92it/s]\u001b[A\n","390it [03:09,  4.03it/s]\u001b[A\n","391it [03:09,  4.12it/s]\u001b[A\n","392it [03:10,  4.18it/s]\u001b[A\n","393it [03:10,  4.22it/s]\u001b[A\n","394it [03:10,  4.26it/s]\u001b[A\n","395it [03:10,  4.28it/s]\u001b[A\n","396it [03:11,  4.30it/s]\u001b[A\n","397it [03:11,  4.31it/s]\u001b[A\n","398it [03:11,  4.32it/s]\u001b[A\n","399it [03:11,  4.32it/s]\u001b[A\n","400it [03:12,  4.32it/s]\u001b[A\n","401it [03:12,  4.33it/s]\u001b[A\n","402it [03:12,  4.33it/s]\u001b[A\n","403it [03:12,  4.33it/s]\u001b[A\n","404it [03:12,  4.33it/s]\u001b[A\n","405it [03:13,  4.33it/s]\u001b[A\n","406it [03:13,  4.33it/s]\u001b[A\n","407it [03:13,  4.34it/s]\u001b[A\n","408it [03:13,  4.33it/s]\u001b[A\n","409it [03:14,  4.33it/s]\u001b[A\n","410it [03:14,  4.33it/s]\u001b[A\n","411it [03:14,  4.33it/s]\u001b[A\n","412it [03:14,  4.33it/s]\u001b[A\n","413it [03:15,  4.33it/s]\u001b[A\n","414it [03:15,  4.33it/s]\u001b[A\n","415it [03:15,  4.33it/s]\u001b[A\n","416it [03:15,  4.33it/s]\u001b[A\n","417it [03:15,  4.33it/s]\u001b[A\n","418it [03:16,  4.33it/s]\u001b[A\n","419it [03:16,  4.33it/s]\u001b[A\n","420it [03:16,  4.33it/s]\u001b[A\n","421it [03:16,  4.33it/s]\u001b[A\n","422it [03:17,  4.33it/s]\u001b[A\n","423it [03:17,  4.33it/s]\u001b[A\n","424it [03:17,  4.33it/s]\u001b[A\n","425it [03:17,  4.33it/s]\u001b[A\n","426it [03:18,  4.33it/s]\u001b[A\n","427it [03:18,  4.33it/s]\u001b[A\n","428it [03:18,  4.33it/s]\u001b[A\n","429it [03:18,  4.33it/s]\u001b[A\n","430it [03:18,  4.33it/s]\u001b[A\n","431it [03:19,  4.33it/s]\u001b[A\n","432it [03:19,  4.33it/s]\u001b[A\n","433it [03:19,  4.33it/s]\u001b[A\n","434it [03:19,  4.33it/s]\u001b[A\n","435it [03:20,  4.33it/s]\u001b[A\n","436it [03:20,  4.33it/s]\u001b[A\n","437it [03:20,  4.33it/s]\u001b[A\n","438it [03:20,  4.33it/s]\u001b[A\n","439it [03:21,  4.33it/s]\u001b[A\n","440it [03:21,  4.33it/s]\u001b[A\n","441it [03:21,  4.33it/s]\u001b[A\n","442it [03:21,  4.33it/s]\u001b[A\n","443it [03:21,  4.33it/s]\u001b[A\n","444it [03:22,  4.33it/s]\u001b[A\n","445it [03:22,  4.33it/s]\u001b[A\n","446it [03:22,  4.33it/s]\u001b[A\n","447it [03:22,  4.33it/s]\u001b[A\n","448it [03:23,  4.33it/s]\u001b[A\n","449it [03:23,  4.33it/s]\u001b[A\n","450it [03:23,  4.33it/s]\u001b[A\n","451it [03:23,  4.33it/s]\u001b[A\n","452it [03:24,  4.33it/s]\u001b[A\n","453it [03:24,  4.33it/s]\u001b[A\n","454it [03:24,  4.33it/s]\u001b[A\n","455it [03:24,  4.33it/s]\u001b[A\n","456it [03:24,  4.33it/s]\u001b[A\n","457it [03:25,  4.33it/s]\u001b[A\n","458it [03:25,  4.33it/s]\u001b[A\n","459it [03:25,  4.33it/s]\u001b[A\n","460it [03:25,  4.33it/s]\u001b[A\n","461it [03:26,  4.33it/s]\u001b[A\n","462it [03:26,  4.33it/s]\u001b[A\n","463it [03:26,  4.33it/s]\u001b[A\n","464it [03:26,  4.33it/s]\u001b[A\n","465it [03:27,  4.33it/s]\u001b[A\n","466it [03:27,  4.33it/s]\u001b[A\n","467it [03:27,  4.33it/s]\u001b[A\n","468it [03:27,  4.33it/s]\u001b[A\n","469it [03:27,  4.33it/s]\u001b[A\n","470it [03:28,  4.33it/s]\u001b[A\n","471it [03:28,  4.33it/s]\u001b[A\n","472it [03:28,  4.33it/s]\u001b[A\n","473it [03:28,  4.33it/s]\u001b[A\n","474it [03:29,  4.33it/s]\u001b[A\n","475it [03:29,  4.33it/s]\u001b[A\n","476it [03:29,  4.33it/s]\u001b[A\n","477it [03:29,  4.33it/s]\u001b[A\n","478it [03:30,  4.33it/s]\u001b[A\n","479it [03:30,  4.33it/s]\u001b[A\n","480it [03:30,  4.33it/s]\u001b[A\n","481it [03:30,  4.33it/s]\u001b[A\n","482it [03:30,  4.33it/s]\u001b[A\n","483it [03:31,  4.33it/s]\u001b[A\n","484it [03:31,  4.33it/s]\u001b[A\n","485it [03:31,  4.33it/s]\u001b[A\n","486it [03:31,  4.33it/s]\u001b[A\n","487it [03:32,  4.33it/s]\u001b[A\n","488it [03:32,  4.33it/s]\u001b[A\n","489it [03:32,  4.33it/s]\u001b[A\n","490it [03:32,  4.33it/s]\u001b[A\n","491it [03:33,  4.33it/s]\u001b[A\n","492it [03:33,  4.33it/s]\u001b[A\n","493it [03:33,  4.33it/s]\u001b[A\n","494it [03:33,  4.33it/s]\u001b[A\n","495it [03:33,  4.33it/s]\u001b[A\n","496it [03:34,  4.33it/s]\u001b[A\n","497it [03:34,  4.33it/s]\u001b[A\n","498it [03:34,  4.33it/s]\u001b[A\n","499it [03:34,  4.34it/s]\u001b[A\n","500it [03:35,  4.34it/s]\u001b[A\n","501it [03:35,  4.33it/s]\u001b[A\n","502it [03:35,  4.33it/s]\u001b[A\n","503it [03:35,  4.33it/s]\u001b[A\n","504it [03:36,  4.33it/s]\u001b[A\n","505it [03:36,  4.33it/s]\u001b[A\n","506it [03:36,  4.33it/s]\u001b[A\n","507it [03:36,  4.34it/s]\u001b[A\n","508it [03:36,  4.33it/s]\u001b[A\n","509it [03:37,  4.33it/s]\u001b[A\n","510it [03:37,  4.33it/s]\u001b[A\n","511it [03:37,  4.33it/s]\u001b[A\n","512it [03:37,  4.33it/s]\u001b[A\n","513it [03:38,  4.33it/s]\u001b[A\n","514it [03:38,  4.33it/s]\u001b[A\n","515it [03:38,  4.33it/s]\u001b[A\n","516it [03:38,  4.33it/s]\u001b[A\n","517it [03:39,  4.33it/s]\u001b[A\n","518it [03:39,  4.33it/s]\u001b[A\n","519it [03:39,  4.33it/s]\u001b[A\n","520it [03:39,  4.33it/s]\u001b[A\n","521it [03:39,  4.33it/s]\u001b[A\n","522it [03:40,  4.33it/s]\u001b[A\n","523it [03:40,  4.33it/s]\u001b[A\n","524it [03:40,  4.33it/s]\u001b[A\n","525it [03:40,  4.33it/s]\u001b[A\n","526it [03:41,  4.33it/s]\u001b[A\n","527it [03:41,  4.33it/s]\u001b[A\n","528it [03:41,  4.33it/s]\u001b[A\n","529it [03:41,  4.33it/s]\u001b[A\n","530it [03:42,  4.33it/s]\u001b[A\n","531it [03:42,  4.33it/s]\u001b[A\n","532it [03:42,  4.33it/s]\u001b[A\n","533it [03:42,  4.33it/s]\u001b[A\n","534it [03:42,  4.33it/s]\u001b[A\n","535it [03:43,  4.33it/s]\u001b[A\n","536it [03:43,  4.33it/s]\u001b[A\n","537it [03:43,  4.33it/s]\u001b[A\n","538it [03:43,  4.33it/s]\u001b[A\n","539it [03:44,  4.33it/s]\u001b[A\n","540it [03:44,  4.33it/s]\u001b[A\n","541it [03:44,  4.33it/s]\u001b[A\n","542it [03:44,  4.33it/s]\u001b[A\n","543it [03:45,  4.33it/s]\u001b[A\n","544it [03:45,  4.33it/s]\u001b[A\n","545it [03:45,  4.33it/s]\u001b[A\n","546it [03:45,  4.33it/s]\u001b[A\n","547it [03:45,  4.33it/s]\u001b[A\n","548it [03:46,  4.33it/s]\u001b[A\n","549it [03:46,  4.33it/s]\u001b[A\n","550it [03:46,  4.33it/s]\u001b[A\n","551it [03:46,  4.33it/s]\u001b[A\n","552it [03:47,  4.33it/s]\u001b[A\n","553it [03:47,  4.32it/s]\u001b[A\n","554it [03:47,  4.33it/s]\u001b[A\n","Epoch:  40% 99/250 [05:10<04:14,  1.69s/it]\n","556it [04:29, 12.80s/it]\u001b[A\n","557it [04:30,  9.03s/it]\u001b[A\n","558it [04:30,  6.39s/it]\u001b[A\n","559it [04:30,  4.54s/it]\u001b[A\n","560it [04:30,  3.25s/it]\u001b[A\n","561it [04:31,  2.34s/it]\u001b[A\n","562it [04:31,  1.71s/it]\u001b[A\n","563it [04:31,  1.27s/it]\u001b[A\n","564it [04:31,  1.05it/s]\u001b[A\n","565it [04:32,  1.36it/s]\u001b[A\n","566it [04:32,  1.71it/s]\u001b[A\n","567it [04:32,  2.09it/s]\u001b[A\n","568it [04:32,  2.47it/s]\u001b[A\n","569it [04:32,  2.84it/s]\u001b[A\n","570it [04:33,  3.16it/s]\u001b[A\n","571it [04:33,  3.44it/s]\u001b[A\n","572it [04:33,  3.67it/s]\u001b[A\n","573it [04:33,  3.85it/s]\u001b[A\n","574it [04:34,  3.98it/s]\u001b[A\n","575it [04:34,  4.08it/s]\u001b[A\n","576it [04:34,  4.15it/s]\u001b[A\n","577it [04:34,  4.20it/s]\u001b[A\n","578it [04:35,  4.24it/s]\u001b[A\n","579it [04:35,  4.27it/s]\u001b[A\n","580it [04:35,  4.29it/s]\u001b[A\n","581it [04:35,  4.30it/s]\u001b[A\n","582it [04:35,  4.31it/s]\u001b[A\n","583it [04:36,  4.32it/s]\u001b[A\n","584it [04:36,  4.33it/s]\u001b[A\n","585it [04:36,  4.33it/s]\u001b[A\n","586it [04:36,  4.33it/s]\u001b[A\n","587it [04:37,  4.33it/s]\u001b[A\n","588it [04:37,  4.33it/s]\u001b[A\n","589it [04:37,  4.33it/s]\u001b[A\n","590it [04:37,  4.33it/s]\u001b[A\n","591it [04:38,  4.33it/s]\u001b[A\n","592it [04:38,  4.33it/s]\u001b[A\n","593it [04:38,  4.33it/s]\u001b[A\n","594it [04:38,  4.34it/s]\u001b[A\n","595it [04:38,  4.34it/s]\u001b[A\n","596it [04:39,  4.33it/s]\u001b[A\n","597it [04:39,  4.34it/s]\u001b[A\n","598it [04:39,  4.33it/s]\u001b[A\n","599it [04:39,  4.33it/s]\u001b[A\n","600it [04:40,  4.33it/s]\u001b[A\n","601it [04:40,  4.33it/s]\u001b[A\n","602it [04:40,  4.34it/s]\u001b[A\n","603it [04:40,  4.34it/s]\u001b[A\n","604it [04:41,  4.34it/s]\u001b[A\n","605it [04:41,  4.33it/s]\u001b[A\n","606it [04:41,  4.33it/s]\u001b[A\n","607it [04:41,  4.33it/s]\u001b[A\n","608it [04:41,  4.33it/s]\u001b[A\n","609it [04:42,  4.33it/s]\u001b[A\n","610it [04:42,  4.33it/s]\u001b[A\n","611it [04:42,  4.33it/s]\u001b[A\n","612it [04:42,  4.34it/s]\u001b[A\n","613it [04:43,  4.34it/s]\u001b[A\n","614it [04:43,  4.34it/s]\u001b[A\n","615it [04:43,  4.33it/s]\u001b[A\n","616it [04:43,  4.33it/s]\u001b[A\n","617it [04:44,  4.33it/s]\u001b[A\n","618it [04:44,  4.33it/s]\u001b[A\n","619it [04:44,  4.33it/s]\u001b[A\n","620it [04:44,  4.33it/s]\u001b[A\n","621it [04:44,  4.33it/s]\u001b[A\n","622it [04:45,  4.33it/s]\u001b[A\n","623it [04:45,  4.33it/s]\u001b[A\n","624it [04:45,  4.33it/s]\u001b[A\n","625it [04:45,  4.33it/s]\u001b[A\n","626it [04:46,  4.33it/s]\u001b[A\n","627it [04:46,  4.33it/s]\u001b[A\n","628it [04:46,  4.33it/s]\u001b[A\n","629it [04:46,  4.33it/s]\u001b[A\n","630it [04:47,  4.33it/s]\u001b[A\n","631it [04:47,  4.33it/s]\u001b[A\n","632it [04:47,  4.34it/s]\u001b[A\n","633it [04:47,  4.34it/s]\u001b[A\n","634it [04:47,  4.34it/s]\u001b[A\n","635it [04:48,  4.33it/s]\u001b[A\n","636it [04:48,  4.33it/s]\u001b[A\n","637it [04:48,  4.34it/s]\u001b[A\n","638it [04:48,  4.33it/s]\u001b[A\n","639it [04:49,  4.33it/s]\u001b[A\n","640it [04:49,  4.33it/s]\u001b[A\n","641it [04:49,  4.34it/s]\u001b[A\n","642it [04:49,  4.34it/s]\u001b[A\n","643it [04:50,  4.34it/s]\u001b[A\n","644it [04:50,  4.34it/s]\u001b[A\n","645it [04:50,  4.33it/s]\u001b[A\n","646it [04:50,  4.33it/s]\u001b[A\n","647it [04:50,  4.34it/s]\u001b[A\n","648it [04:51,  4.33it/s]\u001b[A\n","649it [04:51,  4.33it/s]\u001b[A\n","650it [04:51,  4.33it/s]\u001b[A\n","651it [04:51,  4.33it/s]\u001b[A\n","652it [04:52,  4.34it/s]\u001b[A\n","653it [04:52,  4.34it/s]\u001b[A\n","654it [04:52,  4.34it/s]\u001b[A\n","655it [04:52,  4.34it/s]\u001b[A\n","656it [04:53,  4.33it/s]\u001b[A\n","657it [04:53,  4.34it/s]\u001b[A\n","658it [04:53,  4.33it/s]\u001b[A\n","659it [04:53,  4.33it/s]\u001b[A\n","660it [04:53,  4.33it/s]\u001b[A\n","661it [04:54,  4.33it/s]\u001b[A\n","662it [04:54,  4.33it/s]\u001b[A\n","663it [04:54,  4.33it/s]\u001b[A\n","664it [04:54,  4.34it/s]\u001b[A\n","665it [04:55,  4.34it/s]\u001b[A\n","666it [04:55,  4.33it/s]\u001b[A\n","667it [04:55,  4.33it/s]\u001b[A\n","668it [04:55,  4.33it/s]\u001b[A\n","669it [04:56,  4.33it/s]\u001b[A\n","670it [04:56,  4.33it/s]\u001b[A\n","671it [04:56,  4.34it/s]\u001b[A\n","672it [04:56,  4.34it/s]\u001b[A\n","673it [04:56,  4.34it/s]\u001b[A\n","674it [04:57,  4.34it/s]\u001b[A\n","675it [04:57,  4.34it/s]\u001b[A\n","676it [04:57,  4.33it/s]\u001b[A\n","677it [04:57,  4.33it/s]\u001b[A\n","678it [04:58,  4.33it/s]\u001b[A\n","679it [04:58,  4.33it/s]\u001b[A\n","680it [04:58,  4.33it/s]\u001b[A\n","681it [04:58,  4.34it/s]\u001b[A\n","682it [04:59,  4.34it/s]\u001b[A\n","683it [04:59,  4.34it/s]\u001b[A\n","684it [04:59,  4.34it/s]\u001b[A\n","685it [04:59,  4.33it/s]\u001b[A\n","686it [04:59,  4.33it/s]\u001b[A\n","687it [05:00,  4.33it/s]\u001b[A\n","688it [05:00,  4.33it/s]\u001b[A\n","689it [05:00,  4.33it/s]\u001b[A\n","690it [05:00,  4.33it/s]\u001b[A\n","691it [05:01,  4.33it/s]\u001b[A\n","692it [05:01,  4.33it/s]\u001b[A\n","693it [05:01,  4.34it/s]\u001b[A\n","694it [05:01,  4.34it/s]\u001b[A\n","695it [05:02,  4.33it/s]\u001b[A\n","696it [05:02,  4.33it/s]\u001b[A\n","697it [05:02,  4.33it/s]\u001b[A\n","698it [05:02,  4.33it/s]\u001b[A\n","699it [05:02,  4.33it/s]\u001b[A\n","700it [05:03,  4.33it/s]\u001b[A\n","701it [05:03,  4.33it/s]\u001b[A\n","702it [05:03,  4.33it/s]\u001b[A\n","703it [05:03,  4.33it/s]\u001b[A\n","704it [05:04,  4.34it/s]\u001b[A\n","705it [05:04,  4.34it/s]\u001b[A\n","706it [05:04,  4.34it/s]\u001b[A\n","707it [05:04,  4.33it/s]\u001b[A\n","708it [05:05,  4.33it/s]\u001b[A\n","709it [05:05,  4.33it/s]\u001b[A\n","710it [05:05,  4.33it/s]\u001b[A\n","711it [05:05,  4.33it/s]\u001b[A\n","712it [05:05,  4.33it/s]\u001b[A\n","713it [05:06,  4.34it/s]\u001b[A\n","714it [05:06,  4.34it/s]\u001b[A\n","715it [05:06,  4.34it/s]\u001b[A\n","716it [05:06,  4.34it/s]\u001b[A\n","717it [05:07,  4.34it/s]\u001b[A\n","718it [05:07,  4.33it/s]\u001b[A\n","719it [05:07,  4.33it/s]\u001b[A\n","720it [05:07,  4.33it/s]\u001b[A\n","721it [05:08,  4.34it/s]\u001b[A\n","722it [05:08,  4.34it/s]\u001b[A\n","723it [05:08,  4.34it/s]\u001b[A\n","724it [05:08,  4.34it/s]\u001b[A\n","725it [05:08,  4.33it/s]\u001b[A\n","726it [05:09,  4.33it/s]\u001b[A\n","727it [05:09,  4.33it/s]\u001b[A\n","728it [05:09,  4.33it/s]\u001b[A\n","729it [05:09,  4.33it/s]\u001b[A\n","730it [05:10,  4.33it/s]\u001b[A\n","731it [05:10,  4.33it/s]\u001b[A\n","732it [05:10,  4.33it/s]\u001b[A\n","733it [05:10,  4.34it/s]\u001b[A\n","734it [05:11,  4.34it/s]\u001b[A\n","735it [05:11,  4.33it/s]\u001b[A\n","736it [05:11,  4.33it/s]\u001b[A\n","737it [05:11,  4.33it/s]\u001b[A\n","738it [05:11,  4.33it/s]\u001b[A\n","739it [05:12,  4.33it/s]\u001b[A\n","Epoch:  50% 124/250 [06:35<03:31,  1.68s/it]\n","741it [05:54, 12.79s/it]\u001b[A\n","742it [05:54,  9.02s/it]\u001b[A\n","743it [05:54,  6.39s/it]\u001b[A\n","744it [05:55,  4.54s/it]\u001b[A\n","745it [05:55,  3.25s/it]\u001b[A\n","746it [05:55,  2.34s/it]\u001b[A\n","747it [05:55,  1.71s/it]\u001b[A\n","748it [05:56,  1.27s/it]\u001b[A\n","749it [05:56,  1.05it/s]\u001b[A\n","750it [05:56,  1.36it/s]\u001b[A\n","751it [05:56,  1.71it/s]\u001b[A\n","752it [05:57,  2.09it/s]\u001b[A\n","753it [05:57,  2.47it/s]\u001b[A\n","754it [05:57,  2.84it/s]\u001b[A\n","755it [05:57,  3.16it/s]\u001b[A\n","756it [05:57,  3.44it/s]\u001b[A\n","757it [05:58,  3.67it/s]\u001b[A\n","758it [05:58,  3.85it/s]\u001b[A\n","759it [05:58,  3.98it/s]\u001b[A\n","760it [05:58,  4.08it/s]\u001b[A\n","761it [05:59,  4.15it/s]\u001b[A\n","762it [05:59,  4.20it/s]\u001b[A\n","763it [05:59,  4.24it/s]\u001b[A\n","764it [05:59,  4.26it/s]\u001b[A\n","765it [06:00,  4.28it/s]\u001b[A\n","766it [06:00,  4.30it/s]\u001b[A\n","767it [06:00,  4.31it/s]\u001b[A\n","768it [06:00,  4.32it/s]\u001b[A\n","769it [06:00,  4.32it/s]\u001b[A\n","770it [06:01,  4.32it/s]\u001b[A\n","771it [06:01,  4.32it/s]\u001b[A\n","772it [06:01,  4.33it/s]\u001b[A\n","773it [06:01,  4.33it/s]\u001b[A\n","774it [06:02,  4.32it/s]\u001b[A\n","775it [06:02,  4.33it/s]\u001b[A\n","776it [06:02,  4.33it/s]\u001b[A\n","777it [06:02,  4.33it/s]\u001b[A\n","778it [06:03,  4.33it/s]\u001b[A\n","779it [06:03,  4.33it/s]\u001b[A\n","780it [06:03,  4.33it/s]\u001b[A\n","781it [06:03,  4.33it/s]\u001b[A\n","782it [06:03,  4.33it/s]\u001b[A\n","783it [06:04,  4.33it/s]\u001b[A\n","784it [06:04,  4.33it/s]\u001b[A\n","785it [06:04,  4.33it/s]\u001b[A\n","786it [06:04,  4.33it/s]\u001b[A\n","787it [06:05,  4.33it/s]\u001b[A\n","788it [06:05,  4.33it/s]\u001b[A\n","789it [06:05,  4.33it/s]\u001b[A\n","790it [06:05,  4.33it/s]\u001b[A\n","791it [06:06,  4.33it/s]\u001b[A\n","792it [06:06,  4.33it/s]\u001b[A\n","793it [06:06,  4.33it/s]\u001b[A\n","794it [06:06,  4.33it/s]\u001b[A\n","795it [06:06,  4.33it/s]\u001b[A\n","796it [06:07,  4.33it/s]\u001b[A\n","797it [06:07,  4.33it/s]\u001b[A\n","798it [06:07,  4.33it/s]\u001b[A\n","799it [06:07,  4.33it/s]\u001b[A\n","800it [06:08,  4.33it/s]\u001b[A\n","801it [06:08,  4.33it/s]\u001b[A\n","802it [06:08,  4.33it/s]\u001b[A\n","803it [06:08,  4.33it/s]\u001b[A\n","804it [06:09,  4.33it/s]\u001b[A\n","805it [06:09,  4.33it/s]\u001b[A\n","806it [06:09,  4.33it/s]\u001b[A\n","807it [06:09,  4.33it/s]\u001b[A\n","808it [06:09,  4.33it/s]\u001b[A\n","809it [06:10,  4.33it/s]\u001b[A\n","810it [06:10,  4.33it/s]\u001b[A\n","811it [06:10,  4.33it/s]\u001b[A\n","812it [06:10,  4.33it/s]\u001b[A\n","813it [06:11,  4.33it/s]\u001b[A\n","814it [06:11,  4.33it/s]\u001b[A\n","815it [06:11,  4.33it/s]\u001b[A\n","816it [06:11,  4.33it/s]\u001b[A\n","817it [06:12,  4.33it/s]\u001b[A\n","818it [06:12,  4.33it/s]\u001b[A\n","819it [06:12,  4.33it/s]\u001b[A\n","820it [06:12,  4.33it/s]\u001b[A\n","821it [06:12,  4.33it/s]\u001b[A\n","822it [06:13,  4.33it/s]\u001b[A\n","823it [06:13,  4.33it/s]\u001b[A\n","824it [06:13,  4.33it/s]\u001b[A\n","825it [06:13,  4.33it/s]\u001b[A\n","826it [06:14,  4.33it/s]\u001b[A\n","827it [06:14,  4.33it/s]\u001b[A\n","828it [06:14,  4.33it/s]\u001b[A\n","829it [06:14,  4.33it/s]\u001b[A\n","830it [06:15,  4.33it/s]\u001b[A\n","831it [06:15,  4.33it/s]\u001b[A\n","832it [06:15,  4.33it/s]\u001b[A\n","833it [06:15,  4.33it/s]\u001b[A\n","834it [06:15,  4.33it/s]\u001b[A\n","835it [06:16,  4.33it/s]\u001b[A\n","836it [06:16,  4.33it/s]\u001b[A\n","837it [06:16,  4.33it/s]\u001b[A\n","838it [06:16,  4.33it/s]\u001b[A\n","839it [06:17,  4.33it/s]\u001b[A\n","840it [06:17,  4.33it/s]\u001b[A\n","841it [06:17,  4.33it/s]\u001b[A\n","842it [06:17,  4.33it/s]\u001b[A\n","843it [06:18,  4.33it/s]\u001b[A\n","844it [06:18,  4.33it/s]\u001b[A\n","845it [06:18,  4.33it/s]\u001b[A\n","846it [06:18,  4.33it/s]\u001b[A\n","847it [06:18,  4.33it/s]\u001b[A\n","848it [06:19,  4.33it/s]\u001b[A\n","849it [06:19,  4.33it/s]\u001b[A\n","850it [06:19,  4.33it/s]\u001b[A\n","851it [06:19,  4.33it/s]\u001b[A\n","852it [06:20,  4.33it/s]\u001b[A\n","853it [06:20,  4.33it/s]\u001b[A\n","854it [06:20,  4.33it/s]\u001b[A\n","855it [06:20,  4.33it/s]\u001b[A\n","856it [06:21,  4.33it/s]\u001b[A\n","857it [06:21,  4.32it/s]\u001b[A\n","858it [06:21,  4.33it/s]\u001b[A\n","859it [06:21,  4.33it/s]\u001b[A\n","860it [06:21,  4.33it/s]\u001b[A\n","861it [06:22,  4.33it/s]\u001b[A\n","862it [06:22,  4.33it/s]\u001b[A\n","863it [06:22,  4.33it/s]\u001b[A\n","864it [06:22,  4.33it/s]\u001b[A\n","865it [06:23,  4.33it/s]\u001b[A\n","866it [06:23,  4.33it/s]\u001b[A\n","867it [06:23,  4.33it/s]\u001b[A\n","868it [06:23,  4.33it/s]\u001b[A\n","869it [06:24,  4.33it/s]\u001b[A\n","870it [06:24,  4.33it/s]\u001b[A\n","871it [06:24,  4.33it/s]\u001b[A\n","872it [06:24,  4.33it/s]\u001b[A\n","873it [06:24,  4.33it/s]\u001b[A\n","874it [06:25,  4.32it/s]\u001b[A\n","875it [06:25,  4.33it/s]\u001b[A\n","876it [06:25,  4.33it/s]\u001b[A\n","877it [06:25,  4.33it/s]\u001b[A\n","878it [06:26,  4.33it/s]\u001b[A\n","879it [06:26,  4.33it/s]\u001b[A\n","880it [06:26,  4.33it/s]\u001b[A\n","881it [06:26,  4.33it/s]\u001b[A\n","882it [06:27,  4.33it/s]\u001b[A\n","883it [06:27,  4.33it/s]\u001b[A\n","884it [06:27,  4.33it/s]\u001b[A\n","885it [06:27,  4.33it/s]\u001b[A\n","886it [06:27,  4.33it/s]\u001b[A\n","887it [06:28,  4.33it/s]\u001b[A\n","888it [06:28,  4.33it/s]\u001b[A\n","889it [06:28,  4.33it/s]\u001b[A\n","890it [06:28,  4.33it/s]\u001b[A\n","891it [06:29,  4.33it/s]\u001b[A\n","892it [06:29,  4.33it/s]\u001b[A\n","893it [06:29,  4.33it/s]\u001b[A\n","894it [06:29,  4.33it/s]\u001b[A\n","895it [06:30,  4.33it/s]\u001b[A\n","896it [06:30,  4.33it/s]\u001b[A\n","897it [06:30,  4.33it/s]\u001b[A\n","898it [06:30,  4.33it/s]\u001b[A\n","899it [06:30,  4.33it/s]\u001b[A\n","900it [06:31,  4.33it/s]\u001b[A\n","901it [06:31,  4.33it/s]\u001b[A\n","902it [06:31,  4.33it/s]\u001b[A\n","903it [06:31,  4.33it/s]\u001b[A\n","904it [06:32,  4.33it/s]\u001b[A\n","905it [06:32,  4.33it/s]\u001b[A\n","906it [06:32,  4.33it/s]\u001b[A\n","907it [06:32,  4.32it/s]\u001b[A\n","908it [06:33,  4.33it/s]\u001b[A\n","909it [06:33,  4.32it/s]\u001b[A\n","910it [06:33,  4.32it/s]\u001b[A\n","911it [06:33,  4.33it/s]\u001b[A\n","912it [06:33,  4.33it/s]\u001b[A\n","913it [06:34,  4.33it/s]\u001b[A\n","914it [06:34,  4.33it/s]\u001b[A\n","915it [06:34,  4.33it/s]\u001b[A\n","916it [06:34,  4.33it/s]\u001b[A\n","917it [06:35,  4.33it/s]\u001b[A\n","918it [06:35,  4.33it/s]\u001b[A\n","919it [06:35,  4.33it/s]\u001b[A\n","920it [06:35,  4.33it/s]\u001b[A\n","921it [06:36,  4.33it/s]\u001b[A\n","922it [06:36,  4.33it/s]\u001b[A\n","923it [06:36,  4.33it/s]\u001b[A\n","924it [06:36,  4.33it/s]\u001b[A\n","Epoch:  60% 149/250 [07:59<02:49,  1.68s/it]\n","926it [07:19, 12.81s/it]\u001b[A\n","927it [07:19,  9.04s/it]\u001b[A\n","928it [07:19,  6.39s/it]\u001b[A\n","929it [07:19,  4.55s/it]\u001b[A\n","930it [07:20,  3.25s/it]\u001b[A\n","931it [07:20,  2.35s/it]\u001b[A\n","932it [07:20,  1.71s/it]\u001b[A\n","933it [07:20,  1.27s/it]\u001b[A\n","934it [07:21,  1.05it/s]\u001b[A\n","935it [07:21,  1.35it/s]\u001b[A\n","936it [07:21,  1.71it/s]\u001b[A\n","937it [07:21,  2.08it/s]\u001b[A\n","938it [07:21,  2.47it/s]\u001b[A\n","939it [07:22,  2.83it/s]\u001b[A\n","940it [07:22,  3.16it/s]\u001b[A\n","941it [07:22,  3.44it/s]\u001b[A\n","942it [07:22,  3.67it/s]\u001b[A\n","943it [07:23,  3.84it/s]\u001b[A\n","944it [07:23,  3.98it/s]\u001b[A\n","945it [07:23,  4.08it/s]\u001b[A\n","946it [07:23,  4.15it/s]\u001b[A\n","947it [07:24,  4.20it/s]\u001b[A\n","948it [07:24,  4.24it/s]\u001b[A\n","949it [07:24,  4.27it/s]\u001b[A\n","950it [07:24,  4.28it/s]\u001b[A\n","951it [07:24,  4.30it/s]\u001b[A\n","952it [07:25,  4.31it/s]\u001b[A\n","953it [07:25,  4.32it/s]\u001b[A\n","954it [07:25,  4.32it/s]\u001b[A\n","955it [07:25,  4.33it/s]\u001b[A\n","956it [07:26,  4.33it/s]\u001b[A\n","957it [07:26,  4.33it/s]\u001b[A\n","958it [07:26,  4.33it/s]\u001b[A\n","959it [07:26,  4.33it/s]\u001b[A\n","960it [07:27,  4.33it/s]\u001b[A\n","961it [07:27,  4.33it/s]\u001b[A\n","962it [07:27,  4.33it/s]\u001b[A\n","963it [07:27,  4.33it/s]\u001b[A\n","964it [07:27,  4.33it/s]\u001b[A\n","965it [07:28,  4.34it/s]\u001b[A\n","966it [07:28,  4.33it/s]\u001b[A\n","967it [07:28,  4.33it/s]\u001b[A\n","968it [07:28,  4.33it/s]\u001b[A\n","969it [07:29,  4.33it/s]\u001b[A\n","970it [07:29,  4.33it/s]\u001b[A\n","971it [07:29,  4.33it/s]\u001b[A\n","972it [07:29,  4.33it/s]\u001b[A\n","973it [07:30,  4.33it/s]\u001b[A\n","974it [07:30,  4.33it/s]\u001b[A\n","975it [07:30,  4.33it/s]\u001b[A\n","976it [07:30,  4.33it/s]\u001b[A\n","977it [07:30,  4.33it/s]\u001b[A\n","978it [07:31,  4.33it/s]\u001b[A\n","979it [07:31,  4.33it/s]\u001b[A\n","980it [07:31,  4.33it/s]\u001b[A\n","981it [07:31,  4.33it/s]\u001b[A\n","982it [07:32,  4.33it/s]\u001b[A\n","983it [07:32,  4.33it/s]\u001b[A\n","984it [07:32,  4.33it/s]\u001b[A\n","985it [07:32,  4.33it/s]\u001b[A\n","986it [07:33,  4.33it/s]\u001b[A\n","987it [07:33,  4.33it/s]\u001b[A\n","988it [07:33,  4.33it/s]\u001b[A\n","989it [07:33,  4.33it/s]\u001b[A\n","990it [07:33,  4.33it/s]\u001b[A\n","991it [07:34,  4.33it/s]\u001b[A\n","992it [07:34,  4.33it/s]\u001b[A\n","993it [07:34,  4.33it/s]\u001b[A\n","994it [07:34,  4.33it/s]\u001b[A\n","995it [07:35,  4.33it/s]\u001b[A\n","996it [07:35,  4.33it/s]\u001b[A\n","997it [07:35,  4.33it/s]\u001b[A\n","998it [07:35,  4.33it/s]\u001b[A\n","999it [07:36,  4.33it/s]\u001b[A\n","1000it [07:36,  4.32it/s]\u001b[A\n","1001it [07:36,  4.32it/s]\u001b[A\n","1002it [07:36,  4.32it/s]\u001b[A\n","1003it [07:36,  4.32it/s]\u001b[A\n","1004it [07:37,  4.33it/s]\u001b[A\n","1005it [07:37,  4.33it/s]\u001b[A\n","1006it [07:37,  4.33it/s]\u001b[A\n","1007it [07:37,  4.33it/s]\u001b[A\n","1008it [07:38,  4.33it/s]\u001b[A\n","1009it [07:38,  4.33it/s]\u001b[A\n","1010it [07:38,  4.33it/s]\u001b[A\n","1011it [07:38,  4.33it/s]\u001b[A\n","1012it [07:39,  4.33it/s]\u001b[A\n","1013it [07:39,  4.33it/s]\u001b[A\n","1014it [07:39,  4.33it/s]\u001b[A\n","1015it [07:39,  4.33it/s]\u001b[A\n","1016it [07:39,  4.33it/s]\u001b[A\n","1017it [07:40,  4.33it/s]\u001b[A\n","1018it [07:40,  4.33it/s]\u001b[A\n","1019it [07:40,  4.33it/s]\u001b[A\n","1020it [07:40,  4.33it/s]\u001b[A\n","1021it [07:41,  4.33it/s]\u001b[A\n","1022it [07:41,  4.33it/s]\u001b[A\n","1023it [07:41,  4.33it/s]\u001b[A\n","1024it [07:41,  4.33it/s]\u001b[A\n","1025it [07:42,  4.33it/s]\u001b[A\n","1026it [07:42,  4.33it/s]\u001b[A\n","1027it [07:42,  4.33it/s]\u001b[A\n","1028it [07:42,  4.33it/s]\u001b[A\n","1029it [07:42,  4.33it/s]\u001b[A\n","1030it [07:43,  4.33it/s]\u001b[A\n","1031it [07:43,  4.33it/s]\u001b[A\n","1032it [07:43,  4.33it/s]\u001b[A\n","1033it [07:43,  4.33it/s]\u001b[A\n","1034it [07:44,  4.33it/s]\u001b[A\n","1035it [07:44,  4.33it/s]\u001b[A\n","1036it [07:44,  4.33it/s]\u001b[A\n","1037it [07:44,  4.33it/s]\u001b[A\n","1038it [07:45,  4.33it/s]\u001b[A\n","1039it [07:45,  4.33it/s]\u001b[A\n","1040it [07:45,  4.33it/s]\u001b[A\n","1041it [07:45,  4.33it/s]\u001b[A\n","1042it [07:45,  4.33it/s]\u001b[A\n","1043it [07:46,  4.33it/s]\u001b[A\n","1044it [07:46,  4.33it/s]\u001b[A\n","1045it [07:46,  4.33it/s]\u001b[A\n","1046it [07:46,  4.33it/s]\u001b[A\n","1047it [07:47,  4.33it/s]\u001b[A\n","1048it [07:47,  4.33it/s]\u001b[A\n","1049it [07:47,  4.33it/s]\u001b[A\n","1050it [07:47,  4.33it/s]\u001b[A\n","1051it [07:48,  4.33it/s]\u001b[A\n","1052it [07:48,  4.33it/s]\u001b[A\n","1053it [07:48,  4.33it/s]\u001b[A\n","1054it [07:48,  4.33it/s]\u001b[A\n","1055it [07:48,  4.33it/s]\u001b[A\n","1056it [07:49,  4.33it/s]\u001b[A\n","1057it [07:49,  4.34it/s]\u001b[A\n","1058it [07:49,  4.33it/s]\u001b[A\n","1059it [07:49,  4.33it/s]\u001b[A\n","1060it [07:50,  4.33it/s]\u001b[A\n","1061it [07:50,  4.33it/s]\u001b[A\n","1062it [07:50,  4.33it/s]\u001b[A\n","1063it [07:50,  4.33it/s]\u001b[A\n","1064it [07:51,  4.33it/s]\u001b[A\n","1065it [07:51,  4.33it/s]\u001b[A\n","1066it [07:51,  4.33it/s]\u001b[A\n","1067it [07:51,  4.33it/s]\u001b[A\n","1068it [07:51,  4.33it/s]\u001b[A\n","1069it [07:52,  4.33it/s]\u001b[A\n","1070it [07:52,  4.33it/s]\u001b[A\n","1071it [07:52,  4.33it/s]\u001b[A\n","1072it [07:52,  4.33it/s]\u001b[A\n","1073it [07:53,  4.33it/s]\u001b[A\n","1074it [07:53,  4.33it/s]\u001b[A\n","1075it [07:53,  4.33it/s]\u001b[A\n","1076it [07:53,  4.33it/s]\u001b[A\n","1077it [07:54,  4.33it/s]\u001b[A\n","1078it [07:54,  4.33it/s]\u001b[A\n","1079it [07:54,  4.33it/s]\u001b[A\n","1080it [07:54,  4.33it/s]\u001b[A\n","1081it [07:54,  4.33it/s]\u001b[A\n","1082it [07:55,  4.33it/s]\u001b[A\n","1083it [07:55,  4.33it/s]\u001b[A\n","1084it [07:55,  4.33it/s]\u001b[A\n","1085it [07:55,  4.33it/s]\u001b[A\n","1086it [07:56,  4.33it/s]\u001b[A\n","1087it [07:56,  4.33it/s]\u001b[A\n","1088it [07:56,  4.33it/s]\u001b[A\n","1089it [07:56,  4.33it/s]\u001b[A\n","1090it [07:57,  4.33it/s]\u001b[A\n","1091it [07:57,  4.33it/s]\u001b[A\n","1092it [07:57,  4.33it/s]\u001b[A\n","1093it [07:57,  4.33it/s]\u001b[A\n","1094it [07:57,  4.33it/s]\u001b[A\n","1095it [07:58,  4.33it/s]\u001b[A\n","1096it [07:58,  4.33it/s]\u001b[A\n","1097it [07:58,  4.33it/s]\u001b[A\n","1098it [07:58,  4.33it/s]\u001b[A\n","1099it [07:59,  4.33it/s]\u001b[A\n","1100it [07:59,  4.33it/s]\u001b[A\n","1101it [07:59,  4.32it/s]\u001b[A\n","1102it [07:59,  4.32it/s]\u001b[A\n","1103it [08:00,  4.32it/s]\u001b[A\n","1104it [08:00,  4.33it/s]\u001b[A\n","1105it [08:00,  4.32it/s]\u001b[A\n","1106it [08:00,  4.32it/s]\u001b[A\n","1107it [08:00,  4.32it/s]\u001b[A\n","1108it [08:01,  4.32it/s]\u001b[A\n","1109it [08:01,  4.32it/s]\u001b[A\n","Epoch:  70% 174/250 [09:24<02:08,  1.69s/it]\n","1111it [08:43, 12.83s/it]\u001b[A\n","1112it [08:44,  9.05s/it]\u001b[A\n","1113it [08:44,  6.41s/it]\u001b[A\n","1114it [08:44,  4.55s/it]\u001b[A\n","1115it [08:44,  3.26s/it]\u001b[A\n","1116it [08:45,  2.35s/it]\u001b[A\n","1117it [08:45,  1.71s/it]\u001b[A\n","1118it [08:45,  1.27s/it]\u001b[A\n","1119it [08:45,  1.04it/s]\u001b[A\n","1120it [08:45,  1.35it/s]\u001b[A\n","1121it [08:46,  1.70it/s]\u001b[A\n","1122it [08:46,  2.08it/s]\u001b[A\n","1123it [08:46,  2.47it/s]\u001b[A\n","1124it [08:46,  2.83it/s]\u001b[A\n","1125it [08:47,  3.16it/s]\u001b[A\n","1126it [08:47,  3.44it/s]\u001b[A\n","1127it [08:47,  3.66it/s]\u001b[A\n","1128it [08:47,  3.84it/s]\u001b[A\n","1129it [08:48,  3.98it/s]\u001b[A\n","1130it [08:48,  4.08it/s]\u001b[A\n","1131it [08:48,  4.15it/s]\u001b[A\n","1132it [08:48,  4.20it/s]\u001b[A\n","1133it [08:48,  4.24it/s]\u001b[A\n","1134it [08:49,  4.26it/s]\u001b[A\n","1135it [08:49,  4.28it/s]\u001b[A\n","1136it [08:49,  4.30it/s]\u001b[A\n","1137it [08:49,  4.31it/s]\u001b[A\n","1138it [08:50,  4.31it/s]\u001b[A\n","1139it [08:50,  4.32it/s]\u001b[A\n","1140it [08:50,  4.32it/s]\u001b[A\n","1141it [08:50,  4.32it/s]\u001b[A\n","1142it [08:51,  4.32it/s]\u001b[A\n","1143it [08:51,  4.32it/s]\u001b[A\n","1144it [08:51,  4.32it/s]\u001b[A\n","1145it [08:51,  4.32it/s]\u001b[A\n","1146it [08:51,  4.33it/s]\u001b[A\n","1147it [08:52,  4.33it/s]\u001b[A\n","1148it [08:52,  4.33it/s]\u001b[A\n","1149it [08:52,  4.33it/s]\u001b[A\n","1150it [08:52,  4.33it/s]\u001b[A\n","1151it [08:53,  4.33it/s]\u001b[A\n","1152it [08:53,  4.33it/s]\u001b[A\n","1153it [08:53,  4.33it/s]\u001b[A\n","1154it [08:53,  4.32it/s]\u001b[A\n","1155it [08:54,  4.33it/s]\u001b[A\n","1156it [08:54,  4.33it/s]\u001b[A\n","1157it [08:54,  4.33it/s]\u001b[A\n","1158it [08:54,  4.33it/s]\u001b[A\n","1159it [08:54,  4.33it/s]\u001b[A\n","1160it [08:55,  4.33it/s]\u001b[A\n","1161it [08:55,  4.33it/s]\u001b[A\n","1162it [08:55,  4.33it/s]\u001b[A\n","1163it [08:55,  4.33it/s]\u001b[A\n","1164it [08:56,  4.33it/s]\u001b[A\n","1165it [08:56,  4.33it/s]\u001b[A\n","1166it [08:56,  4.33it/s]\u001b[A\n","1167it [08:56,  4.33it/s]\u001b[A\n","1168it [08:57,  4.33it/s]\u001b[A\n","1169it [08:57,  4.33it/s]\u001b[A\n","1170it [08:57,  4.33it/s]\u001b[A\n","1171it [08:57,  4.33it/s]\u001b[A\n","1172it [08:57,  4.33it/s]\u001b[A\n","1173it [08:58,  4.33it/s]\u001b[A\n","1174it [08:58,  4.33it/s]\u001b[A\n","1175it [08:58,  4.33it/s]\u001b[A\n","1176it [08:58,  4.33it/s]\u001b[A\n","1177it [08:59,  4.33it/s]\u001b[A\n","1178it [08:59,  4.33it/s]\u001b[A\n","1179it [08:59,  4.33it/s]\u001b[A\n","1180it [08:59,  4.33it/s]\u001b[A\n","1181it [09:00,  4.33it/s]\u001b[A\n","1182it [09:00,  4.33it/s]\u001b[A\n","1183it [09:00,  4.33it/s]\u001b[A\n","1184it [09:00,  4.33it/s]\u001b[A\n","1185it [09:00,  4.33it/s]\u001b[A\n","1186it [09:01,  4.33it/s]\u001b[A\n","1187it [09:01,  4.33it/s]\u001b[A\n","1188it [09:01,  4.33it/s]\u001b[A\n","1189it [09:01,  4.33it/s]\u001b[A\n","1190it [09:02,  4.33it/s]\u001b[A\n","1191it [09:02,  4.33it/s]\u001b[A\n","1192it [09:02,  4.33it/s]\u001b[A\n","1193it [09:02,  4.33it/s]\u001b[A\n","1194it [09:03,  4.33it/s]\u001b[A\n","1195it [09:03,  4.33it/s]\u001b[A\n","1196it [09:03,  4.33it/s]\u001b[A\n","1197it [09:03,  4.33it/s]\u001b[A\n","1198it [09:03,  4.33it/s]\u001b[A\n","1199it [09:04,  4.33it/s]\u001b[A\n","1200it [09:04,  4.33it/s]\u001b[A\n","1201it [09:04,  4.33it/s]\u001b[A\n","1202it [09:04,  4.33it/s]\u001b[A\n","1203it [09:05,  4.33it/s]\u001b[A\n","1204it [09:05,  4.33it/s]\u001b[A\n","1205it [09:05,  4.33it/s]\u001b[A\n","1206it [09:05,  4.33it/s]\u001b[A\n","1207it [09:06,  4.33it/s]\u001b[A\n","1208it [09:06,  4.33it/s]\u001b[A\n","1209it [09:06,  4.33it/s]\u001b[A\n","1210it [09:06,  4.33it/s]\u001b[A\n","1211it [09:06,  4.33it/s]\u001b[A\n","1212it [09:07,  4.33it/s]\u001b[A\n","1213it [09:07,  4.33it/s]\u001b[A\n","1214it [09:07,  4.33it/s]\u001b[A\n","1215it [09:07,  4.33it/s]\u001b[A\n","1216it [09:08,  4.33it/s]\u001b[A\n","1217it [09:08,  4.33it/s]\u001b[A\n","1218it [09:08,  4.33it/s]\u001b[A\n","1219it [09:08,  4.33it/s]\u001b[A\n","1220it [09:09,  4.33it/s]\u001b[A\n","1221it [09:09,  4.33it/s]\u001b[A\n","1222it [09:09,  4.33it/s]\u001b[A\n","1223it [09:09,  4.33it/s]\u001b[A\n","1224it [09:09,  4.33it/s]\u001b[A\n","1225it [09:10,  4.33it/s]\u001b[A\n","1226it [09:10,  4.33it/s]\u001b[A\n","1227it [09:10,  4.33it/s]\u001b[A\n","1228it [09:10,  4.33it/s]\u001b[A\n","1229it [09:11,  4.33it/s]\u001b[A\n","1230it [09:11,  4.33it/s]\u001b[A\n","1231it [09:11,  4.33it/s]\u001b[A\n","1232it [09:11,  4.33it/s]\u001b[A\n","1233it [09:12,  4.33it/s]\u001b[A\n","1234it [09:12,  4.33it/s]\u001b[A\n","1235it [09:12,  4.33it/s]\u001b[A\n","1236it [09:12,  4.33it/s]\u001b[A\n","1237it [09:12,  4.33it/s]\u001b[A\n","1238it [09:13,  4.33it/s]\u001b[A\n","1239it [09:13,  4.33it/s]\u001b[A\n","1240it [09:13,  4.33it/s]\u001b[A\n","1241it [09:13,  4.33it/s]\u001b[A\n","1242it [09:14,  4.33it/s]\u001b[A\n","1243it [09:14,  4.33it/s]\u001b[A\n","1244it [09:14,  4.33it/s]\u001b[A\n","1245it [09:14,  4.33it/s]\u001b[A\n","1246it [09:15,  4.33it/s]\u001b[A\n","1247it [09:15,  4.33it/s]\u001b[A\n","1248it [09:15,  4.33it/s]\u001b[A\n","1249it [09:15,  4.33it/s]\u001b[A\n","1250it [09:15,  4.33it/s]\u001b[A\n","1251it [09:16,  4.34it/s]\u001b[A\n","1252it [09:16,  4.34it/s]\u001b[A\n","1253it [09:16,  4.33it/s]\u001b[A\n","1254it [09:16,  4.33it/s]\u001b[A\n","1255it [09:17,  4.33it/s]\u001b[A\n","1256it [09:17,  4.33it/s]\u001b[A\n","1257it [09:17,  4.32it/s]\u001b[A\n","1258it [09:17,  4.33it/s]\u001b[A\n","1259it [09:18,  4.33it/s]\u001b[A\n","1260it [09:18,  4.33it/s]\u001b[A\n","1261it [09:18,  4.33it/s]\u001b[A\n","1262it [09:18,  4.33it/s]\u001b[A\n","1263it [09:18,  4.33it/s]\u001b[A\n","1264it [09:19,  4.33it/s]\u001b[A\n","1265it [09:19,  4.33it/s]\u001b[A\n","1266it [09:19,  4.33it/s]\u001b[A\n","1267it [09:19,  4.32it/s]\u001b[A\n","1268it [09:20,  4.33it/s]\u001b[A\n","1269it [09:20,  4.33it/s]\u001b[A\n","1270it [09:20,  4.33it/s]\u001b[A\n","1271it [09:20,  4.33it/s]\u001b[A\n","1272it [09:21,  4.33it/s]\u001b[A\n","1273it [09:21,  4.33it/s]\u001b[A\n","1274it [09:21,  4.33it/s]\u001b[A\n","1275it [09:21,  4.33it/s]\u001b[A\n","1276it [09:21,  4.33it/s]\u001b[A\n","1277it [09:22,  4.33it/s]\u001b[A\n","1278it [09:22,  4.33it/s]\u001b[A\n","1279it [09:22,  4.33it/s]\u001b[A\n","1280it [09:22,  4.33it/s]\u001b[A\n","1281it [09:23,  4.33it/s]\u001b[A\n","1282it [09:23,  4.33it/s]\u001b[A\n","1283it [09:23,  4.33it/s]\u001b[A\n","1284it [09:23,  4.33it/s]\u001b[A\n","1285it [09:24,  4.33it/s]\u001b[A\n","1286it [09:24,  4.33it/s]\u001b[A\n","1287it [09:24,  4.33it/s]\u001b[A\n","1288it [09:24,  4.33it/s]\u001b[A\n","1289it [09:24,  4.33it/s]\u001b[A\n","1290it [09:25,  4.33it/s]\u001b[A\n","1291it [09:25,  4.33it/s]\u001b[A\n","1292it [09:25,  4.33it/s]\u001b[A\n","1293it [09:25,  4.33it/s]\u001b[A\n","1294it [09:26,  4.33it/s]\u001b[A\n","Epoch:  80% 199/250 [10:49<01:25,  1.68s/it]\n","1296it [10:08, 12.82s/it]\u001b[A\n","1297it [10:08,  9.04s/it]\u001b[A\n","1298it [10:09,  6.40s/it]\u001b[A\n","1299it [10:09,  4.55s/it]\u001b[A\n","1300it [10:09,  3.25s/it]\u001b[A\n","1301it [10:09,  2.35s/it]\u001b[A\n","1302it [10:09,  1.71s/it]\u001b[A\n","1303it [10:10,  1.27s/it]\u001b[A\n","1304it [10:10,  1.05it/s]\u001b[A\n","1305it [10:10,  1.35it/s]\u001b[A\n","1306it [10:10,  1.71it/s]\u001b[A\n","1307it [10:11,  2.08it/s]\u001b[A\n","1308it [10:11,  2.47it/s]\u001b[A\n","1309it [10:11,  2.84it/s]\u001b[A\n","1310it [10:11,  3.16it/s]\u001b[A\n","1311it [10:12,  3.44it/s]\u001b[A\n","1312it [10:12,  3.67it/s]\u001b[A\n","1313it [10:12,  3.84it/s]\u001b[A\n","1314it [10:12,  3.98it/s]\u001b[A\n","1315it [10:12,  4.08it/s]\u001b[A\n","1316it [10:13,  4.15it/s]\u001b[A\n","1317it [10:13,  4.20it/s]\u001b[A\n","1318it [10:13,  4.24it/s]\u001b[A\n","1319it [10:13,  4.27it/s]\u001b[A\n","1320it [10:14,  4.29it/s]\u001b[A\n","1321it [10:14,  4.30it/s]\u001b[A\n","1322it [10:14,  4.30it/s]\u001b[A\n","1323it [10:14,  4.31it/s]\u001b[A\n","1324it [10:15,  4.32it/s]\u001b[A\n","1325it [10:15,  4.32it/s]\u001b[A\n","1326it [10:15,  4.33it/s]\u001b[A\n","1327it [10:15,  4.33it/s]\u001b[A\n","1328it [10:15,  4.33it/s]\u001b[A\n","1329it [10:16,  4.33it/s]\u001b[A\n","1330it [10:16,  4.33it/s]\u001b[A\n","1331it [10:16,  4.33it/s]\u001b[A\n","1332it [10:16,  4.33it/s]\u001b[A\n","1333it [10:17,  4.33it/s]\u001b[A\n","1334it [10:17,  4.33it/s]\u001b[A\n","1335it [10:17,  4.33it/s]\u001b[A\n","1336it [10:17,  4.33it/s]\u001b[A\n","1337it [10:18,  4.33it/s]\u001b[A\n","1338it [10:18,  4.33it/s]\u001b[A\n","1339it [10:18,  4.33it/s]\u001b[A\n","1340it [10:18,  4.33it/s]\u001b[A\n","1341it [10:18,  4.33it/s]\u001b[A\n","1342it [10:19,  4.33it/s]\u001b[A\n","1343it [10:19,  4.33it/s]\u001b[A\n","1344it [10:19,  4.33it/s]\u001b[A\n","1345it [10:19,  4.33it/s]\u001b[A\n","1346it [10:20,  4.33it/s]\u001b[A\n","1347it [10:20,  4.33it/s]\u001b[A\n","1348it [10:20,  4.33it/s]\u001b[A\n","1349it [10:20,  4.33it/s]\u001b[A\n","1350it [10:21,  4.33it/s]\u001b[A\n","1351it [10:21,  4.33it/s]\u001b[A\n","1352it [10:21,  4.33it/s]\u001b[A\n","1353it [10:21,  4.33it/s]\u001b[A\n","1354it [10:21,  4.33it/s]\u001b[A\n","1355it [10:22,  4.33it/s]\u001b[A\n","1356it [10:22,  4.33it/s]\u001b[A\n","1357it [10:22,  4.33it/s]\u001b[A\n","1358it [10:22,  4.33it/s]\u001b[A\n","1359it [10:23,  4.33it/s]\u001b[A\n","1360it [10:23,  4.33it/s]\u001b[A\n","1361it [10:23,  4.33it/s]\u001b[A\n","1362it [10:23,  4.33it/s]\u001b[A\n","1363it [10:24,  4.33it/s]\u001b[A\n","1364it [10:24,  4.33it/s]\u001b[A\n","1365it [10:24,  4.33it/s]\u001b[A\n","1366it [10:24,  4.33it/s]\u001b[A\n","1367it [10:24,  4.33it/s]\u001b[A\n","1368it [10:25,  4.33it/s]\u001b[A\n","1369it [10:25,  4.33it/s]\u001b[A\n","1370it [10:25,  4.33it/s]\u001b[A\n","1371it [10:25,  4.33it/s]\u001b[A\n","1372it [10:26,  4.33it/s]\u001b[A\n","1373it [10:26,  4.33it/s]\u001b[A\n","1374it [10:26,  4.33it/s]\u001b[A\n","1375it [10:26,  4.33it/s]\u001b[A\n","1376it [10:27,  4.33it/s]\u001b[A\n","1377it [10:27,  4.33it/s]\u001b[A\n","1378it [10:27,  4.34it/s]\u001b[A\n","1379it [10:27,  4.34it/s]\u001b[A\n","1380it [10:27,  4.33it/s]\u001b[A\n","1381it [10:28,  4.33it/s]\u001b[A\n","1382it [10:28,  4.33it/s]\u001b[A\n","1383it [10:28,  4.33it/s]\u001b[A\n","1384it [10:28,  4.33it/s]\u001b[A\n","1385it [10:29,  4.33it/s]\u001b[A\n","1386it [10:29,  4.33it/s]\u001b[A\n","1387it [10:29,  4.33it/s]\u001b[A\n","1388it [10:29,  4.33it/s]\u001b[A\n","1389it [10:30,  4.33it/s]\u001b[A\n","1390it [10:30,  4.33it/s]\u001b[A\n","1391it [10:30,  4.33it/s]\u001b[A\n","1392it [10:30,  4.33it/s]\u001b[A\n","1393it [10:30,  4.33it/s]\u001b[A\n","1394it [10:31,  4.33it/s]\u001b[A\n","1395it [10:31,  4.33it/s]\u001b[A\n","1396it [10:31,  4.33it/s]\u001b[A\n","1397it [10:31,  4.33it/s]\u001b[A\n","1398it [10:32,  4.33it/s]\u001b[A\n","1399it [10:32,  4.33it/s]\u001b[A\n","1400it [10:32,  4.33it/s]\u001b[A\n","1401it [10:32,  4.33it/s]\u001b[A\n","1402it [10:33,  4.33it/s]\u001b[A\n","1403it [10:33,  4.33it/s]\u001b[A\n","1404it [10:33,  4.33it/s]\u001b[A\n","1405it [10:33,  4.33it/s]\u001b[A\n","1406it [10:33,  4.33it/s]\u001b[A\n","1407it [10:34,  4.33it/s]\u001b[A\n","1408it [10:34,  4.33it/s]\u001b[A\n","1409it [10:34,  4.33it/s]\u001b[A\n","1410it [10:34,  4.33it/s]\u001b[A\n","1411it [10:35,  4.33it/s]\u001b[A\n","1412it [10:35,  4.33it/s]\u001b[A\n","1413it [10:35,  4.33it/s]\u001b[A\n","1414it [10:35,  4.33it/s]\u001b[A\n","1415it [10:36,  4.33it/s]\u001b[A\n","1416it [10:36,  4.33it/s]\u001b[A\n","1417it [10:36,  4.33it/s]\u001b[A\n","1418it [10:36,  4.33it/s]\u001b[A\n","1419it [10:36,  4.33it/s]\u001b[A\n","1420it [10:37,  4.33it/s]\u001b[A\n","1421it [10:37,  4.33it/s]\u001b[A\n","1422it [10:37,  4.33it/s]\u001b[A\n","1423it [10:37,  4.33it/s]\u001b[A\n","1424it [10:38,  4.33it/s]\u001b[A\n","1425it [10:38,  4.33it/s]\u001b[A\n","1426it [10:38,  4.33it/s]\u001b[A\n","1427it [10:38,  4.33it/s]\u001b[A\n","1428it [10:39,  4.33it/s]\u001b[A\n","1429it [10:39,  4.33it/s]\u001b[A\n","1430it [10:39,  4.33it/s]\u001b[A\n","1431it [10:39,  4.33it/s]\u001b[A\n","1432it [10:39,  4.33it/s]\u001b[A\n","1433it [10:40,  4.33it/s]\u001b[A\n","1434it [10:40,  4.33it/s]\u001b[A\n","1435it [10:40,  4.33it/s]\u001b[A\n","1436it [10:40,  4.33it/s]\u001b[A\n","1437it [10:41,  4.33it/s]\u001b[A\n","1438it [10:41,  4.33it/s]\u001b[A\n","1439it [10:41,  4.33it/s]\u001b[A\n","1440it [10:41,  4.33it/s]\u001b[A\n","1441it [10:42,  4.33it/s]\u001b[A\n","1442it [10:42,  4.33it/s]\u001b[A\n","1443it [10:42,  4.33it/s]\u001b[A\n","1444it [10:42,  4.33it/s]\u001b[A\n","1445it [10:42,  4.33it/s]\u001b[A\n","1446it [10:43,  4.33it/s]\u001b[A\n","1447it [10:43,  4.33it/s]\u001b[A\n","1448it [10:43,  4.33it/s]\u001b[A\n","1449it [10:43,  4.33it/s]\u001b[A\n","1450it [10:44,  4.33it/s]\u001b[A\n","1451it [10:44,  4.33it/s]\u001b[A\n","1452it [10:44,  4.33it/s]\u001b[A\n","1453it [10:44,  4.33it/s]\u001b[A\n","1454it [10:45,  4.33it/s]\u001b[A\n","1455it [10:45,  4.33it/s]\u001b[A\n","1456it [10:45,  4.33it/s]\u001b[A\n","1457it [10:45,  4.33it/s]\u001b[A\n","1458it [10:45,  4.33it/s]\u001b[A\n","1459it [10:46,  4.33it/s]\u001b[A\n","1460it [10:46,  4.33it/s]\u001b[A\n","1461it [10:46,  4.33it/s]\u001b[A\n","1462it [10:46,  4.33it/s]\u001b[A\n","1463it [10:47,  4.33it/s]\u001b[A\n","1464it [10:47,  4.33it/s]\u001b[A\n","1465it [10:47,  4.33it/s]\u001b[A\n","1466it [10:47,  4.33it/s]\u001b[A\n","1467it [10:48,  4.33it/s]\u001b[A\n","1468it [10:48,  4.33it/s]\u001b[A\n","1469it [10:48,  4.33it/s]\u001b[A\n","1470it [10:48,  4.33it/s]\u001b[A\n","1471it [10:48,  4.33it/s]\u001b[A\n","1472it [10:49,  4.33it/s]\u001b[A\n","1473it [10:49,  4.32it/s]\u001b[A\n","1474it [10:49,  4.33it/s]\u001b[A\n","1475it [10:49,  4.32it/s]\u001b[A\n","1476it [10:50,  4.32it/s]\u001b[A\n","1477it [10:50,  4.33it/s]\u001b[A\n","1478it [10:50,  4.33it/s]\u001b[A\n","1479it [10:50,  4.33it/s]\u001b[A\n","Epoch:  90% 224/250 [12:14<00:43,  1.68s/it]\n","1481it [11:33, 12.84s/it]\u001b[A\n","1482it [11:33,  9.06s/it]\u001b[A\n","1483it [11:33,  6.41s/it]\u001b[A\n","1484it [11:34,  4.56s/it]\u001b[A\n","1485it [11:34,  3.26s/it]\u001b[A\n","1486it [11:34,  2.35s/it]\u001b[A\n","1487it [11:34,  1.71s/it]\u001b[A\n","1488it [11:34,  1.27s/it]\u001b[A\n","1489it [11:35,  1.04it/s]\u001b[A\n","1490it [11:35,  1.35it/s]\u001b[A\n","1491it [11:35,  1.70it/s]\u001b[A\n","1492it [11:35,  2.08it/s]\u001b[A\n","1493it [11:36,  2.47it/s]\u001b[A\n","1494it [11:36,  2.83it/s]\u001b[A\n","1495it [11:36,  3.16it/s]\u001b[A\n","1496it [11:36,  3.44it/s]\u001b[A\n","1497it [11:37,  3.66it/s]\u001b[A\n","1498it [11:37,  3.84it/s]\u001b[A\n","1499it [11:37,  3.97it/s]\u001b[A\n","1500it [11:37,  4.07it/s]\u001b[A\n","1501it [11:37,  4.15it/s]\u001b[A\n","1502it [11:38,  4.20it/s]\u001b[A\n","1503it [11:38,  4.24it/s]\u001b[A\n","1504it [11:38,  4.27it/s]\u001b[A\n","1505it [11:38,  4.29it/s]\u001b[A\n","1506it [11:39,  4.30it/s]\u001b[A\n","1507it [11:39,  4.31it/s]\u001b[A\n","1508it [11:39,  4.31it/s]\u001b[A\n","1509it [11:39,  4.31it/s]\u001b[A\n","1510it [11:40,  4.32it/s]\u001b[A\n","1511it [11:40,  4.32it/s]\u001b[A\n","1512it [11:40,  4.32it/s]\u001b[A\n","1513it [11:40,  4.33it/s]\u001b[A\n","1514it [11:40,  4.33it/s]\u001b[A\n","1515it [11:41,  4.33it/s]\u001b[A\n","1516it [11:41,  4.33it/s]\u001b[A\n","1517it [11:41,  4.33it/s]\u001b[A\n","1518it [11:41,  4.33it/s]\u001b[A\n","1519it [11:42,  4.33it/s]\u001b[A\n","1520it [11:42,  4.33it/s]\u001b[A\n","1521it [11:42,  4.33it/s]\u001b[A\n","1522it [11:42,  4.33it/s]\u001b[A\n","1523it [11:43,  4.33it/s]\u001b[A\n","1524it [11:43,  4.33it/s]\u001b[A\n","1525it [11:43,  4.33it/s]\u001b[A\n","1526it [11:43,  4.33it/s]\u001b[A\n","1527it [11:43,  4.33it/s]\u001b[A\n","1528it [11:44,  4.33it/s]\u001b[A\n","1529it [11:44,  4.33it/s]\u001b[A\n","1530it [11:44,  4.33it/s]\u001b[A\n","1531it [11:44,  4.33it/s]\u001b[A\n","1532it [11:45,  4.33it/s]\u001b[A\n","1533it [11:45,  4.33it/s]\u001b[A\n","1534it [11:45,  4.33it/s]\u001b[A\n","1535it [11:45,  4.33it/s]\u001b[A\n","1536it [11:46,  4.33it/s]\u001b[A\n","1537it [11:46,  4.33it/s]\u001b[A\n","1538it [11:46,  4.33it/s]\u001b[A\n","1539it [11:46,  4.33it/s]\u001b[A\n","1540it [11:46,  4.33it/s]\u001b[A\n","1541it [11:47,  4.33it/s]\u001b[A\n","1542it [11:47,  4.33it/s]\u001b[A\n","1543it [11:47,  4.33it/s]\u001b[A\n","1544it [11:47,  4.33it/s]\u001b[A\n","1545it [11:48,  4.33it/s]\u001b[A\n","1546it [11:48,  4.33it/s]\u001b[A\n","1547it [11:48,  4.33it/s]\u001b[A\n","1548it [11:48,  4.33it/s]\u001b[A\n","1549it [11:49,  4.33it/s]\u001b[A\n","1550it [11:49,  4.33it/s]\u001b[A\n","1551it [11:49,  4.33it/s]\u001b[A\n","1552it [11:49,  4.33it/s]\u001b[A\n","1553it [11:49,  4.33it/s]\u001b[A\n","1554it [11:50,  4.33it/s]\u001b[A\n","1555it [11:50,  4.33it/s]\u001b[A\n","1556it [11:50,  4.33it/s]\u001b[A\n","1557it [11:50,  4.33it/s]\u001b[A\n","1558it [11:51,  4.33it/s]\u001b[A\n","1559it [11:51,  4.33it/s]\u001b[A\n","1560it [11:51,  4.33it/s]\u001b[A\n","1561it [11:51,  4.33it/s]\u001b[A\n","1562it [11:52,  4.33it/s]\u001b[A\n","1563it [11:52,  4.33it/s]\u001b[A\n","1564it [11:52,  4.33it/s]\u001b[A\n","1565it [11:52,  4.33it/s]\u001b[A\n","1566it [11:52,  4.33it/s]\u001b[A\n","1567it [11:53,  4.33it/s]\u001b[A\n","1568it [11:53,  4.33it/s]\u001b[A\n","1569it [11:53,  4.33it/s]\u001b[A\n","1570it [11:53,  4.33it/s]\u001b[A\n","1571it [11:54,  4.33it/s]\u001b[A\n","1572it [11:54,  4.33it/s]\u001b[A\n","1573it [11:54,  4.33it/s]\u001b[A\n","1574it [11:54,  4.33it/s]\u001b[A\n","1575it [11:55,  4.33it/s]\u001b[A\n","1576it [11:55,  4.33it/s]\u001b[A\n","1577it [11:55,  4.33it/s]\u001b[A\n","1578it [11:55,  4.33it/s]\u001b[A\n","1579it [11:55,  4.33it/s]\u001b[A\n","1580it [11:56,  4.33it/s]\u001b[A\n","1581it [11:56,  4.33it/s]\u001b[A\n","1582it [11:56,  4.33it/s]\u001b[A\n","1583it [11:56,  4.33it/s]\u001b[A\n","1584it [11:57,  4.33it/s]\u001b[A\n","1585it [11:57,  4.33it/s]\u001b[A\n","1586it [11:57,  4.33it/s]\u001b[A\n","1587it [11:57,  4.33it/s]\u001b[A\n","1588it [11:58,  4.33it/s]\u001b[A\n","1589it [11:58,  4.33it/s]\u001b[A\n","1590it [11:58,  4.33it/s]\u001b[A\n","1591it [11:58,  4.33it/s]\u001b[A\n","1592it [11:58,  4.33it/s]\u001b[A\n","1593it [11:59,  4.33it/s]\u001b[A\n","1594it [11:59,  4.33it/s]\u001b[A\n","1595it [11:59,  4.33it/s]\u001b[A\n","1596it [11:59,  4.33it/s]\u001b[A\n","1597it [12:00,  4.33it/s]\u001b[A\n","1598it [12:00,  4.33it/s]\u001b[A\n","1599it [12:00,  4.33it/s]\u001b[A\n","1600it [12:00,  4.33it/s]\u001b[A\n","1601it [12:01,  4.33it/s]\u001b[A\n","1602it [12:01,  4.33it/s]\u001b[A\n","1603it [12:01,  4.33it/s]\u001b[A\n","1604it [12:01,  4.33it/s]\u001b[A\n","1605it [12:01,  4.34it/s]\u001b[A\n","1606it [12:02,  4.34it/s]\u001b[A\n","1607it [12:02,  4.33it/s]\u001b[A\n","1608it [12:02,  4.33it/s]\u001b[A\n","1609it [12:02,  4.34it/s]\u001b[A\n","1610it [12:03,  4.33it/s]\u001b[A\n","1611it [12:03,  4.33it/s]\u001b[A\n","1612it [12:03,  4.33it/s]\u001b[A\n","1613it [12:03,  4.33it/s]\u001b[A\n","1614it [12:04,  4.33it/s]\u001b[A\n","1615it [12:04,  4.33it/s]\u001b[A\n","1616it [12:04,  4.33it/s]\u001b[A\n","1617it [12:04,  4.33it/s]\u001b[A\n","1618it [12:04,  4.33it/s]\u001b[A\n","1619it [12:05,  4.33it/s]\u001b[A\n","1620it [12:05,  4.33it/s]\u001b[A\n","1621it [12:05,  4.33it/s]\u001b[A\n","1622it [12:05,  4.33it/s]\u001b[A\n","1623it [12:06,  4.33it/s]\u001b[A\n","1624it [12:06,  4.33it/s]\u001b[A\n","1625it [12:06,  4.33it/s]\u001b[A\n","1626it [12:06,  4.33it/s]\u001b[A\n","1627it [12:07,  4.33it/s]\u001b[A\n","1628it [12:07,  4.33it/s]\u001b[A\n","1629it [12:07,  4.33it/s]\u001b[A\n","1630it [12:07,  4.33it/s]\u001b[A\n","1631it [12:07,  4.33it/s]\u001b[A\n","1632it [12:08,  4.33it/s]\u001b[A\n","1633it [12:08,  4.33it/s]\u001b[A\n","1634it [12:08,  4.33it/s]\u001b[A\n","1635it [12:08,  4.33it/s]\u001b[A\n","1636it [12:09,  4.33it/s]\u001b[A\n","1637it [12:09,  4.33it/s]\u001b[A\n","1638it [12:09,  4.33it/s]\u001b[A\n","1639it [12:09,  4.33it/s]\u001b[A\n","1640it [12:10,  4.33it/s]\u001b[A\n","1641it [12:10,  4.33it/s]\u001b[A\n","1642it [12:10,  4.33it/s]\u001b[A\n","1643it [12:10,  4.33it/s]\u001b[A\n","1644it [12:10,  4.33it/s]\u001b[A\n","1645it [12:11,  4.33it/s]\u001b[A\n","1646it [12:11,  4.33it/s]\u001b[A\n","1647it [12:11,  4.33it/s]\u001b[A\n","1648it [12:11,  4.33it/s]\u001b[A\n","1649it [12:12,  4.33it/s]\u001b[A\n","1650it [12:12,  4.33it/s]\u001b[A\n","1651it [12:12,  4.33it/s]\u001b[A\n","1652it [12:12,  4.33it/s]\u001b[A\n","1653it [12:13,  4.33it/s]\u001b[A\n","1654it [12:13,  4.33it/s]\u001b[A\n","1655it [12:13,  4.33it/s]\u001b[A\n","1656it [12:13,  4.33it/s]\u001b[A\n","1657it [12:13,  4.34it/s]\u001b[A\n","1658it [12:14,  4.34it/s]\u001b[A\n","1659it [12:14,  4.34it/s]\u001b[A\n","1660it [12:14,  4.34it/s]\u001b[A\n","1661it [12:14,  4.34it/s]\u001b[A\n","1662it [12:15,  4.34it/s]\u001b[A\n","1663it [12:15,  4.34it/s]\u001b[A\n","1664it [12:15,  4.34it/s]\u001b[A\n","Epoch: 100% 249/250 [13:38<00:01,  1.68s/it]\n","1666it [12:57, 12.81s/it]\u001b[A\n","1667it [12:58,  9.04s/it]\u001b[A\n","1668it [12:58,  6.40s/it]\u001b[A\n","1669it [12:58,  4.55s/it]\u001b[A\n","1670it [12:58,  3.25s/it]\u001b[A\n","1671it [12:59,  2.35s/it]\u001b[A\n","1672it [12:59,  1.71s/it]\u001b[A\n","1673it [12:59,  1.27s/it]\u001b[A\n","1674it [12:59,  1.05it/s]\u001b[A\n","1675it [13:00,  1.35it/s]\u001b[A\n","1676it [13:00,  1.71it/s]\u001b[A\n","1677it [13:00,  2.09it/s]\u001b[A\n","1678it [13:00,  2.47it/s]\u001b[A\n","1679it [13:00,  2.84it/s]\u001b[A\n","1680it [13:01,  3.17it/s]\u001b[A\n","1681it [13:01,  3.45it/s]\u001b[A\n","1682it [13:01,  3.67it/s]\u001b[A\n","1683it [13:01,  3.85it/s]\u001b[A\n","1684it [13:02,  3.98it/s]\u001b[A\n","1685it [13:02,  4.08it/s]\u001b[A\n","1686it [13:02,  4.15it/s]\u001b[A\n","1687it [13:02,  4.21it/s]\u001b[A\n","1688it [13:03,  4.25it/s]\u001b[A\n","1689it [13:03,  4.27it/s]\u001b[A\n","1690it [13:03,  4.29it/s]\u001b[A\n","1691it [13:03,  4.30it/s]\u001b[A\n","1692it [13:03,  4.31it/s]\u001b[A\n","1693it [13:04,  4.32it/s]\u001b[A\n","1694it [13:04,  4.32it/s]\u001b[A\n","1695it [13:04,  4.33it/s]\u001b[A\n","1696it [13:04,  4.33it/s]\u001b[A\n","1697it [13:05,  4.33it/s]\u001b[A\n","1698it [13:05,  4.33it/s]\u001b[A\n","1699it [13:05,  4.33it/s]\u001b[A\n","1700it [13:05,  4.33it/s]\u001b[A\n","1701it [13:06,  4.33it/s]\u001b[A\n","1702it [13:06,  4.33it/s]\u001b[A\n","1703it [13:06,  4.33it/s]\u001b[A\n","1704it [13:06,  4.33it/s]\u001b[A\n","1705it [13:06,  4.33it/s]\u001b[A\n","1706it [13:07,  4.33it/s]\u001b[A\n","1707it [13:07,  4.33it/s]\u001b[A\n","1708it [13:07,  4.33it/s]\u001b[A\n","1709it [13:07,  4.33it/s]\u001b[A\n","1710it [13:08,  4.33it/s]\u001b[A\n","1711it [13:08,  4.33it/s]\u001b[A\n","1712it [13:08,  4.33it/s]\u001b[A\n","1713it [13:08,  4.33it/s]\u001b[A\n","1714it [13:09,  4.33it/s]\u001b[A\n","1715it [13:09,  4.33it/s]\u001b[A\n","1716it [13:09,  4.33it/s]\u001b[A\n","1717it [13:09,  4.33it/s]\u001b[A\n","1718it [13:09,  4.33it/s]\u001b[A\n","1719it [13:10,  4.34it/s]\u001b[A\n","1720it [13:10,  4.34it/s]\u001b[A\n","1721it [13:10,  4.34it/s]\u001b[A\n","1722it [13:10,  4.34it/s]\u001b[A\n","1723it [13:11,  4.33it/s]\u001b[A\n","1724it [13:11,  4.33it/s]\u001b[A\n","1725it [13:11,  4.33it/s]\u001b[A\n","1726it [13:11,  4.33it/s]\u001b[A\n","1727it [13:12,  4.33it/s]\u001b[A\n","1728it [13:12,  4.33it/s]\u001b[A\n","1729it [13:12,  4.34it/s]\u001b[A\n","1730it [13:12,  4.34it/s]\u001b[A\n","1731it [13:12,  4.34it/s]\u001b[A\n","1732it [13:13,  4.34it/s]\u001b[A\n","1733it [13:13,  4.33it/s]\u001b[A\n","1734it [13:13,  4.34it/s]\u001b[A\n","1735it [13:13,  4.33it/s]\u001b[A\n","1736it [13:14,  4.33it/s]\u001b[A\n","1737it [13:14,  4.33it/s]\u001b[A\n","1738it [13:14,  4.34it/s]\u001b[A\n","1739it [13:14,  4.34it/s]\u001b[A\n","1740it [13:15,  4.33it/s]\u001b[A\n","1741it [13:15,  4.34it/s]\u001b[A\n","1742it [13:15,  4.34it/s]\u001b[A\n","1743it [13:15,  4.33it/s]\u001b[A\n","1744it [13:15,  4.34it/s]\u001b[A\n","1745it [13:16,  4.33it/s]\u001b[A\n","1746it [13:16,  4.33it/s]\u001b[A\n","1747it [13:16,  4.34it/s]\u001b[A\n","1748it [13:16,  4.34it/s]\u001b[A\n","1749it [13:17,  4.34it/s]\u001b[A\n","1750it [13:17,  4.34it/s]\u001b[A\n","1751it [13:17,  4.34it/s]\u001b[A\n","1752it [13:17,  4.34it/s]\u001b[A\n","1753it [13:18,  4.33it/s]\u001b[A\n","1754it [13:18,  4.33it/s]\u001b[A\n","1755it [13:18,  4.33it/s]\u001b[A\n","1756it [13:18,  4.33it/s]\u001b[A\n","1757it [13:18,  4.33it/s]\u001b[A\n","1758it [13:19,  4.33it/s]\u001b[A\n","1759it [13:19,  4.33it/s]\u001b[A\n","1760it [13:19,  4.33it/s]\u001b[A\n","1761it [13:19,  4.33it/s]\u001b[A\n","1762it [13:20,  4.33it/s]\u001b[A\n","1763it [13:20,  4.33it/s]\u001b[A\n","1764it [13:20,  4.33it/s]\u001b[A\n","1765it [13:20,  4.33it/s]\u001b[A\n","1766it [13:21,  4.33it/s]\u001b[A\n","1767it [13:21,  4.33it/s]\u001b[A\n","1768it [13:21,  4.33it/s]\u001b[A\n","1769it [13:21,  4.34it/s]\u001b[A\n","1770it [13:21,  4.34it/s]\u001b[A\n","1771it [13:22,  4.34it/s]\u001b[A\n","1772it [13:22,  4.34it/s]\u001b[A\n","1773it [13:22,  4.34it/s]\u001b[A\n","1774it [13:22,  4.34it/s]\u001b[A\n","1775it [13:23,  4.33it/s]\u001b[A\n","1776it [13:23,  4.33it/s]\u001b[A\n","1777it [13:23,  4.33it/s]\u001b[A\n","1778it [13:23,  4.33it/s]\u001b[A\n","1779it [13:24,  4.33it/s]\u001b[A\n","1780it [13:24,  4.33it/s]\u001b[A\n","1781it [13:24,  4.33it/s]\u001b[A\n","1782it [13:24,  4.33it/s]\u001b[A\n","1783it [13:24,  4.33it/s]\u001b[A\n","1784it [13:25,  4.33it/s]\u001b[A\n","1785it [13:25,  4.33it/s]\u001b[A\n","1786it [13:25,  4.33it/s]\u001b[A\n","1787it [13:25,  4.33it/s]\u001b[A\n","1788it [13:26,  4.33it/s]\u001b[A\n","1789it [13:26,  4.33it/s]\u001b[A\n","1790it [13:26,  4.34it/s]\u001b[A\n","1791it [13:26,  4.33it/s]\u001b[A\n","1792it [13:27,  4.33it/s]\u001b[A\n","1793it [13:27,  4.33it/s]\u001b[A\n","1794it [13:27,  4.33it/s]\u001b[A\n","1795it [13:27,  4.33it/s]\u001b[A\n","1796it [13:27,  4.33it/s]\u001b[A\n","1797it [13:28,  4.33it/s]\u001b[A\n","1798it [13:28,  4.33it/s]\u001b[A\n","1799it [13:28,  4.33it/s]\u001b[A\n","1800it [13:28,  4.33it/s]\u001b[A\n","1801it [13:29,  4.33it/s]\u001b[A\n","1802it [13:29,  4.34it/s]\u001b[A\n","1803it [13:29,  4.33it/s]\u001b[A\n","1804it [13:29,  4.33it/s]\u001b[A\n","1805it [13:30,  4.33it/s]\u001b[A\n","1806it [13:30,  4.33it/s]\u001b[A\n","1807it [13:30,  4.33it/s]\u001b[A\n","1808it [13:30,  4.34it/s]\u001b[A\n","1809it [13:30,  4.33it/s]\u001b[A\n","1810it [13:31,  4.33it/s]\u001b[A\n","1811it [13:31,  4.33it/s]\u001b[A\n","1812it [13:31,  4.33it/s]\u001b[A\n","1813it [13:31,  4.33it/s]\u001b[A\n","1814it [13:32,  4.33it/s]\u001b[A\n","1815it [13:32,  4.33it/s]\u001b[A\n","1816it [13:32,  4.33it/s]\u001b[A\n","1817it [13:32,  4.34it/s]\u001b[A\n","1818it [13:33,  4.34it/s]\u001b[A\n","1819it [13:33,  4.33it/s]\u001b[A\n","1820it [13:33,  4.33it/s]\u001b[A\n","1821it [13:33,  4.34it/s]\u001b[A\n","1822it [13:33,  4.34it/s]\u001b[A\n","1823it [13:34,  4.34it/s]\u001b[A\n","1824it [13:34,  4.34it/s]\u001b[A\n","1825it [13:34,  4.34it/s]\u001b[A\n","1826it [13:34,  4.33it/s]\u001b[A\n","1827it [13:35,  4.34it/s]\u001b[A\n","1828it [13:35,  4.34it/s]\u001b[A\n","1829it [13:35,  4.34it/s]\u001b[A\n","1830it [13:35,  4.34it/s]\u001b[A\n","1831it [13:36,  4.34it/s]\u001b[A\n","1832it [13:36,  4.33it/s]\u001b[A\n","1833it [13:36,  4.33it/s]\u001b[A\n","1834it [13:36,  4.34it/s]\u001b[A\n","1835it [13:36,  4.34it/s]\u001b[A\n","1836it [13:37,  4.33it/s]\u001b[A\n","1837it [13:37,  4.33it/s]\u001b[A\n","1838it [13:37,  4.34it/s]\u001b[A\n","1839it [13:37,  4.34it/s]\u001b[A\n","1840it [13:38,  4.34it/s]\u001b[A\n","1841it [13:38,  4.34it/s]\u001b[A\n","1842it [13:38,  4.34it/s]\u001b[A\n","1843it [13:38,  4.33it/s]\u001b[A\n","1844it [13:39,  4.34it/s]\u001b[A\n","1845it [13:39,  4.33it/s]\u001b[A\n","1846it [13:39,  4.33it/s]\u001b[A\n","1847it [13:39,  4.33it/s]\u001b[A\n","1848it [13:39,  4.33it/s]\u001b[A\n","1849it [13:40,  4.34it/s]\u001b[A\n","Epoch: 100% 250/250 [14:23<00:00,  3.45s/it]\n","03/14/2022 09:31:57 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/14/2022 09:32:11 - INFO - __main__ -   *** Validate ***\n","\n","1851it [13:54,  4.30s/it]\u001b[A\n","1852it [13:54,  3.08s/it]\u001b[A\n","1853it [13:54,  2.22s/it]\u001b[A\n","1854it [13:54,  1.63s/it]\u001b[A\n","1855it [13:55,  1.21s/it]\u001b[A\n","1856it [13:55,  1.09it/s]\u001b[A\n","1857it [13:55,  1.41it/s]\u001b[A\n","1858it [13:55,  1.77it/s]\u001b[A\n","1859it [13:56,  2.15it/s]\u001b[A\n","1860it [13:56,  2.53it/s]\u001b[A\n","1861it [13:56,  2.89it/s]\u001b[A\n","1862it [13:56,  3.21it/s]\u001b[A\n","1863it [13:56,  3.48it/s]\u001b[A\n","1864it [13:57,  3.70it/s]\u001b[A\n","1865it [13:57,  3.87it/s]\u001b[A\n","1866it [13:57,  3.99it/s]\u001b[A\n","1867it [13:57,  4.09it/s]\u001b[A\n","1868it [13:58,  4.16it/s]\u001b[A\n","1869it [13:58,  4.21it/s]\u001b[A\n","1870it [13:58,  4.24it/s]\u001b[A\n","1871it [13:58,  4.27it/s]\u001b[A\n","1872it [13:59,  4.29it/s]\u001b[A\n","1873it [13:59,  4.31it/s]\u001b[A\n","1874it [13:59,  4.31it/s]\u001b[A\n","1875it [13:59,  4.32it/s]\u001b[A\n","1876it [13:59,  4.32it/s]\u001b[A\n","1877it [14:00,  4.32it/s]\u001b[A\n","1878it [14:00,  4.32it/s]\u001b[A\n","1879it [14:00,  4.32it/s]\u001b[A\n","1880it [14:00,  4.33it/s]\u001b[A\n","1881it [14:01,  4.33it/s]\u001b[A\n","1882it [14:01,  4.33it/s]\u001b[A\n","1883it [14:01,  4.33it/s]\u001b[A\n","1884it [14:01,  4.33it/s]\u001b[A\n","1885it [14:02,  4.33it/s]\u001b[A\n","1886it [14:02,  4.33it/s]\u001b[A\n","1887it [14:02,  4.33it/s]\u001b[A\n","1888it [14:02,  4.33it/s]\u001b[A\n","1889it [14:02,  4.33it/s]\u001b[A\n","1890it [14:03,  4.33it/s]\u001b[A\n","1891it [14:03,  4.33it/s]\u001b[A\n","1892it [14:03,  4.33it/s]\u001b[A\n","1893it [14:03,  4.33it/s]\u001b[A\n","1894it [14:04,  4.33it/s]\u001b[A\n","1895it [14:04,  4.33it/s]\u001b[A\n","1896it [14:04,  4.33it/s]\u001b[A\n","1897it [14:04,  4.33it/s]\u001b[A\n","1898it [14:05,  4.33it/s]\u001b[A\n","1899it [14:05,  4.33it/s]\u001b[A\n","1900it [14:05,  4.33it/s]\u001b[A\n","1901it [14:05,  4.33it/s]\u001b[A\n","1902it [14:05,  4.33it/s]\u001b[A\n","1903it [14:06,  4.33it/s]\u001b[A\n","1904it [14:06,  4.33it/s]\u001b[A\n","1905it [14:06,  4.33it/s]\u001b[A\n","1906it [14:06,  4.33it/s]\u001b[A\n","1907it [14:07,  4.33it/s]\u001b[A\n","1908it [14:07,  4.33it/s]\u001b[A\n","1909it [14:07,  4.33it/s]\u001b[A\n","1910it [14:07,  4.33it/s]\u001b[A\n","1911it [14:08,  4.33it/s]\u001b[A\n","1912it [14:08,  4.33it/s]\u001b[A\n","1913it [14:08,  4.33it/s]\u001b[A\n","1914it [14:08,  4.33it/s]\u001b[A\n","1915it [14:08,  4.33it/s]\u001b[A\n","1916it [14:09,  4.33it/s]\u001b[A\n","1917it [14:09,  4.33it/s]\u001b[A\n","1918it [14:09,  4.33it/s]\u001b[A\n","1919it [14:09,  4.33it/s]\u001b[A\n","1920it [14:10,  4.33it/s]\u001b[A\n","1921it [14:10,  4.33it/s]\u001b[A\n","1922it [14:10,  4.33it/s]\u001b[A\n","1923it [14:10,  4.33it/s]\u001b[A\n","1924it [14:11,  4.33it/s]\u001b[A\n","1925it [14:11,  4.33it/s]\u001b[A\n","1926it [14:11,  4.33it/s]\u001b[A\n","1927it [14:11,  4.33it/s]\u001b[A\n","1928it [14:11,  4.33it/s]\u001b[A\n","1929it [14:12,  4.33it/s]\u001b[A\n","1930it [14:12,  4.33it/s]\u001b[A\n","1931it [14:12,  4.33it/s]\u001b[A\n","1932it [14:12,  4.33it/s]\u001b[A\n","1933it [14:13,  4.33it/s]\u001b[A\n","1934it [14:13,  4.33it/s]\u001b[A\n","1935it [14:13,  4.33it/s]\u001b[A\n","1936it [14:13,  4.33it/s]\u001b[A\n","1937it [14:14,  4.33it/s]\u001b[A\n","1938it [14:14,  4.33it/s]\u001b[A\n","1939it [14:14,  4.33it/s]\u001b[A\n","1940it [14:14,  4.33it/s]\u001b[A\n","1941it [14:14,  4.33it/s]\u001b[A\n","1942it [14:15,  4.33it/s]\u001b[A\n","1943it [14:15,  4.33it/s]\u001b[A\n","1944it [14:15,  4.33it/s]\u001b[A\n","1945it [14:15,  4.33it/s]\u001b[A\n","1946it [14:16,  4.33it/s]\u001b[A\n","1947it [14:16,  4.33it/s]\u001b[A\n","1948it [14:16,  4.33it/s]\u001b[A\n","1949it [14:16,  4.33it/s]\u001b[A\n","1950it [14:17,  4.33it/s]\u001b[A\n","1951it [14:17,  4.33it/s]\u001b[A\n","1952it [14:17,  4.33it/s]\u001b[A\n","1953it [14:17,  4.33it/s]\u001b[A\n","1954it [14:17,  4.33it/s]\u001b[A\n","1955it [14:18,  4.33it/s]\u001b[A\n","1956it [14:18,  4.33it/s]\u001b[A\n","1957it [14:18,  4.33it/s]\u001b[A\n","1958it [14:18,  4.33it/s]\u001b[A\n","1959it [14:19,  4.33it/s]\u001b[A\n","1960it [14:19,  4.33it/s]\u001b[A\n","1961it [14:19,  4.33it/s]\u001b[A\n","1962it [14:19,  4.33it/s]\u001b[A\n","1963it [14:20,  4.33it/s]\u001b[A\n","1964it [14:20,  4.33it/s]\u001b[A\n","1965it [14:20,  4.33it/s]\u001b[A\n","1966it [14:20,  4.33it/s]\u001b[A\n","1967it [14:20,  4.33it/s]\u001b[A\n","1968it [14:21,  4.32it/s]\u001b[A\n","1969it [14:21,  4.32it/s]\u001b[A\n","1970it [14:21,  4.33it/s]\u001b[A\n","1971it [14:21,  4.33it/s]\u001b[A\n","1972it [14:22,  4.32it/s]\u001b[A\n","1973it [14:22,  4.33it/s]\u001b[A\n","1974it [14:22,  4.33it/s]\u001b[A\n","1975it [14:22,  4.33it/s]\u001b[A\n","1976it [14:23,  4.33it/s]\u001b[A\n","1977it [14:23,  4.33it/s]\u001b[A\n","1978it [14:23,  4.33it/s]\u001b[A\n","1979it [14:23,  4.33it/s]\u001b[A\n","1980it [14:23,  4.33it/s]\u001b[A\n","1981it [14:24,  4.33it/s]\u001b[A\n","1982it [14:24,  4.33it/s]\u001b[A\n","1983it [14:24,  4.33it/s]\u001b[A\n","1984it [14:24,  4.33it/s]\u001b[A\n","1985it [14:25,  4.33it/s]\u001b[A\n","1986it [14:25,  4.33it/s]\u001b[A\n","1987it [14:25,  4.33it/s]\u001b[A\n","1988it [14:25,  4.33it/s]\u001b[A\n","1989it [14:26,  4.33it/s]\u001b[A\n","1990it [14:26,  4.33it/s]\u001b[A\n","1991it [14:26,  4.33it/s]\u001b[A\n","1992it [14:26,  4.33it/s]\u001b[A\n","1993it [14:26,  4.33it/s]\u001b[A\n","1994it [14:27,  4.33it/s]\u001b[A\n","1995it [14:27,  4.33it/s]\u001b[A\n","1996it [14:27,  4.33it/s]\u001b[A\n","1997it [14:27,  4.33it/s]\u001b[A\n","1998it [14:28,  4.33it/s]\u001b[A\n","1999it [14:28,  4.33it/s]\u001b[A\n","2000it [14:28,  4.33it/s]\u001b[A\n","2001it [14:28,  4.33it/s]\u001b[A\n","2002it [14:29,  4.33it/s]\u001b[A\n","2003it [14:29,  4.33it/s]\u001b[A\n","2004it [14:29,  4.33it/s]\u001b[A\n","2005it [14:29,  4.33it/s]\u001b[A\n","2006it [14:29,  4.33it/s]\u001b[A\n","2007it [14:30,  4.33it/s]\u001b[A\n","2008it [14:30,  4.33it/s]\u001b[A\n","2009it [14:30,  4.33it/s]\u001b[A\n","2010it [14:30,  4.33it/s]\u001b[A\n","2011it [14:31,  4.33it/s]\u001b[A\n","2012it [14:31,  4.33it/s]\u001b[A\n","2013it [14:31,  4.33it/s]\u001b[A\n","2014it [14:31,  4.33it/s]\u001b[A\n","2015it [14:32,  4.33it/s]\u001b[A\n","2016it [14:32,  4.33it/s]\u001b[A\n","2017it [14:32,  4.33it/s]\u001b[A\n","2018it [14:32,  4.33it/s]\u001b[A\n","2019it [14:33,  4.33it/s]\u001b[A\n","2020it [14:33,  4.33it/s]\u001b[A\n","2021it [14:33,  4.33it/s]\u001b[A\n","2022it [14:33,  4.33it/s]\u001b[A\n","2023it [14:33,  4.33it/s]\u001b[A\n","2024it [14:34,  4.33it/s]\u001b[A\n","2025it [14:34,  4.33it/s]\u001b[A\n","2026it [14:34,  4.33it/s]\u001b[A\n","2027it [14:34,  4.33it/s]\u001b[A\n","2028it [14:35,  4.33it/s]\u001b[A\n","2029it [14:35,  4.33it/s]\u001b[A\n","2030it [14:35,  4.33it/s]\u001b[A\n","2031it [14:35,  4.33it/s]\u001b[A\n","2032it [14:36,  4.33it/s]\u001b[A\n","2033it [14:36,  4.33it/s]\u001b[A\n","2034it [14:36,  4.33it/s]\u001b[A\n","2035it [14:36,  4.33it/s]\u001b[A03/14/2022 09:32:54 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/14/2022 09:32:54 - INFO - __main__ -     eval_loss = 2.6190853118896484\n","03/14/2022 09:32:54 - INFO - __main__ -     eval_auroc = 0.8730000853538513\n","03/14/2022 09:32:54 - INFO - __main__ -     eval_recall = 0.9200000166893005\n","03/14/2022 09:32:54 - INFO - __main__ -     eval_f1 = 0.47422680258750916\n","03/14/2022 09:32:54 - INFO - root -   *** Test ***\n","\n","2036it [14:36,  4.28it/s]\u001b[A\n","2037it [14:37,  4.29it/s]\u001b[A\n","2038it [14:37,  4.30it/s]\u001b[A\n","2039it [14:37,  4.31it/s]\u001b[A\n","2040it [14:37,  4.32it/s]\u001b[A\n","2041it [14:38,  4.32it/s]\u001b[A\n","2042it [14:38,  4.33it/s]\u001b[A\n","2043it [14:38,  4.33it/s]\u001b[A\n","2044it [14:38,  4.33it/s]\u001b[A\n","2045it [14:39,  4.33it/s]\u001b[A\n","2046it [14:39,  4.33it/s]\u001b[A\n","2047it [14:39,  4.33it/s]\u001b[A\n","2048it [14:39,  4.33it/s]\u001b[A\n","2049it [14:39,  4.33it/s]\u001b[A\n","2050it [14:40,  4.33it/s]\u001b[A\n","2051it [14:40,  4.33it/s]\u001b[A\n","2052it [14:40,  4.33it/s]\u001b[A\n","2053it [14:40,  4.33it/s]\u001b[A\n","2054it [14:41,  4.33it/s]\u001b[A\n","2055it [14:41,  4.33it/s]\u001b[A\n","2056it [14:41,  4.33it/s]\u001b[A\n","2057it [14:41,  4.33it/s]\u001b[A\n","2058it [14:42,  4.33it/s]\u001b[A\n","2059it [14:42,  4.33it/s]\u001b[A\n","2060it [14:42,  4.33it/s]\u001b[A\n","2061it [14:42,  4.33it/s]\u001b[A\n","2062it [14:42,  4.33it/s]\u001b[A\n","2063it [14:43,  4.33it/s]\u001b[A\n","2064it [14:43,  4.33it/s]\u001b[A\n","2065it [14:43,  4.33it/s]\u001b[A\n","2066it [14:43,  4.33it/s]\u001b[A\n","2067it [14:44,  4.33it/s]\u001b[A\n","2068it [14:44,  4.33it/s]\u001b[A\n","2069it [14:44,  4.33it/s]\u001b[A\n","2070it [14:44,  4.33it/s]\u001b[A\n","2071it [14:45,  4.33it/s]\u001b[A\n","2072it [14:45,  4.33it/s]\u001b[A\n","2073it [14:45,  4.33it/s]\u001b[A\n","2074it [14:45,  4.33it/s]\u001b[A\n","2075it [14:45,  4.33it/s]\u001b[A\n","2076it [14:46,  4.33it/s]\u001b[A\n","2077it [14:46,  4.33it/s]\u001b[A\n","2078it [14:46,  4.33it/s]\u001b[A\n","2079it [14:46,  4.33it/s]\u001b[A\n","2080it [14:47,  4.33it/s]\u001b[A\n","2081it [14:47,  4.33it/s]\u001b[A\n","2082it [14:47,  4.33it/s]\u001b[A\n","2083it [14:47,  4.34it/s]\u001b[A\n","2084it [14:48,  4.33it/s]\u001b[A\n","2085it [14:48,  4.33it/s]\u001b[A\n","2086it [14:48,  4.33it/s]\u001b[A\n","2087it [14:48,  4.33it/s]\u001b[A\n","2088it [14:48,  4.33it/s]\u001b[A\n","2089it [14:49,  4.33it/s]\u001b[A\n","2090it [14:49,  4.33it/s]\u001b[A\n","2091it [14:49,  4.33it/s]\u001b[A\n","2092it [14:49,  4.33it/s]\u001b[A\n","2093it [14:50,  4.33it/s]\u001b[A\n","2094it [14:50,  4.33it/s]\u001b[A\n","2095it [14:50,  4.33it/s]\u001b[A\n","2096it [14:50,  4.33it/s]\u001b[A\n","2097it [14:51,  4.33it/s]\u001b[A\n","2098it [14:51,  4.33it/s]\u001b[A\n","2099it [14:51,  4.33it/s]\u001b[A\n","2100it [14:51,  4.33it/s]\u001b[A\n","2101it [14:51,  4.33it/s]\u001b[A\n","2102it [14:52,  4.33it/s]\u001b[A\n","2103it [14:52,  4.33it/s]\u001b[A\n","2104it [14:52,  4.33it/s]\u001b[A\n","2105it [14:52,  4.33it/s]\u001b[A\n","2106it [14:53,  4.33it/s]\u001b[A\n","2107it [14:53,  4.33it/s]\u001b[A\n","2108it [14:53,  4.33it/s]\u001b[A\n","2109it [14:53,  4.33it/s]\u001b[A\n","2110it [14:54,  4.33it/s]\u001b[A\n","2111it [14:54,  4.33it/s]\u001b[A\n","2112it [14:54,  4.33it/s]\u001b[A\n","2113it [14:54,  4.33it/s]\u001b[A\n","2114it [14:54,  4.33it/s]\u001b[A\n","2115it [14:55,  4.33it/s]\u001b[A\n","2116it [14:55,  4.33it/s]\u001b[A\n","2117it [14:55,  4.33it/s]\u001b[A\n","2118it [14:55,  4.33it/s]\u001b[A\n","2119it [14:56,  4.33it/s]\u001b[A\n","2120it [14:56,  4.33it/s]\u001b[A\n","2121it [14:56,  4.33it/s]\u001b[A\n","2122it [14:56,  4.33it/s]\u001b[A\n","2123it [14:57,  4.33it/s]\u001b[A\n","2124it [14:57,  4.33it/s]\u001b[A\n","2125it [14:57,  4.33it/s]\u001b[A\n","2126it [14:57,  4.33it/s]\u001b[A\n","2127it [14:57,  4.33it/s]\u001b[A\n","2128it [14:58,  4.33it/s]\u001b[A\n","2129it [14:58,  4.33it/s]\u001b[A\n","2130it [14:58,  4.33it/s]\u001b[A\n","2131it [14:58,  4.33it/s]\u001b[A\n","2132it [14:59,  4.33it/s]\u001b[A\n","2133it [14:59,  4.33it/s]\u001b[A\n","2134it [14:59,  4.33it/s]\u001b[A\n","2135it [14:59,  4.33it/s]\u001b[A\n","2136it [15:00,  4.33it/s]\u001b[A\n","2137it [15:00,  4.33it/s]\u001b[A\n","2138it [15:00,  4.33it/s]\u001b[A\n","2139it [15:00,  4.33it/s]\u001b[A\n","2140it [15:00,  4.33it/s]\u001b[A\n","2141it [15:01,  4.33it/s]\u001b[A\n","2142it [15:01,  4.33it/s]\u001b[A\n","2143it [15:01,  4.33it/s]\u001b[A\n","2144it [15:01,  4.33it/s]\u001b[A\n","2145it [15:02,  4.33it/s]\u001b[A\n","2146it [15:02,  4.33it/s]\u001b[A\n","2147it [15:02,  4.33it/s]\u001b[A\n","2148it [15:02,  4.33it/s]\u001b[A\n","2149it [15:03,  4.33it/s]\u001b[A\n","2150it [15:03,  4.33it/s]\u001b[A\n","2151it [15:03,  4.33it/s]\u001b[A\n","2152it [15:03,  4.34it/s]\u001b[A\n","2153it [15:03,  4.33it/s]\u001b[A\n","2154it [15:04,  4.33it/s]\u001b[A\n","2155it [15:04,  4.33it/s]\u001b[A\n","2156it [15:04,  4.33it/s]\u001b[A\n","2157it [15:04,  4.33it/s]\u001b[A\n","2158it [15:05,  4.33it/s]\u001b[A\n","2159it [15:05,  4.33it/s]\u001b[A\n","2160it [15:05,  4.33it/s]\u001b[A\n","2161it [15:05,  4.33it/s]\u001b[A\n","2162it [15:06,  4.33it/s]\u001b[A\n","2163it [15:06,  4.33it/s]\u001b[A\n","2164it [15:06,  4.33it/s]\u001b[A\n","2165it [15:06,  4.33it/s]\u001b[A\n","2166it [15:06,  4.33it/s]\u001b[A\n","2167it [15:07,  4.33it/s]\u001b[A\n","2168it [15:07,  4.33it/s]\u001b[A\n","2169it [15:07,  4.33it/s]\u001b[A\n","2170it [15:07,  4.33it/s]\u001b[A\n","2171it [15:08,  4.33it/s]\u001b[A\n","2172it [15:08,  4.33it/s]\u001b[A\n","2173it [15:08,  4.33it/s]\u001b[A\n","2174it [15:08,  4.33it/s]\u001b[A\n","2175it [15:09,  4.33it/s]\u001b[A\n","2176it [15:09,  4.33it/s]\u001b[A\n","2177it [15:09,  4.33it/s]\u001b[A\n","2178it [15:09,  4.33it/s]\u001b[A\n","2179it [15:09,  4.33it/s]\u001b[A\n","2180it [15:10,  4.33it/s]\u001b[A\n","2181it [15:10,  4.33it/s]\u001b[A\n","2182it [15:10,  4.33it/s]\u001b[A\n","2183it [15:10,  4.33it/s]\u001b[A\n","2184it [15:11,  4.33it/s]\u001b[A\n","2185it [15:11,  4.33it/s]\u001b[A\n","2186it [15:11,  4.33it/s]\u001b[A\n","2187it [15:11,  4.33it/s]\u001b[A\n","2188it [15:12,  4.33it/s]\u001b[A\n","2189it [15:12,  4.33it/s]\u001b[A\n","2190it [15:12,  4.33it/s]\u001b[A\n","2191it [15:12,  4.33it/s]\u001b[A\n","2192it [15:12,  4.33it/s]\u001b[A\n","2193it [15:13,  4.33it/s]\u001b[A\n","2194it [15:13,  4.33it/s]\u001b[A\n","2195it [15:13,  4.33it/s]\u001b[A\n","2196it [15:13,  4.33it/s]\u001b[A\n","2197it [15:14,  4.33it/s]\u001b[A\n","2198it [15:14,  4.33it/s]\u001b[A\n","2199it [15:14,  4.33it/s]\u001b[A\n","2200it [15:14,  4.33it/s]\u001b[A\n","2201it [15:15,  4.33it/s]\u001b[A\n","2202it [15:15,  4.33it/s]\u001b[A\n","2203it [15:15,  4.33it/s]\u001b[A\n","2204it [15:15,  4.33it/s]\u001b[A\n","2205it [15:15,  4.33it/s]\u001b[A\n","2206it [15:16,  4.33it/s]\u001b[A\n","2207it [15:16,  4.33it/s]\u001b[A\n","2208it [15:16,  4.33it/s]\u001b[A\n","2209it [15:16,  4.33it/s]\u001b[A\n","2210it [15:17,  4.33it/s]\u001b[A\n","2211it [15:17,  4.33it/s]\u001b[A\n","2212it [15:17,  4.33it/s]\u001b[A\n","2213it [15:17,  4.33it/s]\u001b[A\n","2214it [15:18,  4.33it/s]\u001b[A\n","2215it [15:18,  4.33it/s]\u001b[A\n","2216it [15:18,  4.33it/s]\u001b[A\n","2217it [15:18,  4.33it/s]\u001b[A\n","2218it [15:18,  4.33it/s]\u001b[A\n","2219it [15:19,  4.33it/s]\u001b[A\n","2220it [15:19,  4.33it/s]\u001b[A\n","2221it [15:19,  4.33it/s]\u001b[A\n","2222it [15:19,  4.33it/s]\u001b[A\n","2223it [15:20,  4.33it/s]\u001b[A\n","2224it [15:20,  4.33it/s]\u001b[A\n","2225it [15:20,  4.33it/s]\u001b[A\n","2226it [15:20,  4.33it/s]\u001b[A\n","2227it [15:21,  4.33it/s]\u001b[A\n","2228it [15:21,  4.33it/s]\u001b[A\n","2229it [15:21,  4.33it/s]\u001b[A\n","2230it [15:21,  4.33it/s]\u001b[A\n","2231it [15:21,  4.32it/s]\u001b[A\n","2232it [15:22,  4.33it/s]\u001b[A\n","2233it [15:22,  4.33it/s]\u001b[A\n","2234it [15:22,  4.33it/s]\u001b[A\n","2235it [15:22,  4.33it/s]\u001b[A\n","2236it [15:23,  4.33it/s]\u001b[A\n","2237it [15:23,  4.33it/s]\u001b[A\n","2238it [15:23,  4.33it/s]\u001b[A\n","2239it [15:23,  4.33it/s]\u001b[A\n","2240it [15:24,  4.33it/s]\u001b[A\n","2241it [15:24,  4.33it/s]\u001b[A\n","2242it [15:24,  4.33it/s]\u001b[A\n","2243it [15:24,  4.33it/s]\u001b[A\n","2244it [15:24,  4.33it/s]\u001b[A\n","2245it [15:25,  4.33it/s]\u001b[A\n","2246it [15:25,  4.33it/s]\u001b[A\n","2247it [15:25,  4.33it/s]\u001b[A\n","2248it [15:25,  4.33it/s]\u001b[A\n","2249it [15:26,  4.33it/s]\u001b[A\n","2250it [15:26,  4.33it/s]\u001b[A\n","2251it [15:26,  4.33it/s]\u001b[A\n","2252it [15:26,  4.33it/s]\u001b[A\n","2253it [15:27,  4.33it/s]\u001b[A\n","2254it [15:27,  4.33it/s]\u001b[A\n","2255it [15:27,  4.33it/s]\u001b[A\n","2256it [15:27,  4.33it/s]\u001b[A\n","2257it [15:27,  4.33it/s]\u001b[A\n","2258it [15:28,  4.33it/s]\u001b[A\n","2259it [15:28,  4.33it/s]\u001b[A\n","2260it [15:28,  4.33it/s]\u001b[A\n","2261it [15:28,  4.33it/s]\u001b[A\n","2262it [15:29,  4.33it/s]\u001b[A\n","2263it [15:29,  4.33it/s]\u001b[A\n","2264it [15:29,  4.33it/s]\u001b[A\n","2265it [15:29,  4.33it/s]\u001b[A\n","2266it [15:30,  4.33it/s]\u001b[A\n","2267it [15:30,  4.33it/s]\u001b[A\n","2268it [15:30,  4.33it/s]\u001b[A\n","2269it [15:30,  4.33it/s]\u001b[A\n","2270it [15:30,  4.33it/s]\u001b[A\n","2271it [15:31,  4.33it/s]\u001b[A\n","2272it [15:31,  4.33it/s]\u001b[A\n","2273it [15:31,  4.33it/s]\u001b[A\n","2274it [15:31,  4.33it/s]\u001b[A\n","2275it [15:32,  4.33it/s]\u001b[A\n","2276it [15:32,  4.33it/s]\u001b[A\n","2277it [15:32,  4.33it/s]\u001b[A\n","2278it [15:32,  4.33it/s]\u001b[A\n","2279it [15:33,  4.33it/s]\u001b[A\n","2280it [15:33,  4.33it/s]\u001b[A\n","2281it [15:33,  4.33it/s]\u001b[A\n","2282it [15:33,  4.33it/s]\u001b[A\n","2283it [15:33,  4.33it/s]\u001b[A\n","2284it [15:34,  4.33it/s]\u001b[A\n","2285it [15:34,  4.33it/s]\u001b[A\n","2286it [15:34,  4.33it/s]\u001b[A\n","2287it [15:34,  4.33it/s]\u001b[A\n","2288it [15:35,  4.33it/s]\u001b[A\n","2289it [15:35,  4.33it/s]\u001b[A\n","2290it [15:35,  4.33it/s]\u001b[A\n","2291it [15:35,  4.33it/s]\u001b[A\n","2292it [15:36,  4.33it/s]\u001b[A\n","2293it [15:36,  4.33it/s]\u001b[A\n","2294it [15:36,  4.33it/s]\u001b[A\n","2295it [15:36,  4.33it/s]\u001b[A\n","2296it [15:36,  4.33it/s]\u001b[A\n","2297it [15:37,  4.33it/s]\u001b[A\n","2298it [15:37,  4.33it/s]\u001b[A\n","2299it [15:37,  4.34it/s]\u001b[A\n","2300it [15:37,  4.33it/s]\u001b[A\n","2301it [15:38,  4.33it/s]\u001b[A\n","2302it [15:38,  4.33it/s]\u001b[A\n","2303it [15:38,  4.33it/s]\u001b[A\n","2304it [15:38,  4.33it/s]\u001b[A\n","2305it [15:39,  4.33it/s]\u001b[A\n","2306it [15:39,  4.33it/s]\u001b[A\n","2307it [15:39,  4.33it/s]\u001b[A\n","2308it [15:39,  4.33it/s]\u001b[A\n","2309it [15:39,  4.33it/s]\u001b[A\n","2310it [15:40,  4.33it/s]\u001b[A\n","2311it [15:40,  4.33it/s]\u001b[A\n","2312it [15:40,  4.33it/s]\u001b[A\n","2313it [15:40,  4.33it/s]\u001b[A\n","2314it [15:41,  4.33it/s]\u001b[A\n","2315it [15:41,  4.33it/s]\u001b[A\n","2316it [15:41,  4.33it/s]\u001b[A\n","2317it [15:41,  4.33it/s]\u001b[A\n","2318it [15:42,  4.33it/s]\u001b[A\n","2319it [15:42,  4.33it/s]\u001b[A\n","2320it [15:42,  4.33it/s]\u001b[A\n","2321it [15:42,  4.33it/s]\u001b[A\n","2322it [15:42,  4.33it/s]\u001b[A\n","2323it [15:43,  4.33it/s]\u001b[A\n","2324it [15:43,  4.33it/s]\u001b[A\n","2325it [15:43,  4.33it/s]\u001b[A\n","2326it [15:43,  4.33it/s]\u001b[A\n","2327it [15:44,  4.33it/s]\u001b[A\n","2328it [15:44,  4.33it/s]\u001b[A\n","2329it [15:44,  4.33it/s]\u001b[A\n","2330it [15:44,  4.33it/s]\u001b[A\n","2331it [15:45,  4.33it/s]\u001b[A\n","2332it [15:45,  4.33it/s]\u001b[A\n","2333it [15:45,  4.33it/s]\u001b[A\n","2334it [15:45,  4.33it/s]\u001b[A\n","2335it [15:45,  4.33it/s]\u001b[A\n","2336it [15:46,  4.33it/s]\u001b[A\n","2337it [15:46,  4.33it/s]\u001b[A\n","2338it [15:46,  4.33it/s]\u001b[A\n","2339it [15:46,  4.33it/s]\u001b[A\n","2340it [15:47,  4.33it/s]\u001b[A\n","2341it [15:47,  4.33it/s]\u001b[A\n","2342it [15:47,  4.33it/s]\u001b[A\n","2343it [15:47,  4.33it/s]\u001b[A\n","2344it [15:48,  4.33it/s]\u001b[A\n","2345it [15:48,  4.33it/s]\u001b[A\n","2346it [15:48,  4.33it/s]\u001b[A\n","2347it [15:48,  4.33it/s]\u001b[A\n","2348it [15:48,  4.33it/s]\u001b[A\n","2349it [15:49,  4.33it/s]\u001b[A\n","2350it [15:49,  4.33it/s]\u001b[A\n","2351it [15:49,  4.33it/s]\u001b[A\n","2352it [15:49,  4.33it/s]\u001b[A\n","2353it [15:50,  4.33it/s]\u001b[A\n","2354it [15:50,  4.33it/s]\u001b[A03/14/2022 09:34:07 - INFO - __main__ -   ***** Test results spoilers *****\n","03/14/2022 09:34:07 - INFO - __main__ -     eval_loss = 3.1177148818969727\n","03/14/2022 09:34:07 - INFO - __main__ -     eval_auroc = 0.8237287998199463\n","03/14/2022 09:34:07 - INFO - __main__ -     eval_recall = 0.8333333134651184\n","03/14/2022 09:34:07 - INFO - __main__ -     eval_f1 = 0.26490065455436707\n","03/14/2022 09:34:07 - INFO - filelock -   Lock 140288413982160 acquired on log.lock\n","03/14/2022 09:34:07 - INFO - filelock -   Lock 140288413982160 released on log.lock\n","2354it [15:50,  2.48it/s]\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n","  FutureWarning,\n","03/14/2022 09:34:13 - INFO - __main__ -   Load top-100 templates from spoilers_auto_template/yes_no/16-21.sort.txt\n","03/14/2022 09:34:13 - INFO - __main__ -   Specify load the 19-th template: *cls**sent_0*,*mask*?*sep+*\n","03/14/2022 09:34:13 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","03/14/2022 09:34:13 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/spoilers-prompt-demo-16-21-roberta-large-29866', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Mar14_09-34-13_eda9c4d80d5c', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=21, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/spoilers-prompt-demo-16-21-roberta-large-29866', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=0, model_id=19, save_logit=True, save_logit_dir='ensemble_predict_results', fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n","03/14/2022 09:34:13 - INFO - __main__ -   Task name: spoilers, number of labels: 2, output mode: classification\n","03/14/2022 09:34:13 - INFO - __main__ -   Automatically convert the template to using demonstrations.\n","03/14/2022 09:34:13 - INFO - __main__ -   | *cls**sent_0*,*mask*?*sep+* => *cls**sent_0*,*mask*?*sep+**sent_1*,*label_0*?*sep+**sent_2*,*label_1*?*sep+*\n","03/14/2022 09:34:14 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:34:14 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/14/2022 09:34:14 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/14/2022 09:34:14 - INFO - src.dataset -   Total num_sample for mode train: 1\n","03/14/2022 09:34:14 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:34:14 - INFO - filelock -   Lock 140578892290384 acquired on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:14 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers [took 0.002 s]\n","03/14/2022 09:34:14 - INFO - filelock -   Lock 140578892290384 released on data/k-shot-10x/spoilers/16-21/cached_train_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:14 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:34:14 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/14/2022 09:34:14 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/14/2022 09:34:14 - INFO - src.dataset -   Total num_sample for mode dev: 16\n","03/14/2022 09:34:14 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:34:15 - INFO - filelock -   Lock 140578879090960 acquired on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:15 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/14/2022 09:34:15 - INFO - filelock -   Lock 140578879090960 released on data/k-shot-10x/spoilers/16-21/cached_dev_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:15 - INFO - src.dataset -   *** Example ***\n","03/14/2022 09:34:15 - INFO - src.dataset -   guid: dev-0\n","03/14/2022 09:34:15 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 14484, 2459, 6, 596, 821, 6157, 475, 967, 8810, 21, 2343, 116, 6, 50264, 116, 2, 5488, 74, 28, 6474, 9724, 6, 440, 116, 2, 571, 10810, 31985, 364, 329, 6, 3216, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None)\n","03/14/2022 09:34:15 - INFO - src.dataset -   text: <s>Guys, why goth mikasa was shown?,<mask>?</s>which would be lit af, No?</s>gabi sniper ez, Yes?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","03/14/2022 09:34:17 - INFO - src.dataset -   Use demonstrations\n","03/14/2022 09:34:17 - INFO - src.dataset -   Label 0 to word ƒ†No (440)\n","03/14/2022 09:34:17 - INFO - src.dataset -   Label 1 to word ƒ†Yes (3216)\n","03/14/2022 09:34:17 - INFO - src.dataset -   Total num_sample for mode test: 16\n","03/14/2022 09:34:17 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-10x/spoilers/16-21\n","03/14/2022 09:34:17 - INFO - filelock -   Lock 140578879090960 acquired on data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:17 - INFO - src.dataset -   Loading features from cached file data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers [took 0.003 s]\n","03/14/2022 09:34:17 - INFO - filelock -   Lock 140578879090960 released on data/k-shot-10x/spoilers/16-21/cached_test_RobertaTokenizer_128_spoilers.lock\n","03/14/2022 09:34:17 - INFO - src.dataset -   *** Example ***\n","03/14/2022 09:34:17 - INFO - src.dataset -   guid: test-0\n","03/14/2022 09:34:17 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 8663, 18, 269, 15867, 154, 123, 6, 50264, 116, 2, 3463, 45548, 6, 440, 116, 2, 571, 10810, 31985, 364, 329, 6, 3216, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0, mask_pos=[8], label_word_list=None)\n","03/14/2022 09:34:17 - INFO - src.dataset -   text: <s>eren's really battering him,<mask>?</s>exactly, No?</s>gabi sniper ez, Yes?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'lm_head.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/14/2022 09:34:37 - INFO - src.trainer -   ***** Running training *****\n","03/14/2022 09:34:37 - INFO - src.trainer -     Num examples = 32\n","03/14/2022 09:34:37 - INFO - src.trainer -     Num Epochs = 250\n","03/14/2022 09:34:37 - INFO - src.trainer -     Instantaneous batch size per device = 8\n","03/14/2022 09:34:37 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","03/14/2022 09:34:37 - INFO - src.trainer -     Gradient Accumulation steps = 1\n","03/14/2022 09:34:37 - INFO - src.trainer -     Total optimization steps = 1000\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1096: FutureWarning: This method is deprecated, use `Trainer.is_local_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_local_process_zero()` instead.\", FutureWarning)\n","Epoch:  10% 24/250 [00:40<06:17,  1.67s/it]\n","  0% 0/185 [00:00<?, ?it/s]\u001b[A\n","  1% 2/185 [00:00<00:21,  8.67it/s]\u001b[A\n","  2% 3/185 [00:00<00:27,  6.67it/s]\u001b[A\n","  2% 4/185 [00:00<00:31,  5.74it/s]\u001b[A\n","  3% 5/185 [00:00<00:34,  5.23it/s]\u001b[A\n","  3% 6/185 [00:01<00:36,  4.92it/s]\u001b[A\n","  4% 7/185 [00:01<00:37,  4.72it/s]\u001b[A\n","  4% 8/185 [00:01<00:38,  4.60it/s]\u001b[A\n","  5% 9/185 [00:01<00:38,  4.51it/s]\u001b[A\n","  5% 10/185 [00:02<00:39,  4.46it/s]\u001b[A\n","  6% 11/185 [00:02<00:39,  4.42it/s]\u001b[A\n","  6% 12/185 [00:02<00:39,  4.39it/s]\u001b[A\n","  7% 13/185 [00:02<00:39,  4.38it/s]\u001b[A\n","  8% 14/185 [00:03<00:39,  4.36it/s]\u001b[A\n","  8% 15/185 [00:03<00:39,  4.35it/s]\u001b[A\n","  9% 16/185 [00:03<00:38,  4.34it/s]\u001b[A\n","  9% 17/185 [00:03<00:38,  4.33it/s]\u001b[A\n"," 10% 18/185 [00:03<00:38,  4.33it/s]\u001b[A\n"," 10% 19/185 [00:04<00:38,  4.33it/s]\u001b[A\n"," 11% 20/185 [00:04<00:38,  4.33it/s]\u001b[A\n"," 11% 21/185 [00:04<00:37,  4.33it/s]\u001b[A\n"," 12% 22/185 [00:04<00:37,  4.33it/s]\u001b[A\n"," 12% 23/185 [00:05<00:37,  4.33it/s]\u001b[A\n"," 13% 24/185 [00:05<00:37,  4.33it/s]\u001b[A\n"," 14% 25/185 [00:05<00:36,  4.33it/s]\u001b[A\n"," 14% 26/185 [00:05<00:36,  4.33it/s]\u001b[A\n"," 15% 27/185 [00:06<00:36,  4.32it/s]\u001b[A\n"," 15% 28/185 [00:06<00:36,  4.32it/s]\u001b[A\n"," 16% 29/185 [00:06<00:36,  4.33it/s]\u001b[A\n"," 16% 30/185 [00:06<00:35,  4.33it/s]\u001b[A\n"," 17% 31/185 [00:06<00:35,  4.32it/s]\u001b[A\n"," 17% 32/185 [00:07<00:35,  4.33it/s]\u001b[A\n"," 18% 33/185 [00:07<00:35,  4.32it/s]\u001b[A\n"," 18% 34/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 35/185 [00:07<00:34,  4.33it/s]\u001b[A\n"," 19% 36/185 [00:08<00:34,  4.33it/s]\u001b[A\n"," 20% 37/185 [00:08<00:34,  4.32it/s]\u001b[A\n"," 21% 38/185 [00:08<00:33,  4.32it/s]\u001b[A\n"," 21% 39/185 [00:08<00:33,  4.33it/s]\u001b[A\n"," 22% 40/185 [00:09<00:33,  4.32it/s]\u001b[A\n"," 22% 41/185 [00:09<00:33,  4.32it/s]\u001b[A\n"," 23% 42/185 [00:09<00:33,  4.33it/s]\u001b[A\n"," 23% 43/185 [00:09<00:32,  4.33it/s]\u001b[A\n"," 24% 44/185 [00:09<00:32,  4.33it/s]\u001b[A\n"," 24% 45/185 [00:10<00:32,  4.33it/s]\u001b[A\n"," 25% 46/185 [00:10<00:32,  4.33it/s]\u001b[A\n"," 25% 47/185 [00:10<00:31,  4.33it/s]\u001b[A\n"," 26% 48/185 [00:10<00:31,  4.33it/s]\u001b[A\n"," 26% 49/185 [00:11<00:31,  4.33it/s]\u001b[A\n"," 27% 50/185 [00:11<00:31,  4.33it/s]\u001b[A\n"," 28% 51/185 [00:11<00:30,  4.32it/s]\u001b[A\n"," 28% 52/185 [00:11<00:30,  4.32it/s]\u001b[A\n"," 29% 53/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 29% 54/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 30% 55/185 [00:12<00:30,  4.33it/s]\u001b[A\n"," 30% 56/185 [00:12<00:29,  4.33it/s]\u001b[A\n"," 31% 57/185 [00:12<00:29,  4.33it/s]\u001b[A\n"," 31% 58/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 59/185 [00:13<00:29,  4.33it/s]\u001b[A\n"," 32% 60/185 [00:13<00:28,  4.32it/s]\u001b[A\n"," 33% 61/185 [00:13<00:28,  4.32it/s]\u001b[A\n"," 34% 62/185 [00:14<00:28,  4.32it/s]\u001b[A\n"," 34% 63/185 [00:14<00:28,  4.32it/s]\u001b[A\n"," 35% 64/185 [00:14<00:27,  4.32it/s]\u001b[A\n"," 35% 65/185 [00:14<00:27,  4.33it/s]\u001b[A\n"," 36% 66/185 [00:15<00:27,  4.33it/s]\u001b[A\n"," 36% 67/185 [00:15<00:27,  4.32it/s]\u001b[A\n"," 37% 68/185 [00:15<00:27,  4.32it/s]\u001b[A\n"," 37% 69/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 38% 70/185 [00:15<00:26,  4.33it/s]\u001b[A\n"," 38% 71/185 [00:16<00:26,  4.33it/s]\u001b[A\n"," 39% 72/185 [00:16<00:26,  4.33it/s]\u001b[A\n"," 39% 73/185 [00:16<00:25,  4.33it/s]\u001b[A\n"," 40% 74/185 [00:16<00:25,  4.32it/s]\u001b[A\n"," 41% 75/185 [00:17<00:25,  4.33it/s]\u001b[A\n"," 41% 76/185 [00:17<00:25,  4.33it/s]\u001b[A\n"," 42% 77/185 [00:17<00:24,  4.33it/s]\u001b[A\n"," 42% 78/185 [00:17<00:24,  4.33it/s]\u001b[A\n"," 43% 79/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 43% 80/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 44% 81/185 [00:18<00:24,  4.33it/s]\u001b[A\n"," 44% 82/185 [00:18<00:23,  4.33it/s]\u001b[A\n"," 45% 83/185 [00:18<00:23,  4.33it/s]\u001b[A\n"," 45% 84/185 [00:19<00:23,  4.32it/s]\u001b[A\n"," 46% 85/185 [00:19<00:23,  4.32it/s]\u001b[A\n"," 46% 86/185 [00:19<00:22,  4.33it/s]\u001b[A\n"," 47% 87/185 [00:19<00:22,  4.32it/s]\u001b[A\n"," 48% 88/185 [00:20<00:22,  4.32it/s]\u001b[A\n"," 48% 89/185 [00:20<00:22,  4.32it/s]\u001b[A\n"," 49% 90/185 [00:20<00:21,  4.32it/s]\u001b[A\n"," 49% 91/185 [00:20<00:21,  4.32it/s]\u001b[A\n"," 50% 92/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 50% 93/185 [00:21<00:21,  4.33it/s]\u001b[A\n"," 51% 94/185 [00:21<00:21,  4.32it/s]\u001b[A\n"," 51% 95/185 [00:21<00:20,  4.33it/s]\u001b[A\n"," 52% 96/185 [00:21<00:20,  4.32it/s]\u001b[A\n"," 52% 97/185 [00:22<00:20,  4.32it/s]\u001b[A\n"," 53% 98/185 [00:22<00:20,  4.32it/s]\u001b[A\n"," 54% 99/185 [00:22<00:19,  4.33it/s]\u001b[A\n"," 54% 100/185 [00:22<00:19,  4.33it/s]\u001b[A\n"," 55% 101/185 [00:23<00:19,  4.33it/s]\u001b[A\n"," 55% 102/185 [00:23<00:19,  4.33it/s]\u001b[A\n"," 56% 103/185 [00:23<00:18,  4.33it/s]\u001b[A\n"," 56% 104/185 [00:23<00:18,  4.33it/s]\u001b[A\n"," 57% 105/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 57% 106/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 58% 107/185 [00:24<00:18,  4.33it/s]\u001b[A\n"," 58% 108/185 [00:24<00:17,  4.33it/s]\u001b[A\n"," 59% 109/185 [00:24<00:17,  4.33it/s]\u001b[A\n"," 59% 110/185 [00:25<00:17,  4.33it/s]\u001b[A\n"," 60% 111/185 [00:25<00:17,  4.33it/s]\u001b[A\n"," 61% 112/185 [00:25<00:16,  4.33it/s]\u001b[A\n"," 61% 113/185 [00:25<00:16,  4.33it/s]\u001b[A\n"," 62% 114/185 [00:26<00:16,  4.33it/s]\u001b[A\n"," 62% 115/185 [00:26<00:16,  4.33it/s]\u001b[A\n"," 63% 116/185 [00:26<00:15,  4.33it/s]\u001b[A\n"," 63% 117/185 [00:26<00:15,  4.33it/s]\u001b[A\n"," 64% 118/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 64% 119/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 65% 120/185 [00:27<00:15,  4.33it/s]\u001b[A\n"," 65% 121/185 [00:27<00:14,  4.33it/s]\u001b[A\n"," 66% 122/185 [00:27<00:14,  4.33it/s]\u001b[A\n"," 66% 123/185 [00:28<00:14,  4.33it/s]\u001b[A\n"," 67% 124/185 [00:28<00:14,  4.32it/s]\u001b[A\n"," 68% 125/185 [00:28<00:13,  4.32it/s]\u001b[A\n"," 68% 126/185 [00:28<00:13,  4.32it/s]\u001b[A\n"," 69% 127/185 [00:29<00:13,  4.32it/s]\u001b[A\n"," 69% 128/185 [00:29<00:13,  4.32it/s]\u001b[A\n"," 70% 129/185 [00:29<00:12,  4.32it/s]\u001b[A\n"," 70% 130/185 [00:29<00:12,  4.32it/s]\u001b[A\n"," 71% 131/185 [00:30<00:12,  4.32it/s]\u001b[A\n"," 71% 132/185 [00:30<00:12,  4.32it/s]\u001b[A\n"," 72% 133/185 [00:30<00:12,  4.33it/s]\u001b[A\n"," 72% 134/185 [00:30<00:11,  4.33it/s]\u001b[A\n"," 73% 135/185 [00:30<00:11,  4.33it/s]\u001b[A\n"," 74% 136/185 [00:31<00:11,  4.33it/s]\u001b[A\n"," 74% 137/185 [00:31<00:11,  4.33it/s]\u001b[A\n"," 75% 138/185 [00:31<00:10,  4.33it/s]\u001b[A\n"," 75% 139/185 [00:31<00:10,  4.33it/s]\u001b[A\n"," 76% 140/185 [00:32<00:10,  4.33it/s]\u001b[A\n"," 76% 141/185 [00:32<00:10,  4.33it/s]\u001b[A\n"," 77% 142/185 [00:32<00:09,  4.33it/s]\u001b[A\n"," 77% 143/185 [00:32<00:09,  4.33it/s]\u001b[A\n"," 78% 144/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 78% 145/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 79% 146/185 [00:33<00:09,  4.33it/s]\u001b[A\n"," 79% 147/185 [00:33<00:08,  4.33it/s]\u001b[A\n"," 80% 148/185 [00:33<00:08,  4.33it/s]\u001b[A\n"," 81% 149/185 [00:34<00:08,  4.33it/s]\u001b[A\n"," 81% 150/185 [00:34<00:08,  4.33it/s]\u001b[A\n"," 82% 151/185 [00:34<00:07,  4.33it/s]\u001b[A\n"," 82% 152/185 [00:34<00:07,  4.33it/s]\u001b[A\n"," 83% 153/185 [00:35<00:07,  4.33it/s]\u001b[A\n"," 83% 154/185 [00:35<00:07,  4.32it/s]\u001b[A\n"," 84% 155/185 [00:35<00:06,  4.32it/s]\u001b[A\n"," 84% 156/185 [00:35<00:06,  4.33it/s]\u001b[A\n"," 85% 157/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 85% 158/185 [00:36<00:06,  4.33it/s]\u001b[A\n"," 86% 159/185 [00:36<00:06,  4.32it/s]\u001b[A\n"," 86% 160/185 [00:36<00:05,  4.32it/s]\u001b[A\n"," 87% 161/185 [00:36<00:05,  4.32it/s]\u001b[A\n"," 88% 162/185 [00:37<00:05,  4.32it/s]\u001b[A\n"," 88% 163/185 [00:37<00:05,  4.32it/s]\u001b[A\n"," 89% 164/185 [00:37<00:04,  4.32it/s]\u001b[A\n"," 89% 165/185 [00:37<00:04,  4.32it/s]\u001b[A\n"," 90% 166/185 [00:38<00:04,  4.32it/s]\u001b[A\n"," 90% 167/185 [00:38<00:04,  4.32it/s]\u001b[A\n"," 91% 168/185 [00:38<00:03,  4.33it/s]\u001b[A\n"," 91% 169/185 [00:38<00:03,  4.33it/s]\u001b[A\n"," 92% 170/185 [00:39<00:03,  4.32it/s]\u001b[A\n"," 92% 171/185 [00:39<00:03,  4.33it/s]\u001b[A\n"," 93% 172/185 [00:39<00:03,  4.33it/s]\u001b[A\n"," 94% 173/185 [00:39<00:02,  4.32it/s]\u001b[A\n"," 94% 174/185 [00:39<00:02,  4.33it/s]\u001b[A\n"," 95% 175/185 [00:40<00:02,  4.33it/s]\u001b[A\n"," 95% 176/185 [00:40<00:02,  4.33it/s]\u001b[A\n"," 96% 177/185 [00:40<00:01,  4.33it/s]\u001b[A\n"," 96% 178/185 [00:40<00:01,  4.33it/s]\u001b[A\n"," 97% 179/185 [00:41<00:01,  4.33it/s]\u001b[A\n"," 97% 180/185 [00:41<00:01,  4.33it/s]\u001b[A\n"," 98% 181/185 [00:41<00:00,  4.33it/s]\u001b[A\n"," 98% 182/185 [00:41<00:00,  4.33it/s]\u001b[A\n"," 99% 183/185 [00:42<00:00,  4.32it/s]\u001b[A\n"," 99% 184/185 [00:42<00:00,  4.33it/s]\u001b[A\n","100% 185/185 [00:42<00:00,  4.33it/s]\u001b[A03/14/2022 09:36:02 - INFO - src.trainer -   Best dev result: 0.9031249284744263\n","Epoch:  20% 49/250 [02:13<05:37,  1.68s/it]\n","186it [01:33, 15.38s/it]             \u001b[A\n","187it [01:33, 10.84s/it]\u001b[A\n","188it [01:33,  7.66s/it]\u001b[A\n","189it [01:33,  5.43s/it]\u001b[A\n","190it [01:34,  3.87s/it]\u001b[A\n","191it [01:34,  2.78s/it]\u001b[A\n","192it [01:34,  2.01s/it]\u001b[A\n","193it [01:34,  1.48s/it]\u001b[A\n","194it [01:35,  1.10s/it]\u001b[A\n","195it [01:35,  1.19it/s]\u001b[A\n","196it [01:35,  1.52it/s]\u001b[A\n","197it [01:35,  1.88it/s]\u001b[A\n","198it [01:36,  2.27it/s]\u001b[A\n","199it [01:36,  2.65it/s]\u001b[A\n","200it [01:36,  3.00it/s]\u001b[A\n","201it [01:36,  3.30it/s]\u001b[A\n","202it [01:36,  3.55it/s]\u001b[A\n","203it [01:37,  3.75it/s]\u001b[A\n","204it [01:37,  3.91it/s]\u001b[A\n","205it [01:37,  4.03it/s]\u001b[A\n","206it [01:37,  4.11it/s]\u001b[A\n","207it [01:38,  4.17it/s]\u001b[A\n","208it [01:38,  4.21it/s]\u001b[A\n","209it [01:38,  4.25it/s]\u001b[A\n","210it [01:38,  4.27it/s]\u001b[A\n","211it [01:39,  4.29it/s]\u001b[A\n","212it [01:39,  4.30it/s]\u001b[A\n","213it [01:39,  4.31it/s]\u001b[A\n","214it [01:39,  4.31it/s]\u001b[A\n","215it [01:39,  4.32it/s]\u001b[A\n","216it [01:40,  4.32it/s]\u001b[A\n","217it [01:40,  4.32it/s]\u001b[A\n","218it [01:40,  4.32it/s]\u001b[A\n","219it [01:40,  4.33it/s]\u001b[A\n","220it [01:41,  4.33it/s]\u001b[A\n","221it [01:41,  4.33it/s]\u001b[A\n","222it [01:41,  4.33it/s]\u001b[A\n","223it [01:41,  4.33it/s]\u001b[A\n","224it [01:42,  4.33it/s]\u001b[A\n","225it [01:42,  4.33it/s]\u001b[A\n","226it [01:42,  4.33it/s]\u001b[A\n","227it [01:42,  4.32it/s]\u001b[A\n","228it [01:42,  4.32it/s]\u001b[A\n","229it [01:43,  4.32it/s]\u001b[A\n","230it [01:43,  4.32it/s]\u001b[A\n","231it [01:43,  4.32it/s]\u001b[A\n","232it [01:43,  4.33it/s]\u001b[A\n","233it [01:44,  4.33it/s]\u001b[A\n","234it [01:44,  4.33it/s]\u001b[A\n","235it [01:44,  4.33it/s]\u001b[A\n","236it [01:44,  4.33it/s]\u001b[A\n","237it [01:45,  4.33it/s]\u001b[A\n","238it [01:45,  4.33it/s]\u001b[A\n","239it [01:45,  4.33it/s]\u001b[A\n","240it [01:45,  4.33it/s]\u001b[A\n","241it [01:45,  4.33it/s]\u001b[A\n","242it [01:46,  4.33it/s]\u001b[A\n","243it [01:46,  4.33it/s]\u001b[A\n","244it [01:46,  4.33it/s]\u001b[A\n","245it [01:46,  4.33it/s]\u001b[A\n","246it [01:47,  4.33it/s]\u001b[A\n","247it [01:47,  4.33it/s]\u001b[A\n","248it [01:47,  4.33it/s]\u001b[A\n","249it [01:47,  4.33it/s]\u001b[A\n","250it [01:48,  4.33it/s]\u001b[A\n","251it [01:48,  4.33it/s]\u001b[A\n","252it [01:48,  4.33it/s]\u001b[A\n","253it [01:48,  4.33it/s]\u001b[A\n","254it [01:48,  4.33it/s]\u001b[A\n","255it [01:49,  4.33it/s]\u001b[A\n","256it [01:49,  4.33it/s]\u001b[A\n","257it [01:49,  4.33it/s]\u001b[A\n","258it [01:49,  4.33it/s]\u001b[A\n","259it [01:50,  4.33it/s]\u001b[A\n","260it [01:50,  4.33it/s]\u001b[A\n","261it [01:50,  4.33it/s]\u001b[A\n","262it [01:50,  4.33it/s]\u001b[A\n","263it [01:51,  4.32it/s]\u001b[A\n","264it [01:51,  4.33it/s]\u001b[A\n","265it [01:51,  4.33it/s]\u001b[A\n","266it [01:51,  4.33it/s]\u001b[A\n","267it [01:51,  4.33it/s]\u001b[A\n","268it [01:52,  4.33it/s]\u001b[A\n","269it [01:52,  4.33it/s]\u001b[A\n","270it [01:52,  4.32it/s]\u001b[A\n","271it [01:52,  4.33it/s]\u001b[A\n","272it [01:53,  4.32it/s]\u001b[A\n","273it [01:53,  4.32it/s]\u001b[A\n","274it [01:53,  4.33it/s]\u001b[A\n","275it [01:53,  4.33it/s]\u001b[A\n","276it [01:54,  4.33it/s]\u001b[A\n","277it [01:54,  4.33it/s]\u001b[A\n","278it [01:54,  4.33it/s]\u001b[A\n","279it [01:54,  4.33it/s]\u001b[A\n","280it [01:54,  4.33it/s]\u001b[A\n","281it [01:55,  4.33it/s]\u001b[A\n","282it [01:55,  4.33it/s]\u001b[A\n","283it [01:55,  4.32it/s]\u001b[A\n","284it [01:55,  4.33it/s]\u001b[A\n","285it [01:56,  4.33it/s]\u001b[A\n","286it [01:56,  4.33it/s]\u001b[A\n","287it [01:56,  4.33it/s]\u001b[A\n","288it [01:56,  4.33it/s]\u001b[A\n","289it [01:57,  4.33it/s]\u001b[A\n","290it [01:57,  4.33it/s]\u001b[A\n","291it [01:57,  4.33it/s]\u001b[A\n","292it [01:57,  4.33it/s]\u001b[A\n","293it [01:58,  4.32it/s]\u001b[A\n","294it [01:58,  4.32it/s]\u001b[A\n","295it [01:58,  4.32it/s]\u001b[A\n","296it [01:58,  4.32it/s]\u001b[A\n","297it [01:58,  4.32it/s]\u001b[A\n","298it [01:59,  4.32it/s]\u001b[A\n","299it [01:59,  4.32it/s]\u001b[A\n","300it [01:59,  4.33it/s]\u001b[A\n","301it [01:59,  4.33it/s]\u001b[A\n","302it [02:00,  4.33it/s]\u001b[A\n","303it [02:00,  4.33it/s]\u001b[A\n","304it [02:00,  4.33it/s]\u001b[A\n","305it [02:00,  4.32it/s]\u001b[A\n","306it [02:01,  4.32it/s]\u001b[A\n","307it [02:01,  4.32it/s]\u001b[A\n","308it [02:01,  4.33it/s]\u001b[A\n","309it [02:01,  4.33it/s]\u001b[A\n","310it [02:01,  4.33it/s]\u001b[A\n","311it [02:02,  4.33it/s]\u001b[A\n","312it [02:02,  4.33it/s]\u001b[A\n","313it [02:02,  4.33it/s]\u001b[A\n","314it [02:02,  4.33it/s]\u001b[A\n","315it [02:03,  4.32it/s]\u001b[A\n","316it [02:03,  4.32it/s]\u001b[A\n","317it [02:03,  4.33it/s]\u001b[A\n","318it [02:03,  4.33it/s]\u001b[A\n","319it [02:04,  4.33it/s]\u001b[A\n","320it [02:04,  4.33it/s]\u001b[A\n","321it [02:04,  4.33it/s]\u001b[A\n","322it [02:04,  4.33it/s]\u001b[A\n","323it [02:04,  4.33it/s]\u001b[A\n","324it [02:05,  4.33it/s]\u001b[A\n","325it [02:05,  4.33it/s]\u001b[A\n","326it [02:05,  4.33it/s]\u001b[A\n","327it [02:05,  4.33it/s]\u001b[A\n","328it [02:06,  4.33it/s]\u001b[A\n","329it [02:06,  4.33it/s]\u001b[A\n","330it [02:06,  4.33it/s]\u001b[A\n","331it [02:06,  4.33it/s]\u001b[A\n","332it [02:07,  4.33it/s]\u001b[A\n","333it [02:07,  4.33it/s]\u001b[A\n","334it [02:07,  4.33it/s]\u001b[A\n","335it [02:07,  4.33it/s]\u001b[A\n","336it [02:07,  4.33it/s]\u001b[A\n","337it [02:08,  4.33it/s]\u001b[A\n","338it [02:08,  4.33it/s]\u001b[A\n","339it [02:08,  4.32it/s]\u001b[A\n","340it [02:08,  4.32it/s]\u001b[A\n","341it [02:09,  4.33it/s]\u001b[A\n","342it [02:09,  4.33it/s]\u001b[A\n","343it [02:09,  4.33it/s]\u001b[A\n","344it [02:09,  4.33it/s]\u001b[A\n","345it [02:10,  4.33it/s]\u001b[A\n","346it [02:10,  4.33it/s]\u001b[A\n","347it [02:10,  4.33it/s]\u001b[A\n","348it [02:10,  4.33it/s]\u001b[A\n","349it [02:10,  4.32it/s]\u001b[A\n","350it [02:11,  4.33it/s]\u001b[A\n","351it [02:11,  4.33it/s]\u001b[A\n","352it [02:11,  4.33it/s]\u001b[A\n","353it [02:11,  4.33it/s]\u001b[A\n","354it [02:12,  4.33it/s]\u001b[A\n","355it [02:12,  4.33it/s]\u001b[A\n","356it [02:12,  4.33it/s]\u001b[A\n","357it [02:12,  4.33it/s]\u001b[A\n","358it [02:13,  4.32it/s]\u001b[A\n","359it [02:13,  4.33it/s]\u001b[A\n","360it [02:13,  4.33it/s]\u001b[A\n","361it [02:13,  4.33it/s]\u001b[A\n","362it [02:13,  4.33it/s]\u001b[A\n","363it [02:14,  4.33it/s]\u001b[A\n","364it [02:14,  4.33it/s]\u001b[A\n","365it [02:14,  4.33it/s]\u001b[A\n","366it [02:14,  4.33it/s]\u001b[A\n","367it [02:15,  4.32it/s]\u001b[A\n","368it [02:15,  4.32it/s]\u001b[A\n","369it [02:15,  4.32it/s]\u001b[A\n","Epoch:  30% 74/250 [03:38<04:55,  1.68s/it]\n","371it [02:57, 12.76s/it]\u001b[A\n","372it [02:58,  9.00s/it]\u001b[A\n","373it [02:58,  6.37s/it]\u001b[A\n","374it [02:58,  4.53s/it]\u001b[A\n","375it [02:58,  3.24s/it]\u001b[A\n","376it [02:58,  2.34s/it]\u001b[A\n","377it [02:59,  1.71s/it]\u001b[A\n","378it [02:59,  1.26s/it]\u001b[A\n","379it [02:59,  1.05it/s]\u001b[A\n","380it [02:59,  1.36it/s]\u001b[A\n","381it [03:00,  1.71it/s]\u001b[A\n","382it [03:00,  2.09it/s]\u001b[A\n","383it [03:00,  2.47it/s]\u001b[A\n","384it [03:00,  2.84it/s]\u001b[A\n","385it [03:01,  3.16it/s]\u001b[A\n","386it [03:01,  3.44it/s]\u001b[A\n","387it [03:01,  3.67it/s]\u001b[A\n","388it [03:01,  3.84it/s]\u001b[A\n","389it [03:01,  3.98it/s]\u001b[A\n","390it [03:02,  4.08it/s]\u001b[A\n","391it [03:02,  4.15it/s]\u001b[A\n","392it [03:02,  4.20it/s]\u001b[A\n","393it [03:02,  4.24it/s]\u001b[A\n","394it [03:03,  4.26it/s]\u001b[A\n","395it [03:03,  4.28it/s]\u001b[A\n","396it [03:03,  4.30it/s]\u001b[A\n","397it [03:03,  4.31it/s]\u001b[A\n","398it [03:04,  4.31it/s]\u001b[A\n","399it [03:04,  4.31it/s]\u001b[A\n","400it [03:04,  4.32it/s]\u001b[A\n","401it [03:04,  4.32it/s]\u001b[A\n","402it [03:04,  4.32it/s]\u001b[A\n","403it [03:05,  4.32it/s]\u001b[A\n","404it [03:05,  4.32it/s]\u001b[A\n","405it [03:05,  4.32it/s]\u001b[A\n","406it [03:05,  4.33it/s]\u001b[A\n","407it [03:06,  4.32it/s]\u001b[A\n","408it [03:06,  4.32it/s]\u001b[A\n","409it [03:06,  4.32it/s]\u001b[A\n","410it [03:06,  4.33it/s]\u001b[A\n","411it [03:07,  4.32it/s]\u001b[A\n","412it [03:07,  4.32it/s]\u001b[A\n","413it [03:07,  4.33it/s]\u001b[A\n","414it [03:07,  4.32it/s]\u001b[A\n","415it [03:07,  4.32it/s]\u001b[A\n","416it [03:08,  4.32it/s]\u001b[A\n","417it [03:08,  4.32it/s]\u001b[A\n","418it [03:08,  4.33it/s]\u001b[A\n","419it [03:08,  4.33it/s]\u001b[A\n","420it [03:09,  4.33it/s]\u001b[A\n","421it [03:09,  4.33it/s]\u001b[A\n","422it [03:09,  4.33it/s]\u001b[A\n","423it [03:09,  4.32it/s]\u001b[A\n","424it [03:10,  4.32it/s]\u001b[A\n","425it [03:10,  4.32it/s]\u001b[A\n","426it [03:10,  4.32it/s]\u001b[A\n","427it [03:10,  4.33it/s]\u001b[A\n","428it [03:10,  4.33it/s]\u001b[A\n","429it [03:11,  4.33it/s]\u001b[A\n","430it [03:11,  4.33it/s]\u001b[A\n","431it [03:11,  4.33it/s]\u001b[A\n","432it [03:11,  4.32it/s]\u001b[A\n","433it [03:12,  4.33it/s]\u001b[A\n","434it [03:12,  4.32it/s]\u001b[A\n","435it [03:12,  4.32it/s]\u001b[A\n","436it [03:12,  4.33it/s]\u001b[A\n","437it [03:13,  4.32it/s]\u001b[A\n","438it [03:13,  4.32it/s]\u001b[A\n","439it [03:13,  4.32it/s]\u001b[A\n","440it [03:13,  4.33it/s]\u001b[A\n","441it [03:13,  4.33it/s]\u001b[A\n","442it [03:14,  4.33it/s]\u001b[A\n","443it [03:14,  4.33it/s]\u001b[A\n","444it [03:14,  4.33it/s]\u001b[A\n","445it [03:14,  4.32it/s]\u001b[A\n","446it [03:15,  4.32it/s]\u001b[A\n","447it [03:15,  4.32it/s]\u001b[A\n","448it [03:15,  4.32it/s]\u001b[A\n","449it [03:15,  4.32it/s]\u001b[A\n","450it [03:16,  4.33it/s]\u001b[A\n","451it [03:16,  4.33it/s]\u001b[A\n","452it [03:16,  4.33it/s]\u001b[A\n","453it [03:16,  4.33it/s]\u001b[A\n","454it [03:16,  4.33it/s]\u001b[A\n","455it [03:17,  4.33it/s]\u001b[A\n","456it [03:17,  4.33it/s]\u001b[A\n","457it [03:17,  4.33it/s]\u001b[A\n","458it [03:17,  4.32it/s]\u001b[A\n","459it [03:18,  4.32it/s]\u001b[A\n","460it [03:18,  4.32it/s]\u001b[A\n","461it [03:18,  4.32it/s]\u001b[A\n","462it [03:18,  4.32it/s]\u001b[A\n","463it [03:19,  4.33it/s]\u001b[A\n","464it [03:19,  4.33it/s]\u001b[A\n","465it [03:19,  4.33it/s]\u001b[A\n","466it [03:19,  4.33it/s]\u001b[A\n","467it [03:19,  4.33it/s]\u001b[A\n","468it [03:20,  4.33it/s]\u001b[A\n","469it [03:20,  4.33it/s]\u001b[A\n","470it [03:20,  4.32it/s]\u001b[A\n","471it [03:20,  4.32it/s]\u001b[A\n","472it [03:21,  4.33it/s]\u001b[A\n","473it [03:21,  4.33it/s]\u001b[A\n","474it [03:21,  4.33it/s]\u001b[A\n","475it [03:21,  4.33it/s]\u001b[A\n","476it [03:22,  4.33it/s]\u001b[A\n","477it [03:22,  4.32it/s]\u001b[A\n","478it [03:22,  4.32it/s]\u001b[A\n","479it [03:22,  4.33it/s]\u001b[A\n","480it [03:22,  4.32it/s]\u001b[A\n","481it [03:23,  4.32it/s]\u001b[A\n","482it [03:23,  4.32it/s]\u001b[A\n","483it [03:23,  4.33it/s]\u001b[A\n","484it [03:23,  4.33it/s]\u001b[A\n","485it [03:24,  4.33it/s]\u001b[A\n","486it [03:24,  4.33it/s]\u001b[A\n","487it [03:24,  4.33it/s]\u001b[A\n","488it [03:24,  4.32it/s]\u001b[A\n","489it [03:25,  4.33it/s]\u001b[A\n","490it [03:25,  4.33it/s]\u001b[A\n","491it [03:25,  4.32it/s]\u001b[A\n","492it [03:25,  4.33it/s]\u001b[A\n","493it [03:25,  4.33it/s]\u001b[A\n","494it [03:26,  4.32it/s]\u001b[A\n","495it [03:26,  4.33it/s]\u001b[A\n","496it [03:26,  4.33it/s]\u001b[A\n","497it [03:26,  4.33it/s]\u001b[A\n","498it [03:27,  4.32it/s]\u001b[A\n","499it [03:27,  4.32it/s]\u001b[A\n","500it [03:27,  4.32it/s]\u001b[A\n","501it [03:27,  4.32it/s]\u001b[A\n","502it [03:28,  4.32it/s]\u001b[A\n","503it [03:28,  4.32it/s]\u001b[A\n","504it [03:28,  4.32it/s]\u001b[A\n","505it [03:28,  4.32it/s]\u001b[A\n","506it [03:29,  4.32it/s]\u001b[A\n","507it [03:29,  4.32it/s]\u001b[A\n","508it [03:29,  4.33it/s]\u001b[A\n","509it [03:29,  4.32it/s]\u001b[A\n","510it [03:29,  4.32it/s]\u001b[A\n","511it [03:30,  4.32it/s]\u001b[A\n","512it [03:30,  4.32it/s]\u001b[A\n","513it [03:30,  4.32it/s]\u001b[A\n","514it [03:30,  4.32it/s]\u001b[A\n","515it [03:31,  4.32it/s]\u001b[A\n","516it [03:31,  4.32it/s]\u001b[A\n","517it [03:31,  4.32it/s]\u001b[A\n","518it [03:31,  4.33it/s]\u001b[A\n","519it [03:32,  4.33it/s]\u001b[A\n","520it [03:32,  4.33it/s]\u001b[A\n","521it [03:32,  4.33it/s]\u001b[A\n","522it [03:32,  4.33it/s]\u001b[A\n","523it [03:32,  4.33it/s]\u001b[A\n","524it [03:33,  4.33it/s]\u001b[A\n","525it [03:33,  4.32it/s]\u001b[A\n","526it [03:33,  4.32it/s]\u001b[A\n","527it [03:33,  4.32it/s]\u001b[A\n","528it [03:34,  4.32it/s]\u001b[A\n","529it [03:34,  4.33it/s]\u001b[A\n","530it [03:34,  4.33it/s]\u001b[A\n","531it [03:34,  4.33it/s]\u001b[A\n","532it [03:35,  4.33it/s]\u001b[A\n","533it [03:35,  4.33it/s]\u001b[A\n","534it [03:35,  4.33it/s]\u001b[A\n","535it [03:35,  4.33it/s]\u001b[A\n","536it [03:35,  4.32it/s]\u001b[A\n","537it [03:36,  4.32it/s]\u001b[A\n","538it [03:36,  4.32it/s]\u001b[A\n","539it [03:36,  4.33it/s]\u001b[A\n","540it [03:36,  4.33it/s]\u001b[A\n","541it [03:37,  4.33it/s]\u001b[A\n","542it [03:37,  4.33it/s]\u001b[A\n","543it [03:37,  4.33it/s]\u001b[A\n","544it [03:37,  4.33it/s]\u001b[A\n","545it [03:38,  4.33it/s]\u001b[A\n","546it [03:38,  4.33it/s]\u001b[A\n","547it [03:38,  4.32it/s]\u001b[A\n","548it [03:38,  4.33it/s]\u001b[A\n","549it [03:38,  4.33it/s]\u001b[A\n","550it [03:39,  4.33it/s]\u001b[A\n","551it [03:39,  4.32it/s]\u001b[A\n","552it [03:39,  4.32it/s]\u001b[A\n","553it [03:39,  4.32it/s]\u001b[A\n","554it [03:40,  4.32it/s]\u001b[A\n","555it [03:40,  4.32it/s]\u001b[A03/14/2022 09:39:00 - INFO - src.trainer -   Best dev result: 0.937250018119812\n","Epoch:  40% 99/250 [05:10<04:13,  1.68s/it]\n","556it [04:29, 15.05s/it]\u001b[A\n","557it [04:30, 10.60s/it]\u001b[A\n","558it [04:30,  7.49s/it]\u001b[A\n","559it [04:30,  5.31s/it]\u001b[A\n","560it [04:30,  3.79s/it]\u001b[A\n","561it [04:31,  2.72s/it]\u001b[A\n","562it [04:31,  1.97s/it]\u001b[A\n","563it [04:31,  1.45s/it]\u001b[A\n","564it [04:31,  1.09s/it]\u001b[A\n","565it [04:32,  1.21it/s]\u001b[A\n","566it [04:32,  1.54it/s]\u001b[A\n","567it [04:32,  1.91it/s]\u001b[A\n","568it [04:32,  2.29it/s]\u001b[A\n","569it [04:32,  2.67it/s]\u001b[A\n","570it [04:33,  3.02it/s]\u001b[A\n","571it [04:33,  3.32it/s]\u001b[A\n","572it [04:33,  3.57it/s]\u001b[A\n","573it [04:33,  3.76it/s]\u001b[A\n","574it [04:34,  3.92it/s]\u001b[A\n","575it [04:34,  4.03it/s]\u001b[A\n","576it [04:34,  4.11it/s]\u001b[A\n","577it [04:34,  4.18it/s]\u001b[A\n","578it [04:35,  4.22it/s]\u001b[A\n","579it [04:35,  4.25it/s]\u001b[A\n","580it [04:35,  4.27it/s]\u001b[A\n","581it [04:35,  4.29it/s]\u001b[A\n","582it [04:35,  4.30it/s]\u001b[A\n","583it [04:36,  4.31it/s]\u001b[A\n","584it [04:36,  4.31it/s]\u001b[A\n","585it [04:36,  4.32it/s]\u001b[A\n","586it [04:36,  4.32it/s]\u001b[A\n","587it [04:37,  4.32it/s]\u001b[A\n","588it [04:37,  4.32it/s]\u001b[A\n","589it [04:37,  4.32it/s]\u001b[A\n","590it [04:37,  4.32it/s]\u001b[A\n","591it [04:38,  4.32it/s]\u001b[A\n","592it [04:38,  4.32it/s]\u001b[A\n","593it [04:38,  4.33it/s]\u001b[A\n","594it [04:38,  4.33it/s]\u001b[A\n","595it [04:38,  4.33it/s]\u001b[A\n","596it [04:39,  4.33it/s]\u001b[A\n","597it [04:39,  4.33it/s]\u001b[A\n","598it [04:39,  4.33it/s]\u001b[A\n","599it [04:39,  4.32it/s]\u001b[A\n","600it [04:40,  4.33it/s]\u001b[A\n","601it [04:40,  4.33it/s]\u001b[A\n","602it [04:40,  4.33it/s]\u001b[A\n","603it [04:40,  4.33it/s]\u001b[A\n","604it [04:41,  4.33it/s]\u001b[A\n","605it [04:41,  4.33it/s]\u001b[A\n","606it [04:41,  4.33it/s]\u001b[A\n","607it [04:41,  4.33it/s]\u001b[A\n","608it [04:41,  4.33it/s]\u001b[A\n","609it [04:42,  4.32it/s]\u001b[A\n","610it [04:42,  4.33it/s]\u001b[A\n","611it [04:42,  4.33it/s]\u001b[A\n","612it [04:42,  4.32it/s]\u001b[A\n","613it [04:43,  4.33it/s]\u001b[A\n","614it [04:43,  4.33it/s]\u001b[A\n","615it [04:43,  4.33it/s]\u001b[A\n","616it [04:43,  4.33it/s]\u001b[A\n","617it [04:44,  4.33it/s]\u001b[A\n","618it [04:44,  4.33it/s]\u001b[A\n","619it [04:44,  4.33it/s]\u001b[A\n","620it [04:44,  4.33it/s]\u001b[A\n","621it [04:44,  4.32it/s]\u001b[A\n","622it [04:45,  4.32it/s]\u001b[A\n","623it [04:45,  4.33it/s]\u001b[A\n","624it [04:45,  4.33it/s]\u001b[A\n","625it [04:45,  4.33it/s]\u001b[A\n","626it [04:46,  4.33it/s]\u001b[A\n","627it [04:46,  4.33it/s]\u001b[A\n","628it [04:46,  4.33it/s]\u001b[A\n","629it [04:46,  4.33it/s]\u001b[A\n","630it [04:47,  4.33it/s]\u001b[A\n","631it [04:47,  4.32it/s]\u001b[A\n","632it [04:47,  4.32it/s]\u001b[A\n","633it [04:47,  4.32it/s]\u001b[A\n","634it [04:47,  4.33it/s]\u001b[A\n","635it [04:48,  4.33it/s]\u001b[A\n","636it [04:48,  4.33it/s]\u001b[A\n","637it [04:48,  4.33it/s]\u001b[A\n","638it [04:48,  4.33it/s]\u001b[A\n","639it [04:49,  4.33it/s]\u001b[A\n","640it [04:49,  4.33it/s]\u001b[A\n","641it [04:49,  4.33it/s]\u001b[A\n","642it [04:49,  4.32it/s]\u001b[A\n","643it [04:50,  4.32it/s]\u001b[A\n","644it [04:50,  4.33it/s]\u001b[A\n","645it [04:50,  4.33it/s]\u001b[A\n","646it [04:50,  4.33it/s]\u001b[A\n","647it [04:50,  4.33it/s]\u001b[A\n","648it [04:51,  4.33it/s]\u001b[A\n","649it [04:51,  4.33it/s]\u001b[A\n","650it [04:51,  4.33it/s]\u001b[A\n","651it [04:51,  4.33it/s]\u001b[A\n","652it [04:52,  4.32it/s]\u001b[A\n","653it [04:52,  4.32it/s]\u001b[A\n","654it [04:52,  4.32it/s]\u001b[A\n","655it [04:52,  4.32it/s]\u001b[A\n","656it [04:53,  4.32it/s]\u001b[A\n","657it [04:53,  4.33it/s]\u001b[A\n","658it [04:53,  4.33it/s]\u001b[A\n","659it [04:53,  4.33it/s]\u001b[A\n","660it [04:53,  4.33it/s]\u001b[A\n","661it [04:54,  4.33it/s]\u001b[A\n","662it [04:54,  4.32it/s]\u001b[A\n","663it [04:54,  4.33it/s]\u001b[A\n","664it [04:54,  4.32it/s]\u001b[A\n","665it [04:55,  4.32it/s]\u001b[A\n","666it [04:55,  4.33it/s]\u001b[A\n","667it [04:55,  4.33it/s]\u001b[A\n","668it [04:55,  4.33it/s]\u001b[A\n","669it [04:56,  4.33it/s]\u001b[A\n","670it [04:56,  4.33it/s]\u001b[A\n","671it [04:56,  4.33it/s]\u001b[A\n","672it [04:56,  4.33it/s]\u001b[A\n","673it [04:56,  4.33it/s]\u001b[A\n","674it [04:57,  4.33it/s]\u001b[A\n","675it [04:57,  4.33it/s]\u001b[A\n","676it [04:57,  4.33it/s]\u001b[A\n","677it [04:57,  4.33it/s]\u001b[A\n","678it [04:58,  4.33it/s]\u001b[A\n","679it [04:58,  4.33it/s]\u001b[A\n","680it [04:58,  4.33it/s]\u001b[A\n","681it [04:58,  4.33it/s]\u001b[A\n","682it [04:59,  4.32it/s]\u001b[A\n","683it [04:59,  4.33it/s]\u001b[A\n","684it [04:59,  4.33it/s]\u001b[A\n","685it [04:59,  4.32it/s]\u001b[A\n","686it [04:59,  4.33it/s]\u001b[A\n","687it [05:00,  4.32it/s]\u001b[A\n","688it [05:00,  4.32it/s]\u001b[A\n","689it [05:00,  4.32it/s]\u001b[A\n","690it [05:00,  4.32it/s]\u001b[A\n","691it [05:01,  4.33it/s]\u001b[A\n","692it [05:01,  4.33it/s]\u001b[A\n","693it [05:01,  4.33it/s]\u001b[A\n","694it [05:01,  4.33it/s]\u001b[A\n","695it [05:02,  4.33it/s]\u001b[A\n","696it [05:02,  4.33it/s]\u001b[A\n","697it [05:02,  4.32it/s]\u001b[A\n","698it [05:02,  4.32it/s]\u001b[A\n","699it [05:03,  4.33it/s]\u001b[A\n","700it [05:03,  4.33it/s]\u001b[A\n","701it [05:03,  4.33it/s]\u001b[A\n","702it [05:03,  4.33it/s]\u001b[A\n","703it [05:03,  4.33it/s]\u001b[A\n","704it [05:04,  4.33it/s]\u001b[A\n","705it [05:04,  4.33it/s]\u001b[A\n","706it [05:04,  4.33it/s]\u001b[A\n","707it [05:04,  4.32it/s]\u001b[A\n","708it [05:05,  4.32it/s]\u001b[A\n","709it [05:05,  4.32it/s]\u001b[A\n","710it [05:05,  4.33it/s]\u001b[A\n","711it [05:05,  4.33it/s]\u001b[A\n","712it [05:06,  4.33it/s]\u001b[A\n","713it [05:06,  4.33it/s]\u001b[A\n","714it [05:06,  4.33it/s]\u001b[A\n","715it [05:06,  4.33it/s]\u001b[A\n","716it [05:06,  4.33it/s]\u001b[A\n","717it [05:07,  4.33it/s]\u001b[A\n","718it [05:07,  4.32it/s]\u001b[A\n","719it [05:07,  4.32it/s]\u001b[A\n","720it [05:07,  4.32it/s]\u001b[A\n","721it [05:08,  4.32it/s]\u001b[A\n","722it [05:08,  4.32it/s]\u001b[A\n","723it [05:08,  4.32it/s]\u001b[A\n","724it [05:08,  4.33it/s]\u001b[A\n","725it [05:09,  4.33it/s]\u001b[A\n","726it [05:09,  4.33it/s]\u001b[A\n","727it [05:09,  4.33it/s]\u001b[A\n","728it [05:09,  4.32it/s]\u001b[A\n","729it [05:09,  4.32it/s]\u001b[A\n","730it [05:10,  4.32it/s]\u001b[A\n","731it [05:10,  4.32it/s]\u001b[A\n","732it [05:10,  4.32it/s]\u001b[A\n","733it [05:10,  4.32it/s]\u001b[A\n","734it [05:11,  4.32it/s]\u001b[A\n","735it [05:11,  4.33it/s]\u001b[A\n","736it [05:11,  4.33it/s]\u001b[A\n","737it [05:11,  4.33it/s]\u001b[A\n","738it [05:12,  4.32it/s]\u001b[A\n","739it [05:12,  4.33it/s]\u001b[A\n","Epoch:  50% 124/250 [06:34<03:30,  1.67s/it]\n","741it [05:54, 12.76s/it]\u001b[A\n","742it [05:54,  9.01s/it]\u001b[A\n","743it [05:54,  6.37s/it]\u001b[A\n","744it [05:55,  4.53s/it]\u001b[A\n","745it [05:55,  3.24s/it]\u001b[A\n","746it [05:55,  2.34s/it]\u001b[A\n","747it [05:55,  1.71s/it]\u001b[A\n","748it [05:56,  1.26s/it]\u001b[A\n","749it [05:56,  1.05it/s]\u001b[A\n","750it [05:56,  1.36it/s]\u001b[A\n","751it [05:56,  1.71it/s]\u001b[A\n","752it [05:57,  2.09it/s]\u001b[A\n","753it [05:57,  2.47it/s]\u001b[A\n","754it [05:57,  2.84it/s]\u001b[A\n","755it [05:57,  3.16it/s]\u001b[A\n","756it [05:57,  3.44it/s]\u001b[A\n","757it [05:58,  3.67it/s]\u001b[A\n","758it [05:58,  3.84it/s]\u001b[A\n","759it [05:58,  3.98it/s]\u001b[A\n","760it [05:58,  4.07it/s]\u001b[A\n","761it [05:59,  4.15it/s]\u001b[A\n","762it [05:59,  4.20it/s]\u001b[A\n","763it [05:59,  4.23it/s]\u001b[A\n","764it [05:59,  4.26it/s]\u001b[A\n","765it [06:00,  4.28it/s]\u001b[A\n","766it [06:00,  4.29it/s]\u001b[A\n","767it [06:00,  4.30it/s]\u001b[A\n","768it [06:00,  4.31it/s]\u001b[A\n","769it [06:00,  4.32it/s]\u001b[A\n","770it [06:01,  4.32it/s]\u001b[A\n","771it [06:01,  4.32it/s]\u001b[A\n","772it [06:01,  4.32it/s]\u001b[A\n","773it [06:01,  4.32it/s]\u001b[A\n","774it [06:02,  4.32it/s]\u001b[A\n","775it [06:02,  4.32it/s]\u001b[A\n","776it [06:02,  4.32it/s]\u001b[A\n","777it [06:02,  4.32it/s]\u001b[A\n","778it [06:03,  4.33it/s]\u001b[A\n","779it [06:03,  4.33it/s]\u001b[A\n","780it [06:03,  4.33it/s]\u001b[A\n","781it [06:03,  4.33it/s]\u001b[A\n","782it [06:03,  4.32it/s]\u001b[A\n","783it [06:04,  4.32it/s]\u001b[A\n","784it [06:04,  4.33it/s]\u001b[A\n","785it [06:04,  4.32it/s]\u001b[A\n","786it [06:04,  4.32it/s]\u001b[A\n","787it [06:05,  4.32it/s]\u001b[A\n","788it [06:05,  4.33it/s]\u001b[A\n","789it [06:05,  4.33it/s]\u001b[A\n","790it [06:05,  4.33it/s]\u001b[A\n","791it [06:06,  4.33it/s]\u001b[A\n","792it [06:06,  4.33it/s]\u001b[A\n","793it [06:06,  4.33it/s]\u001b[A\n","794it [06:06,  4.33it/s]\u001b[A\n","795it [06:06,  4.32it/s]\u001b[A\n","796it [06:07,  4.32it/s]\u001b[A\n","797it [06:07,  4.32it/s]\u001b[A\n","798it [06:07,  4.32it/s]\u001b[A\n","799it [06:07,  4.32it/s]\u001b[A\n","800it [06:08,  4.32it/s]\u001b[A\n","801it [06:08,  4.32it/s]\u001b[A\n","802it [06:08,  4.32it/s]\u001b[A\n","803it [06:08,  4.33it/s]\u001b[A\n","804it [06:09,  4.33it/s]\u001b[A\n","805it [06:09,  4.33it/s]\u001b[A\n","806it [06:09,  4.33it/s]\u001b[A\n","807it [06:09,  4.33it/s]\u001b[A\n","808it [06:09,  4.33it/s]\u001b[A\n","809it [06:10,  4.32it/s]\u001b[A\n","810it [06:10,  4.32it/s]\u001b[A\n","811it [06:10,  4.33it/s]\u001b[A\n","812it [06:10,  4.33it/s]\u001b[A\n","813it [06:11,  4.33it/s]\u001b[A\n","814it [06:11,  4.33it/s]\u001b[A\n","815it [06:11,  4.32it/s]\u001b[A\n","816it [06:11,  4.32it/s]\u001b[A\n","817it [06:12,  4.32it/s]\u001b[A\n","818it [06:12,  4.32it/s]\u001b[A\n","819it [06:12,  4.32it/s]\u001b[A\n","820it [06:12,  4.32it/s]\u001b[A\n","821it [06:12,  4.32it/s]\u001b[A\n","822it [06:13,  4.32it/s]\u001b[A\n","823it [06:13,  4.32it/s]\u001b[A\n","824it [06:13,  4.32it/s]\u001b[A\n","825it [06:13,  4.33it/s]\u001b[A\n","826it [06:14,  4.33it/s]\u001b[A\n","827it [06:14,  4.33it/s]\u001b[A\n","828it [06:14,  4.33it/s]\u001b[A\n","829it [06:14,  4.32it/s]\u001b[A\n","830it [06:15,  4.33it/s]\u001b[A\n","831it [06:15,  4.32it/s]\u001b[A\n","832it [06:15,  4.32it/s]\u001b[A\n","833it [06:15,  4.33it/s]\u001b[A\n","834it [06:15,  4.33it/s]\u001b[A\n","835it [06:16,  4.32it/s]\u001b[A\n","836it [06:16,  4.33it/s]\u001b[A\n","837it [06:16,  4.33it/s]\u001b[A\n","838it [06:16,  4.33it/s]\u001b[A\n","839it [06:17,  4.32it/s]\u001b[A\n","840it [06:17,  4.33it/s]\u001b[A\n","841it [06:17,  4.32it/s]\u001b[A\n","842it [06:17,  4.32it/s]\u001b[A\n","843it [06:18,  4.33it/s]\u001b[A\n","844it [06:18,  4.33it/s]\u001b[A\n","845it [06:18,  4.33it/s]\u001b[A\n","846it [06:18,  4.33it/s]\u001b[A\n","847it [06:19,  4.33it/s]\u001b[A\n","848it [06:19,  4.33it/s]\u001b[A\n","849it [06:19,  4.32it/s]\u001b[A\n","850it [06:19,  4.33it/s]\u001b[A\n","851it [06:19,  4.33it/s]\u001b[A\n","852it [06:20,  4.33it/s]\u001b[A\n","853it [06:20,  4.33it/s]\u001b[A\n","854it [06:20,  4.33it/s]\u001b[A\n","855it [06:20,  4.33it/s]\u001b[A\n","856it [06:21,  4.33it/s]\u001b[A\n","857it [06:21,  4.33it/s]\u001b[A\n","858it [06:21,  4.33it/s]\u001b[A\n","859it [06:21,  4.33it/s]\u001b[A\n","860it [06:22,  4.33it/s]\u001b[A\n","861it [06:22,  4.33it/s]\u001b[A\n","862it [06:22,  4.32it/s]\u001b[A\n","863it [06:22,  4.32it/s]\u001b[A\n","864it [06:22,  4.32it/s]\u001b[A\n","865it [06:23,  4.32it/s]\u001b[A\n","866it [06:23,  4.32it/s]\u001b[A\n","867it [06:23,  4.33it/s]\u001b[A\n","868it [06:23,  4.33it/s]\u001b[A\n","869it [06:24,  4.33it/s]\u001b[A\n","870it [06:24,  4.33it/s]\u001b[A\n","871it [06:24,  4.33it/s]\u001b[A\n","872it [06:24,  4.32it/s]\u001b[A\n","873it [06:25,  4.33it/s]\u001b[A\n","874it [06:25,  4.32it/s]\u001b[A\n","875it [06:25,  4.32it/s]\u001b[A\n","876it [06:25,  4.33it/s]\u001b[A\n","877it [06:25,  4.33it/s]\u001b[A\n","878it [06:26,  4.33it/s]\u001b[A\n","879it [06:26,  4.33it/s]\u001b[A\n","880it [06:26,  4.33it/s]\u001b[A\n","881it [06:26,  4.33it/s]\u001b[A\n","882it [06:27,  4.33it/s]\u001b[A\n","883it [06:27,  4.33it/s]\u001b[A\n","884it [06:27,  4.32it/s]\u001b[A\n","885it [06:27,  4.32it/s]\u001b[A\n","886it [06:28,  4.32it/s]\u001b[A\n","887it [06:28,  4.32it/s]\u001b[A\n","888it [06:28,  4.33it/s]\u001b[A\n","889it [06:28,  4.32it/s]\u001b[A\n","890it [06:28,  4.32it/s]\u001b[A\n","891it [06:29,  4.32it/s]\u001b[A\n","892it [06:29,  4.32it/s]\u001b[A\n","893it [06:29,  4.32it/s]\u001b[A\n","894it [06:29,  4.32it/s]\u001b[A\n","895it [06:30,  4.32it/s]\u001b[A\n","896it [06:30,  4.32it/s]\u001b[A\n","897it [06:30,  4.32it/s]\u001b[A\n","898it [06:30,  4.32it/s]\u001b[A\n","899it [06:31,  4.32it/s]\u001b[A\n","900it [06:31,  4.33it/s]\u001b[A\n","901it [06:31,  4.33it/s]\u001b[A\n","902it [06:31,  4.33it/s]\u001b[A\n","903it [06:31,  4.33it/s]\u001b[A\n","904it [06:32,  4.33it/s]\u001b[A\n","905it [06:32,  4.33it/s]\u001b[A\n","906it [06:32,  4.33it/s]\u001b[A\n","907it [06:32,  4.32it/s]\u001b[A\n","908it [06:33,  4.32it/s]\u001b[A\n","909it [06:33,  4.32it/s]\u001b[A\n","910it [06:33,  4.32it/s]\u001b[A\n","911it [06:33,  4.33it/s]\u001b[A\n","912it [06:34,  4.33it/s]\u001b[A\n","913it [06:34,  4.33it/s]\u001b[A\n","914it [06:34,  4.33it/s]\u001b[A\n","915it [06:34,  4.33it/s]\u001b[A\n","916it [06:34,  4.33it/s]\u001b[A\n","917it [06:35,  4.33it/s]\u001b[A\n","918it [06:35,  4.32it/s]\u001b[A\n","919it [06:35,  4.33it/s]\u001b[A\n","920it [06:35,  4.32it/s]\u001b[A\n","921it [06:36,  4.32it/s]\u001b[A\n","922it [06:36,  4.33it/s]\u001b[A\n","923it [06:36,  4.33it/s]\u001b[A\n","924it [06:36,  4.33it/s]\u001b[A\n","Epoch:  60% 149/250 [07:59<02:48,  1.67s/it]\n","926it [07:19, 12.76s/it]\u001b[A\n","927it [07:19,  9.00s/it]\u001b[A\n","928it [07:19,  6.37s/it]\u001b[A\n","929it [07:19,  4.53s/it]\u001b[A\n","930it [07:19,  3.24s/it]\u001b[A\n","931it [07:20,  2.34s/it]\u001b[A\n","932it [07:20,  1.71s/it]\u001b[A\n","933it [07:20,  1.26s/it]\u001b[A\n","934it [07:20,  1.05it/s]\u001b[A\n","935it [07:21,  1.36it/s]\u001b[A\n","936it [07:21,  1.71it/s]\u001b[A\n","937it [07:21,  2.09it/s]\u001b[A\n","938it [07:21,  2.47it/s]\u001b[A\n","939it [07:22,  2.84it/s]\u001b[A\n","940it [07:22,  3.16it/s]\u001b[A\n","941it [07:22,  3.44it/s]\u001b[A\n","942it [07:22,  3.66it/s]\u001b[A\n","943it [07:22,  3.84it/s]\u001b[A\n","944it [07:23,  3.97it/s]\u001b[A\n","945it [07:23,  4.07it/s]\u001b[A\n","946it [07:23,  4.15it/s]\u001b[A\n","947it [07:23,  4.20it/s]\u001b[A\n","948it [07:24,  4.23it/s]\u001b[A\n","949it [07:24,  4.26it/s]\u001b[A\n","950it [07:24,  4.28it/s]\u001b[A\n","951it [07:24,  4.29it/s]\u001b[A\n","952it [07:25,  4.30it/s]\u001b[A\n","953it [07:25,  4.31it/s]\u001b[A\n","954it [07:25,  4.32it/s]\u001b[A\n","955it [07:25,  4.32it/s]\u001b[A\n","956it [07:25,  4.32it/s]\u001b[A\n","957it [07:26,  4.32it/s]\u001b[A\n","958it [07:26,  4.33it/s]\u001b[A\n","959it [07:26,  4.33it/s]\u001b[A\n","960it [07:26,  4.32it/s]\u001b[A\n","961it [07:27,  4.32it/s]\u001b[A\n","962it [07:27,  4.32it/s]\u001b[A\n","963it [07:27,  4.32it/s]\u001b[A\n","964it [07:27,  4.32it/s]\u001b[A\n","965it [07:28,  4.32it/s]\u001b[A\n","966it [07:28,  4.33it/s]\u001b[A\n","967it [07:28,  4.33it/s]\u001b[A\n","968it [07:28,  4.33it/s]\u001b[A\n","969it [07:28,  4.33it/s]\u001b[A\n","970it [07:29,  4.33it/s]\u001b[A\n","971it [07:29,  4.33it/s]\u001b[A\n","972it [07:29,  4.33it/s]\u001b[A\n","973it [07:29,  4.32it/s]\u001b[A\n","974it [07:30,  4.32it/s]\u001b[A\n","975it [07:30,  4.32it/s]\u001b[A\n","976it [07:30,  4.33it/s]\u001b[A\n","977it [07:30,  4.33it/s]\u001b[A\n","978it [07:31,  4.33it/s]\u001b[A\n","979it [07:31,  4.33it/s]\u001b[A\n","980it [07:31,  4.33it/s]\u001b[A\n","981it [07:31,  4.33it/s]\u001b[A\n","982it [07:31,  4.33it/s]\u001b[A\n","983it [07:32,  4.33it/s]\u001b[A\n","984it [07:32,  4.32it/s]\u001b[A\n","985it [07:32,  4.33it/s]\u001b[A\n","986it [07:32,  4.33it/s]\u001b[A\n","987it [07:33,  4.33it/s]\u001b[A\n","988it [07:33,  4.33it/s]\u001b[A\n","989it [07:33,  4.33it/s]\u001b[A\n","990it [07:33,  4.33it/s]\u001b[A\n","991it [07:34,  4.33it/s]\u001b[A\n","992it [07:34,  4.33it/s]\u001b[A\n","993it [07:34,  4.33it/s]\u001b[A\n","994it [07:34,  4.32it/s]\u001b[A\n","995it [07:34,  4.33it/s]\u001b[A\n","996it [07:35,  4.32it/s]\u001b[A\n","997it [07:35,  4.32it/s]\u001b[A\n","998it [07:35,  4.32it/s]\u001b[A\n","999it [07:35,  4.33it/s]\u001b[A\n","1000it [07:36,  4.33it/s]\u001b[A\n","1001it [07:36,  4.33it/s]\u001b[A\n","1002it [07:36,  4.33it/s]\u001b[A\n","1003it [07:36,  4.33it/s]\u001b[A\n","1004it [07:37,  4.33it/s]\u001b[A\n","1005it [07:37,  4.33it/s]\u001b[A\n","1006it [07:37,  4.32it/s]\u001b[A\n","1007it [07:37,  4.32it/s]\u001b[A\n","1008it [07:37,  4.33it/s]\u001b[A\n","1009it [07:38,  4.33it/s]\u001b[A\n","1010it [07:38,  4.33it/s]\u001b[A\n","1011it [07:38,  4.33it/s]\u001b[A\n","1012it [07:38,  4.33it/s]\u001b[A\n","1013it [07:39,  4.33it/s]\u001b[A\n","1014it [07:39,  4.33it/s]\u001b[A\n","1015it [07:39,  4.33it/s]\u001b[A\n","1016it [07:39,  4.32it/s]\u001b[A\n","1017it [07:40,  4.32it/s]\u001b[A\n","1018it [07:40,  4.33it/s]\u001b[A\n","1019it [07:40,  4.33it/s]\u001b[A\n","1020it [07:40,  4.33it/s]\u001b[A\n","1021it [07:40,  4.33it/s]\u001b[A\n","1022it [07:41,  4.33it/s]\u001b[A\n","1023it [07:41,  4.32it/s]\u001b[A\n","1024it [07:41,  4.32it/s]\u001b[A\n","1025it [07:41,  4.33it/s]\u001b[A\n","1026it [07:42,  4.33it/s]\u001b[A\n","1027it [07:42,  4.32it/s]\u001b[A\n","1028it [07:42,  4.33it/s]\u001b[A\n","1029it [07:42,  4.33it/s]\u001b[A\n","1030it [07:43,  4.33it/s]\u001b[A\n","1031it [07:43,  4.33it/s]\u001b[A\n","1032it [07:43,  4.33it/s]\u001b[A\n","1033it [07:43,  4.33it/s]\u001b[A\n","1034it [07:43,  4.33it/s]\u001b[A\n","1035it [07:44,  4.33it/s]\u001b[A\n","1036it [07:44,  4.33it/s]\u001b[A\n","1037it [07:44,  4.32it/s]\u001b[A\n","1038it [07:44,  4.33it/s]\u001b[A\n","1039it [07:45,  4.32it/s]\u001b[A\n","1040it [07:45,  4.32it/s]\u001b[A\n","1041it [07:45,  4.32it/s]\u001b[A\n","1042it [07:45,  4.33it/s]\u001b[A\n","1043it [07:46,  4.33it/s]\u001b[A\n","1044it [07:46,  4.33it/s]\u001b[A\n","1045it [07:46,  4.33it/s]\u001b[A\n","1046it [07:46,  4.33it/s]\u001b[A\n","1047it [07:46,  4.33it/s]\u001b[A\n","1048it [07:47,  4.33it/s]\u001b[A\n","1049it [07:47,  4.33it/s]\u001b[A\n","1050it [07:47,  4.32it/s]\u001b[A\n","1051it [07:47,  4.33it/s]\u001b[A\n","1052it [07:48,  4.33it/s]\u001b[A\n","1053it [07:48,  4.33it/s]\u001b[A\n","1054it [07:48,  4.33it/s]\u001b[A\n","1055it [07:48,  4.33it/s]\u001b[A\n","1056it [07:49,  4.33it/s]\u001b[A\n","1057it [07:49,  4.32it/s]\u001b[A\n","1058it [07:49,  4.32it/s]\u001b[A\n","1059it [07:49,  4.32it/s]\u001b[A\n","1060it [07:50,  4.32it/s]\u001b[A\n","1061it [07:50,  4.33it/s]\u001b[A\n","1062it [07:50,  4.33it/s]\u001b[A\n","1063it [07:50,  4.32it/s]\u001b[A\n","1064it [07:50,  4.33it/s]\u001b[A\n","1065it [07:51,  4.33it/s]\u001b[A\n","1066it [07:51,  4.33it/s]\u001b[A\n","1067it [07:51,  4.33it/s]\u001b[A\n","1068it [07:51,  4.33it/s]\u001b[A\n","1069it [07:52,  4.33it/s]\u001b[A\n","1070it [07:52,  4.32it/s]\u001b[A\n","1071it [07:52,  4.32it/s]\u001b[A\n","1072it [07:52,  4.32it/s]\u001b[A\n","1073it [07:53,  4.32it/s]\u001b[A\n","1074it [07:53,  4.32it/s]\u001b[A\n","1075it [07:53,  4.32it/s]\u001b[A\n","1076it [07:53,  4.33it/s]\u001b[A\n","1077it [07:53,  4.33it/s]\u001b[A\n","1078it [07:54,  4.33it/s]\u001b[A\n","1079it [07:54,  4.32it/s]\u001b[A\n","1080it [07:54,  4.32it/s]\u001b[A\n","1081it [07:54,  4.32it/s]\u001b[A\n","1082it [07:55,  4.32it/s]\u001b[A\n","1083it [07:55,  4.32it/s]\u001b[A\n","1084it [07:55,  4.32it/s]\u001b[A\n","1085it [07:55,  4.32it/s]\u001b[A\n","1086it [07:56,  4.33it/s]\u001b[A\n","1087it [07:56,  4.33it/s]\u001b[A\n","1088it [07:56,  4.33it/s]\u001b[A\n","1089it [07:56,  4.33it/s]\u001b[A\n","1090it [07:56,  4.32it/s]\u001b[A\n","1091it [07:57,  4.33it/s]\u001b[A\n","1092it [07:57,  4.32it/s]\u001b[A\n","1093it [07:57,  4.32it/s]\u001b[A\n","1094it [07:57,  4.32it/s]\u001b[A\n","1095it [07:58,  4.33it/s]\u001b[A\n","1096it [07:58,  4.32it/s]\u001b[A\n","1097it [07:58,  4.33it/s]\u001b[A\n","1098it [07:58,  4.33it/s]\u001b[A\n","1099it [07:59,  4.33it/s]\u001b[A\n","1100it [07:59,  4.33it/s]\u001b[A\n","1101it [07:59,  4.33it/s]\u001b[A\n","1102it [07:59,  4.32it/s]\u001b[A\n","1103it [07:59,  4.32it/s]\u001b[A\n","1104it [08:00,  4.32it/s]\u001b[A\n","1105it [08:00,  4.32it/s]\u001b[A\n","1106it [08:00,  4.32it/s]\u001b[A\n","1107it [08:00,  4.32it/s]\u001b[A\n","1108it [08:01,  4.32it/s]\u001b[A\n","1109it [08:01,  4.32it/s]\u001b[A\n","Epoch:  70% 174/250 [09:24<02:07,  1.67s/it]\n","1111it [08:43, 12.76s/it]\u001b[A\n","1112it [08:43,  9.00s/it]\u001b[A\n","1113it [08:44,  6.37s/it]\u001b[A\n","1114it [08:44,  4.53s/it]\u001b[A\n","1115it [08:44,  3.24s/it]\u001b[A\n","1116it [08:44,  2.34s/it]\u001b[A\n","1117it [08:44,  1.71s/it]\u001b[A\n","1118it [08:45,  1.26s/it]\u001b[A\n","1119it [08:45,  1.05it/s]\u001b[A\n","1120it [08:45,  1.36it/s]\u001b[A\n","1121it [08:45,  1.71it/s]\u001b[A\n","1122it [08:46,  2.09it/s]\u001b[A\n","1123it [08:46,  2.47it/s]\u001b[A\n","1124it [08:46,  2.84it/s]\u001b[A\n","1125it [08:46,  3.16it/s]\u001b[A\n","1126it [08:47,  3.44it/s]\u001b[A\n","1127it [08:47,  3.66it/s]\u001b[A\n","1128it [08:47,  3.84it/s]\u001b[A\n","1129it [08:47,  3.97it/s]\u001b[A\n","1130it [08:47,  4.07it/s]\u001b[A\n","1131it [08:48,  4.14it/s]\u001b[A\n","1132it [08:48,  4.20it/s]\u001b[A\n","1133it [08:48,  4.23it/s]\u001b[A\n","1134it [08:48,  4.26it/s]\u001b[A\n","1135it [08:49,  4.28it/s]\u001b[A\n","1136it [08:49,  4.29it/s]\u001b[A\n","1137it [08:49,  4.30it/s]\u001b[A\n","1138it [08:49,  4.31it/s]\u001b[A\n","1139it [08:50,  4.31it/s]\u001b[A\n","1140it [08:50,  4.32it/s]\u001b[A\n","1141it [08:50,  4.32it/s]\u001b[A\n","1142it [08:50,  4.32it/s]\u001b[A\n","1143it [08:50,  4.32it/s]\u001b[A\n","1144it [08:51,  4.32it/s]\u001b[A\n","1145it [08:51,  4.33it/s]\u001b[A\n","1146it [08:51,  4.33it/s]\u001b[A\n","1147it [08:51,  4.33it/s]\u001b[A\n","1148it [08:52,  4.33it/s]\u001b[A\n","1149it [08:52,  4.33it/s]\u001b[A\n","1150it [08:52,  4.33it/s]\u001b[A\n","1151it [08:52,  4.33it/s]\u001b[A\n","1152it [08:53,  4.32it/s]\u001b[A\n","1153it [08:53,  4.33it/s]\u001b[A\n","1154it [08:53,  4.33it/s]\u001b[A\n","1155it [08:53,  4.33it/s]\u001b[A\n","1156it [08:53,  4.33it/s]\u001b[A\n","1157it [08:54,  4.33it/s]\u001b[A\n","1158it [08:54,  4.33it/s]\u001b[A\n","1159it [08:54,  4.33it/s]\u001b[A\n","1160it [08:54,  4.33it/s]\u001b[A\n","1161it [08:55,  4.33it/s]\u001b[A\n","1162it [08:55,  4.32it/s]\u001b[A\n","1163it [08:55,  4.33it/s]\u001b[A\n","1164it [08:55,  4.33it/s]\u001b[A\n","1165it [08:56,  4.32it/s]\u001b[A\n","1166it [08:56,  4.33it/s]\u001b[A\n","1167it [08:56,  4.33it/s]\u001b[A\n","1168it [08:56,  4.33it/s]\u001b[A\n","1169it [08:56,  4.33it/s]\u001b[A\n","1170it [08:57,  4.33it/s]\u001b[A\n","1171it [08:57,  4.33it/s]\u001b[A\n","1172it [08:57,  4.33it/s]\u001b[A\n","1173it [08:57,  4.33it/s]\u001b[A\n","1174it [08:58,  4.32it/s]\u001b[A\n","1175it [08:58,  4.32it/s]\u001b[A\n","1176it [08:58,  4.32it/s]\u001b[A\n","1177it [08:58,  4.32it/s]\u001b[A\n","1178it [08:59,  4.32it/s]\u001b[A\n","1179it [08:59,  4.33it/s]\u001b[A\n","1180it [08:59,  4.33it/s]\u001b[A\n","1181it [08:59,  4.33it/s]\u001b[A\n","1182it [08:59,  4.32it/s]\u001b[A\n","1183it [09:00,  4.33it/s]\u001b[A\n","1184it [09:00,  4.32it/s]\u001b[A\n","1185it [09:00,  4.32it/s]\u001b[A\n","1186it [09:00,  4.32it/s]\u001b[A\n","1187it [09:01,  4.32it/s]\u001b[A\n","1188it [09:01,  4.33it/s]\u001b[A\n","1189it [09:01,  4.33it/s]\u001b[A\n","1190it [09:01,  4.33it/s]\u001b[A\n","1191it [09:02,  4.33it/s]\u001b[A\n","1192it [09:02,  4.33it/s]\u001b[A\n","1193it [09:02,  4.33it/s]\u001b[A\n","1194it [09:02,  4.33it/s]\u001b[A\n","1195it [09:02,  4.32it/s]\u001b[A\n","1196it [09:03,  4.33it/s]\u001b[A\n","1197it [09:03,  4.33it/s]\u001b[A\n","1198it [09:03,  4.33it/s]\u001b[A\n","1199it [09:03,  4.33it/s]\u001b[A\n","1200it [09:04,  4.33it/s]\u001b[A\n","1201it [09:04,  4.33it/s]\u001b[A\n","1202it [09:04,  4.32it/s]\u001b[A\n","1203it [09:04,  4.32it/s]\u001b[A\n","1204it [09:05,  4.33it/s]\u001b[A\n","1205it [09:05,  4.33it/s]\u001b[A\n","1206it [09:05,  4.33it/s]\u001b[A\n","1207it [09:05,  4.32it/s]\u001b[A\n","1208it [09:05,  4.32it/s]\u001b[A\n","1209it [09:06,  4.33it/s]\u001b[A\n","1210it [09:06,  4.33it/s]\u001b[A\n","1211it [09:06,  4.33it/s]\u001b[A\n","1212it [09:06,  4.33it/s]\u001b[A\n","1213it [09:07,  4.33it/s]\u001b[A\n","1214it [09:07,  4.33it/s]\u001b[A\n","1215it [09:07,  4.33it/s]\u001b[A\n","1216it [09:07,  4.33it/s]\u001b[A\n","1217it [09:08,  4.33it/s]\u001b[A\n","1218it [09:08,  4.32it/s]\u001b[A\n","1219it [09:08,  4.33it/s]\u001b[A\n","1220it [09:08,  4.33it/s]\u001b[A\n","1221it [09:08,  4.33it/s]\u001b[A\n","1222it [09:09,  4.33it/s]\u001b[A\n","1223it [09:09,  4.33it/s]\u001b[A\n","1224it [09:09,  4.33it/s]\u001b[A\n","1225it [09:09,  4.33it/s]\u001b[A\n","1226it [09:10,  4.33it/s]\u001b[A\n","1227it [09:10,  4.33it/s]\u001b[A\n","1228it [09:10,  4.33it/s]\u001b[A\n","1229it [09:10,  4.32it/s]\u001b[A\n","1230it [09:11,  4.32it/s]\u001b[A\n","1231it [09:11,  4.32it/s]\u001b[A\n","1232it [09:11,  4.33it/s]\u001b[A\n","1233it [09:11,  4.33it/s]\u001b[A\n","1234it [09:11,  4.32it/s]\u001b[A\n","1235it [09:12,  4.32it/s]\u001b[A\n","1236it [09:12,  4.32it/s]\u001b[A\n","1237it [09:12,  4.32it/s]\u001b[A\n","1238it [09:12,  4.32it/s]\u001b[A\n","1239it [09:13,  4.32it/s]\u001b[A\n","1240it [09:13,  4.32it/s]\u001b[A\n","1241it [09:13,  4.32it/s]\u001b[A\n","1242it [09:13,  4.32it/s]\u001b[A\n","1243it [09:14,  4.33it/s]\u001b[A\n","1244it [09:14,  4.33it/s]\u001b[A\n","1245it [09:14,  4.33it/s]\u001b[A\n","1246it [09:14,  4.33it/s]\u001b[A\n","1247it [09:15,  4.32it/s]\u001b[A\n","1248it [09:15,  4.32it/s]\u001b[A\n","1249it [09:15,  4.32it/s]\u001b[A\n","1250it [09:15,  4.32it/s]\u001b[A\n","1251it [09:15,  4.32it/s]\u001b[A\n","1252it [09:16,  4.32it/s]\u001b[A\n","1253it [09:16,  4.32it/s]\u001b[A\n","1254it [09:16,  4.32it/s]\u001b[A\n","1255it [09:16,  4.32it/s]\u001b[A\n","1256it [09:17,  4.32it/s]\u001b[A\n","1257it [09:17,  4.32it/s]\u001b[A\n","1258it [09:17,  4.32it/s]\u001b[A\n","1259it [09:17,  4.32it/s]\u001b[A\n","1260it [09:18,  4.32it/s]\u001b[A\n","1261it [09:18,  4.32it/s]\u001b[A\n","1262it [09:18,  4.33it/s]\u001b[A\n","1263it [09:18,  4.32it/s]\u001b[A\n","1264it [09:18,  4.32it/s]\u001b[A\n","1265it [09:19,  4.32it/s]\u001b[A\n","1266it [09:19,  4.32it/s]\u001b[A\n","1267it [09:19,  4.33it/s]\u001b[A\n","1268it [09:19,  4.33it/s]\u001b[A\n","1269it [09:20,  4.33it/s]\u001b[A\n","1270it [09:20,  4.33it/s]\u001b[A\n","1271it [09:20,  4.33it/s]\u001b[A\n","1272it [09:20,  4.33it/s]\u001b[A\n","1273it [09:21,  4.32it/s]\u001b[A\n","1274it [09:21,  4.32it/s]\u001b[A\n","1275it [09:21,  4.33it/s]\u001b[A\n","1276it [09:21,  4.33it/s]\u001b[A\n","1277it [09:21,  4.33it/s]\u001b[A\n","1278it [09:22,  4.33it/s]\u001b[A\n","1279it [09:22,  4.33it/s]\u001b[A\n","1280it [09:22,  4.33it/s]\u001b[A\n","1281it [09:22,  4.32it/s]\u001b[A\n","1282it [09:23,  4.33it/s]\u001b[A\n","1283it [09:23,  4.32it/s]\u001b[A\n","1284it [09:23,  4.32it/s]\u001b[A\n","1285it [09:23,  4.33it/s]\u001b[A\n","1286it [09:24,  4.33it/s]\u001b[A\n","1287it [09:24,  4.32it/s]\u001b[A\n","1288it [09:24,  4.33it/s]\u001b[A\n","1289it [09:24,  4.33it/s]\u001b[A\n","1290it [09:24,  4.33it/s]\u001b[A\n","1291it [09:25,  4.33it/s]\u001b[A\n","1292it [09:25,  4.33it/s]\u001b[A\n","1293it [09:25,  4.33it/s]\u001b[A\n","1294it [09:25,  4.33it/s]\u001b[A\n","Epoch:  80% 199/250 [10:48<01:25,  1.67s/it]\n","1296it [10:08, 12.76s/it]\u001b[A\n","1297it [10:08,  9.00s/it]\u001b[A\n","1298it [10:08,  6.37s/it]\u001b[A\n","1299it [10:08,  4.53s/it]\u001b[A\n","1300it [10:09,  3.24s/it]\u001b[A\n","1301it [10:09,  2.34s/it]\u001b[A\n","1302it [10:09,  1.71s/it]\u001b[A\n","1303it [10:09,  1.26s/it]\u001b[A\n","1304it [10:09,  1.05it/s]\u001b[A\n","1305it [10:10,  1.36it/s]\u001b[A\n","1306it [10:10,  1.71it/s]\u001b[A\n","1307it [10:10,  2.09it/s]\u001b[A\n","1308it [10:10,  2.47it/s]\u001b[A\n","1309it [10:11,  2.83it/s]\u001b[A\n","1310it [10:11,  3.16it/s]\u001b[A\n","1311it [10:11,  3.44it/s]\u001b[A\n","1312it [10:11,  3.67it/s]\u001b[A\n","1313it [10:12,  3.84it/s]\u001b[A\n","1314it [10:12,  3.98it/s]\u001b[A\n","1315it [10:12,  4.07it/s]\u001b[A\n","1316it [10:12,  4.14it/s]\u001b[A\n","1317it [10:12,  4.20it/s]\u001b[A\n","1318it [10:13,  4.24it/s]\u001b[A\n","1319it [10:13,  4.26it/s]\u001b[A\n","1320it [10:13,  4.28it/s]\u001b[A\n","1321it [10:13,  4.30it/s]\u001b[A\n","1322it [10:14,  4.30it/s]\u001b[A\n","1323it [10:14,  4.31it/s]\u001b[A\n","1324it [10:14,  4.32it/s]\u001b[A\n","1325it [10:14,  4.32it/s]\u001b[A\n","1326it [10:15,  4.31it/s]\u001b[A\n","1327it [10:15,  4.31it/s]\u001b[A\n","1328it [10:15,  4.32it/s]\u001b[A\n","1329it [10:15,  4.32it/s]\u001b[A\n","1330it [10:15,  4.32it/s]\u001b[A\n","1331it [10:16,  4.32it/s]\u001b[A\n","1332it [10:16,  4.32it/s]\u001b[A\n","1333it [10:16,  4.32it/s]\u001b[A\n","1334it [10:16,  4.33it/s]\u001b[A\n","1335it [10:17,  4.32it/s]\u001b[A\n","1336it [10:17,  4.32it/s]\u001b[A\n","1337it [10:17,  4.33it/s]\u001b[A\n","1338it [10:17,  4.33it/s]\u001b[A\n","1339it [10:18,  4.32it/s]\u001b[A\n","1340it [10:18,  4.33it/s]\u001b[A\n","1341it [10:18,  4.33it/s]\u001b[A\n","1342it [10:18,  4.33it/s]\u001b[A\n","1343it [10:18,  4.33it/s]\u001b[A\n","1344it [10:19,  4.33it/s]\u001b[A\n","1345it [10:19,  4.32it/s]\u001b[A\n","1346it [10:19,  4.32it/s]\u001b[A\n","1347it [10:19,  4.33it/s]\u001b[A\n","1348it [10:20,  4.32it/s]\u001b[A\n","1349it [10:20,  4.32it/s]\u001b[A\n","1350it [10:20,  4.33it/s]\u001b[A\n","1351it [10:20,  4.33it/s]\u001b[A\n","1352it [10:21,  4.33it/s]\u001b[A\n","1353it [10:21,  4.33it/s]\u001b[A\n","1354it [10:21,  4.33it/s]\u001b[A\n","1355it [10:21,  4.33it/s]\u001b[A\n","1356it [10:21,  4.33it/s]\u001b[A\n","1357it [10:22,  4.33it/s]\u001b[A\n","1358it [10:22,  4.33it/s]\u001b[A\n","1359it [10:22,  4.33it/s]\u001b[A\n","1360it [10:22,  4.32it/s]\u001b[A\n","1361it [10:23,  4.33it/s]\u001b[A\n","1362it [10:23,  4.33it/s]\u001b[A\n","1363it [10:23,  4.33it/s]\u001b[A\n","1364it [10:23,  4.33it/s]\u001b[A\n","1365it [10:24,  4.33it/s]\u001b[A\n","1366it [10:24,  4.33it/s]\u001b[A\n","1367it [10:24,  4.33it/s]\u001b[A\n","1368it [10:24,  4.33it/s]\u001b[A\n","1369it [10:24,  4.33it/s]\u001b[A\n","1370it [10:25,  4.33it/s]\u001b[A\n","1371it [10:25,  4.33it/s]\u001b[A\n","1372it [10:25,  4.33it/s]\u001b[A\n","1373it [10:25,  4.33it/s]\u001b[A\n","1374it [10:26,  4.33it/s]\u001b[A\n","1375it [10:26,  4.33it/s]\u001b[A\n","1376it [10:26,  4.33it/s]\u001b[A\n","1377it [10:26,  4.33it/s]\u001b[A\n","1378it [10:27,  4.33it/s]\u001b[A\n","1379it [10:27,  4.33it/s]\u001b[A\n","1380it [10:27,  4.33it/s]\u001b[A\n","1381it [10:27,  4.32it/s]\u001b[A\n","1382it [10:27,  4.32it/s]\u001b[A\n","1383it [10:28,  4.32it/s]\u001b[A\n","1384it [10:28,  4.33it/s]\u001b[A\n","1385it [10:28,  4.33it/s]\u001b[A\n","1386it [10:28,  4.33it/s]\u001b[A\n","1387it [10:29,  4.33it/s]\u001b[A\n","1388it [10:29,  4.33it/s]\u001b[A\n","1389it [10:29,  4.33it/s]\u001b[A\n","1390it [10:29,  4.33it/s]\u001b[A\n","1391it [10:30,  4.32it/s]\u001b[A\n","1392it [10:30,  4.32it/s]\u001b[A\n","1393it [10:30,  4.32it/s]\u001b[A\n","1394it [10:30,  4.33it/s]\u001b[A\n","1395it [10:30,  4.33it/s]\u001b[A\n","1396it [10:31,  4.33it/s]\u001b[A\n","1397it [10:31,  4.33it/s]\u001b[A\n","1398it [10:31,  4.33it/s]\u001b[A\n","1399it [10:31,  4.33it/s]\u001b[A\n","1400it [10:32,  4.33it/s]\u001b[A\n","1401it [10:32,  4.33it/s]\u001b[A\n","1402it [10:32,  4.33it/s]\u001b[A\n","1403it [10:32,  4.33it/s]\u001b[A\n","1404it [10:33,  4.33it/s]\u001b[A\n","1405it [10:33,  4.33it/s]\u001b[A\n","1406it [10:33,  4.33it/s]\u001b[A\n","1407it [10:33,  4.33it/s]\u001b[A\n","1408it [10:33,  4.33it/s]\u001b[A\n","1409it [10:34,  4.33it/s]\u001b[A\n","1410it [10:34,  4.33it/s]\u001b[A\n","1411it [10:34,  4.33it/s]\u001b[A\n","1412it [10:34,  4.32it/s]\u001b[A\n","1413it [10:35,  4.33it/s]\u001b[A\n","1414it [10:35,  4.32it/s]\u001b[A\n","1415it [10:35,  4.32it/s]\u001b[A\n","1416it [10:35,  4.32it/s]\u001b[A\n","1417it [10:36,  4.33it/s]\u001b[A\n","1418it [10:36,  4.33it/s]\u001b[A\n","1419it [10:36,  4.33it/s]\u001b[A\n","1420it [10:36,  4.33it/s]\u001b[A\n","1421it [10:36,  4.33it/s]\u001b[A\n","1422it [10:37,  4.33it/s]\u001b[A\n","1423it [10:37,  4.33it/s]\u001b[A\n","1424it [10:37,  4.33it/s]\u001b[A\n","1425it [10:37,  4.32it/s]\u001b[A\n","1426it [10:38,  4.33it/s]\u001b[A\n","1427it [10:38,  4.33it/s]\u001b[A\n","1428it [10:38,  4.33it/s]\u001b[A\n","1429it [10:38,  4.33it/s]\u001b[A\n","1430it [10:39,  4.33it/s]\u001b[A\n","1431it [10:39,  4.33it/s]\u001b[A\n","1432it [10:39,  4.33it/s]\u001b[A\n","1433it [10:39,  4.33it/s]\u001b[A\n","1434it [10:40,  4.33it/s]\u001b[A\n","1435it [10:40,  4.33it/s]\u001b[A\n","1436it [10:40,  4.33it/s]\u001b[A\n","1437it [10:40,  4.33it/s]\u001b[A\n","1438it [10:40,  4.33it/s]\u001b[A\n","1439it [10:41,  4.33it/s]\u001b[A\n","1440it [10:41,  4.33it/s]\u001b[A\n","1441it [10:41,  4.33it/s]\u001b[A\n","1442it [10:41,  4.33it/s]\u001b[A\n","1443it [10:42,  4.33it/s]\u001b[A\n","1444it [10:42,  4.33it/s]\u001b[A\n","1445it [10:42,  4.33it/s]\u001b[A\n","1446it [10:42,  4.33it/s]\u001b[A\n","1447it [10:43,  4.33it/s]\u001b[A\n","1448it [10:43,  4.33it/s]\u001b[A\n","1449it [10:43,  4.33it/s]\u001b[A\n","1450it [10:43,  4.33it/s]\u001b[A\n","1451it [10:43,  4.33it/s]\u001b[A\n","1452it [10:44,  4.33it/s]\u001b[A\n","1453it [10:44,  4.33it/s]\u001b[A\n","1454it [10:44,  4.32it/s]\u001b[A\n","1455it [10:44,  4.32it/s]\u001b[A\n","1456it [10:45,  4.33it/s]\u001b[A\n","1457it [10:45,  4.33it/s]\u001b[A\n","1458it [10:45,  4.32it/s]\u001b[A\n","1459it [10:45,  4.33it/s]\u001b[A\n","1460it [10:46,  4.33it/s]\u001b[A\n","1461it [10:46,  4.33it/s]\u001b[A\n","1462it [10:46,  4.33it/s]\u001b[A\n","1463it [10:46,  4.33it/s]\u001b[A\n","1464it [10:46,  4.33it/s]\u001b[A\n","1465it [10:47,  4.33it/s]\u001b[A\n","1466it [10:47,  4.33it/s]\u001b[A\n","1467it [10:47,  4.33it/s]\u001b[A\n","1468it [10:47,  4.33it/s]\u001b[A\n","1469it [10:48,  4.33it/s]\u001b[A\n","1470it [10:48,  4.33it/s]\u001b[A\n","1471it [10:48,  4.33it/s]\u001b[A\n","1472it [10:48,  4.33it/s]\u001b[A\n","1473it [10:49,  4.33it/s]\u001b[A\n","1474it [10:49,  4.33it/s]\u001b[A\n","1475it [10:49,  4.33it/s]\u001b[A\n","1476it [10:49,  4.33it/s]\u001b[A\n","1477it [10:49,  4.33it/s]\u001b[A\n","1478it [10:50,  4.33it/s]\u001b[A\n","1479it [10:50,  4.33it/s]\u001b[A\n","Epoch:  90% 224/250 [12:12<00:43,  1.67s/it]\n","1481it [11:32, 12.73s/it]\u001b[A\n","1482it [11:32,  8.98s/it]\u001b[A\n","1483it [11:32,  6.35s/it]\u001b[A\n","1484it [11:33,  4.52s/it]\u001b[A\n","1485it [11:33,  3.23s/it]\u001b[A\n","1486it [11:33,  2.33s/it]\u001b[A\n","1487it [11:33,  1.70s/it]\u001b[A\n","1488it [11:34,  1.26s/it]\u001b[A\n","1489it [11:34,  1.05it/s]\u001b[A\n","1490it [11:34,  1.36it/s]\u001b[A\n","1491it [11:34,  1.71it/s]\u001b[A\n","1492it [11:35,  2.09it/s]\u001b[A\n","1493it [11:35,  2.48it/s]\u001b[A\n","1494it [11:35,  2.84it/s]\u001b[A\n","1495it [11:35,  3.17it/s]\u001b[A\n","1496it [11:35,  3.44it/s]\u001b[A\n","1497it [11:36,  3.67it/s]\u001b[A\n","1498it [11:36,  3.85it/s]\u001b[A\n","1499it [11:36,  3.98it/s]\u001b[A\n","1500it [11:36,  4.08it/s]\u001b[A\n","1501it [11:37,  4.15it/s]\u001b[A\n","1502it [11:37,  4.20it/s]\u001b[A\n","1503it [11:37,  4.24it/s]\u001b[A\n","1504it [11:37,  4.27it/s]\u001b[A\n","1505it [11:38,  4.29it/s]\u001b[A\n","1506it [11:38,  4.30it/s]\u001b[A\n","1507it [11:38,  4.31it/s]\u001b[A\n","1508it [11:38,  4.31it/s]\u001b[A\n","1509it [11:38,  4.32it/s]\u001b[A\n","1510it [11:39,  4.32it/s]\u001b[A\n","1511it [11:39,  4.33it/s]\u001b[A\n","1512it [11:39,  4.33it/s]\u001b[A\n","1513it [11:39,  4.33it/s]\u001b[A\n","1514it [11:40,  4.33it/s]\u001b[A\n","1515it [11:40,  4.33it/s]\u001b[A\n","1516it [11:40,  4.33it/s]\u001b[A\n","1517it [11:40,  4.33it/s]\u001b[A\n","1518it [11:41,  4.33it/s]\u001b[A\n","1519it [11:41,  4.33it/s]\u001b[A\n","1520it [11:41,  4.33it/s]\u001b[A\n","1521it [11:41,  4.33it/s]\u001b[A\n","1522it [11:41,  4.33it/s]\u001b[A\n","1523it [11:42,  4.33it/s]\u001b[A\n","1524it [11:42,  4.33it/s]\u001b[A\n","1525it [11:42,  4.33it/s]\u001b[A\n","1526it [11:42,  4.33it/s]\u001b[A\n","1527it [11:43,  4.33it/s]\u001b[A\n","1528it [11:43,  4.33it/s]\u001b[A\n","1529it [11:43,  4.32it/s]\u001b[A\n","1530it [11:43,  4.32it/s]\u001b[A\n","1531it [11:44,  4.33it/s]\u001b[A\n","1532it [11:44,  4.33it/s]\u001b[A\n","1533it [11:44,  4.33it/s]\u001b[A\n","1534it [11:44,  4.33it/s]\u001b[A\n","1535it [11:44,  4.33it/s]\u001b[A\n","1536it [11:45,  4.33it/s]\u001b[A\n","1537it [11:45,  4.33it/s]\u001b[A\n","1538it [11:45,  4.33it/s]\u001b[A\n","1539it [11:45,  4.33it/s]\u001b[A\n","1540it [11:46,  4.33it/s]\u001b[A\n","1541it [11:46,  4.33it/s]\u001b[A\n","1542it [11:46,  4.33it/s]\u001b[A\n","1543it [11:46,  4.33it/s]\u001b[A\n","1544it [11:47,  4.33it/s]\u001b[A\n","1545it [11:47,  4.33it/s]\u001b[A\n","1546it [11:47,  4.33it/s]\u001b[A\n","1547it [11:47,  4.33it/s]\u001b[A\n","1548it [11:47,  4.33it/s]\u001b[A\n","1549it [11:48,  4.33it/s]\u001b[A\n","1550it [11:48,  4.33it/s]\u001b[A\n","1551it [11:48,  4.33it/s]\u001b[A\n","1552it [11:48,  4.33it/s]\u001b[A\n","1553it [11:49,  4.33it/s]\u001b[A\n","1554it [11:49,  4.33it/s]\u001b[A\n","1555it [11:49,  4.33it/s]\u001b[A\n","1556it [11:49,  4.33it/s]\u001b[A\n","1557it [11:50,  4.33it/s]\u001b[A\n","1558it [11:50,  4.33it/s]\u001b[A\n","1559it [11:50,  4.33it/s]\u001b[A\n","1560it [11:50,  4.33it/s]\u001b[A\n","1561it [11:50,  4.33it/s]\u001b[A\n","1562it [11:51,  4.33it/s]\u001b[A\n","1563it [11:51,  4.33it/s]\u001b[A\n","1564it [11:51,  4.33it/s]\u001b[A\n","1565it [11:51,  4.33it/s]\u001b[A\n","1566it [11:52,  4.33it/s]\u001b[A\n","1567it [11:52,  4.33it/s]\u001b[A\n","1568it [11:52,  4.33it/s]\u001b[A\n","1569it [11:52,  4.33it/s]\u001b[A\n","1570it [11:53,  4.33it/s]\u001b[A\n","1571it [11:53,  4.33it/s]\u001b[A\n","1572it [11:53,  4.33it/s]\u001b[A\n","1573it [11:53,  4.33it/s]\u001b[A\n","1574it [11:53,  4.33it/s]\u001b[A\n","1575it [11:54,  4.33it/s]\u001b[A\n","1576it [11:54,  4.33it/s]\u001b[A\n","1577it [11:54,  4.33it/s]\u001b[A\n","1578it [11:54,  4.33it/s]\u001b[A\n","1579it [11:55,  4.33it/s]\u001b[A\n","1580it [11:55,  4.33it/s]\u001b[A\n","1581it [11:55,  4.33it/s]\u001b[A\n","1582it [11:55,  4.33it/s]\u001b[A\n","1583it [11:56,  4.33it/s]\u001b[A\n","1584it [11:56,  4.33it/s]\u001b[A\n","1585it [11:56,  4.33it/s]\u001b[A\n","1586it [11:56,  4.33it/s]\u001b[A\n","1587it [11:56,  4.33it/s]\u001b[A\n","1588it [11:57,  4.33it/s]\u001b[A\n","1589it [11:57,  4.33it/s]\u001b[A\n","1590it [11:57,  4.33it/s]\u001b[A\n","1591it [11:57,  4.33it/s]\u001b[A\n","1592it [11:58,  4.33it/s]\u001b[A\n","1593it [11:58,  4.33it/s]\u001b[A\n","1594it [11:58,  4.33it/s]\u001b[A\n","1595it [11:58,  4.33it/s]\u001b[A\n","1596it [11:59,  4.33it/s]\u001b[A\n","1597it [11:59,  4.33it/s]\u001b[A\n","1598it [11:59,  4.33it/s]\u001b[A\n","1599it [11:59,  4.33it/s]\u001b[A\n","1600it [11:59,  4.33it/s]\u001b[A\n","1601it [12:00,  4.33it/s]\u001b[A\n","1602it [12:00,  4.33it/s]\u001b[A\n","1603it [12:00,  4.33it/s]\u001b[A\n","1604it [12:00,  4.33it/s]\u001b[A\n","1605it [12:01,  4.33it/s]\u001b[A\n","1606it [12:01,  4.33it/s]\u001b[A\n","1607it [12:01,  4.33it/s]\u001b[A\n","1608it [12:01,  4.33it/s]\u001b[A\n","1609it [12:02,  4.33it/s]\u001b[A\n","1610it [12:02,  4.33it/s]\u001b[A\n","1611it [12:02,  4.33it/s]\u001b[A\n","1612it [12:02,  4.33it/s]\u001b[A\n","1613it [12:02,  4.33it/s]\u001b[A\n","1614it [12:03,  4.33it/s]\u001b[A\n","1615it [12:03,  4.33it/s]\u001b[A\n","1616it [12:03,  4.33it/s]\u001b[A\n","1617it [12:03,  4.33it/s]\u001b[A\n","1618it [12:04,  4.33it/s]\u001b[A\n","1619it [12:04,  4.33it/s]\u001b[A\n","1620it [12:04,  4.33it/s]\u001b[A\n","1621it [12:04,  4.33it/s]\u001b[A\n","1622it [12:05,  4.33it/s]\u001b[A\n","1623it [12:05,  4.33it/s]\u001b[A\n","1624it [12:05,  4.33it/s]\u001b[A\n","1625it [12:05,  4.33it/s]\u001b[A\n","1626it [12:06,  4.33it/s]\u001b[A\n","1627it [12:06,  4.33it/s]\u001b[A\n","1628it [12:06,  4.33it/s]\u001b[A\n","1629it [12:06,  4.33it/s]\u001b[A\n","1630it [12:06,  4.33it/s]\u001b[A\n","1631it [12:07,  4.33it/s]\u001b[A\n","1632it [12:07,  4.33it/s]\u001b[A\n","1633it [12:07,  4.33it/s]\u001b[A\n","1634it [12:07,  4.33it/s]\u001b[A\n","1635it [12:08,  4.33it/s]\u001b[A\n","1636it [12:08,  4.32it/s]\u001b[A\n","1637it [12:08,  4.33it/s]\u001b[A\n","1638it [12:08,  4.33it/s]\u001b[A\n","1639it [12:09,  4.33it/s]\u001b[A\n","1640it [12:09,  4.33it/s]\u001b[A\n","1641it [12:09,  4.33it/s]\u001b[A\n","1642it [12:09,  4.33it/s]\u001b[A\n","1643it [12:09,  4.33it/s]\u001b[A\n","1644it [12:10,  4.33it/s]\u001b[A\n","1645it [12:10,  4.33it/s]\u001b[A\n","1646it [12:10,  4.32it/s]\u001b[A\n","1647it [12:10,  4.33it/s]\u001b[A\n","1648it [12:11,  4.33it/s]\u001b[A\n","1649it [12:11,  4.33it/s]\u001b[A\n","1650it [12:11,  4.33it/s]\u001b[A\n","1651it [12:11,  4.33it/s]\u001b[A\n","1652it [12:12,  4.33it/s]\u001b[A\n","1653it [12:12,  4.33it/s]\u001b[A\n","1654it [12:12,  4.33it/s]\u001b[A\n","1655it [12:12,  4.33it/s]\u001b[A\n","1656it [12:12,  4.33it/s]\u001b[A\n","1657it [12:13,  4.33it/s]\u001b[A\n","1658it [12:13,  4.33it/s]\u001b[A\n","1659it [12:13,  4.33it/s]\u001b[A\n","1660it [12:13,  4.33it/s]\u001b[A\n","1661it [12:14,  4.33it/s]\u001b[A\n","1662it [12:14,  4.33it/s]\u001b[A\n","1663it [12:14,  4.33it/s]\u001b[A\n","1664it [12:14,  4.33it/s]\u001b[A\n","Epoch: 100% 249/250 [13:37<00:01,  1.67s/it]\n","1666it [12:56, 12.73s/it]\u001b[A\n","1667it [12:57,  8.98s/it]\u001b[A\n","1668it [12:57,  6.36s/it]\u001b[A\n","1669it [12:57,  4.52s/it]\u001b[A\n","1670it [12:57,  3.23s/it]\u001b[A\n","1671it [12:58,  2.33s/it]\u001b[A\n","1672it [12:58,  1.70s/it]\u001b[A\n","1673it [12:58,  1.26s/it]\u001b[A\n","1674it [12:58,  1.05it/s]\u001b[A\n","1675it [12:58,  1.36it/s]\u001b[A\n","1676it [12:59,  1.71it/s]\u001b[A\n","1677it [12:59,  2.09it/s]\u001b[A\n","1678it [12:59,  2.48it/s]\u001b[A\n","1679it [12:59,  2.84it/s]\u001b[A\n","1680it [13:00,  3.17it/s]\u001b[A\n","1681it [13:00,  3.44it/s]\u001b[A\n","1682it [13:00,  3.67it/s]\u001b[A\n","1683it [13:00,  3.84it/s]\u001b[A\n","1684it [13:01,  3.97it/s]\u001b[A\n","1685it [13:01,  4.07it/s]\u001b[A\n","1686it [13:01,  4.15it/s]\u001b[A\n","1687it [13:01,  4.20it/s]\u001b[A\n","1688it [13:01,  4.24it/s]\u001b[A\n","1689it [13:02,  4.27it/s]\u001b[A\n","1690it [13:02,  4.29it/s]\u001b[A\n","1691it [13:02,  4.30it/s]\u001b[A\n","1692it [13:02,  4.31it/s]\u001b[A\n","1693it [13:03,  4.32it/s]\u001b[A\n","1694it [13:03,  4.32it/s]\u001b[A\n","1695it [13:03,  4.32it/s]\u001b[A\n","1696it [13:03,  4.33it/s]\u001b[A\n","1697it [13:04,  4.33it/s]\u001b[A\n","1698it [13:04,  4.33it/s]\u001b[A\n","1699it [13:04,  4.33it/s]\u001b[A\n","1700it [13:04,  4.33it/s]\u001b[A\n","1701it [13:04,  4.33it/s]\u001b[A\n","1702it [13:05,  4.33it/s]\u001b[A\n","1703it [13:05,  4.33it/s]\u001b[A\n","1704it [13:05,  4.33it/s]\u001b[A\n","1705it [13:05,  4.33it/s]\u001b[A\n","1706it [13:06,  4.33it/s]\u001b[A\n","1707it [13:06,  4.33it/s]\u001b[A\n","1708it [13:06,  4.33it/s]\u001b[A\n","1709it [13:06,  4.33it/s]\u001b[A\n","1710it [13:07,  4.33it/s]\u001b[A\n","1711it [13:07,  4.33it/s]\u001b[A\n","1712it [13:07,  4.33it/s]\u001b[A\n","1713it [13:07,  4.33it/s]\u001b[A\n","1714it [13:07,  4.33it/s]\u001b[A\n","1715it [13:08,  4.33it/s]\u001b[A\n","1716it [13:08,  4.33it/s]\u001b[A\n","1717it [13:08,  4.33it/s]\u001b[A\n","1718it [13:08,  4.33it/s]\u001b[A\n","1719it [13:09,  4.33it/s]\u001b[A\n","1720it [13:09,  4.33it/s]\u001b[A\n","1721it [13:09,  4.33it/s]\u001b[A\n","1722it [13:09,  4.33it/s]\u001b[A\n","1723it [13:10,  4.33it/s]\u001b[A\n","1724it [13:10,  4.33it/s]\u001b[A\n","1725it [13:10,  4.33it/s]\u001b[A\n","1726it [13:10,  4.33it/s]\u001b[A\n","1727it [13:10,  4.33it/s]\u001b[A\n","1728it [13:11,  4.33it/s]\u001b[A\n","1729it [13:11,  4.33it/s]\u001b[A\n","1730it [13:11,  4.33it/s]\u001b[A\n","1731it [13:11,  4.33it/s]\u001b[A\n","1732it [13:12,  4.33it/s]\u001b[A\n","1733it [13:12,  4.33it/s]\u001b[A\n","1734it [13:12,  4.33it/s]\u001b[A\n","1735it [13:12,  4.33it/s]\u001b[A\n","1736it [13:13,  4.33it/s]\u001b[A\n","1737it [13:13,  4.33it/s]\u001b[A\n","1738it [13:13,  4.33it/s]\u001b[A\n","1739it [13:13,  4.33it/s]\u001b[A\n","1740it [13:14,  4.33it/s]\u001b[A\n","1741it [13:14,  4.33it/s]\u001b[A\n","1742it [13:14,  4.33it/s]\u001b[A\n","1743it [13:14,  4.33it/s]\u001b[A\n","1744it [13:14,  4.33it/s]\u001b[A\n","1745it [13:15,  4.33it/s]\u001b[A\n","1746it [13:15,  4.33it/s]\u001b[A\n","1747it [13:15,  4.33it/s]\u001b[A\n","1748it [13:15,  4.33it/s]\u001b[A\n","1749it [13:16,  4.33it/s]\u001b[A\n","1750it [13:16,  4.33it/s]\u001b[A\n","1751it [13:16,  4.33it/s]\u001b[A\n","1752it [13:16,  4.33it/s]\u001b[A\n","1753it [13:17,  4.33it/s]\u001b[A\n","1754it [13:17,  4.33it/s]\u001b[A\n","1755it [13:17,  4.33it/s]\u001b[A\n","1756it [13:17,  4.33it/s]\u001b[A\n","1757it [13:17,  4.33it/s]\u001b[A\n","1758it [13:18,  4.33it/s]\u001b[A\n","1759it [13:18,  4.33it/s]\u001b[A\n","1760it [13:18,  4.33it/s]\u001b[A\n","1761it [13:18,  4.33it/s]\u001b[A\n","1762it [13:19,  4.33it/s]\u001b[A\n","1763it [13:19,  4.33it/s]\u001b[A\n","1764it [13:19,  4.33it/s]\u001b[A\n","1765it [13:19,  4.33it/s]\u001b[A\n","1766it [13:20,  4.33it/s]\u001b[A\n","1767it [13:20,  4.33it/s]\u001b[A\n","1768it [13:20,  4.33it/s]\u001b[A\n","1769it [13:20,  4.33it/s]\u001b[A\n","1770it [13:20,  4.33it/s]\u001b[A\n","1771it [13:21,  4.33it/s]\u001b[A\n","1772it [13:21,  4.33it/s]\u001b[A\n","1773it [13:21,  4.33it/s]\u001b[A\n","1774it [13:21,  4.33it/s]\u001b[A\n","1775it [13:22,  4.33it/s]\u001b[A\n","1776it [13:22,  4.33it/s]\u001b[A\n","1777it [13:22,  4.33it/s]\u001b[A\n","1778it [13:22,  4.33it/s]\u001b[A\n","1779it [13:23,  4.33it/s]\u001b[A\n","1780it [13:23,  4.33it/s]\u001b[A\n","1781it [13:23,  4.33it/s]\u001b[A\n","1782it [13:23,  4.33it/s]\u001b[A\n","1783it [13:23,  4.33it/s]\u001b[A\n","1784it [13:24,  4.33it/s]\u001b[A\n","1785it [13:24,  4.33it/s]\u001b[A\n","1786it [13:24,  4.33it/s]\u001b[A\n","1787it [13:24,  4.33it/s]\u001b[A\n","1788it [13:25,  4.33it/s]\u001b[A\n","1789it [13:25,  4.33it/s]\u001b[A\n","1790it [13:25,  4.33it/s]\u001b[A\n","1791it [13:25,  4.33it/s]\u001b[A\n","1792it [13:26,  4.33it/s]\u001b[A\n","1793it [13:26,  4.33it/s]\u001b[A\n","1794it [13:26,  4.33it/s]\u001b[A\n","1795it [13:26,  4.33it/s]\u001b[A\n","1796it [13:26,  4.33it/s]\u001b[A\n","1797it [13:27,  4.33it/s]\u001b[A\n","1798it [13:27,  4.33it/s]\u001b[A\n","1799it [13:27,  4.33it/s]\u001b[A\n","1800it [13:27,  4.33it/s]\u001b[A\n","1801it [13:28,  4.33it/s]\u001b[A\n","1802it [13:28,  4.33it/s]\u001b[A\n","1803it [13:28,  4.33it/s]\u001b[A\n","1804it [13:28,  4.33it/s]\u001b[A\n","1805it [13:29,  4.33it/s]\u001b[A\n","1806it [13:29,  4.33it/s]\u001b[A\n","1807it [13:29,  4.33it/s]\u001b[A\n","1808it [13:29,  4.33it/s]\u001b[A\n","1809it [13:29,  4.33it/s]\u001b[A\n","1810it [13:30,  4.33it/s]\u001b[A\n","1811it [13:30,  4.33it/s]\u001b[A\n","1812it [13:30,  4.33it/s]\u001b[A\n","1813it [13:30,  4.33it/s]\u001b[A\n","1814it [13:31,  4.33it/s]\u001b[A\n","1815it [13:31,  4.33it/s]\u001b[A\n","1816it [13:31,  4.33it/s]\u001b[A\n","1817it [13:31,  4.33it/s]\u001b[A\n","1818it [13:32,  4.33it/s]\u001b[A\n","1819it [13:32,  4.33it/s]\u001b[A\n","1820it [13:32,  4.33it/s]\u001b[A\n","1821it [13:32,  4.33it/s]\u001b[A\n","1822it [13:32,  4.33it/s]\u001b[A\n","1823it [13:33,  4.33it/s]\u001b[A\n","1824it [13:33,  4.33it/s]\u001b[A\n","1825it [13:33,  4.33it/s]\u001b[A\n","1826it [13:33,  4.33it/s]\u001b[A\n","1827it [13:34,  4.33it/s]\u001b[A\n","1828it [13:34,  4.33it/s]\u001b[A\n","1829it [13:34,  4.33it/s]\u001b[A\n","1830it [13:34,  4.33it/s]\u001b[A\n","1831it [13:35,  4.33it/s]\u001b[A\n","1832it [13:35,  4.33it/s]\u001b[A\n","1833it [13:35,  4.33it/s]\u001b[A\n","1834it [13:35,  4.33it/s]\u001b[A\n","1835it [13:35,  4.33it/s]\u001b[A\n","1836it [13:36,  4.33it/s]\u001b[A\n","1837it [13:36,  4.33it/s]\u001b[A\n","1838it [13:36,  4.33it/s]\u001b[A\n","1839it [13:36,  4.33it/s]\u001b[A\n","1840it [13:37,  4.33it/s]\u001b[A\n","1841it [13:37,  4.33it/s]\u001b[A\n","1842it [13:37,  4.33it/s]\u001b[A\n","1843it [13:37,  4.33it/s]\u001b[A\n","1844it [13:38,  4.33it/s]\u001b[A\n","1845it [13:38,  4.33it/s]\u001b[A\n","1846it [13:38,  4.33it/s]\u001b[A\n","1847it [13:38,  4.33it/s]\u001b[A\n","1848it [13:38,  4.33it/s]\u001b[A\n","1849it [13:39,  4.33it/s]\u001b[A\n","Epoch: 100% 250/250 [14:21<00:00,  3.45s/it]\n","03/14/2022 09:48:59 - INFO - src.trainer -   \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","/content/drive/MyDrive/Spoiler Detection/LM-BFF/env/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","03/14/2022 09:49:12 - INFO - __main__ -   *** Validate ***\n","\n","1851it [13:52,  4.18s/it]\u001b[A\n","1852it [13:53,  2.99s/it]\u001b[A\n","1853it [13:53,  2.16s/it]\u001b[A\n","1854it [13:53,  1.58s/it]\u001b[A\n","1855it [13:53,  1.18s/it]\u001b[A\n","1856it [13:53,  1.12it/s]\u001b[A\n","1857it [13:54,  1.44it/s]\u001b[A\n","1858it [13:54,  1.80it/s]\u001b[A\n","1859it [13:54,  2.18it/s]\u001b[A\n","1860it [13:54,  2.56it/s]\u001b[A\n","1861it [13:55,  2.92it/s]\u001b[A\n","1862it [13:55,  3.24it/s]\u001b[A\n","1863it [13:55,  3.50it/s]\u001b[A\n","1864it [13:55,  3.71it/s]\u001b[A\n","1865it [13:56,  3.88it/s]\u001b[A\n","1866it [13:56,  4.00it/s]\u001b[A\n","1867it [13:56,  4.10it/s]\u001b[A\n","1868it [13:56,  4.16it/s]\u001b[A\n","1869it [13:56,  4.21it/s]\u001b[A\n","1870it [13:57,  4.25it/s]\u001b[A\n","1871it [13:57,  4.27it/s]\u001b[A\n","1872it [13:57,  4.29it/s]\u001b[A\n","1873it [13:57,  4.30it/s]\u001b[A\n","1874it [13:58,  4.31it/s]\u001b[A\n","1875it [13:58,  4.31it/s]\u001b[A\n","1876it [13:58,  4.32it/s]\u001b[A\n","1877it [13:58,  4.32it/s]\u001b[A\n","1878it [13:59,  4.32it/s]\u001b[A\n","1879it [13:59,  4.32it/s]\u001b[A\n","1880it [13:59,  4.33it/s]\u001b[A\n","1881it [13:59,  4.33it/s]\u001b[A\n","1882it [13:59,  4.33it/s]\u001b[A\n","1883it [14:00,  4.33it/s]\u001b[A\n","1884it [14:00,  4.33it/s]\u001b[A\n","1885it [14:00,  4.33it/s]\u001b[A\n","1886it [14:00,  4.33it/s]\u001b[A\n","1887it [14:01,  4.33it/s]\u001b[A\n","1888it [14:01,  4.33it/s]\u001b[A\n","1889it [14:01,  4.33it/s]\u001b[A\n","1890it [14:01,  4.33it/s]\u001b[A\n","1891it [14:02,  4.33it/s]\u001b[A\n","1892it [14:02,  4.33it/s]\u001b[A\n","1893it [14:02,  4.33it/s]\u001b[A\n","1894it [14:02,  4.33it/s]\u001b[A\n","1895it [14:02,  4.33it/s]\u001b[A\n","1896it [14:03,  4.33it/s]\u001b[A\n","1897it [14:03,  4.33it/s]\u001b[A\n","1898it [14:03,  4.33it/s]\u001b[A\n","1899it [14:03,  4.33it/s]\u001b[A\n","1900it [14:04,  4.33it/s]\u001b[A\n","1901it [14:04,  4.33it/s]\u001b[A\n","1902it [14:04,  4.33it/s]\u001b[A\n","1903it [14:04,  4.33it/s]\u001b[A\n","1904it [14:05,  4.33it/s]\u001b[A\n","1905it [14:05,  4.33it/s]\u001b[A\n","1906it [14:05,  4.33it/s]\u001b[A\n","1907it [14:05,  4.33it/s]\u001b[A\n","1908it [14:05,  4.33it/s]\u001b[A\n","1909it [14:06,  4.33it/s]\u001b[A\n","1910it [14:06,  4.33it/s]\u001b[A\n","1911it [14:06,  4.33it/s]\u001b[A\n","1912it [14:06,  4.33it/s]\u001b[A\n","1913it [14:07,  4.33it/s]\u001b[A\n","1914it [14:07,  4.33it/s]\u001b[A\n","1915it [14:07,  4.33it/s]\u001b[A\n","1916it [14:07,  4.33it/s]\u001b[A\n","1917it [14:08,  4.33it/s]\u001b[A\n","1918it [14:08,  4.32it/s]\u001b[A\n","1919it [14:08,  4.32it/s]\u001b[A\n","1920it [14:08,  4.33it/s]\u001b[A\n","1921it [14:08,  4.33it/s]\u001b[A\n","1922it [14:09,  4.33it/s]\u001b[A\n","1923it [14:09,  4.33it/s]\u001b[A\n","1924it [14:09,  4.33it/s]\u001b[A\n","1925it [14:09,  4.33it/s]\u001b[A\n","1926it [14:10,  4.33it/s]\u001b[A\n","1927it [14:10,  4.33it/s]\u001b[A\n","1928it [14:10,  4.33it/s]\u001b[A\n","1929it [14:10,  4.33it/s]\u001b[A\n","1930it [14:11,  4.33it/s]\u001b[A\n","1931it [14:11,  4.33it/s]\u001b[A\n","1932it [14:11,  4.33it/s]\u001b[A\n","1933it [14:11,  4.33it/s]\u001b[A\n","1934it [14:11,  4.33it/s]\u001b[A\n","1935it [14:12,  4.33it/s]\u001b[A\n","1936it [14:12,  4.33it/s]\u001b[A\n","1937it [14:12,  4.33it/s]\u001b[A\n","1938it [14:12,  4.33it/s]\u001b[A\n","1939it [14:13,  4.33it/s]\u001b[A\n","1940it [14:13,  4.33it/s]\u001b[A\n","1941it [14:13,  4.33it/s]\u001b[A\n","1942it [14:13,  4.33it/s]\u001b[A\n","1943it [14:14,  4.33it/s]\u001b[A\n","1944it [14:14,  4.33it/s]\u001b[A\n","1945it [14:14,  4.33it/s]\u001b[A\n","1946it [14:14,  4.33it/s]\u001b[A\n","1947it [14:14,  4.33it/s]\u001b[A\n","1948it [14:15,  4.33it/s]\u001b[A\n","1949it [14:15,  4.33it/s]\u001b[A\n","1950it [14:15,  4.33it/s]\u001b[A\n","1951it [14:15,  4.33it/s]\u001b[A\n","1952it [14:16,  4.33it/s]\u001b[A\n","1953it [14:16,  4.33it/s]\u001b[A\n","1954it [14:16,  4.33it/s]\u001b[A\n","1955it [14:16,  4.33it/s]\u001b[A\n","1956it [14:17,  4.33it/s]\u001b[A\n","1957it [14:17,  4.33it/s]\u001b[A\n","1958it [14:17,  4.33it/s]\u001b[A\n","1959it [14:17,  4.33it/s]\u001b[A\n","1960it [14:17,  4.33it/s]\u001b[A\n","1961it [14:18,  4.33it/s]\u001b[A\n","1962it [14:18,  4.33it/s]\u001b[A\n","1963it [14:18,  4.33it/s]\u001b[A\n","1964it [14:18,  4.33it/s]\u001b[A\n","1965it [14:19,  4.33it/s]\u001b[A\n","1966it [14:19,  4.33it/s]\u001b[A\n","1967it [14:19,  4.33it/s]\u001b[A\n","1968it [14:19,  4.33it/s]\u001b[A\n","1969it [14:20,  4.33it/s]\u001b[A\n","1970it [14:20,  4.33it/s]\u001b[A\n","1971it [14:20,  4.33it/s]\u001b[A\n","1972it [14:20,  4.33it/s]\u001b[A\n","1973it [14:20,  4.33it/s]\u001b[A\n","1974it [14:21,  4.33it/s]\u001b[A\n","1975it [14:21,  4.33it/s]\u001b[A\n","1976it [14:21,  4.33it/s]\u001b[A\n","1977it [14:21,  4.33it/s]\u001b[A\n","1978it [14:22,  4.33it/s]\u001b[A\n","1979it [14:22,  4.33it/s]\u001b[A\n","1980it [14:22,  4.33it/s]\u001b[A\n","1981it [14:22,  4.33it/s]\u001b[A\n","1982it [14:23,  4.32it/s]\u001b[A\n","1983it [14:23,  4.32it/s]\u001b[A\n","1984it [14:23,  4.33it/s]\u001b[A\n","1985it [14:23,  4.32it/s]\u001b[A\n","1986it [14:23,  4.32it/s]\u001b[A\n","1987it [14:24,  4.33it/s]\u001b[A\n","1988it [14:24,  4.33it/s]\u001b[A\n","1989it [14:24,  4.33it/s]\u001b[A\n","1990it [14:24,  4.33it/s]\u001b[A\n","1991it [14:25,  4.33it/s]\u001b[A\n","1992it [14:25,  4.33it/s]\u001b[A\n","1993it [14:25,  4.33it/s]\u001b[A\n","1994it [14:25,  4.33it/s]\u001b[A\n","1995it [14:26,  4.33it/s]\u001b[A\n","1996it [14:26,  4.33it/s]\u001b[A\n","1997it [14:26,  4.33it/s]\u001b[A\n","1998it [14:26,  4.33it/s]\u001b[A\n","1999it [14:26,  4.33it/s]\u001b[A\n","2000it [14:27,  4.33it/s]\u001b[A\n","2001it [14:27,  4.33it/s]\u001b[A\n","2002it [14:27,  4.33it/s]\u001b[A\n","2003it [14:27,  4.33it/s]\u001b[A\n","2004it [14:28,  4.33it/s]\u001b[A\n","2005it [14:28,  4.33it/s]\u001b[A\n","2006it [14:28,  4.33it/s]\u001b[A\n","2007it [14:28,  4.33it/s]\u001b[A\n","2008it [14:29,  4.33it/s]\u001b[A\n","2009it [14:29,  4.33it/s]\u001b[A\n","2010it [14:29,  4.33it/s]\u001b[A\n","2011it [14:29,  4.33it/s]\u001b[A\n","2012it [14:29,  4.33it/s]\u001b[A\n","2013it [14:30,  4.33it/s]\u001b[A\n","2014it [14:30,  4.33it/s]\u001b[A\n","2015it [14:30,  4.33it/s]\u001b[A\n","2016it [14:30,  4.33it/s]\u001b[A\n","2017it [14:31,  4.33it/s]\u001b[A\n","2018it [14:31,  4.33it/s]\u001b[A\n","2019it [14:31,  4.33it/s]\u001b[A\n","2020it [14:31,  4.33it/s]\u001b[A\n","2021it [14:32,  4.33it/s]\u001b[A\n","2022it [14:32,  4.33it/s]\u001b[A\n","2023it [14:32,  4.33it/s]\u001b[A\n","2024it [14:32,  4.33it/s]\u001b[A\n","2025it [14:32,  4.33it/s]\u001b[A\n","2026it [14:33,  4.33it/s]\u001b[A\n","2027it [14:33,  4.33it/s]\u001b[A\n","2028it [14:33,  4.33it/s]\u001b[A\n","2029it [14:33,  4.33it/s]\u001b[A\n","2030it [14:34,  4.33it/s]\u001b[A\n","2031it [14:34,  4.33it/s]\u001b[A\n","2032it [14:34,  4.33it/s]\u001b[A\n","2033it [14:34,  4.33it/s]\u001b[A\n","2034it [14:35,  4.33it/s]\u001b[A\n","2035it [14:35,  4.33it/s]\u001b[A03/14/2022 09:49:55 - INFO - __main__ -   ***** Eval results spoilers *****\n","03/14/2022 09:49:55 - INFO - __main__ -     eval_loss = 4.215115070343018\n","03/14/2022 09:49:55 - INFO - __main__ -     eval_auroc = 0.937250018119812\n","03/14/2022 09:49:55 - INFO - __main__ -     eval_recall = 1.0\n","03/14/2022 09:49:55 - INFO - __main__ -     eval_f1 = 0.39370080828666687\n","03/14/2022 09:49:55 - INFO - root -   *** Test ***\n","\n","2036it [14:35,  4.26it/s]\u001b[A\n","2037it [14:35,  4.28it/s]\u001b[A\n","2038it [14:36,  4.29it/s]\u001b[A\n","2039it [14:36,  4.30it/s]\u001b[A\n","2040it [14:36,  4.31it/s]\u001b[A\n","2041it [14:36,  4.31it/s]\u001b[A\n","2042it [14:36,  4.32it/s]\u001b[A\n","2043it [14:37,  4.32it/s]\u001b[A\n","2044it [14:37,  4.32it/s]\u001b[A\n","2045it [14:37,  4.33it/s]\u001b[A\n","2046it [14:37,  4.33it/s]\u001b[A\n","2047it [14:38,  4.33it/s]\u001b[A\n","2048it [14:38,  4.33it/s]\u001b[A\n","2049it [14:38,  4.33it/s]\u001b[A\n","2050it [14:38,  4.33it/s]\u001b[A\n","2051it [14:39,  4.33it/s]\u001b[A\n","2052it [14:39,  4.33it/s]\u001b[A\n","2053it [14:39,  4.33it/s]\u001b[A\n","2054it [14:39,  4.33it/s]\u001b[A\n","2055it [14:39,  4.33it/s]\u001b[A\n","2056it [14:40,  4.33it/s]\u001b[A\n","2057it [14:40,  4.33it/s]\u001b[A\n","2058it [14:40,  4.33it/s]\u001b[A\n","2059it [14:40,  4.33it/s]\u001b[A\n","2060it [14:41,  4.33it/s]\u001b[A\n","2061it [14:41,  4.33it/s]\u001b[A\n","2062it [14:41,  4.33it/s]\u001b[A\n","2063it [14:41,  4.33it/s]\u001b[A\n","2064it [14:42,  4.33it/s]\u001b[A\n","2065it [14:42,  4.32it/s]\u001b[A\n","2066it [14:42,  4.32it/s]\u001b[A\n","2067it [14:42,  4.33it/s]\u001b[A\n","2068it [14:42,  4.33it/s]\u001b[A\n","2069it [14:43,  4.33it/s]\u001b[A\n","2070it [14:43,  4.33it/s]\u001b[A\n","2071it [14:43,  4.33it/s]\u001b[A\n","2072it [14:43,  4.33it/s]\u001b[A\n","2073it [14:44,  4.33it/s]\u001b[A\n","2074it [14:44,  4.33it/s]\u001b[A\n","2075it [14:44,  4.33it/s]\u001b[A\n","2076it [14:44,  4.33it/s]\u001b[A\n","2077it [14:45,  4.33it/s]\u001b[A\n","2078it [14:45,  4.33it/s]\u001b[A\n","2079it [14:45,  4.33it/s]\u001b[A\n","2080it [14:45,  4.33it/s]\u001b[A\n","2081it [14:45,  4.33it/s]\u001b[A\n","2082it [14:46,  4.33it/s]\u001b[A\n","2083it [14:46,  4.33it/s]\u001b[A\n","2084it [14:46,  4.33it/s]\u001b[A\n","2085it [14:46,  4.33it/s]\u001b[A\n","2086it [14:47,  4.33it/s]\u001b[A\n","2087it [14:47,  4.33it/s]\u001b[A\n","2088it [14:47,  4.33it/s]\u001b[A\n","2089it [14:47,  4.33it/s]\u001b[A\n","2090it [14:48,  4.33it/s]\u001b[A\n","2091it [14:48,  4.33it/s]\u001b[A\n","2092it [14:48,  4.33it/s]\u001b[A\n","2093it [14:48,  4.33it/s]\u001b[A\n","2094it [14:48,  4.33it/s]\u001b[A\n","2095it [14:49,  4.33it/s]\u001b[A\n","2096it [14:49,  4.33it/s]\u001b[A\n","2097it [14:49,  4.33it/s]\u001b[A\n","2098it [14:49,  4.33it/s]\u001b[A\n","2099it [14:50,  4.33it/s]\u001b[A\n","2100it [14:50,  4.33it/s]\u001b[A\n","2101it [14:50,  4.33it/s]\u001b[A\n","2102it [14:50,  4.33it/s]\u001b[A\n","2103it [14:51,  4.33it/s]\u001b[A\n","2104it [14:51,  4.33it/s]\u001b[A\n","2105it [14:51,  4.33it/s]\u001b[A\n","2106it [14:51,  4.33it/s]\u001b[A\n","2107it [14:51,  4.33it/s]\u001b[A\n","2108it [14:52,  4.33it/s]\u001b[A\n","2109it [14:52,  4.33it/s]\u001b[A\n","2110it [14:52,  4.33it/s]\u001b[A\n","2111it [14:52,  4.33it/s]\u001b[A\n","2112it [14:53,  4.33it/s]\u001b[A\n","2113it [14:53,  4.33it/s]\u001b[A\n","2114it [14:53,  4.33it/s]\u001b[A\n","2115it [14:53,  4.33it/s]\u001b[A\n","2116it [14:54,  4.33it/s]\u001b[A\n","2117it [14:54,  4.33it/s]\u001b[A\n","2118it [14:54,  4.32it/s]\u001b[A\n","2119it [14:54,  4.33it/s]\u001b[A\n","2120it [14:54,  4.33it/s]\u001b[A\n","2121it [14:55,  4.33it/s]\u001b[A\n","2122it [14:55,  4.33it/s]\u001b[A\n","2123it [14:55,  4.33it/s]\u001b[A\n","2124it [14:55,  4.33it/s]\u001b[A\n","2125it [14:56,  4.33it/s]\u001b[A\n","2126it [14:56,  4.33it/s]\u001b[A\n","2127it [14:56,  4.33it/s]\u001b[A\n","2128it [14:56,  4.33it/s]\u001b[A\n","2129it [14:57,  4.33it/s]\u001b[A\n","2130it [14:57,  4.33it/s]\u001b[A\n","2131it [14:57,  4.33it/s]\u001b[A\n","2132it [14:57,  4.33it/s]\u001b[A\n","2133it [14:57,  4.33it/s]\u001b[A\n","2134it [14:58,  4.33it/s]\u001b[A\n","2135it [14:58,  4.33it/s]\u001b[A\n","2136it [14:58,  4.33it/s]\u001b[A\n","2137it [14:58,  4.33it/s]\u001b[A\n","2138it [14:59,  4.33it/s]\u001b[A\n","2139it [14:59,  4.33it/s]\u001b[A\n","2140it [14:59,  4.33it/s]\u001b[A\n","2141it [14:59,  4.33it/s]\u001b[A\n","2142it [15:00,  4.33it/s]\u001b[A\n","2143it [15:00,  4.33it/s]\u001b[A\n","2144it [15:00,  4.33it/s]\u001b[A\n","2145it [15:00,  4.33it/s]\u001b[A\n","2146it [15:00,  4.33it/s]\u001b[A\n","2147it [15:01,  4.33it/s]\u001b[A\n","2148it [15:01,  4.33it/s]\u001b[A\n","2149it [15:01,  4.33it/s]\u001b[A\n","2150it [15:01,  4.32it/s]\u001b[A\n","2151it [15:02,  4.32it/s]\u001b[A\n","2152it [15:02,  4.33it/s]\u001b[A\n","2153it [15:02,  4.33it/s]\u001b[A\n","2154it [15:02,  4.33it/s]\u001b[A\n","2155it [15:03,  4.33it/s]\u001b[A\n","2156it [15:03,  4.33it/s]\u001b[A\n","2157it [15:03,  4.33it/s]\u001b[A\n","2158it [15:03,  4.33it/s]\u001b[A\n","2159it [15:03,  4.33it/s]\u001b[A\n","2160it [15:04,  4.33it/s]\u001b[A\n","2161it [15:04,  4.33it/s]\u001b[A\n","2162it [15:04,  4.33it/s]\u001b[A\n","2163it [15:04,  4.33it/s]\u001b[A\n","2164it [15:05,  4.33it/s]\u001b[A\n","2165it [15:05,  4.33it/s]\u001b[A\n","2166it [15:05,  4.33it/s]\u001b[A\n","2167it [15:05,  4.33it/s]\u001b[A\n","2168it [15:06,  4.33it/s]\u001b[A\n","2169it [15:06,  4.33it/s]\u001b[A\n","2170it [15:06,  4.33it/s]\u001b[A\n","2171it [15:06,  4.33it/s]\u001b[A\n","2172it [15:06,  4.33it/s]\u001b[A\n","2173it [15:07,  4.33it/s]\u001b[A\n","2174it [15:07,  4.33it/s]\u001b[A\n","2175it [15:07,  4.33it/s]\u001b[A\n","2176it [15:07,  4.33it/s]\u001b[A\n","2177it [15:08,  4.33it/s]\u001b[A\n","2178it [15:08,  4.33it/s]\u001b[A\n","2179it [15:08,  4.33it/s]\u001b[A\n","2180it [15:08,  4.33it/s]\u001b[A\n","2181it [15:09,  4.33it/s]\u001b[A\n","2182it [15:09,  4.33it/s]\u001b[A\n","2183it [15:09,  4.33it/s]\u001b[A\n","2184it [15:09,  4.32it/s]\u001b[A\n","2185it [15:09,  4.33it/s]\u001b[A\n","2186it [15:10,  4.33it/s]\u001b[A\n","2187it [15:10,  4.33it/s]\u001b[A\n","2188it [15:10,  4.33it/s]\u001b[A\n","2189it [15:10,  4.33it/s]\u001b[A\n","2190it [15:11,  4.33it/s]\u001b[A\n","2191it [15:11,  4.33it/s]\u001b[A\n","2192it [15:11,  4.33it/s]\u001b[A\n","2193it [15:11,  4.33it/s]\u001b[A\n","2194it [15:12,  4.33it/s]\u001b[A\n","2195it [15:12,  4.33it/s]\u001b[A\n","2196it [15:12,  4.33it/s]\u001b[A\n","2197it [15:12,  4.33it/s]\u001b[A\n","2198it [15:12,  4.33it/s]\u001b[A\n","2199it [15:13,  4.33it/s]\u001b[A\n","2200it [15:13,  4.33it/s]\u001b[A\n","2201it [15:13,  4.33it/s]\u001b[A\n","2202it [15:13,  4.33it/s]\u001b[A\n","2203it [15:14,  4.32it/s]\u001b[A\n","2204it [15:14,  4.32it/s]\u001b[A\n","2205it [15:14,  4.33it/s]\u001b[A\n","2206it [15:14,  4.33it/s]\u001b[A\n","2207it [15:15,  4.33it/s]\u001b[A\n","2208it [15:15,  4.33it/s]\u001b[A\n","2209it [15:15,  4.33it/s]\u001b[A\n","2210it [15:15,  4.33it/s]\u001b[A\n","2211it [15:15,  4.33it/s]\u001b[A\n","2212it [15:16,  4.33it/s]\u001b[A\n","2213it [15:16,  4.33it/s]\u001b[A\n","2214it [15:16,  4.33it/s]\u001b[A\n","2215it [15:16,  4.33it/s]\u001b[A\n","2216it [15:17,  4.33it/s]\u001b[A\n","2217it [15:17,  4.33it/s]\u001b[A\n","2218it [15:17,  4.33it/s]\u001b[A\n","2219it [15:17,  4.33it/s]\u001b[A\n","2220it [15:18,  4.32it/s]\u001b[A\n","2221it [15:18,  4.33it/s]\u001b[A\n","2222it [15:18,  4.33it/s]\u001b[A\n","2223it [15:18,  4.32it/s]\u001b[A\n","2224it [15:18,  4.32it/s]\u001b[A\n","2225it [15:19,  4.33it/s]\u001b[A\n","2226it [15:19,  4.32it/s]\u001b[A\n","2227it [15:19,  4.32it/s]\u001b[A\n","2228it [15:19,  4.33it/s]\u001b[A\n","2229it [15:20,  4.33it/s]\u001b[A\n","2230it [15:20,  4.33it/s]\u001b[A\n","2231it [15:20,  4.33it/s]\u001b[A\n","2232it [15:20,  4.33it/s]\u001b[A\n","2233it [15:21,  4.33it/s]\u001b[A\n","2234it [15:21,  4.33it/s]\u001b[A\n","2235it [15:21,  4.33it/s]\u001b[A\n","2236it [15:21,  4.33it/s]\u001b[A\n","2237it [15:21,  4.33it/s]\u001b[A\n","2238it [15:22,  4.33it/s]\u001b[A\n","2239it [15:22,  4.33it/s]\u001b[A\n","2240it [15:22,  4.33it/s]\u001b[A\n","2241it [15:22,  4.33it/s]\u001b[A\n","2242it [15:23,  4.32it/s]\u001b[A\n","2243it [15:23,  4.33it/s]\u001b[A\n","2244it [15:23,  4.33it/s]\u001b[A\n","2245it [15:23,  4.33it/s]\u001b[A\n","2246it [15:24,  4.33it/s]\u001b[A\n","2247it [15:24,  4.33it/s]\u001b[A\n","2248it [15:24,  4.33it/s]\u001b[A\n","2249it [15:24,  4.33it/s]\u001b[A\n","2250it [15:24,  4.33it/s]\u001b[A\n","2251it [15:25,  4.33it/s]\u001b[A\n","2252it [15:25,  4.33it/s]\u001b[A\n","2253it [15:25,  4.33it/s]\u001b[A\n","2254it [15:25,  4.33it/s]\u001b[A\n","2255it [15:26,  4.33it/s]\u001b[A\n","2256it [15:26,  4.33it/s]\u001b[A\n","2257it [15:26,  4.33it/s]\u001b[A\n","2258it [15:26,  4.33it/s]\u001b[A\n","2259it [15:27,  4.33it/s]\u001b[A\n","2260it [15:27,  4.33it/s]\u001b[A\n","2261it [15:27,  4.33it/s]\u001b[A\n","2262it [15:27,  4.33it/s]\u001b[A\n","2263it [15:27,  4.33it/s]\u001b[A\n","2264it [15:28,  4.33it/s]\u001b[A\n","2265it [15:28,  4.33it/s]\u001b[A\n","2266it [15:28,  4.33it/s]\u001b[A\n","2267it [15:28,  4.33it/s]\u001b[A\n","2268it [15:29,  4.33it/s]\u001b[A\n","2269it [15:29,  4.33it/s]\u001b[A\n","2270it [15:29,  4.33it/s]\u001b[A\n","2271it [15:29,  4.33it/s]\u001b[A\n","2272it [15:30,  4.33it/s]\u001b[A\n","2273it [15:30,  4.33it/s]\u001b[A\n","2274it [15:30,  4.33it/s]\u001b[A\n","2275it [15:30,  4.33it/s]\u001b[A\n","2276it [15:30,  4.33it/s]\u001b[A\n","2277it [15:31,  4.33it/s]\u001b[A\n","2278it [15:31,  4.33it/s]\u001b[A\n","2279it [15:31,  4.33it/s]\u001b[A\n","2280it [15:31,  4.33it/s]\u001b[A\n","2281it [15:32,  4.33it/s]\u001b[A\n","2282it [15:32,  4.33it/s]\u001b[A\n","2283it [15:32,  4.33it/s]\u001b[A\n","2284it [15:32,  4.33it/s]\u001b[A\n","2285it [15:33,  4.33it/s]\u001b[A\n","2286it [15:33,  4.33it/s]\u001b[A\n","2287it [15:33,  4.33it/s]\u001b[A\n","2288it [15:33,  4.33it/s]\u001b[A\n","2289it [15:33,  4.33it/s]\u001b[A\n","2290it [15:34,  4.33it/s]\u001b[A\n","2291it [15:34,  4.33it/s]\u001b[A\n","2292it [15:34,  4.33it/s]\u001b[A\n","2293it [15:34,  4.33it/s]\u001b[A\n","2294it [15:35,  4.33it/s]\u001b[A\n","2295it [15:35,  4.33it/s]\u001b[A\n","2296it [15:35,  4.33it/s]\u001b[A\n","2297it [15:35,  4.33it/s]\u001b[A\n","2298it [15:36,  4.33it/s]\u001b[A\n","2299it [15:36,  4.33it/s]\u001b[A\n","2300it [15:36,  4.33it/s]\u001b[A\n","2301it [15:36,  4.33it/s]\u001b[A\n","2302it [15:36,  4.33it/s]\u001b[A\n","2303it [15:37,  4.33it/s]\u001b[A\n","2304it [15:37,  4.33it/s]\u001b[A\n","2305it [15:37,  4.33it/s]\u001b[A\n","2306it [15:37,  4.33it/s]\u001b[A\n","2307it [15:38,  4.33it/s]\u001b[A\n","2308it [15:38,  4.33it/s]\u001b[A\n","2309it [15:38,  4.33it/s]\u001b[A\n","2310it [15:38,  4.33it/s]\u001b[A\n","2311it [15:39,  4.33it/s]\u001b[A\n","2312it [15:39,  4.33it/s]\u001b[A\n","2313it [15:39,  4.33it/s]\u001b[A\n","2314it [15:39,  4.33it/s]\u001b[A\n","2315it [15:40,  4.33it/s]\u001b[A\n","2316it [15:40,  4.33it/s]\u001b[A\n","2317it [15:40,  4.33it/s]\u001b[A\n","2318it [15:40,  4.33it/s]\u001b[A\n","2319it [15:40,  4.33it/s]\u001b[A\n","2320it [15:41,  4.33it/s]\u001b[A\n","2321it [15:41,  4.33it/s]\u001b[A\n","2322it [15:41,  4.33it/s]\u001b[A\n","2323it [15:41,  4.33it/s]\u001b[A\n","2324it [15:42,  4.33it/s]\u001b[A\n","2325it [15:42,  4.33it/s]\u001b[A\n","2326it [15:42,  4.33it/s]\u001b[A\n","2327it [15:42,  4.33it/s]\u001b[A\n","2328it [15:43,  4.33it/s]\u001b[A\n","2329it [15:43,  4.33it/s]\u001b[A\n","2330it [15:43,  4.33it/s]\u001b[A\n","2331it [15:43,  4.33it/s]\u001b[A\n","2332it [15:43,  4.33it/s]\u001b[A\n","2333it [15:44,  4.33it/s]\u001b[A\n","2334it [15:44,  4.33it/s]\u001b[A\n","2335it [15:44,  4.33it/s]\u001b[A\n","2336it [15:44,  4.33it/s]\u001b[A\n","2337it [15:45,  4.33it/s]\u001b[A\n","2338it [15:45,  4.33it/s]\u001b[A\n","2339it [15:45,  4.33it/s]\u001b[A\n","2340it [15:45,  4.33it/s]\u001b[A\n","2341it [15:46,  4.33it/s]\u001b[A\n","2342it [15:46,  4.33it/s]\u001b[A\n","2343it [15:46,  4.33it/s]\u001b[A\n","2344it [15:46,  4.33it/s]\u001b[A\n","2345it [15:46,  4.33it/s]\u001b[A\n","2346it [15:47,  4.33it/s]\u001b[A\n","2347it [15:47,  4.33it/s]\u001b[A\n","2348it [15:47,  4.33it/s]\u001b[A\n","2349it [15:47,  4.33it/s]\u001b[A\n","2350it [15:48,  4.33it/s]\u001b[A\n","2351it [15:48,  4.33it/s]\u001b[A\n","2352it [15:48,  4.33it/s]\u001b[A\n","2353it [15:48,  4.33it/s]\u001b[A\n","2354it [15:49,  4.33it/s]\u001b[A03/14/2022 09:51:09 - INFO - __main__ -   ***** Test results spoilers *****\n","03/14/2022 09:51:09 - INFO - __main__ -     eval_loss = 5.07611083984375\n","03/14/2022 09:51:09 - INFO - __main__ -     eval_auroc = 0.852401077747345\n","03/14/2022 09:51:09 - INFO - __main__ -     eval_recall = 0.9583333134651184\n","03/14/2022 09:51:09 - INFO - __main__ -     eval_f1 = 0.2266009896993637\n","03/14/2022 09:51:09 - INFO - filelock -   Lock 140578843819600 acquired on log.lock\n","03/14/2022 09:51:09 - INFO - filelock -   Lock 140578843819600 released on log.lock\n","2354it [15:49,  2.48it/s]\n"]}],"source":["!source env/bin/activate; bash ensemble.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5845,"status":"ok","timestamp":1647253078620,"user":{"displayName":"Ryan Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1-IudxHvO0CBMUTkOJsFEFNw0PLxKkEPh2bI8=s64","userId":"14098298700710797497"},"user_tz":420},"id":"T49oI5hLkC3d","outputId":"f21b669f-075a-4774-9eed-58de7fb8f2a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model 0 | val: mean +- std: 94.4 +- 0.0 | test: mean +- std: 88.4 (0.0) (median 88.4) / 95.8 +- 0.0 (median 95.8)\n","Model 1 | val: mean +- std: 94.4 +- 0.0 | test: mean +- std: 90.3 (0.0) (median 90.3) / 95.8 +- 0.0 (median 95.8)\n","Model 2 | val: mean +- std: 95.2 +- 0.0 | test: mean +- std: 89.7 (0.0) (median 89.7) / 95.8 +- 0.0 (median 95.8)\n","Model 3 | val: mean +- std: 88.0 +- 0.0 | test: mean +- std: 83.4 (0.0) (median 83.4) / 91.7 +- 0.0 (median 91.7)\n","Model 4 | val: mean +- std: 91.6 +- 0.0 | test: mean +- std: 85.1 (0.0) (median 85.1) / 95.8 +- 0.0 (median 95.8)\n","Model 5 | val: mean +- std: 86.5 +- 0.0 | test: mean +- std: 86.7 (0.0) (median 86.7) / 75.0 +- 0.0 (median 75.0)\n","Model 6 | val: mean +- std: 87.7 +- 0.0 | test: mean +- std: 84.4 (0.0) (median 84.4) / 100.0 +- 0.0 (median 100.0)\n","Model 7 | val: mean +- std: 93.1 +- 0.0 | test: mean +- std: 90.2 (0.0) (median 90.2) / 83.3 +- 0.0 (median 83.3)\n","Model 8 | val: mean +- std: 86.0 +- 0.0 | test: mean +- std: 81.8 (0.0) (median 81.8) / 95.8 +- 0.0 (median 95.8)\n","Model 9 | val: mean +- std: 89.8 +- 0.0 | test: mean +- std: 83.6 (0.0) (median 83.6) / 95.8 +- 0.0 (median 95.8)\n","Model 10 | val: mean +- std: 89.9 +- 0.0 | test: mean +- std: 79.1 (0.0) (median 79.1) / 75.0 +- 0.0 (median 75.0)\n","Model 11 | val: mean +- std: 93.6 +- 0.0 | test: mean +- std: 89.2 (0.0) (median 89.2) / 91.7 +- 0.0 (median 91.7)\n","Model 12 | val: mean +- std: 96.0 +- 0.0 | test: mean +- std: 93.4 (0.0) (median 93.4) / 83.3 +- 0.0 (median 83.3)\n","Model 13 | val: mean +- std: 87.5 +- 0.0 | test: mean +- std: 87.2 (0.0) (median 87.2) / 87.5 +- 0.0 (median 87.5)\n","Model 14 | val: mean +- std: 85.3 +- 0.0 | test: mean +- std: 77.1 (0.0) (median 77.1) / 95.8 +- 0.0 (median 95.8)\n","Model 15 | val: mean +- std: 92.0 +- 0.0 | test: mean +- std: 83.3 (0.0) (median 83.3) / 83.3 +- 0.0 (median 83.3)\n","Model 16 | val: mean +- std: 85.3 +- 0.0 | test: mean +- std: 77.5 (0.0) (median 77.5) / 95.8 +- 0.0 (median 95.8)\n","Model 17 | val: mean +- std: 88.3 +- 0.0 | test: mean +- std: 82.4 (0.0) (median 82.4) / 91.7 +- 0.0 (median 91.7)\n","Model 18 | val: mean +- std: 87.3 +- 0.0 | test: mean +- std: 82.4 (0.0) (median 82.4) / 83.3 +- 0.0 (median 83.3)\n","Model 19 | val: mean +- std: 93.7 +- 0.0 | test: mean +- std: 85.2 (0.0) (median 85.2) / 95.8 +- 0.0 (median 95.8)\n","mean +- std: 78.4 (0.0) (median 78.4) / 91.7 (0.0) (median 91.7)\n"]}],"source":["!source env/bin/activate; python tools/ensemble.py --condition \"{'tag': 'yes_no_ensemble', 'task_name': 'spoilers', 'few_shot_type': 'prompt-demo'}\" --n_models 20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39wlurug4F2j"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyN07gogmGuKyBsrd1NpadaT"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}